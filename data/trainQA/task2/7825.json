{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A framework for extraction and transformation of documents\n\nPaper Abstract: \n\nPaper Section Text:\nInformation extraction (IE) of text documents found its theoretical foundations in the framework of document spanners . Introduced by Fagin, Kimelfeld, Reiss, and Vansummeren one decade ago , this framework sets the grounds for rule-based IE through the notion of spanners and how to combine them by using relational operators. Moreover, it inspired a lot of research on languages [ref]25 [ref]35, expressiveness  [ref]31   ,\nevaluation       , provenance  , and compressed evaluation    for IE. Initially conceived to understand SystemT, IBM’s rule-based IE system, it has found some recent promising implementations in . See   for surveys of the area. Although IE is a crucial task for data management, document extraction is usually followed by a transformation into new documents. Indeed, data transformation is essential for communicating middleware systems, where data does not conform to the required format of a third-party system and needs to be converted. This forms the main task of the so-called ETL technologies  (for Extract-Transform-Load), which are crucial parts of today’s data management workflows. Even the search-and-replace operation over text, a ubiquitous task in all text systems, can be conceived as an extraction (i.e., search) followed by a transformation (i.e., replace). Hence, although document spanners provided the formal ground of ruled-based IE, these grounds are incomplete without understanding the following transformation process. In this paper, we study the extraction and transformation of text documents through the lens of document spanners. We propose to use a two-phase process where the first phase extracts span-tuples from a document, and the second phase maps the content of the span-tuples into new documents. The main idea here is to use the framework of document spanners for the extraction phase, completing it with an transformation phase using the theory of polyregular functions. Let us now explain our proposed framework and respective results with several examples. We consider the following document 𝐃𝐃\\mathbf{D} over alphabet ΣΣ\\Sigma that lists famous English-speaking singers in the format ##\\# [person] ##\\# [person] ##\\# …##\\# [person] ##\\# with [person] == [last name]; [name]; [birthplace]; [opt]; [opt];…, where [opt] are optional attributes like, e. g., age, nickname, honorary titles etc, which follow no fixed scheme. This means that ‘##\\#’ and ‘;’ are used as separators, and for convenience, we set Σ^=Σ∖{;,#}\\widehat{\\Sigma}=\\Sigma\\setminus\\{;,\\#\\}. 𝐃=𝐃absent\\mathbf{D}= # Holiday; Billie; USA # Bush; Kate; England # Young; Neil; Canada; 78; ‘‘Godfather of Grunge’’ # King; Carole; USA; 81 # McCartney; Paul; England; Sir; CH; MBE # Mitchell; Joni; Canada; painter # Franklin; Aretha; USA; ‘‘Queen of Soul’’ # O’Riordan; Dolores; Ireland; †01/15/2018 # Bowie; David; England # Dylan; Bob; USA; 82 # Young; Neil; USA # Gallagher; Rory; Ireland # Our general approach is to first perform information extraction on 𝐃𝐃\\mathbf{D} with (an extension of) regex formulas (see ). For example, E1:=Σ∗#x{Σ^∗};y{Σ^∗};Σ∗E_{1}\\ :=\\ \\ \\Sigma^{*}\\#\\,\\textsf{x}\\{\\widehat{\\Sigma}^{*}\\};\\textsf{y}\\{\\widehat{\\Sigma}^{*}\\};\\Sigma^{*}\nis a regex formula that uses a variable x to extract just any factor over Σ^^Σ\\widehat{\\Sigma} that occurs between separators ‘##\\#’ and ‘;’, and a variable y to extract the following factor over Σ^^Σ\\widehat{\\Sigma} between the next two occurrences of ‘;’. The construct x​{r}x𝑟\\textsf{x}\\{r\\} creates a span [i,j⟩𝑖𝑗[i,j\\rangle pointing to factor 𝐃​[i]​𝐃​[i+1]​…​𝐃​[j−1]𝐃delimited-[]𝑖𝐃delimited-[]𝑖1…𝐃delimited-[]𝑗1\\mathbf{D}[i]\\mathbf{D}[i+1]\\ldots\\mathbf{D}[j-1], satisfying the subexpression r𝑟r.\nThe regex formula E1subscript𝐸1E_{1} specifies a spanner that, on our example document 𝐃𝐃\\mathbf{D}\nproduces the set {([2,9⟩,[10,16⟩),\\{([2,9\\rangle,[10,16\\rangle), ([21,25⟩,[26,30⟩),21252630([21,25\\rangle,[26,30\\rangle), ([39,44⟩,[45,49⟩),…}([39,44\\rangle,[45,49\\rangle),\\ldots\\}\nof span-tuples. Every span-tuple t𝑡t represents an annotated version of our document: the factor corresponding to t​(x)𝑡xt(\\textsf{x}) is annotated by (or allocated to) x and the factor corresponding to t​(y)𝑡yt(\\textsf{y}) is annotated by y. And each such annotation represents another finding of relevant information in our document. As the second step of our model, we map each annotated version of our document to another document, which therefore describes a transformation of our document. This mapping will explicitly use the information represented by the annotation from the extraction phase. Formally, we use a string-to-string function for the transformation phase that gets as argument a single string that represents the document and the information given by a span-tuple.\nThe transformation function is applied to each of the annotated documents. Since the transformation function can be non-injective, we may obtain the same output string several times. In our model, we keep such duplicates (by using bag semantics), since they represent the situation that the same output string can be constructed from completely different annotations.\nConsidering our example, the annotated documents are as follows\n(we use ||\\textbar as separator between strings for better readability): They are obtained by simply representing the span for\neach variable z∈{x,y}zxy\\textsf{z}\\in\\{\\textsf{x},\\textsf{y}\\} by a pair of parentheses ⊢z⁡…​⊣zsubscript⊢z…subscript⊣z\\prescript{}{\\textsf{z}}{\\textsf{\\vdash}}\\ldots\\textsf{\\dashv}_{\\textsf{z}}.\nIf we couple the spanner E1subscript𝐸1E_{1} with the string-to-string function: where u,u′∈Σ∗𝑢superscript𝑢′superscriptΣu,u^{\\prime}\\in\\Sigma^{*} and vx,vy∈Σ^∗subscript𝑣xsubscript𝑣ysuperscript^Σv_{\\textsf{x}},v_{\\textsf{y}}\\in\\widehat{\\Sigma}^{*},\nthen we obtain what we call an extract-transform program (ET program). This ET program (E1,T1)subscript𝐸1subscript𝑇1(E_{1},T_{1}) maps our document 𝐃𝐃\\mathbf{D} to the following collection of strings: Note that these are the names of all singers from the document, but in the format “[first name] [last name]” (i. e., the order of the names is swapped). Moreover, “Neil Young” constitutes a duplicate, since it is the image under different extractions of the regex formula E1subscript𝐸1E_{1}.\nThis is an intended feature of our model, since every distinct way of information extraction should translate into a distinct output document of the transformation phase. This example already points out that classical one-way transducers \nare not suitable transformation functions, since they cannot change the order of the extracted factors.\nWe propose, instead, to\nuse the class of polyregular functions  , which is a robust class of regular string-to-string functions with\n(at most)\npolynomial growth. The function from above is a polyregular function that even has linear growth; a subclass of special interest for this work. We will discuss polyregular functions (and representations for them) later in this paper. In the examples here, we describe them sufficiently formally but on an intuitive level. Let us move on to a more complicated example. We want to define an ET program which maps 𝐃𝐃\\mathbf{D} to the following strings: Specifically, we want to\nmap each singer entry [last name]; [name]; [birthplace]; [opt-1]; …; [opt-k] to the string [name] [last name]: [birthplace] # [name] [last name]: [opt-1] # …# [name] [last name]: [opt-k] #,\nwhich is something like the cross-product between [name] [last name] and the list [birthplace], [opt-1], …, [opt-k]. For example, the substring # Young; Neil; Canada; 78; ‘‘Godfather of Grunge’’ # is mapped to\nNeil Young: Canada # Neil Young: 78 # Neil Young: ‘‘Godfather of Crunge’’ #. Intuitively, we would have to extract each element of the unbounded list [birthplace], [opt-1], …, [opt-k] in its own variable, which goes beyond the capability of document spanners.\nAs a remedy,\nin this paper we propose to extend the classical spanner framework to multispanners, which can extract in each variable a set of spans instead of only a single span. Now assume that we apply to the document the expression: Then for the substring\n#Young; Neil; Canada; 78; ‘‘Godfather of Grunge’’#, we will extract each attribute as a single span of variable z, so the corresponding multispan tuple and the corresponding string representation would be: Thus, by defining a suitable transformation function T2subscript𝑇2T_{2}, we get an ET program (E2,T2)subscript𝐸2subscript𝑇2(E_{2},T_{2}) for our task. This is again possible by a polyregular function, but not with linear growth. These examples show that typical string transduction tasks can often be naturally divided into the job of locating the relevant parts of our input, followed by their manipulation (e. g., reordering, deleting, copying, etc.), and our framework is particularly tailored to this situation. Moreover, we can easily construct new ET programs by combining specifications for an extract task (i. e., a multispanner) with specifications for a transform task (a string-to-string function) of existing ET programs. For example, the same spanner that finds relevant text blocks, like names, email addresses, URLs, etc. can be combined with different string-to-string functions that pick and arrange some of these text blocks in a certain format. Or, for cleaning textual data, we use a string-to-string function that deletes all the annotated factors, and we combine this with different spanners for finding the factors to be deleted. As also pointed out by our examples, it is a desired feature that the output of an ET program can contain duplicates. Let us motivate this by a more practical example. Assume that the extraction phase marks the addresses in a list of customer orders, while the transformation phase then transforms these addresses into a format suitable for printing on the parcels to be shipped to the customers. In the likely situation that there are different orders by the same person, the extraction phase produces different markings that will all be transformed into the same address label. Such duplicates, however, are important, since we actually want to obtain an address label for each distinct order (i. e., distinct marking), even if these addresses are the same (although the orders are not). This means that the output sets of our ET programs should actually be bags. Our main conceptual contribution is a sound formalisation of our ET framework, which includes the extension of the classical spanner framework to multispanners (which we believe are interesting in their own right). Our main algorithmic result is about linear ET programs, i. e., the subclass where the multispanner of the extraction can be described by a regular expression (similar to the ones used above) and the transformation function is a polyregular function with linear growth (e.g., the program (E1,T1)subscript𝐸1subscript𝑇1(E_{1},T_{1})). We show that we can enumerate the output documents of linear ET programs with linear preprocessing and output-linear delay (in data complexity). Furthermore, we show that ET programs are closed under composition.\nThis implies\nthat we can enumerate the output of a composition of programs efficiently.\nAs an important tool for our enumeration algorithm, we show that linear ET programs have the same expressive power as nondeterministic streaming string transducers (NSST). This also gives an important insight with respect to the expressive power of linear ET programs. Specifically, the contributions are as follows. We introduce the formalism of multispanners, for extracting an unbounded number of spans in a single tuple (Section 2). By combining multispanners and polyregular functions, we introduce ET programs as a framework for extraction and transformation of documents (Section 3). We restrict our analysis to the linear case, and show that linear ET programs are equally expressive as NSSTs under bag-semantics (Section 4). We present an enumeration algorithm for evaluating linear ET programs with linear-time preprocessing and output-linear delay (Section 5). We show\nthat linear ET programs are closed under composition and, therefore, we can evaluate the composition of linear ET programs efficiently (Section 6). We conclude in Section 7 with some remarks\non future work.\nFurther details are provided in an appendix.\n\nReference markers found: [25]\n\nContext windows for each [ref]:\n[ref]25: ago , this framework sets the grounds for rule-based IE through the notion of spanners and how to combine them by using relational operators. Moreover, it inspired a lot of research on languages [ref]25 [ref]35, expressiveness  [ref]31   ,\nevaluation       , provenance  , and compressed evaluation    for IE. Initially conceived to understand SystemT, IBM’s rule-based IE system, it has found\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 25,\n            \"titles\": [\n                \"Joining Extractions of Regular Expressions\",\n                \"The design and analysis of computer algorithms\",\n                \"Regular Transformations (Dagstuhl Seminar 23202)\",\n                \"Expressiveness of streaming string transducers\",\n                \"Nondeterministic streaming string transducers\",\n                \"Composing Copyless Streaming String Transducers\",\n                \"Enumeration on trees with tractable combined complexity and efficient updates\",\n                \"Constant-delay enumeration for nondeterministic document spanners\",\n                \"Transductions and context-free languages\",\n                \"ℤ-polyregular functions\",\n                \"Transducers of polynomial growth\",\n                \"Ranked enumeration of MSO logic on words\",\n                \"The complexity of aggregates over extractions by regular expressions\",\n                \"Split-Correctness in Information Extraction\",\n                \"Weight annotation in information extraction\",\n                \"Making data structures persistent\",\n                \"MSO definable string transductions and two-way finite-state transducers\",\n                \"Two-way finite state transducers with nested pebbles\",\n                \"Spanners: a formal framework for information extraction\",\n                \"Document spanners: A formal approach to information extraction\",\n                \"Efficient enumeration algorithms for regular document spanners\",\n                \"Document spanners: From expressive power to decision problems\",\n                \"A logic for document spanners\",\n                \"Dynamic complexity of document spanners\",\n                \"Splitting Spanner Atoms: A Tool for Acyclic Core Spanners\",\n                \"FASTUS: A cascaded finite-state transducer for extracting information from natural-language text\",\n                \"The replace operator\",\n                \"Document Spanners for Extracting Incomplete Information: Expressiveness and Complexity\",\n                \"Streaming enumeration on nested documents\",\n                \"Constant-delay enumeration for SLP-compressed documents\",\n                \"The many facets of string transducers (invited talk)\",\n                \"Grammars for document spanners\",\n                \"Complexity Bounds for Relational Algebra over Document Spanners\",\n                \"Recursive Programs for Document Spanners\",\n                \"Rematch: a novel regex engine for finding all matches\",\n                \"A purely regular approach to non-regular core spanners\",\n                \"Spanner evaluation over slp-compressed documents\",\n                \"Document spanners - A brief overview of concepts, results, and recent developments\",\n                \"Query evaluation over slp-represented document databases with complex document editing\",\n                \"Enumerating with constant delay the answers to a query\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]25 based on the context and research area of A framework for extraction and transformation of documents. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}