{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Applying Deutsch’s concept of good explanations to artificial intelligence and neuroscience - an initial exploration\n\nPaper Abstract: \n\nPaper Section Text:\nThis section may appear to be a lengthy digression, but we believe it is helpful to study the best solution to the intelligence problem that evolution has discovered - the human brain. We distinguish two types of process that may occur in the brain. The first is the varying of randomly-initialized easy-to-vary neuronal models with many parameters to fit incoming sensory data, very similar to what deep learning does. The second process is Popperian in nature and involves the generation of hard-to-vary explanations which must be thrown out “whole cloth” if falsified by experience. Distinguishing the relative importance of these two processes for human intelligence has import for futurists and AI safety researchers looking to predict the development trajectory of AI and how long we have until superintelligence is developed. The differing views on the relative importance of these two types of processes can explain a lot of the variance between Deutsch and other experts regarding the predicted date superintelligent AI will be developed. If the brain works entirely by fitting easy-to-vary models to big data, then reaching human-level intelligence will be possible by scaling up deep learning with larger and larger models and datasets, suggesting a sooner time-to-arrival for superintelligent AI. However, if hard-to-vary explanations are an important part of human intelligence, the problem of how to program an AI to generate/discover such explanations must be solved first before truly human-like AI can be produced. Of course, just as the way airplanes fly is much different from birds, superintelligent general-purpose AI might operate in a very different way from humans and this scenario should not be discounted. In passing we note that Christopher Olah of OpenAI has posited something similar – in an interview published online he speculates that as deep learning models are scaled up they will eventually reach a point where they start to generate “crisp abstractions”. Given that currently many of the abstractions (features) being learned by today’s overparametrized deep learning systems are very “uncrisp” and brittle, we find this to be a questionable theory. The brain has about 8.6x1010superscript101010^{10} neurons and an average of 103superscript10310^{3} connections (synapses) per neuron. Each synapse contains at least 4.7 bits of information. Information is encoded in the brain both in the synaptic weights and the pattern of connections, which is arguably much more important. About 80% of those neurons are in the cerebellum, which is responsible for motor control with almost all the others in the cerebral cortex. Which neurons are connected to which is determined by a mix of genetic mechanisms which control the placement of connections during development and processes of dendritic growth and pruning during the lifetime of any given individual in response to its environment. The former can be viewed as an “outer-loop” optimization that occurs via evolution over many generations while the latter is part of an “inner-loop” optimization that occurs during an individual’s lifetime. As a side note, Popper has noted that evolution, since it is based on random mutations which happen “from within” operates along the lines of his epistemology (conjecture and criticism/falsification). The “inner loop” optimization, by contrast, appears to be largely based on fitting sense data. Both processes are known to occur although the relative importance of each is far from understood (the nature-vs-nurture debate). We conjecture that to the extent the HTV principle is involved in human cognition it would have to have arisen from genetics, having been stumbled on by evolution and not learned by fitting highly variable neural models to sense data. The amount of data that can inform neural structure from genetics is limited however by size of the genome. This “genetic bottleneck” has been suggested to serve as a form of regularization on how evolution has shaped the brain. The human genome contains about 750 megabytes of information total. About 2% of the genome is coding for proteins (15 Mb). While the degree of importance of the non-coding genome is largely unknown, if we assume 25% of the non-coding genome is used for regulatory purposes, that is about 200 Mb for the genetic bottleneck. In what follows, we ignore the complex question of the importance of how much is learned from genetics vs environment. Instead we focus on the question of how far brute-force fitting to sense data alone can go. Conventional thinking in neuroscience has been that the brain cannot operate on brute-force fitting of sense data alone, because the amount of data provided is too scant to train the number of parameters involved. As an aside, this “poverty of the stimulus” was the key motivation for Chomsky to propose a genetic component to human grammar which he called “universal grammar”. However, recently Hasson et al. have argued that the amount of data streaming in though the senses is enough to allow for brute-force fitting.[ref]13 Quantifying the amount of information streaming into the brain through the senses is very difficult, but it may be quite large – Zimmerman estimates that the channel capacity of the human visual system is 107 bits/s while the channel capacity of the auditory system is 105 bits/s. The actual amount of information is much lower due to redundancy effects, however if we assume it is 100 times slower than learning the 4x1014superscript101410^{14} bits necessary to specify all 9x1013superscript101310^{13} synapses in the brain is theoretically possible in 133 years. Regardless of the relevance of this back-of-the envelope calculation, it is safe to say that the brain does process very large amounts of data. For instance a child has seen many faces by an early age under a variety of angles, distances, and lighting conditions. As Hasson points out, humans are subject to an “other-race” effect whereby they find faces of different ethnicities more difficult to distinguish, suggesting a type of brute force fitting which struggles to extrapolate even slightly.[ref]13 A study based on audio recording in a child’s home suggests that children hear at least 5-10 million words per year.[ref]51 By age 10, this suggests 100 million words may have been heard. This is very large but still 3000 times smaller than the natural language model GPT3’s training corpus, which contained 300 billion tokens. Levy, Hasson, and Malach have estimated that two hundred million neurons in the visual cortex are involved in the representation of a single-object image. Thus, it seems that a large part of the brain, at least the part dedicated to raw perception of objects and possibly a large part of language ability, is learned by fitting large easy-to-vary neural models to sensory data. However even Uri Hasson, who forcefully argued for this position recently,[ref]13 points out that humans have additional abilities which go beyond this, pointing to our ability to learn to mathematics such as calculus and perform abstract symbolic reasoning. What about “common sense” knowledge, or “background knowledge”? Some common sense knowledge comes in the form of “rules of thumb”, which are rules extracted from experience through induction. They are brute facts, often “free floating” and unconnected to other facts. Suppose we are talking to someone who has no knowledge of how the human body works. They still may be able to treat certain diseases using a long list of rules of thumb gained from experience. For instance: Aspirin helps with headaches. If Aspirin doesn’t work, try ibuprofen next. Drinking lots of water helps ameliorate cold symptoms. Homeopathic remedies do not work. (note one can mistakenly go the wrong way here!) The person wouldn’t be able to explain any of these facts but instead would justify them by pointing to how often they have been confirmed by prior experience. Explanatory theories, on the other hand, are required whenever trying to predict something you haven’t seen before. Consider the following questions: What would happen if you poured a bottle of bleach into the fuel tank of a car? What would happen if the United States cut off all trade with France tomorrow? What would happen if you diluted hot sauce 10 times and repeated this 10x dilution process 10 times? We must use some form of common sense reasoning to answer these questions, reasoning which uses explanatory theories. The GPT2 language model (which is a type of brute-force fit, highly-variable model) is very bad at answering these sorts of questions. GPT3 does much better, but still stumbles in this particular area. The two processes discussed in the last section map nicely onto Kahneman’s distinction between System 1 and System 2. Some unique characteristics of both of these two processes are summarized in figure 1. To summarize our view on the titular question of this section, it appears that large parts of the brain such as the perceptual and motor systems are learned by brute-force fitting. However, our capacity to build both common sense explanatory theories and scientific explanatory theories is where the HTV principle seems to be important.\n\nReference markers found: [13]\n\nContext windows for each [ref]:\n[ref]13: to human grammar which he called “universal grammar”. However, recently Hasson et al. have argued that the amount of data streaming in though the senses is enough to allow for brute-force fitting.[ref]13 Quantifying the amount of information streaming into the brain through the senses is very difficult, but it may be quite large – Zimmerman estimates that the channel capacity of the human\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 13,\n            \"titles\": [\n                \"Self-explaining AI as an alternative to interpretable AI\",\n                \"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification\",\n                \"A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis\",\n                \"Mastering the game of Go with deep neural networks and tree search\",\n                \"The Beginning of Infinity: Explanations That Transform the World\",\n                \"The Elements of Statistical Learning\",\n                \"Fantastic generalization measures and where to find them\",\n                \"A human-centered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy\",\n                \"When it comes to gorillas, google photos remains blind\",\n                \"Human-level control through deep reinforcement learning\",\n                \"Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics\",\n                \"Deepmind’s losses and the future of artificial intelligence\",\n                \"The Logic of Scientific Discovery\",\n                \"Direct fit to nature: An evolutionary perspective on biological and artificial neural networks\",\n                \"‘Less Than One’-Shot Learning: Learning N Classes From M<N Samples\",\n                \"A determination of the deflection of light by the sun’s gravitational field, from observations made at the total eclipse of May 29, 1919\",\n                \"Applying machine learning techniques to predict the properties of energetic materials\",\n                \"Machine learning of energetic material properties\",\n                \"Deep learning enables rapid identification of potent DDR1 kinase inhibitors\",\n                \"Assessing the impact of generative AI on medicinal chemistry\",\n                \"Deep learning for molecular design—a review of the state of the art\",\n                \"Creative blocks\",\n                \"Conjectures and Refutations: The Growth of Scientific Knowledge\",\n                \"Pattern Recognition and Machine Learning\",\n                \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\",\n                \"A theory of universal artificial intelligence based on algorithmic complexity\",\n                \"Novum Organum\",\n                \"Objective Knowledge: An Evolutionary Approach\",\n                \"The Myth of the Framework: In Defence of Science and Rationality, In Defence of Science and Rationality\",\n                \"A new way to explain explanation\",\n                \"Reconciling modern machine-learning practice and the classical bias–variance trade-off\",\n                \"Deep Double Descent: Where Bigger Models and More Data Hurt\",\n                \"Summa totius logicae\",\n                \"Quaestiones et decisiones in quattuor libros Sententiarum Petri Lombardi: Centilogium theologicum\",\n                \"Modeling by shortest data description\",\n                \"Keeping the neural networks simple by minimizing the description length of the weights\",\n                \"A philosophical treatise of universal induction\",\n                \"Flat minima\",\n                \"Sharp Minima Can Generalize For Deep Nets\",\n                \"On the Measure of Intelligence\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]13 based on the context and research area of Applying Deutsch’s concept of good explanations to artificial intelligence and neuroscience - an initial exploration. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}