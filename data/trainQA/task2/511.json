{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Parameterized MDPs and Reinforcement Learning Problems - A Maximum Entropy Principle Based Framework\n\nPaper Abstract: \n\nPaper Section Text:\nWe broadly classify our simulations into two categories. Firstly, in the model-free RL setting we demonstrate our Algorithm 1 to determine the control policy Î¼âˆ—superscriptğœ‡\\mu^{*} for the finite and infinite Shannon entropy variants of the Gridworld environment in Figure 2. Each cell in the Gridworld denotes a state. The cells colored black are invalid states. An agent can choose to move vertically, horizontal, diagonally or stay at the current cell. Each action is followed by a probability to slip in the neighbouring states (probability of 0.05 to slip in each of the vertical and horizontal directions, and probability of 0.025 to slip in each of the diagonal directions - cumulative pâ€‹(sâ€‹lâ€‹iâ€‹p)â‰ˆ0.3ğ‘ğ‘ ğ‘™ğ‘–ğ‘0.3p(slip)\\approx 0.3). For the finite entropy case - each step incurs a unit cost. The process terminates when the agent reaches the terminal state ğ“ğ“\\mathbf{T}. For the infinite entropy case - each step incurs a unit reward. Secondly, in the parameterized MDPs and RL setting we demonstrate our Algorithms 2 and 3 in designing a 5G small cell network. This involves simultaneously determining the locations of the small cells in the network as well as the optimal routing path of the communication packets from the base station to the users. We compare our MEP-based Algorithm 1 with the benchmark algorithms Entropy Regularized (ER) G-learning (also referred to as Soft Q-learning) , Q-learning  and Double Q-learning [ref]13. Note that our choice of the benchmark algorithm G-learning (or, entropy regularized Soft Q) presented in ) is based on its commonality to our framework as discussed in the Section III-B, and the choice of algorithms Q-learning and Double Q-learning is based on their widespread utility in the literature. Also, note that the work done in  already establishes the efficacy of the G-learning algorithm over the following algorithms in literature Q-learning, Double Q-learning, Î¨Î¨\\Psi-learning , Speedy Q-learning [ref]46, and the consistent Bellman Operator ğ’¯Csubscriptğ’¯ğ¶\\mathcal{T}_{C} of . Below we highlight features and advantages of our MEP-based Algorithm 1. Faster Convergence to Optimal Jâˆ—superscriptğ½J^{*}:  Figures 3(a1)-(a3) (finite entropy variant of Gridworld) and Figures 3 (b1)-(b3) (infinite entropy variant of Gridworld) illustrate the faster convergence of our MEP-based Algorithm 1 for different discount factor Î³ğ›¾\\gamma values. Here, at each episode the percentage error Î”â€‹V/Jâˆ—Î”ğ‘‰superscriptğ½\\Delta V/J^{*} between the value function VÎ²Î¼superscriptsubscriptğ‘‰ğ›½ğœ‡V_{\\beta}^{\\mu} corresponding to learned policy Î¼=Î¼â€‹(eâ€‹p)ğœ‡ğœ‡ğ‘’ğ‘\\mu=\\mu(ep) in the episode eâ€‹pğ‘’ğ‘ep, and the optimal value function Jâˆ—superscriptğ½J^{*} is given by where Nğ‘N denotes the total experimental runs and iğ‘–i indexes the value function VÎ²,iÎ¼superscriptsubscriptğ‘‰ğ›½ğ‘–ğœ‡V_{\\beta,i}^{\\mu} for each run. As observed in Figures 3(a1)-(a3), and Figures 3(b1)-(b3), our Algorithm 1 converges even faster as the discount factor Î³ğ›¾\\gamma decreases. We characterize the faster convergence rates also in terms of the convergence time - more precisely the percentage EÂ¯pâ€‹rsubscriptÂ¯ğ¸ğ‘ğ‘Ÿ\\bar{E}_{pr} of total episodes taken for the learning error Î”â€‹V/Jâˆ—Î”ğ‘‰superscriptğ½\\Delta V/J^{*} to reach within 5%percent55\\% of the best (see Figures 3(a4) and 3(b4)). As is observed in the figures, the performance of our (MEP-based) algorithm in comparison to entropy regularized G learning is better across all values (0.650.650.65 to 0.950.950.95) of discount factor Î³ğ›¾\\gamma. Note that the performance of Algorithm 1 gets even better with decreasing Î³ğ›¾\\gamma values where the smaller discount factor values occur in instances such as the context of recommendation systems , and teaching RL-agents using human-generated rewards . Robustness to noise in data: Figures 3(c1)-(c4) demonstrate robustness to noisy environments; here the instantaneous cost câ€‹(s,a,sâ€²)ğ‘ğ‘ ğ‘superscriptğ‘ â€²c(s,a,s^{\\prime}) in the finite horizon variant of Gridworld is noisy. For the purpose of simulations, we add Gaussian noise ğ’©â€‹(0,Ïƒ2)ğ’©0superscriptğœ2\\mathcal{N}(0,\\sigma^{2}) with Ïƒ=1ğœ1\\sigma=1 for vertical and horizontal actions, and Ïƒ=0.5ğœ0.5\\sigma=0.5 for diagonal movements. Here, at each episode we compare the percentage error Î”â€‹V/Jâˆ—Î”ğ‘‰superscriptğ½\\Delta V/J^{*} in the learned value functions VÎ²subscriptğ‘‰ğ›½V_{\\beta} (corresponding to the state-action value estimate in (III-C)) of the respective algorithms. Similar to our observations and conclusions in Figures 3(a1)-(a3) and Figures 3(b1)-(b3) we see faster convergence of our MEP-based algorithm over the benchmark algorithms in Figures 3(c1)-(c3) in the case of noisy environment. Also, Figure 3(c4) demonstrates that across all discount factor values (0.650.650.65 to 0.950.950.95), Algorithm 1 converges faster than the entropy regularized Soft Q learning. Simultaneously determining the unknown parameters and policy in Parameterized MDPs:  We design the 5G Small Cell Network (see Figure 1) both when the underlying model (csâ€‹sâ€²asuperscriptsubscriptğ‘ğ‘ superscriptğ‘ â€²ğ‘c_{ss^{\\prime}}^{a} and psâ€‹sâ€²asuperscriptsubscriptğ‘ğ‘ superscriptğ‘ â€²ğ‘p_{ss^{\\prime}}^{a}) is known (using Algorithm 2) and as well as unknown (using Algorithm 3). In our simulations we randomly distribute 464646 user nodes {ni}subscriptğ‘›ğ‘–\\{n_{i}\\} at {xi}subscriptğ‘¥ğ‘–\\{x_{i}\\} and the base station Î´ğ›¿\\delta at zğ‘§z in the domain Î©âŠ‚â„2Î©superscriptâ„2\\Omega\\subset\\mathbb{R}^{2} as shown in Figure 4(a). The objective is to determine the locations {yj}j=15superscriptsubscriptsubscriptğ‘¦ğ‘—ğ‘—15\\{y_{j}\\}_{j=1}^{5} (parameters) of the small cells {fj}j=15superscriptsubscriptsubscriptğ‘“ğ‘—ğ‘—15\\{f_{j}\\}_{j=1}^{5} and determine the corresponding communication routes (policy). Here, the state space of the underlying MDP is ğ’®={n1,â€¦,n46,f1,â€¦,f5}ğ’®subscriptğ‘›1â€¦subscriptğ‘›46subscriptğ‘“1â€¦subscriptğ‘“5\\mathcal{S}=\\{n_{1},\\ldots,n_{46},f_{1},\\ldots,f_{5}\\} where the locations y1,â€¦,y5subscriptğ‘¦1â€¦subscriptğ‘¦5y_{1},\\ldots,y_{5} of the small cells are the unknown parameters {Î¶s}subscriptğœğ‘ \\{\\zeta_{s}\\} of the MDP, the action space is ğ’œ={f1,â€¦,f5}ğ’œsubscriptğ‘“1â€¦subscriptğ‘“5\\mathcal{A}=\\{f_{1},\\ldots,f_{5}\\}, and the cost function câ€‹(s,a,sâ€²)=â€–Ïâ€‹(s)âˆ’Ïâ€‹(sâ€²)â€–22ğ‘ğ‘ ğ‘superscriptğ‘ â€²superscriptsubscriptnormğœŒğ‘ ğœŒsuperscriptğ‘ â€²22c(s,a,s^{\\prime})=\\|\\rho(s)-\\rho(s^{\\prime})\\|_{2}^{2} where Ïâ€‹(â‹…)ğœŒâ‹…\\rho(\\cdot) denotes the spatial location of the respective states. The objective is to simultaneously determine the parameters (unknown small cell locations) and the control policy (communication routes in the 5G network). We consider two cases where (a) psâ€‹sâ€²asuperscriptsubscriptğ‘ğ‘ superscriptğ‘ â€²ğ‘p_{ss^{\\prime}}^{a} is deterministic, i.e. an action ağ‘a at the state sğ‘ s results into sâ€²=asuperscriptğ‘ â€²ğ‘s^{\\prime}=a with probability 111, and (b) psâ€‹sâ€²asuperscriptsubscriptğ‘ğ‘ superscriptğ‘ â€²ğ‘p_{ss^{\\prime}}^{a} is probabilistic such that action ağ‘a at the state sğ‘ s results into sâ€²=asuperscriptğ‘ â€²ğ‘s^{\\prime}=a with probability 0.90.90.9 or to the state sâ€²=f1superscriptğ‘ â€²subscriptğ‘“1s^{\\prime}=f_{1} with probability 0.10.10.1. Additionally, due to absence of prior work in literature on network design problems modeled as parameterized MDPs, we compare our results only with the solution resulting from a straightforward sequential methodology (as shown in Figure 4(a)) where we first partition the user nodes into 555 distinct clusters to allocate a small cells in each cluster, and then determine optimal routes in the network. Deterministic pâ€‹(s,a,sâ€²)ğ‘ğ‘ ğ‘superscriptğ‘ â€²p(s,a,s^{\\prime}): Figure 4(b) illustrates the allocation of small cells and the corresponding communication routes (resulting from optimal policy Î¼âˆ—superscriptğœ‡\\mu^{*}) as determined by the Algorithm 2. Here, the network is designed to minimize the cumulative cost of communication from each user node and small cell. As denoted in the Figure, the route Î´â†’y3â†’y4â†’y1â†’niâ†’ğ›¿subscriptğ‘¦3â†’subscriptğ‘¦4â†’subscriptğ‘¦1â†’subscriptğ‘›ğ‘–\\delta\\rightarrow y_{3}\\rightarrow y_{4}\\rightarrow y_{1}\\rightarrow n_{i} carries the communication packet from the base station Î´ğ›¿\\delta to the respective user nodes nisubscriptğ‘›ğ‘–n_{i} as indicated by the grey arrow from y1subscriptğ‘¦1y_{1}. The cost incurred here is approximately 180%percent180180\\% lesser than that in Figure 4(a) clearly indicating the advantage obtained from simultaneously determining the parameters and policy over a sequential methodology. In the model-free RL setting where the functions câ€‹(s,a)ğ‘ğ‘ ğ‘c(s,a), psâ€‹sâ€²asuperscriptsubscriptğ‘ğ‘ superscriptğ‘ â€²ğ‘p_{ss^{\\prime}}^{a}, and the locations {xi}subscriptğ‘¥ğ‘–\\{x_{i}\\} of the user nodes {ni}subscriptğ‘›ğ‘–\\{n_{i}\\} are not known, we employ our Algorithm 3 to determine the small cell locations {yj}j=15superscriptsubscriptsubscriptğ‘¦ğ‘—ğ‘—15\\{y_{j}\\}_{j=1}^{5} and as well as the optimal policy {Î¼âˆ—â€‹(a|s)}superscriptğœ‡conditionalğ‘ğ‘ \\{\\mu^{*}(a|s)\\} as demonstrated in the Figure 4(c). It is evident from Figures 4(b) and (c) that the solutions obtained when the model is completely known and unknown are approximately same. In fact, the solutions obtained differ only by 1.9%percent1.91.9\\% in terms of the total cost âˆ‘sâˆˆğ’®JÎ¶â€‹Î·Î¼â€‹(s)subscriptğ‘ ğ’®subscriptsuperscriptğ½ğœ‡ğœğœ‚ğ‘ \\sum_{s\\in\\mathcal{S}}J^{\\mu}_{\\zeta\\eta}(s) (22) incurred, clearly indicating the efficacy of our model-free learning Algorithm 3. Probabilistic pâ€‹(s,a,sâ€²)ğ‘ğ‘ ğ‘superscriptğ‘ â€²p(s,a,s^{\\prime}): Figure 4(d) illustrates the solution as obtained by our Algorithm 2 when the underlying model (câ€‹(s,a)ğ‘ğ‘ ğ‘c(s,a), psâ€‹sâ€²asuperscriptsubscriptğ‘ğ‘ superscriptğ‘ â€²ğ‘p_{ss^{\\prime}}^{a}, and {xi}subscriptğ‘¥ğ‘–\\{x_{i}\\}) is known. As before, here the network is designed to minimize the cumulative cost of communication from each user node and small cell. The cost associated to the network design is approximately 127%percent127127\\% lesser than in Figure 4(a). Figure 4(e) illustrates the solution as obtained by the Algorithm 3 for the model-free case (câ€‹(s,a)ğ‘ğ‘ ğ‘c(s,a), psâ€‹sâ€²asuperscriptsubscriptğ‘ğ‘ superscriptğ‘ â€²ğ‘p_{ss^{\\prime}}^{a}, and {xi}subscriptğ‘¥ğ‘–\\{x_{i}\\} are unknown). Similar to the above scenario, the solutions obtained for this case using the Algorithms 2 and 3 are also approximately the same and differ only by 0.3%percent0.30.3\\% in terms of the total cost âˆ‘sJÎ¶â€‹Î·Î¼â€‹(s)subscriptğ‘ subscriptsuperscriptğ½ğœ‡ğœğœ‚ğ‘ \\sum_{s}J^{\\mu}_{\\zeta\\eta}(s) incurred; thereby, substantiating the efficacy of our proposed model-free learning Algorithm 3. Sensitivity Analysis:  Our algorithms enable categorizing the user nodes {ni}subscriptğ‘›ğ‘–\\{n_{i}\\} in Figure 4(b) into the categories of (i) low, (ii) medium, and (iii) high sensitiveness such that the final solution is least susceptible to the user nodes in (i) and most susceptible to the nodes in (iii). Note that the above sensitivity analysis requires to compute the derivative âˆ‘sâ€²âˆ‚VÎ²Î¼â€‹(sâ€²)/âˆ‚Î¶ssubscriptsuperscriptğ‘ â€²superscriptsubscriptğ‘‰ğ›½ğœ‡superscriptğ‘ â€²subscriptğœğ‘ \\sum_{s^{\\prime}}\\partial V_{\\beta}^{\\mu}(s^{\\prime})/\\partial\\zeta_{s}, and we determine it by solving for the fixed point of the Bellman equation in (24). The derivative âˆ‘sâ€²âˆ‚VÎ²Î¼â€‹(sâ€²)/âˆ‚Î¶ssubscriptsuperscriptğ‘ â€²superscriptsubscriptğ‘‰ğ›½ğœ‡superscriptğ‘ â€²subscriptğœğ‘ \\sum_{s^{\\prime}}{\\partial V_{\\beta}^{\\mu}(s^{\\prime})}/{\\partial\\zeta_{s}} computed at Î²â†’âˆâ†’ğ›½\\beta\\rightarrow\\infty is a measure of sensitivity of the solution to the cost function âˆ‘sJÎ¶â€‹Î·Î¼â€‹(s)subscriptğ‘ subscriptsuperscriptğ½ğœ‡ğœğœ‚ğ‘ \\sum_{s}J^{\\mu}_{\\zeta\\eta}(s) in (22) since VÎ²Î¼superscriptsubscriptğ‘‰ğ›½ğœ‡V_{\\beta}^{\\mu} in (11) is a smooth approximation of JÎ¶â€‹Î·Î¼â€‹(s)subscriptsuperscriptğ½ğœ‡ğœğœ‚ğ‘ J^{\\mu}_{\\zeta\\eta}(s) in (22) and VÎ²Î¼â†’JÎ¶â€‹Î·Î¼â€‹(s)â†’superscriptsubscriptğ‘‰ğ›½ğœ‡subscriptsuperscriptğ½ğœ‡ğœğœ‚ğ‘ V_{\\beta}^{\\mu}\\rightarrow J^{\\mu}_{\\zeta\\eta}(s) as Î²â†’âˆâ†’ğ›½\\beta\\rightarrow\\infty. A similar analysis for Figure 4(c)-(e) can be done if the locations {xi}subscriptğ‘¥ğ‘–\\{x_{i}\\} of the user nodes {ni}subscriptğ‘›ğ‘–\\{n_{i}\\} are known to the agent. The sensitivity of the final solution to the locations {yj}subscriptğ‘¦ğ‘—\\{y_{j}\\}, zğ‘§z of the small cells and the base station can also be determined in a similar manner. Entropy over paths versus entropy of the policy:  We demonstrate the benefit of maximizing the entropy of the distribution {pÎ¼â€‹(Ï‰|s)}subscriptğ‘ğœ‡conditionalğœ”ğ‘ \\{p_{\\mu}(\\omega|s)\\} over the paths of an MDP as compared to the distribution {Î¼â€‹(a|s)}ğœ‡conditionalğ‘ğ‘ \\{\\mu(a|s)\\} over the control actions. Figure 4(g) demonstrates the 5G network obtained by considering the distribution over the control policy, and the Figure 4(h) illustrates the network obtained by considering the distribution over the entire paths. The network cost incurred in the Figure 4(h) is 5%percent55\\% less than the cost incurred in Figure 4(g). Here, we have considered the above demonstrated probabilistic psâ€‹sâ€²asuperscriptsubscriptğ‘ğ‘ superscriptğ‘ â€²ğ‘p_{ss^{\\prime}}^{a} scenario and minimized the cumulative communication cost incurred only from the user nodes. Avoiding poor local minima and large scale setups:  As noted in the Section V-B, annealing Î²ğ›½\\beta from a small value Î²min(â‰ˆ0)annotatedsubscriptğ›½absent0\\beta_{\\min}(\\approx 0) to a large value Î²maxsubscriptğ›½\\beta_{\\max} prevents the algorithm from getting stuck at a poor local minima. Figure 4(i) demonstrates the network design obtained where the Algorithm 2 does not anneal Î²ğ›½\\beta, and iteratively solves the optimization problem at Î²=Î²maxğ›½subscriptğ›½\\beta=\\beta_{\\max}. The resulting network incurs a 11%percent1111\\% higher cost in comparison to the network obtained in 4(h) where the Algorithm 2 anneals Î²ğ›½\\beta from a small to a large value. Figure 4(j) demonstrate the 5G network design obtained using Algorithm 2 when the user nodes are increased by around 121212 times (610610610), and the allocated small cells are doubled to 101010.\n\nReference markers found: [13]\n\nContext windows for each [ref]:\n[ref]13: station to the users. We compare our MEP-based Algorithm 1 with the benchmark algorithms Entropy Regularized (ER) G-learning (also referred to as Soft Q-learning) , Q-learning  and Double Q-learning [ref]13. Note that our choice of the benchmark algorithm G-learning (or, entropy regularized Soft Q) presented in ) is based on its commonality to our framework as discussed in the Section III-B, and\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 13,\n            \"titles\": [\n                \"Taming the Noise in Reinforcement Learning via Soft Updates\",\n                \"Handbook of Markov decision processes: methods and applications\",\n                \"Neuro-dynamic programming\",\n                \"Linear programming and markov decision chains\",\n                \"Linear Programming for Large-Scale Markov Decision Problems\",\n                \"Q-learning\",\n                \"Information theory and statistical mechanics\",\n                \"Deterministic annealing, clustering, and optimization\",\n                \"A scalable approach to combinatorial library design for drug discovery\",\n                \"Maximal entropy random walk for region-based visual saliency\",\n                \"Aggregation of graph models and markov chains by deterministic annealing\",\n                \"Protein structure alignment by deterministic annealing\",\n                \"Maximum entropy inverse reinforcement learning.\",\n                \"Double q-learning\",\n                \"Soft q-learning with mutual-information regularization\",\n                \"Relative entropy policy search\",\n                \"A Unified View of Entropy-Regularized Markov Decision Processes\",\n                \"An Alternative Softmax Operator for Reinforcement Learning\",\n                \"Bridging the Gap Between Value and Policy Based Reinforcement Learning\",\n                \"Sbeed: Convergent reinforcement learning with nonlinear function approximation\",\n                \"Trust Region Policy Optimization\",\n                \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\n                \"Location-aware self-organizing methods in femtocell networks\",\n                \"Wireless backhauling of 5g small cells: Challenges and solution approaches\",\n                \"Policy search for the optimal control of markov decision processes: A novel particle-based iterative scheme\",\n                \"Simultaneous facility location and path optimization in static and dynamic networks\",\n                \"The planar k-means problem is np-hard\",\n                \"Querying beneficial constraints before clustering using facility location analysis\",\n                \"Locally Weighted Ensemble Clustering\",\n                \"Soft policy gradient method for maximum entropy deep reinforcement learning\",\n                \"Task-oriented deep reinforcement learning for robotic skill acquisition and control\",\n                \"Parameterized markov decision process and its application to service rate control\",\n                \"Deep Reinforcement Learning in Parameterized Action Space\",\n                \"Reinforcement Learning with Parameterized Actions\",\n                \"Hierarchical Approaches for Reinforcement Learning in Parameterized Action Space\",\n                \"Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space\",\n                \"Event-triggered distributed control of nonlinear interconnected systems using online reinforcement learning with exploration\",\n                \"Toward generalization of automated temporal abstraction to partially observable reinforcement learning\",\n                \"Probability theory: The logic of science\",\n                \"Maximizing entropy over markov processes\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]13 based on the context and research area of Parameterized MDPs and Reinforcement Learning Problems - A Maximum Entropy Principle Based Framework. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}