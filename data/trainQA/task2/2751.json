{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Intelligent Decision Assistance Versus Automated Decision-Making: Enhancing Knowledge Work Through Explainable Artificial Intelligence\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, we use the previously depicted research streams of DSS and automation and synthesize them to conceptualize a solution against the disadvantages of automation.\nSubsequently, we discuss three particular techniques of this concept\n We see two main dimensions that influence the undesired effects of automation, which we discuss in more detail below: First, the general level of human control and agency  and, second, the form and degree of explainability . Which level of human agency in automation should be implemented is a notorious discussion in automation literature .\nAsatiani et al.  have discussed that retaining control of human workers may help to sustain their skill level. Similar, Endsley et al. [ref]39 argued that lower automation levels, in general, can keep them cognitively engaged. Regarding the second dimension, the literature suggests “that a seamless, collaborative interaction between human agents and automated tools, as opposed to using automation as an isolated “black box”, could help to prevent the ill effects of deskilling” [6, p. 6].\nAs discussed, the research stream of XAI addresses this “black box” issue in AI-based automated decision-making. Recent examples  \ndemonstrate the capability of XAI to support end-users in their decision-making. By varying the “degree” of explanations, i.e. the system’s transparency , we believe different effects on the negative aspects of automation could be influenced.\nOn the one hand, some might argue that more explainability is always better. However, the latest research suggests that a high level of automation paired with high explainability might just result in automation bias . Furthermore, the degree of explainability should be adapted to the profession and experience of the end-user, e.g. novice users might need more intuitive and simpler explanations while data scientists can get the full degree of potential explanations .\nThese examples show that also the degree of explainability needs to be chosen thoughtfully. As introduced, there are many forms of guidance—suggestive, quasi-suggestive, and informative guidance .\nSuggestive guidance provides the decision-maker with explicit recommendations and tries to increase the guidance of this recommendation. However, as Parasuraman et al.  states, also partially automated systems can lead to automation bias and skill degradation.\nIn contrast, as mentioned, informative decisional guidance is a form of guidance where users do not receive explicit recommendations .\nWe follow this line of reasoning and propose a system could simply withhold its recommendation—although it is aware of that recommendation. Parkes  validates that suggestive guidance—which is actually a form of automated decision making—can lead to automation bias, while informative guidance does not have such effects. Research also shows that the effects of the types of guidance vary depending on the task complexity. Montazemi et al.  found that suggestive guidance is better for less complex tasks and informative guidance is better with increasing task complexity. This argument strengthens our derivation. Following this line of thought gives rise to the idea to set the degree of automation to almost zero and withhold explicit AI recommendations while keeping support through explanations up.\nBy doing so, we can minimize the drawbacks of automation while still assisting human decision-making. We are creating intelligent systems that are fully capable of solving issues on their own but use their capabilities to inspire and support instead of automating.\nBased on the derivation, we name this new class of DSS, Intelligent Decision Assistance (IDA) and define it as follows: Definition: Intelligent Decision Assistance (IDA) is an AI that a) supports humans, b) does not recommend explicit decisions or actions, and c) explains its reasoning Referring to the three phases of decision making—intelligence, design, and choice—we mainly support with this approach the intelligence and to some extent the design phase.\nIn terms of final effects on the human, we derive three hypotheses (engagement, performance, automation disadvantages). First, IDA provides decision-makers with options to actively engage with the task by interactively requesting explanations, interpreting them and essentially communicating with the AI. As Asatiani et al.   have discussed providing explanations instead of using automation as an isolated “blackbox” could result in an engaged human-AI collaboration. Thus, we hypothesize: H1: IDA increases engagement with the task. Beyond that, we hypothesize that IDA should increase human performance. While especially, if the automation is far better than the human, IDA will most likely not exceed automated decision-making, it should still improve the performance by providing guidance and especially insights.\nTherefore, we formulate: H2: IDA performance outperforms the human alone. Lastly, because IDA does not incorporate higher levels of automation it should reduce automation disadvantages and especially prevent deskilling. Therefore, we formulate the following hypothesis: H3: IDA reduces automation induced disadvantages. In the next section, we are going to test these hypotheses based on empirical studies in the literature. Figure 1 depicts IDA in the continuum of both discussed dimensions. We depict different types of systems for decision-making. At a high level of automation and almost no explanations, we position automation . Traditional DSS come also usually with a higher level of automation, through providing explicit recommendation, but additionally provide explanations for the decision-maker.\nWe delimit ourselves from DSS that use AI to transform unstructured data into structured data and DSS that use AI to produce a pre-decision output, e.g. a forecast. As stated, Parasuraman et al.  define four stages of automation—information acquisition, information analysis, decision-making, and actions. Following this classification, we focus on the decision-making level. This classification allows us also to differentiate IDA from Advanced Analytics [ref]47.\nWhile advanced analytics may incorporate AI solutions they are always on the information acquisition or analysis level. In contrast, IDA allows the decision-maker to actively engage on the decision level and is positioned in the right top corner of Figure 1 with high explainability and full human autonomy. Now that we derived, defined, and delimit IDA, we discuss specific explanation techniques that support IDA and consequently pose valid implementation options. Specifically, we discuss feature importance, example-based explanations, and counterfactual explanations. We explain these features based on the example of a loan approval decision-making task. Feature importance:\nFeature importance is a model-agnostic technique that gives the decision-maker information about the importance of specific data points. Two famous algorithms of feature importance are LIME [ref]48 and SHAP .\nIn a loan approval decision where the banker has information about past credits, expenses, demographics, etc., one could now train artificial intelligence to make this decision and recommend explicit decisions.\nIn contrast, IDA would withhold the specific AI decision but provide the decision-maker, i.e. the banker, with information on which data was in particular important for the AI’s decision.\nIn an IDA this information could now be used for various use cases. Now in the time of big data, e.g. having many information on customers, one particular great use case would be to filter or sort the features in an intelligent way based on the feature importance. Example-based explanations:\nExample-based explanations provide historical data that is similar to the current instance .\nExample-based explanations, therefore essentially represent some form of information retrieval.\nResearch in psychology states that humans prefer explanations that show examples .\nFurthermore, examples can be used within complex tasks .\nReferring to our loan approval case, the decision-maker would receive information on past approvals that were similar.\nIn an IDA, the decision-maker would get information about similar historical cases that are labeled. Based on these examples, the decision-maker should be able to infer differences or similarities.\n Counterfactual explanation:\nCounterfactual explanations give information on what the smallest change would be to get a different AI decision .\nCounterfactual explanations take a similar form to the statement :\n“You were denied a loan because your annual income was 20,000. If your income had been 45,000, you would have been offered a loan.”\nIn an IDA a counterfactual explanation would look like the following: “Your current annual income is £30,000. If your income would be £45,000, the AI’s decision would change.”\nThis type of non-intrusive explanation would lead to an increased thought process of the decision-maker. Figure 2 highlights the idea of IDA for a credit allowance example. On the left side, we display a traditional interface for automated decision-making. On the right side, IDA is visualized. In the traditional interface, the decision-maker gets a specific recommendation. Additionally, the decision-maker gets the available information on the credit applicant, the importance of the features for the decision, and optional explanation options. In contrast, an IDA does not provide a specific recommendation, but rather various XAI techniques that allow the decision-maker to “brainstorm” with the AI.\n\nReference markers found: [39]\n\nContext windows for each [ref]:\n[ref]39: is a notorious discussion in automation literature .\nAsatiani et al.  have discussed that retaining control of human workers may help to sustain their skill level. Similar, Endsley et al. [ref]39 argued that lower automation levels, in general, can keep them cognitively engaged. Regarding the second dimension, the literature suggests “that a seamless, collaborative interaction between\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 39,\n            \"titles\": [\n                \"On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection\",\n                \"The strategic impacts of intelligent automation for knowledge and service work: An interdisciplinary review\",\n                \"Decision support and business intelligence systems (required)\",\n                \"A framework for management information systems\",\n                \"A new approach to automating services\",\n                \"The future of employment: How susceptible are jobs to computerisation?\",\n                \"Implementation of automation as distributed cognition in knowledge work organizations: Six recommendations for managers\",\n                \"How FinTechs can enable better support to FIs’ credit decisioning?\",\n                \"Ai in talent acquisition: a review of ai-applications used in recruitment and selection\",\n                \"Automation use and automation bias\",\n                \"A model for types and levels of human interaction with automation\",\n                \"Disconnecting knowledge from the knower: the knowledge worker as icarus\",\n                \"How to cope with change?-preserving validity of predictive services over time\",\n                \"A critical analysis of decision support systems research revisited: the rise of design science\",\n                \"A review of the nature and effects of guidance design features\",\n                \"Explainable artificial intelligence: Objectives, stakeholders, and future research opportunities\",\n                \"Decisional guidance for computer-based decision support\",\n                \"Peeking inside the black-box: a survey on explainable artificial intelligence (xai)\",\n                \"Automate/informate: The two faces of intelligent technology\",\n                \"A brief history of decision support systems\",\n                \"Right-brained decision support systems\",\n                \"Assessing dss effectiveness using evaluation research methods\",\n                \"The new science of management decision\",\n                \"Decisional guidance\",\n                \"Explanations from intelligent systems: Theoretical foundations and implications for practice\",\n                \"An explainable artificial intelligence system for small-unit tactical behavior\",\n                \"White, grey, black: Effects of xai augmentation on the confidence in ai-based decision support systems\",\n                \"Conceptual complexity and the bias/variance tradeoff\",\n                \"Increasing the level of automation in organisations: Some remarks on formalisation, contingency and the social construction of reality\",\n                \"Automation—its concept and a short history\",\n                \"Agency plus automation: Designing artificial intelligence into interactive systems\",\n                \"Automated decision making comes of age\",\n                \"Level of automation effects on performance, situation awareness and workload in a dynamic control task\",\n                \"Ironies of automation\",\n                \"Supporting decision making and action selection under time pressure and uncertainty: The case of in-flight icing\",\n                \"Automation bias: a systematic review of frequency, effect mediators, and mitigators\",\n                \"How much automation is too much? keeping the human relevant in knowledge work\",\n                \"Google effects on memory: Cognitive consequences of having information at our fingertips\",\n                \"Business-unit-level relationship between employee satisfaction, employee engagement, and business outcomes: a meta-analysis\",\n                \"The role of situation awareness in naturalistic decision making\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]39 based on the context and research area of Intelligent Decision Assistance Versus Automated Decision-Making: Enhancing Knowledge Work Through Explainable Artificial Intelligence. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}