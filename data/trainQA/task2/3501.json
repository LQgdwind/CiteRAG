{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Art Creation with Multi-Conditional StyleGANs\n\nPaper Abstract: \n\nPaper Section Text:\nGenerative adversarial networks (GANs)  are among the most well-known family of network architectures.\nThe objective of the architecture is to approximate a target distribution, which,\nin our setting, implies that the GAN seeks to produce images similar to those in the target distribution given by a set of training images. GANs achieve this through the interaction of two neural networks, the generator Gùê∫G and the discriminator Dùê∑D.\nThe generator produces fake data, while the discriminator attempts to tell apart such generated data from genuine original training images. During training, as the two networks are tightly coupled, they both improve over time until Gùê∫G is ideally able to approximate the target distribution to a degree that makes it hard for Dùê∑D to distinguish between genuine original data and fake generated data. Building on this idea, Radford¬†et al. combined convolutional networks with GANs to produce images of higher quality¬†.\nArjovsky¬†et al. proposed the Wasserstein distance, a new loss function under which the training of a Wasserstein GAN (WGAN) improves in stability and the generated images increase in quality¬†.\nKarras¬†et al. introduced progressive growing of GANs, a new training technique that allowed the generation of images of unprecedented resolution¬†.\nThey achieved this by progressively increasing the number of layers in the model to reduce the training time and to produce finer details. With StyleGAN, that is based on style transfer, Karras¬†et al. presented a new GAN architecture¬†\nthat improved the state-of-the-art image quality and provides control over both high-level attributes as well as finer details.\nKarras¬†et al. further improved the StyleGAN architecture with StyleGAN2, which removes characteristic artifacts from generated images¬†.\nLater on, they additionally introduced an adaptive augmentation algorithm (ADA) to StyleGAN2 in order to reduce the amount of data needed during training¬†.\nDue to its high image quality and the increasing research interest around it, we base our work on the StyleGAN2-ADA model. In recent years, different architectures have been proposed to incorporate conditions into the GAN architecture.\nThe first conditional GAN (cGAN) was proposed by Mirza and Osindero, where the condition information is one-hot (or otherwise) encoded into a vector¬†.\nThis encoding is concatenated with the other inputs before being fed into the generator and discriminator.\nAnother approach uses an auxiliary classification head in the discriminator¬†.\nThe cross-entropy between the predicted and actual conditions is added to the GAN loss formulation to guide the generator towards conditional generation. Current state-of-the-art architectures employ a projection-based discriminator that computes the dot product between the last discriminator layer and a learned embedding of the conditions¬†.\nAdditionally, the generator typically applies conditional normalization in each layer with condition-specific, learned scale and shift parameters¬†.\nThe conditional StyleGAN2 architecture also incorporates a projection-based discriminator and conditional normalization in the generator. Less attention has been given to multi-conditional GANs, where the conditioning is made up of multiple distinct categories of conditions that apply to each sample. Yildirim¬†et al. hand-crafted loss functions for different parts of the conditioning, such as shape, color, or texture on a fashion dataset¬†.\nPark¬†et al. proposed a GAN conditioned on a base image and a textual editing instruction to generate the corresponding edited image¬†. There is a long history of attempts to emulate human creativity by means of AI methods such as neural networks.\nSome studies focus on more practical aspects, whereas others consider philosophical questions such as whether machines are able to create artifacts that evoke human emotions in the same way as human-created art does.\nRecent developments include the work of Mohammed and Kiritchenko, who collected annotations, including perceived emotions and preference ratings, for over 4,000 artworks¬†.\nAchlioptas¬†et al. introduced a dataset with less annotation variety, but were able to gather perceived emotions for over 80,000 paintings¬†.\nIn addition, they solicited explanation utterances from the annotators about why they felt a certain emotion in response to an artwork, leading to around 455,000 annotations.\nElgammal¬†et al. presented a Creative Adversarial Network (CAN) architecture that is encouraged to produce more novel forms of artistic images by deviating from style norms rather than simply reproducing the target distribution¬†.\nLiu¬†et al. proposed a new method to generate art images from sketches given a specific art style¬†.\nAs certain paintings produced by GANs have been sold for high prices,111https://www.christies.com/features/a-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx McCormack¬†et al. raise important questions about issues such as authorship and copyrights of generated art¬†. GAN inversion is a rapidly growing branch of GAN research.\nThe objective of GAN inversion is to find a reverse mapping from a given genuine input image into the latent space of a trained GAN.\nXia¬†et al. provide a survey of prominent inversion methods and their applications¬†2.\nIn the context of StyleGAN, Abdal¬†et al. proposed Image2StyleGAN, which was one of the first feasible methods to invert an image into the extended latent space W+superscriptùëäW^{+} of StyleGAN¬†[ref]1.\nKarras¬†et al. instead opted to embed images into the smaller WùëäW space so as to improve the editing quality at the cost of reconstruction¬†.\nWith the latent code for an image, it is possible to navigate in the latent space and modify the produced image.\nApplications of such latent space navigation include image manipulation¬†[ref]1 [ref]2  8 7 0 3, image restoration¬†7 3 9 4, and image interpolation¬†[ref]2 1 3 1.\nWe would like to highlight the work of Zhu¬†et al., which introduced two new embedding spaces for GAN inversion, the PùëÉP space and the improved PNsubscriptùëÉùëÅP_{N} space¬†9.\nThe PùëÉP space eliminates the skew of marginal distributions in the more widely used WùëäW space. There are many evaluation techniques for GANs that attempt to assess the visual quality of generated images¬†.\nDue to the different focus of each metric, there is not just one accepted definition of visual quality.\nAlso, many of the metrics solely focus on unconditional generation and evaluate the separability between generated images and real images, as for example the approach from Zhou et al.¬†7.\nHowever, the Fr√©chet Inception Distance (FID) score by Heusel¬†et al.¬† has become commonly accepted and computes the distance between two distributions.\nVariations of the FID such as the Fr√©chet Joint Distance FJD¬† and the Intra-Fr√©chet Inception Distance (I-FID)¬† additionally enable an assessment of whether the conditioning of a GAN was successful.\nThis allows us to also assess desirable properties such as conditional consistency and intra-condition diversity of our GAN models¬†.\nAnother frequently used metric to benchmark GANs is the Inception Score (IS)¬†6, which primarily considers the diversity of samples.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: the context of StyleGAN, Abdal¬†et al. proposed Image2StyleGAN, which was one of the first feasible methods to invert an image into the extended latent space W+superscriptùëäW^{+} of StyleGAN¬†[ref]1.\nKarras¬†et al. instead opted to embed images into the smaller WùëäW space so as to improve the editing quality at the cost of reconstruction¬†.\nWith the latent code for an image, it is possible\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"Image2StyleGAN++: How to Edit the Embedded Images?\",\n                \"Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?\",\n                \"StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows\",\n                \"ArtEmis: Affective Language for Visual Art\",\n                \"Wasserstein t-SNE\",\n                \"Towards automated artificial evolution for computer-generated images\",\n                \"Demystifying MMD GANs\",\n                \"Evaluating options by combined qualitative and quantitative methods\",\n                \"Large Scale GAN Training for High Fidelity Natural Image Synthesis\",\n                \"The Cross-Depiction Problem: Computer Vision Algorithms for Recognising Objects in Artwork and in Photographs\",\n                \"Modulating early visual processing by language\",\n                \"On the Evaluation of Conditional GANs\",\n                \"Improbable creativity\",\n                \"The Fr√©chet distance between multivariate normal distributions\",\n                \"CAN: Creative adversarial networks, generating ‚Äùart‚Äù by learning about styles and deviating from style norms\",\n                \"Generative Adversarial Networks\",\n                \"GANs trained by a two time-scale update rule converge to a Nash equilibrium\",\n                \"TinyBERT: Distilling BERT for natural language understanding\",\n                \"Progressive Growing of GANs for Improved Quality, Stability, and Variation\",\n                \"Training Generative Adversarial Networks with Limited Data\",\n                \"A Style-Based Generator Architecture for Generative Adversarial Networks\",\n                \"Analyzing and Improving the Image Quality of StyleGAN\",\n                \"Sketch-to-Art: Synthesizing Stylized Art Images From Sketches\",\n                \"Autonomy, Authenticity, Authorship and Intention in computer generated art\",\n                \"Conditional Generative Adversarial Nets\",\n                \"cGANs with Projection Discriminator\",\n                \"An annotated dataset of emotions evoked by art\",\n                \"Face Identity Disentanglement via Latent Space Mapping\",\n                \"Conditional Image Synthesis with Auxiliary Classifier GANs\",\n                \"Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation\",\n                \"MC-GAN: Multi-conditional Generative Adversarial Network for Image Synthesis\",\n                \"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\",\n                \"Improved Techniques for Training GANs\",\n                \"Interpreting the Latent Space of GANs for Semantic Face Editing\",\n                \"Rethinking the Inception architecture for computer vision\",\n                \"Deep Image Prior\",\n                \"Unsupervised Discovery of Interpretable Directions in the GAN Latent Space\",\n                \"Controllable Continuous Gaze Redirection\",\n                \"GAN inversion: A survey\",\n                \"Generative Hierarchical Features from Synthesizing Images\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of Art Creation with Multi-Conditional StyleGANs. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}