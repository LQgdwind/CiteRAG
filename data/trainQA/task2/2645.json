{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: 3D Human Texture Estimation from a Single Image with Transformers\n\nPaper Abstract: \n\nPaper Section Text:\nQuery. As introduced in SectionÂ 1, the Query of our model represents a color-encoding of the output UV space, and each pixel in the Query map should characterize a vertex on the 3D human mesh.\nTo this end, we use the 3D coordinates of a standard human body mesh from the SMPL modelÂ [ref]31 as the color encoding of each vertex. The color values of each pixel in the Query can be obtained by first mapping the 3D coordinates to the UV space and then interpolating the coordinates with KD-TreeÂ [ref]33. We pre-compute the Query map before network training, and the same color encoding of the UV space is used for all input images. Key. Recall that the Key is used to correlate with the Query elements to obtain the attention map for the input (FigureÂ 1), which is crucial for connecting the image space and the UV space.\nMotivated by recent studiesÂ  , we use 2D part-segmentation as the Key to learn a mapping from the input image to the output UV map.\nOur 2D part-segmentation is obtained with an off-the-shelf modelÂ .\nAs shown in FigureÂ 2, we also include the input image as additional channels of the Key to provide contextual information. Value. The Value represents the source information indexed by the Key and is aggregated into the UV space to generate the output using the attention map.\nThere are two possible options for the Value depending on the form of the Transformer output: 1) using the RGB input image as the Value when the Transformer directly outputs the RGB UV map; 2) using the flow field of the image, i.e., 2D coordinates for each pixel, when the model first produces the texture flow and then generates the UV map by sampling the input image with the predicted flow.\nAs will be introduced later in this section, our Transformer predicts both RGB values and texture flow, and thus, we concatenate both the RGB image and the flow field as the Value of our model (FigureÂ 2). The Query, Key, and Value cannot be directly used by the Transformer with their raw forms;\nthis is especially the case for the Query and Key which will be compared in the attention module and thereby should have the same feature dimension.\nThus, we feed them in CNNs to transform them into feature space as shown in FigureÂ 2.\nThen the resulting features Qâˆˆâ„vâ€‹uÃ—dğ‘„superscriptâ„ğ‘£ğ‘¢ğ‘‘Q\\in\\mathbb{R}^{vu\\times d}, Kâˆˆâ„hâ€‹wÃ—dğ¾superscriptâ„â„ğ‘¤ğ‘‘K\\in\\mathbb{R}^{hw\\times d}, and Vâˆˆâ„hâ€‹wÃ—cğ‘‰superscriptâ„â„ğ‘¤ğ‘V\\in\\mathbb{R}^{hw\\times c} are sent into the Transformer unit, which produces the Output features Oâˆˆâ„vâ€‹uÃ—cğ‘‚superscriptâ„ğ‘£ğ‘¢ğ‘O\\in\\mathbb{R}^{vu\\times c}.\nHere, v,uğ‘£ğ‘¢v,u are the height and width of the output UV map, h,wâ„ğ‘¤h,w are the height and width of the input image, while dğ‘‘d, cğ‘c are the feature dimensions of Kğ¾K and Vğ‘‰V, respectively.\nNote that the CNN of Qğ‘„Q is used to find a better representation of the fixed Query map and only needed in the training phase. During deployment, we can drop the Query CNN by pre-computing the feature encodings. The Transformer unit in FigureÂ 2 is the central part of the proposed network, which effectively distributes the image features into suitable locations of the UV map and enables global information interchange between the input space and the output space.\nSpecifically, it can be written as: where fAttnsubscriptğ‘“Attnf_{\\text{Attn}} is the multi-head attention module fromÂ [ref]43.\nfres-MLPsubscriptğ‘“res-MLPf_{\\text{res-MLP}} is a two-layer MLP with a residual connection between its input and output.\nEQsubscriptğ¸ğ‘„E_{Q} and EKsubscriptğ¸ğ¾E_{K} are sinusoidal positional encodingsÂ [ref]43 for the Query and Key features, respectively. Low-rank attention.\nAs introduced inÂ [ref]43, the multi-head attention fAttnsubscriptğ‘“Attnf_{\\text{Attn}} is based on a single-head attention module: where Q~âˆˆâ„v~â€‹u~Ã—d~~ğ‘„superscriptâ„~ğ‘£~ğ‘¢~ğ‘‘\\tilde{Q}\\in\\mathbb{R}^{\\tilde{v}\\tilde{u}\\times\\tilde{d}}, K~âˆˆâ„h~â€‹w~Ã—d~~ğ¾superscriptâ„~â„~ğ‘¤~ğ‘‘\\tilde{K}\\in\\mathbb{R}^{\\tilde{h}\\tilde{w}\\times\\tilde{d}}, and V~âˆˆâ„h~â€‹w~Ã—c~~ğ‘‰superscriptâ„~â„~ğ‘¤~ğ‘\\tilde{V}\\in\\mathbb{R}^{\\tilde{h}\\tilde{w}\\times\\tilde{c}}.\nThe denominator Î±ğ›¼\\alpha is incorporated to avoid large values.\nFor clarity, we do not use the symbols Qğ‘„Q, Kğ¾K, and Vğ‘‰V here as these features need to be first processed by linear projection layers in the multi-head attention fAttnsubscriptğ‘“Attnf_{\\text{Attn}}Â [ref]43. One important problem of the normal attention formulation is the high memory complexity (ğ’ªâ€‹(v~â€‹u~â€‹h~â€‹w~)ğ’ª~ğ‘£~ğ‘¢~â„~ğ‘¤\\mathcal{O}(\\tilde{v}\\tilde{u}\\tilde{h}\\tilde{w}) for Eq.Â 2), which makes the training infeasible for common GPUs.\nTo remedy this issue, we propose a low-rank attention (LoRA) module inspired byÂ : where the softmax function is removed, and the attention is approximated by pure matrix multiplication.\nThis strategy allows more efficient computations by manipulating the order of the matrix multiplicationsÂ  (i.e., computing K~âŠ¤â€‹V~superscript~ğ¾top~ğ‘‰\\tilde{K}^{\\top}\\tilde{V} first).\nThe memory footprint can be significantly reduced to ğ’ªâ€‹(maxâ¡(v~â€‹u~,h~â€‹w~)â‹…maxâ¡(d~,c~))ğ’ªâ‹…~ğ‘£~ğ‘¢~â„~ğ‘¤~ğ‘‘~ğ‘\\mathcal{O}(\\max(\\tilde{v}\\tilde{u},\\tilde{h}\\tilde{w})\\cdot\\max(\\tilde{d},\\tilde{c})), which makes the training more feasible.\nNote that  uses this low-rank strategy for matrix factorization while we apply it to multi-head attention for efficient Transformers. Multi-scale feature fusion.\nInstead of applying the Transformer unit at one single scale, we compute the Output features with a three-level feature pyramid.\nFor the first two levels where the features have large spatial sizes, we use the proposed LoRA (Eq.Â 3) to reduce memory cost.\nFor the small features on the third level, we simply use the softmax attention module (Eq.Â 2).\nThen the multi-scale Output features Oi,i=1,2,3formulae-sequencesubscriptğ‘‚ğ‘–ğ‘–123O_{i},i=1,2,3 can be easily fused within a CNN as shown in FigureÂ 2.\nThis multi-scale approach can help the proposed Transformer better exploit contextual information and predict higher-quality human textures for invisible regions (FigureÂ 9). Mask-fusion of RGB and texture flow.\nThere are two possible options for the output of the network: 1) directly generating the RGB textures TRGBsubscriptğ‘‡RGBT_{\\text{RGB}}Â  , or 2) predicting the texture flow Fğ¹F that can be used to generate the final textures by sampling from the input image Iğ¼IÂ   .\nAs shown in FigureÂ 3, both these two strategies have their advantages and limitations:\nDirectly synthesizing TRGBsubscriptğ‘‡RGBT_{\\text{RGB}} can well reconstruct visually-pleasant 3D human textures, which however often leads to loss of fine details (FigureÂ 3(b)).\nOn the other hand, the texture flow is able to preserve fine details in the input image, but the results could suffer from severe artifacts (FigureÂ 3(c)). To solve this problem, we propose a mask-fusion method to combine the advantages of both RGB values and texture flow.\nSpecifically, our Transformer generates three outputs: the RGB UV map TRGBsubscriptğ‘‡RGBT_{\\text{RGB}}, the texture flow Fğ¹F, and a fusion mask Mğ‘€M.\nThen the mask-fusion process can be written as: where fsamplesubscriptğ‘“samplef_{\\text{sample}} is the bilinear sampling function that is used to sample textures from Iğ¼I with Fğ¹F, and âŠ™direct-product\\odot denotes element-wise multiplication.\nThe final texture Tğ‘‡T is computed by fusing TRGBsubscriptğ‘‡RGBT_{\\text{RGB}} and fsampleâ€‹(F,I)subscriptğ‘“sampleğ¹ğ¼f_{\\text{sample}}(F,I) via weighted sum.\nAs shown in FigureÂ 3(f), the mask-fusion method is able to significantly reduce visual artifacts while preserving fine details.\nIn addition, by observing the learned fusion mask (FigureÂ 3(d)), we can find that the network tends to use sampled textures fsampleâ€‹(F,I)subscriptğ‘“sampleğ¹ğ¼f_{\\text{sample}}(F,I) for visible regions such as the human chest, and use synthesized textures TRGBsubscriptğ‘‡RGBT_{\\text{RGB}} for invisible regions such as the human back as well as other hard regions with sophisticated structures such as the face and hands. Relationship with conventional Transformers.\nTypically, existing worksÂ [ref]43    construct the Transformer by stacking multiple Transformer units, where the Output features of the current unit become the Query features for the next one.\nThus, the data flow in the Transformer needs to be able to serve as two different roles (i.e., Query and Output) simultaneously.\nThis is a reasonable choice when either 1) the Query and Output are in the same space, for example, in natural language processingÂ [ref]43 the Query is the shifted Output of the previous time step, or 2) the physical meaning of the Query cannot be concretely defined, for instance, in object detectionÂ  the Query is randomly-initialized vectors that are also optimized during training. However, in this work we have a clear definition of the Query which is a color encoding map obtained from 3D coordinates of a standard 3D body mesh, and each element in the Query corresponds to a physical vertex;\nand this Query map is in a space different from the Output (i.e., the RGB values and texture flow).\nTherefore, in our network (FigureÂ 2), we avoid stacking multiple attention modules and use the Transformer units more efficiently in a multi-scale manner.\nThrough this way, the features of our Texformer do not need to serve as two different roles simultaneously anymore, which effectively alleviates the difficulties of training.\nAs shown in SectionÂ 4.1, the conventional Transformer cannot provide as high-quality results as the proposed model, which demonstrates the effectiveness of our design.\n\nReference markers found: [31]\n\nContext windows for each [ref]:\n[ref]31: of the output UV space, and each pixel in the Query map should characterize a vertex on the 3D human mesh.\nTo this end, we use the 3D coordinates of a standard human body mesh from the SMPL modelÂ [ref]31 as the color encoding of each vertex. The color values of each pixel in the Query can be obtained by first mapping the 3D coordinates to the UV space and then interpolating the coordinates\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 31,\n            \"titles\": [\n                \"Pose Guided Person Image Generation\",\n                \"Learning to Reconstruct People in Clothing from a Single RGB Camera\",\n                \"Detailed Human Avatars from Monocular Video\",\n                \"Video Based Reconstruction of 3D People Models\",\n                \"Tex2Shape: Detailed Full Human Body Geometry From a Single Image\",\n                \"Layer normalization\",\n                \"Multi-garment net: Learning to dress 3D people from images\",\n                \"Fast approximate energy minimization via graph cuts\",\n                \"End-to-end object detection with transformers\",\n                \"Pre-trained image processing transformer\",\n                \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n                \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n                \"Image style transfer using convolutional neural networks\",\n                \"Shape and viewpoint without keypoints\",\n                \"A Survey on Visual Transformer\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Eanet: Enhancing alignment for cross-domain person re-identification\",\n                \"Hand-transformer: Non-autoregressive structured modeling for 3D hand pose estimation\",\n                \"Deep volumetric video from very sparse multi-view performance capture\",\n                \"ARCH: Animatable Reconstruction of Clothed Humans\",\n                \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\n                \"Image-to-Image Translation with Conditional Adversarial Networks\",\n                \"End-to-end Recovery of Human Shape and Pose\",\n                \"Learning Category-Specific Mesh Reconstruction from Image Collections\",\n                \"Learning 3D Human Dynamics from Video\",\n                \"Neural 3D Mesh Renderer\",\n                \"VIBE: Video Inference for Human Body Pose and Shape Estimation\",\n                \"Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop\",\n                \"360-Degree Textures of People in Clothing from a Single Image\",\n                \"Self-supervised Single-view 3D Reconstruction via Semantic Consistency\",\n                \"Robust 3D Self-portraits in Seconds\",\n                \"Smpl: A skinned multi-person linear model\",\n                \"Analysis of approximate nearest neighbor searching with clustered point sets\",\n                \"Learning to Transfer Texture from Clothing Images to 3D Humans\",\n                \"Siclope: Silhouette-based clothed people\",\n                \"Dense Pose Transfer\",\n                \"3DPeople: Modeling the Geometry of Dressed Humans\",\n                \"U-Net: Convolutional Networks for Biomedical Image Segmentation\",\n                \"Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization\",\n                \"Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3D human digitization\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]31 based on the context and research area of 3D Human Texture Estimation from a Single Image with Transformers. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}