{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Maximally Extendable Sheaf Codes\n\nPaper Abstract: \n\nPaper Section Text:\nIf we equip the cochain complex 𝐂∙=𝐂∙​(X;ℱ)superscript𝐂∙superscript𝐂∙𝑋ℱ\\mathbf{C}^{\\bullet}=\\mathbf{C}^{\\bullet}(X;\\mathcal{F}) with a norm, we can naturally transfer all the standard notions, such as coboundary expansion and local minimality, from theory of high-dimensional expanders (HDXs)  , originally developed for cohomology groups over 𝔽2subscript𝔽2\\mathbb{F}_{2} to the much more general realm of cohomology groups with local coefficients in the sheaf ℱℱ\\mathcal{F}. Here we use the block (Hamming) norm333333This norm is also sometimes called the support norm 8. from [ref]2, naturally generalizing the standard Hamming norm, defined for c∈𝐂∙𝑐superscript𝐂∙c\\in\\mathbf{C}^{\\bullet} as |c|X=|{σ∈X∣cσ≠0}|subscript𝑐𝑋conditional-set𝜎𝑋subscript𝑐𝜎0\\left\\lvert c\\right\\rvert_{X}=\\left\\lvert\\{\\sigma\\in X\\mid c_{\\sigma}\\neq 0\\}\\right\\rvert, and also consider its normalized version343434This normalization is often used for simplicial complexes . as ∥c∥X=∑cσ≠0wσsubscriptdelimited-∥∥𝑐𝑋subscriptsubscript𝑐𝜎0subscript𝑤𝜎\\left\\lVert c\\right\\rVert_{X}=\\sum_{c_{\\sigma}\\neq 0}w_{\\sigma}, where wσ=|Xσ|/∑σ∈X​(i)|Xσ|subscript𝑤𝜎subscript𝑋𝜎subscript𝜎𝑋𝑖subscript𝑋𝜎w_{\\sigma}=\\left\\lvert X_{\\sigma}\\right\\rvert/\\sum_{\\sigma\\in X(i)}\\left\\lvert X_{\\sigma}\\right\\rvert, dimσ=idimension𝜎𝑖\\dim\\sigma=i. In fact, in all examples we are interested, the poset X𝑋X is sufficiently regular, and wσ=1/|X​(i)|subscript𝑤𝜎1𝑋𝑖w_{\\sigma}=1/\\left\\lvert X(i)\\right\\rvert for σ∈X​(i)𝜎𝑋𝑖\\sigma\\in X(i). Both these norms can be naturally extended to subspaces S⩽𝐂∙𝑆superscript𝐂∙S\\leqslant\\mathbf{C}^{\\bullet} as |S|X=minσ∈S|σ|X\\left\\lvert S\\right\\rvert_{X}=\\min_{\\sigma\\in S}\\left\\lvert\\sigma\\right\\rvert_{X} and ∥S∥X=minσ∈S∥σ∥X\\left\\lVert S\\right\\rVert_{X}=\\min_{\\sigma\\in S}\\left\\lVert\\sigma\\right\\rVert_{X}. Now, we are ready to define the i𝑖i-th coboundary expansion ηi​(ℱ)=ηi​(X;ℱ)superscript𝜂𝑖ℱsuperscript𝜂𝑖𝑋ℱ\\eta^{i}(\\mathcal{F})=\\eta^{i}(X;\\mathcal{F}) of ℱℱ\\mathcal{F} as and also its normalized version η~i​(ℱ)=η~i​(X;ℱ)superscript~𝜂𝑖ℱsuperscript~𝜂𝑖𝑋ℱ\\tilde{\\eta}^{i}(\\mathcal{F})=\\tilde{\\eta}^{i}(X;\\mathcal{F}), where instead of |⋅|Xsubscript⋅𝑋\\left\\lvert\\cdot\\right\\rvert_{X} we use ∥⋅∥Xsubscriptdelimited-∥∥⋅𝑋\\left\\lVert\\cdot\\right\\rVert_{X}. Here we assume that the minimum over the empty set is equal to ∞\\infty, and also put η​(ℱ)=mini∈ℤ⁡ηi​(ℱ)𝜂ℱsubscript𝑖ℤsuperscript𝜂𝑖ℱ\\eta(\\mathcal{F})=\\min_{i\\in\\mathbb{Z}}\\eta^{i}(\\mathcal{F}) and η~​(ℱ)=mini∈ℤ⁡η~i​(ℱ)~𝜂ℱsubscript𝑖ℤsuperscript~𝜂𝑖ℱ\\tilde{\\eta}(\\mathcal{F})=\\min_{i\\in\\mathbb{Z}}\\tilde{\\eta}^{i}(\\mathcal{F}). Note that it is also possible to define the small set versions [ref]7 of all these quantities: In the special case when ℱℱ\\mathcal{F} is the constant sheaf 𝔽2¯¯subscript𝔽2\\underline{\\mathbb{F}_{2}} on a simplicial complex X𝑋X, we get exactly the coboundary expansion from the theory of HDXs, which in turn generalizes the Cheeger constant h​(X)ℎ𝑋h(X) (e.g., see  8) of a graph X𝑋X since we have h​(X)=η0​(X;𝔽2¯)ℎ𝑋superscript𝜂0𝑋¯subscript𝔽2h(X)=\\eta^{0}(X;\\underline{\\mathbb{F}_{2}}) when we view graphs as simplicial complexes353535This assumption is very important here since simplicial complexes also have the empty face ∅\\varnothing with dim∅=−1dimension1\\dim\\varnothing=-1 (e.g., see ).. At the same time, when ℱℱ\\mathcal{F} is a default sheaf code of length n𝑛n, then η​(ℱ)𝜂ℱ\\eta(\\mathcal{F}) and η~​(ℱ)~𝜂ℱ\\tilde{\\eta}(\\mathcal{F}) coincide respectively with the minimum distance d​(ℱ)𝑑ℱd(\\mathcal{F}) and its normalized version 1n​d​(ℱ)1𝑛𝑑ℱ\\frac{1}{n}d(\\mathcal{F}). In the case of tensor product code 𝒜⊗ℬtensor-product𝒜ℬ\\mathcal{A}\\otimes\\mathcal{B}, we have that η​(𝒜⊗ℬ)𝜂tensor-product𝒜ℬ\\eta(\\mathcal{A}\\otimes\\mathcal{B}) is, up to normalization factors, the agreement testability , which is in turn related with the robust testability  of 𝒜⊗ℬtensor-product𝒜ℬ\\mathcal{A}\\otimes\\mathcal{B} (e.g., see  ). The quantity η​(𝒜⊗ℬ)𝜂tensor-product𝒜ℬ\\eta(\\mathcal{A}\\otimes\\mathcal{B}) was an important ingredient in the first constructions of good LTCs and qLDPC codes  [ref]2, where it played a role similar to the minimal distance in the Sipser-Spielman codes.\nNote, that in [ref]2 it was called product expansion, and also considered in the case when we have some puncturing in the product code 𝒜⊗ℬtensor-product𝒜ℬ\\mathcal{A}\\otimes\\mathcal{B}. Later Leverrier and Zémor found an elegant equivalent reformulation of the product expansion, and finally in   it was directly connected to the (co)boundary expansion, and also generalized to D>2𝐷2D>2 dimensions. It is important to note that when D>2𝐷2D>2 it is a strictly stronger property than both the agreement and the robust testability 5. Finally, in 8 the authors studied coboundary expansion for arbitrary sheaves of vector spaces on simplicial complexes, and also used it to construct LTCs and qLDPC codes. Let us also say that a sheaf code ℱℱ\\mathcal{F} is an η𝜂\\eta-expander if η​(ℱ)⩾η𝜂ℱ𝜂\\eta(\\mathcal{F})\\geqslant\\eta, and a two-way η𝜂\\eta-expander if both ℱℱ\\mathcal{F} and ℱTsuperscriptℱT\\mathcal{F}^{\\mathrm{T}} are η𝜂\\eta-expanders, where ℱTsuperscriptℱT\\mathcal{F}^{\\mathrm{T}} is the dual sheaf code. Note that if ℱ=⨂i∈[D]𝒞iℱsubscripttensor-product𝑖delimited-[]𝐷subscript𝒞𝑖\\mathcal{F}=\\bigotimes_{i\\in[D]}\\mathcal{C}_{i}, then two-way η𝜂\\eta-expansion requires that both ⨂i∈[D]𝒞isubscripttensor-product𝑖delimited-[]𝐷subscript𝒞𝑖\\bigotimes_{i\\in[D]}\\mathcal{C}_{i} and ⨂i∈[D]𝒞i⟂subscripttensor-product𝑖delimited-[]𝐷superscriptsubscript𝒞𝑖perpendicular-to\\bigotimes_{i\\in[D]}\\mathcal{C}_{i}^{\\perp} are η𝜂\\eta-expanders, and when D=1𝐷1D=1 it requires that the minimal distance of a code and its dual is at least η𝜂\\eta. The case D=1𝐷1D=1 was essential for a construction of Sipser-Spielman codes with good bidirectional small-set expansion properties in , 7. At the same time, the case D=2𝐷2D=2 was essential for expander LP codes [ref]2 and their simplification called quantum Tanner codes  to get the linear minimum distance, and also good bidirectional small-set expansion [ref]7, which in turn implies the NLTS theorem 1.\n\nReference markers found: [2]\n\nContext windows for each [ref]:\n[ref]2: much more general realm of cohomology groups with local coefficients in the sheaf ℱℱ\\mathcal{F}. Here we use the block (Hamming) norm333333This norm is also sometimes called the support norm 8. from [ref]2, naturally generalizing the standard Hamming norm, defined for c∈𝐂∙𝑐superscript𝐂∙c\\in\\mathbf{C}^{\\bullet} as |c|X=|{σ∈X∣cσ≠0}|subscript𝑐𝑋conditional-set𝜎𝑋subscript𝑐𝜎0\\left\\lvert\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 2,\n            \"titles\": [\n                \"Quantum Tanner codes\",\n                \"Locally Testable Codes with constant rate, distance, and locality\",\n                \"Asymptotically Good Quantum and Locally Testable Classical LDPC Codes\",\n                \"Good Quantum LDPC Codes with Linear Time Decoders\",\n                \"Two-sided Robustly Testable Codes\",\n                \"NLTS Hamiltonians and Strongly-Explicit SoS Lower Bounds from Low-Rate Quantum LDPC Codes\",\n                \"Explicit Lower Bounds Against ΩΩ\\\\Omega(n)-Rounds of Sum-of-Squares\",\n                \"Single-shot decoding of good quantum LDPC codes\",\n                \"An efficient decoder for a linear distance quantum LDPC code\",\n                \"Efficient decoding up to a constant fraction of the code length for asymptotically good quantum codes\",\n                \"NLTS Hamiltonians from good quantum codes\",\n                \"Layer Codes\",\n                \"Local Quantum Codes from Subdivided Manifolds\",\n                \"Geometrically Local Quantum and Classical Codes from Subdivision\",\n                \"High Dimensional Expanders Imply Agreement Expanders\",\n                \"Graph codes and local systems\",\n                \"Balanced product quantum codes\",\n                \"On Good 2-Query Locally Testable Codes from Sheaves on High Dimensional Expanders\",\n                \"New Codes on High Dimensional Expanders\",\n                \"Expansion of higher-dimensional cubical complexes with application to quantum locally testable codes\",\n                \"Topological quantum memory\",\n                \"Quantum LDPC Codes with Almost Linear Minimum Distance\",\n                \"Fiber bundle codes: breaking the N1/2​polylog⁡(N)superscript𝑁12polylog𝑁N^{1/2}\\\\operatorname{polylog}(N) barrier for quantum LDPC codes\",\n                \"On expander codes\",\n                \"On the Maximally Recoverable Property for Multi-Protection Group Codes\",\n                \"Explicit Maximally Recoverable Codes With Locality\",\n                \"A Family of Optimal Locally Recoverable Codes\",\n                \"Quantum Locally Recoverable Codes\",\n                \"Maximally extendable product codes are good coboundary expanders\",\n                \"Quantum locally testable codes\",\n                \"Local Hamiltonians Whose Ground States Are Hard to Approximate\",\n                \"Quantum Locally Testable Code with Constant Soundness\",\n                \"Infinite series of quaternionic 1-vertex cube complexes, the doubling construction, and explicit cubical Ramanujan complexes\",\n                \"Good quantum error-correcting codes exist\",\n                \"Error correcting codes in quantum theory\",\n                \"The category of linear codes\",\n                \"Combinatorial problems of elementary Abelian groups\",\n                \"Sheaf codes\",\n                \"A recursive approach to low complexity codes\",\n                \"Expander codes\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]2 based on the context and research area of Maximally Extendable Sheaf Codes. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}