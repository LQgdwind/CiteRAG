{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Provably Efficient Learning in Partially Observable Contextual Bandit\n\nPaper Abstract: \n\nPaper Section Text:\nWe first consider an off-policy learning problem between an contextual bandit expert and a MAB agent.\nContextual bandits   [ref]4 is a variation of MAB where the expert can observe extra contextual information W𝑊W associated with the reward signal Y𝑌Y.\nThe expert follows certain policy and summarizes its experiences as the joint distribution F^​(a,y,w)^𝐹𝑎𝑦𝑤\\hat{F}(a,y,w).\nHowever, there exists a latent confounder U𝑈U that affects the causal effects between actions and rewards, making the observed contextual bandit model incomplete.\nThe agent, who is observing the expert interacting with the environment,\nwants to be more efficient and reuse the observed F^​(a,y,w)^𝐹𝑎𝑦𝑤\\hat{F}(a,y,w) with the help of extra prior knowledge about U𝑈U, i.e., F^​(u)^𝐹𝑢\\hat{F}(u),\nto find the optimal policy faster. Causal bounds.\nThis transfer scenario is depicted in Figure 1,\nwhere the actions A𝐴A, outcomes Y𝑌Y, context W𝑊W and U𝑈U,\nand causal structures used by the expert and the agent are identical.\nIn the first task, the MAB agent is unable to observe contexts,\nbut we will also examine the case where the agent can partially observe contexts in the task 2 and 3.\nThe missing contexts may be due to privacy protection or legal restrictions.\nAs the new agent cannot observe W𝑊W or U𝑈U, its objective is to minimize regret as defined in equation (1). One can find the optimal action d​o​(A=a∗)𝑑𝑜𝐴superscript𝑎do(A=a^{*}) by evaluating 𝔼​[Y|d​o​(a)]𝔼delimited-[]conditional𝑌𝑑𝑜𝑎\\mathbb{E}[Y|do(a)].\nTherefore, the off-policy learning problem is equivalent to identifying the causal effect 𝔼​[Y|d​o​(a)]𝔼delimited-[]conditional𝑌𝑑𝑜𝑎\\mathbb{E}[Y|do(a)] given the observational distribution F​(a,y,w)𝐹𝑎𝑦𝑤{F}(a,y,w) and prior knowledge F​(u)𝐹𝑢F(u).\nAlthough U𝑈U is unobserved during online learning, its distribution can be usually known regardless of the model itself (e.g., U𝑈U stands for gender, gene\ntype, blood type, or age). Alternatively, if costs permit, one can estimate F​(u)𝐹𝑢{F}(u) by re-testing within a small sampled sub-population.\nHowever, even though W∪U𝑊𝑈W\\cup U is assumed to be sufficient,\nthe causal effects of the action A𝐴A on the reward Y𝑌Y is non-identifiable from incomplete experiences F​(a,y,w)𝐹𝑎𝑦𝑤{F}(a,y,w) and F​(u)𝐹𝑢{F}(u),\nbecause the back-door path A←U→Y←𝐴𝑈→𝑌A\\leftarrow U\\rightarrow Y is still unblocked.\nThis means that the causal effects of A𝐴A on Y𝑌Y cannot be estimated without bias, regardless of sample size. However, we can tight obtain bounds on the causal effects via an optimization problem when given a prior distribution F​(u)𝐹𝑢F(u) and F​(a,y,w)𝐹𝑎𝑦𝑤F(a,y,w).\nThe optimization problem is as follows: Here, the sup/inf is taken over all causal models ℳℳ\\mathcal{M}, and Fℳsubscript𝐹ℳF_{\\mathcal{M}} represents the distributions in the causal model ℳℳ\\mathcal{M}.\nDenote the solutions to (4) as h​(a)ℎ𝑎h(a) and l​(a)𝑙𝑎l(a), respectively.\nSince the optimization is performed over all compatible causal models, the resulting bounds are guaranteed to be tight.\nIt is worth noting that the sufficient and necessary condition for causal identification is that h​(a)ℎ𝑎h(a) is equal to l​(a)𝑙𝑎l(a).\nWe provide a formal statement of this result in the appendix.\nThis idea is similar to that proposed in other works, such as  [ref]42 . In Theorem 3.1, we provide a specific version of the optimization problem (4) to obtain bounds on the causal effects of actions on rewards.\nWe consider the estimation error ϵitalic-ϵ\\epsilon and distinguish between known and unknown variables by using F^^𝐹\\hat{F} to represent the observed distributions and F𝐹F to represent the unknown variables. Given a causal diagram 𝒢𝒢\\mathcal{G} and a distribution compatible with 𝒢𝒢\\mathcal{G}, let {W,U}𝑊𝑈\\{W,U\\} be a set of variables satisfying\nthe back-door criterion in 𝒢𝒢\\mathcal{G} relative to an ordered pair (A,Y)𝐴𝑌(A,Y), where {W,U}𝑊𝑈\\{W,U\\} is partially observable, i.e., only\nprobabilities F^​(a,y,w)^𝐹𝑎𝑦𝑤\\hat{F}(a,y,w) and F^​(u)^𝐹𝑢\\hat{F}(u) with the maximum estimation error ϵitalic-ϵ\\epsilon,\nthe causal effects of A𝐴A on Y𝑌Y are then bounded as follows: where l​(a0)𝑙subscript𝑎0l(a_{0}) and h​(a0)ℎsubscript𝑎0h(a_{0}) are solutions to the following functional optimization problem for any given a0subscript𝑎0a_{0} Here the inf/sup is taken with respect to all unknown cumulative distribution functions F​(a,y,w,u)𝐹𝑎𝑦𝑤𝑢F(a,y,w,u), F​(a,w,u)𝐹𝑎𝑤𝑢F(a,w,u), F​(y|a,w,u)𝐹conditional𝑦𝑎𝑤𝑢F(y|a,w,u), F​(w,u)𝐹𝑤𝑢F(w,u), F​(a,y,w)𝐹𝑎𝑦𝑤F(a,y,w) and F​(u)𝐹𝑢F(u). The functional optimization problem presented in Theorem 3.1 provides tight lower and upper bounds on the average causal effect l​(a0)𝑙subscript𝑎0l(a_{0}) and h​(a0)ℎsubscript𝑎0h(a_{0}), respectively,\nfor any given a0∈𝒜subscript𝑎0𝒜a_{0}\\in\\mathcal{A}.\nOur results generalize those of 9 to random variables with estimation error,\nwhile 9 only considers the case of discrete random variables without estimation error.\nMoreover, this optimization problem can be applied to both parametric and non-parametric cases. It is essential to consider constraints for all a∈𝒜𝑎𝒜a\\in\\mathcal{A}, rather than just the given a0subscript𝑎0a_{0} of interest,\nas only considering constraints for a0subscript𝑎0a_{0} would result in non-tight bounds.\nAs an example, the optimal solutions generated by the non-linear optimization problem in 9 may not belong to any valid causal models,\nleading to non-tight bounds.\nTherefore, one need to consider all values of the given random variable X𝑋X to obtain tight bounds,\neven if only causal bounds under a specific intervention d​o​(X=x)𝑑𝑜𝑋𝑥do(X=x) are required. The original optimization problem is challenging to solve due to its inefficiency and intractability.\nFirstly, the functional constraints for unknown distributions make the optimization problem intractable.\nSecondly, solving a non-linear optimization problem can often get trapped in local optima.\nIn some cases, narrow causal bounds that do not include the true causal effects may lead to critical false decisions.\nTo overcome these challenges, it is necessary to approximate the problem to make it easier to solve and to obtain global solutions with high probability. The optimization problem can be naturally simplified by probability mass functions for discrete random variables with bounded support.\nTo extend this approach to general random variables, we only consider random variables with bounded support\nand discretize 𝒜𝒜\\mathcal{A}, 𝒲𝒲\\mathcal{W}, 𝒰𝒰\\mathcal{U}, and 𝒴𝒴\\mathcal{Y} into n𝒜subscript𝑛𝒜n_{\\mathcal{A}}, n𝒲subscript𝑛𝒲n_{\\mathcal{W}}, n𝒰subscript𝑛𝒰n_{\\mathcal{U}}, and n𝒴subscript𝑛𝒴n_{\\mathcal{Y}} disjoint blocks, respectively.\nFor random variables that take in finite values,\nthey have natural discretization.\nWe first define and then integrate both sides of the first equality over 𝒜isubscript𝒜𝑖\\mathcal{A}_{i}, 𝒴jsubscript𝒴𝑗\\mathcal{Y}_{j}, and 𝒲ksubscript𝒲𝑘\\mathcal{W}_{k} to obtain This allows us to reformulate the first equality in terms of integration instead of the original pointwise equality. As the discretization becomes finer,\ni.e., d​i​a​m​(𝒜i)𝑑𝑖𝑎𝑚subscript𝒜𝑖diam(\\mathcal{A}_{i}), d​i​a​m​(𝒴j)𝑑𝑖𝑎𝑚subscript𝒴𝑗diam(\\mathcal{Y}_{j}), d​i​a​m​(𝒲k)𝑑𝑖𝑎𝑚subscript𝒲𝑘diam(\\mathcal{W}_{k}), and d​i​a​m​(𝒰l)𝑑𝑖𝑎𝑚subscript𝒰𝑙diam(\\mathcal{U}_{l}) approach zero for all (i,j,k,l)𝑖𝑗𝑘𝑙(i,j,k,l),\nthe approximation error converges to zero for good distributions.\nFor discrete and finite random variables,\nthe number of constraints is finite so the approximation error can be exactly zero for natural discretization.\nThe other equalities hold naturally if the first constraint is satisfied.\nFor the constraints for all u∈𝒰𝑢𝒰u\\in\\mathcal{U},\nwe integrate these constraints using the same approach as Compared with the original functional constraints in distributions, the linear constraints for xi​j​k​lsubscript𝑥𝑖𝑗𝑘𝑙x_{ijkl} are much easier to handle. After discretization, the objective of the original optimization problem is where yjsubscript𝑦𝑗y_{j} is chosen to be ∫y∈𝒴jy​𝑑y∫y∈𝒴j𝑑ysubscript𝑦subscript𝒴𝑗𝑦differential-d𝑦subscript𝑦subscript𝒴𝑗differential-d𝑦\\frac{\\int_{y\\in\\mathcal{Y}_{j}}ydy}{\\int_{y\\in\\mathcal{Y}_{j}}dy},\nas this option can reduce the approximation error (see the appendix for more details).\nIf 𝒴jsubscript𝒴𝑗\\mathcal{Y}_{j} is an interval, then the above choice of yjsubscript𝑦𝑗y_{j} is just the midpoint of 𝒴jsubscript𝒴𝑗\\mathcal{Y}_{j}, The first transfer learning task requires obtaining causal bounds from the number of n𝒜subscript𝑛𝒜n_{\\mathcal{A}} optimization problems.\nSince every optimization problem shares the same feasible region, it is efficient to solve all optimization problems in a unified way.\nTo obtain the global solutions, we use Monto-Carlo method to approximate the above discrete optimization problem. Monte-Carlo algorithm.\nMonte Carlo (MC) algorithms are a key approach for sampling causal models subject to constraints.\nHowever, efficiently sampling probability tables with row and column constraints can be challenging.\nAfter discretization, the problem of sampling xi​j​k​lsubscript𝑥𝑖𝑗𝑘𝑙x_{ijkl} is equivalent to sampling from a simplex 𝒟𝒟\\mathcal{D} defined by the linear constraints Let Similarly, we can define θlsubscript𝜃𝑙\\theta_{l} and θ^lsubscript^𝜃𝑙\\hat{\\theta}_{l}, so we rewrite the linear equations as The overall equalities provide n𝒜​n𝒴​n𝒲+n𝒰−1subscript𝑛𝒜subscript𝑛𝒴subscript𝑛𝒲subscript𝑛𝒰1n_{\\mathcal{A}}n_{\\mathcal{Y}}n_{\\mathcal{W}}+n_{\\mathcal{U}}-1 linearly independent constraints,\nas both types of equalities imply ∑i​j​k​lxi​j​k​l=1subscript𝑖𝑗𝑘𝑙subscript𝑥𝑖𝑗𝑘𝑙1\\sum_{ijkl}x_{ijkl}=1.\nSince there are n𝒜​n𝒴​n𝒲​n𝒰subscript𝑛𝒜subscript𝑛𝒴subscript𝑛𝒲subscript𝑛𝒰n_{\\mathcal{A}}n_{\\mathcal{Y}}n_{\\mathcal{W}}n_{\\mathcal{U}} unknown variables,\nwe need to determine the value of n𝒜​n𝒴​n𝒲​n𝒰−n𝒜​n𝒴​n𝒲−n𝒰+1subscript𝑛𝒜subscript𝑛𝒴subscript𝑛𝒲subscript𝑛𝒰subscript𝑛𝒜subscript𝑛𝒴subscript𝑛𝒲subscript𝑛𝒰1n_{\\mathcal{A}}n_{\\mathcal{Y}}n_{\\mathcal{W}}n_{\\mathcal{U}}-n_{\\mathcal{A}}n_{\\mathcal{Y}}n_{\\mathcal{W}}-n_{\\mathcal{U}}+1\nunknowns. Directly sampling xi​j​k​lsubscript𝑥𝑖𝑗𝑘𝑙x_{ijkl} from distributions supported on  01  and rejecting invalid samples that do not satisfy the inequality constraints can be sample-inefficient.\nTo address this issue, we can incorporate inequalities to shrink the support of each unknown variable. For instance, Li and Pearl 9 proposed using the bounds and in their optimization problem.\nHowever, the support of each unknown xi​j​k​lsubscript𝑥𝑖𝑗𝑘𝑙x_{ijkl} in 9: [max⁡{0,θi​j​k+θl−1},min⁡{θi​j​k,θl}]0subscript𝜃𝑖𝑗𝑘subscript𝜃𝑙1subscript𝜃𝑖𝑗𝑘subscript𝜃𝑙[\\max\\{0,\\theta_{ijk}+\\theta_{l}-1\\},\\min\\{\\theta_{ijk},\\theta_{l}\\}],\nis not as tight as those found by solving the linear programming problem (9): Clearly, the solutions generated by 9 may not satisfy all the constraints in (6),\nwhich further contributes to the lack of tightness in their approach.\nWe provide evidence for this claim by reporting the proportion of valid samples obtained with different sample spaces in Table 5 for the example discussed in Section 4. We also report the simulation results in Section 4 for causal bounds,\nwhich evidently support our arguments. While the individual bounds for xi​j​k​lsubscript𝑥𝑖𝑗𝑘𝑙x_{ijkl} obtained by (9) are tight,\nthe sample efficiency is still unsatisfactory.\nTo improve sample efficiency, we introduce the sampling algorithm based on sequential LP.\nSuppose that we find a linearly independent variable index set S:={n1,n2,⋯,n|S|}assign𝑆subscript𝑛1subscript𝑛2⋯subscript𝑛𝑆S:=\\{n_{1},n_{2},\\cdots,n_{|S|}\\} with cardinality n𝒜​n𝒴​n𝒲​n𝒰−n𝒜​n𝒴​n𝒲−n𝒰+1subscript𝑛𝒜subscript𝑛𝒴subscript𝑛𝒲subscript𝑛𝒰subscript𝑛𝒜subscript𝑛𝒴subscript𝑛𝒲subscript𝑛𝒰1n_{\\mathcal{A}}n_{\\mathcal{Y}}n_{\\mathcal{W}}n_{\\mathcal{U}}-n_{\\mathcal{A}}n_{\\mathcal{Y}}n_{\\mathcal{W}}-n_{\\mathcal{U}}+1 of (8).\nFor the first variable xn1subscript𝑥subscript𝑛1x_{n_{1}},\nwe solve (9) to find its support [ln1,hn1]subscript𝑙subscript𝑛1subscriptℎsubscript𝑛1[l_{n_{1}},h_{n_{1}}],\nand we sample a x^n1subscript^𝑥subscript𝑛1\\hat{x}_{n_{1}} from a user-given distribution truncated to [ln1,hn1]subscript𝑙subscript𝑛1subscriptℎsubscript𝑛1[l_{n_{1}},h_{n_{1}}].\nThen, for the nisubscript𝑛𝑖n_{i} step, the previous values of xn1,⋯,xni−1subscript𝑥subscript𝑛1⋯subscript𝑥subscript𝑛𝑖1x_{n_{1}},\\cdots,x_{n_{i-1}} have been determined by sampling.\nWe add the constraints for xnjsubscript𝑥subscript𝑛𝑗x_{n_{j}}, j=1,2,⋯,i−1𝑗12⋯𝑖1j=1,2,\\cdots,i-1, where it is equal to its corresponding value.\nTogether with new constraints in (9), we can find a support [lni,hni]subscript𝑙subscript𝑛𝑖subscriptℎsubscript𝑛𝑖[l_{n_{i}},h_{n_{i}}]\nand sample a valid value for xnisubscript𝑥subscript𝑛𝑖x_{n_{i}} by solving where x^njsubscript^𝑥subscript𝑛𝑗\\hat{x}_{n_{j}} is a sampled value for xnjsubscript𝑥subscript𝑛𝑗x_{n_{j}}.\nAfter |S|𝑆|S| steps, the remaining xi​j​k​lsubscript𝑥𝑖𝑗𝑘𝑙x_{ijkl} with its index (i,j,k,l)∉S𝑖𝑗𝑘𝑙𝑆(i,j,k,l)\\notin S can be solved by linear equations (8).\nThese sequential LP steps ensure that we always find valid samples in the simplex defined by (9). Sequentially solving LPs and sampling components of 𝐱𝐱\\mathbf{x} induce a distribution supported on the simplex defined by (​9​)italic-(9italic-)\\eqref{eq: linear programming to find support}.\nAlthough this is the most computationally extensive step, it can significantly improve the sample efficiency.\nIn comparison, the sample efficiency of directly sampling xi​j​k​lsubscript𝑥𝑖𝑗𝑘𝑙x_{ijkl} from distributions supported on  01  or using the bounds proposed in 9 is much lower,\nas shown in Table 5.\nWe will discuss our sampling method on general simplex in appendix. The remaining concern is the estimation error in RHS of (6).\nDenote Since θi​j​k∈ subscript𝜃𝑖𝑗𝑘01\\theta_{ijk}\\in , the variance of one sample is at most 1414\\frac{1}{4}.\nIf θi​j​ksubscript𝜃𝑖𝑗𝑘{\\theta}_{ijk} has n𝑛n i.i.d. samples, then its largest variance of θ^i​j​ksubscript^𝜃𝑖𝑗𝑘\\hat{\\theta}_{ijk} is 14​n14𝑛\\frac{1}{4n}.\nWe can simply set ϵ=12​nitalic-ϵ12𝑛\\epsilon=\\frac{1}{2\\sqrt{n}} if n𝑛n is given.\nWe can sample θlsubscript𝜃𝑙\\theta_{l} and θi​j​ksubscript𝜃𝑖𝑗𝑘\\theta_{ijk} from distributions with known parameters θ^lsubscript^𝜃𝑙\\hat{\\theta}_{l} and θ^i​j​ksubscript^𝜃𝑖𝑗𝑘\\hat{\\theta}_{ijk}: For example, if one want to reflect the concentration property,\nthen one can choose the truncate Gaussian distribution 𝒩​(θ^x,14​nx)𝒩subscript^𝜃𝑥14subscript𝑛𝑥\\mathcal{N}(\\hat{\\theta}_{x},\\frac{1}{4n_{x}}) for x=i​j​k𝑥𝑖𝑗𝑘x=ijk or x=l𝑥𝑙x=l.\nIf one expects the worst cases, then one can choose the uniform distribution for θxsubscript𝜃𝑥\\theta_{x} with discrepancy parameter ϵitalic-ϵ\\epsilon. The optimization problem (4) after discretization is If all referred random variables are discrete, the optimization (12) can be exactly the same as (4) for natural discretization.\nFor general random variables, it is still an open problem that whether the solution to (12) will converge to\nthe solution to (4) as the discretization becomes finer.\nHowever, we have the following probability convergence results for variables 𝐱:=(xi​j​k​l)assign𝐱subscript𝑥𝑖𝑗𝑘𝑙\\mathbf{x}:=(x_{ijkl}).\nDenote the solutions to (12) as l^​(a)^𝑙𝑎\\hat{l}(a) and h^​(a)^ℎ𝑎\\hat{h}(a).\nFor simplicity, we denote the induced sampling measure for 𝐱𝐱\\mathbf{x} as ℙssubscriptℙ𝑠\\mathbb{P}_{s}.\nThe existence and uniqueness can be easily proved by Kolmogorov’s extension theorem. Assume that\nthe sampling measure ℙssubscriptℙ𝑠\\mathbb{P}_{s} satisfies ∀𝐱∈𝒟for-all𝐱𝒟\\forall\\mathbf{x}\\in\\mathcal{D}, and ∀δ>0for-all𝛿0\\forall\\delta>0, where ℬ​(𝐱,δ)ℬ𝐱𝛿\\mathcal{B}(\\mathbf{x},\\delta) is a ball centered at 𝐱𝐱\\mathbf{x} with radius δ𝛿\\delta.\nIf the discrepancy parameter is set to 00,\nthen b(1)​(a)subscript𝑏1𝑎b_{(1)}(a) converges to l^​(a)^𝑙𝑎\\hat{l}(a) in probability\nand b(B)​(a)subscript𝑏𝐵𝑎b_{(B)}(a) converges to h^​(a)^ℎ𝑎\\hat{h}(a) in probability for B→∞→𝐵B\\to\\infty. The condition ℙs​(ℬ​(𝐱,δ)∩𝒟)>0subscriptℙ𝑠ℬ𝐱𝛿𝒟0\\mathbb{P}_{s}(\\mathcal{B}(\\mathbf{x},\\delta)\\cap\\mathcal{D})>0 means that\nthe sampling distribution can cover all feasible models.\nThis condition can be satisfied by various continuous distributions such as the uniform distribution, truncated Gaussian distribution, and others. Similar to [ref]42, we can establish a concentration result for our sampling algorithm using Chernoff’s bound.\nHowever, this result is not as useful as Proposition 3.1.\nFirstly, the concentration property relies on the distribution of causal bounds,\nwhich is often complex and difficult to know in practice.\nSecondly, people are usually more interested in convergence results than concentration results in approximation problems.\nFor example, if one has several l​(a)𝑙𝑎l(a) samples, it is more reasonable to use the minimum value among them rather than the average results. Regarding other statistical properties,\nMosteller  shows that if the distribution on causal bounds Fb​(x)subscript𝐹𝑏𝑥F_{b}(x) is continuous and has a density pb​(x)subscript𝑝𝑏𝑥p_{b}(x),\nthen the q𝑞q-th quantile b[B​q]subscript𝑏delimited-[]𝐵𝑞b_{[Bq]} asymptotically follows the normal distribution 𝒩​(Fb−1​(q),q​(1−q)B​[pb​(Fb−1​(q))]2)𝒩superscriptsubscript𝐹𝑏1𝑞𝑞1𝑞𝐵superscriptdelimited-[]subscript𝑝𝑏superscriptsubscript𝐹𝑏1𝑞2\\mathcal{N}(F_{b}^{-1}(q),\\frac{q(1-q)}{B[p_{b}(F_{b}^{-1}(q))]^{2}}). To accelerate the convergence speed of Algorithm 1,\nwe can incorporate an optimization procedure OPT(min/max, object, feasible domain 𝒟𝒟\\mathcal{D}, initial guess 𝐱0subscript𝐱0\\mathbf{x}_{0}).\nThe optimization procedure OPT takes as input the objective function to minimize/maximize, the feasible domain 𝒟𝒟\\mathcal{D}, and an initial guess 𝐱0subscript𝐱0\\mathbf{x}_{0}.\nThe procedure OPT seeks to find local optima 𝐱l​o​c∈𝒟subscript𝐱𝑙𝑜𝑐𝒟\\mathbf{x}_{loc}\\in\\mathcal{D} that optimizes the objective function locally.\nSpecifically, in each iteration of Algorithm 1, we use the sampled value 𝐱𝐱\\mathbf{x} as the initial guess 𝐱0subscript𝐱0\\mathbf{x}_{0} and feed it to OPT.\nThe output of OPT is then used as the updated estimate of causal bounds.\nThe modified algorithm with OPT can be found in the appendix.\nBy leveraging the optimization procedure, we can further enhance the result in Proposition 3.1 under mild assumptions on OPT. Assume that\nthe sampling measure ℙssubscriptℙ𝑠\\mathbb{P}_{s} satisfies ∀𝐱∈𝒟for-all𝐱𝒟\\forall\\mathbf{x}\\in\\mathcal{D}, and ∀δ>0for-all𝛿0\\forall\\delta>0, where ℬ​(𝐱,δ)ℬ𝐱𝛿\\mathcal{B}(\\mathbf{x},\\delta) is a ball centered at 𝐱𝐱\\mathbf{x} with radius δ𝛿\\delta.\nGiven a deterministic procedure OPT which satisfies for any local optima 𝐱l​o​csubscript𝐱𝑙𝑜𝑐\\mathbf{x}_{loc},\nthere exists δ>0𝛿0\\delta>0 such that for any initial guess 𝐱0∈ℬ​(𝐱l​o​c,δ)∩𝒟subscript𝐱0ℬsubscript𝐱𝑙𝑜𝑐𝛿𝒟\\mathbf{x}_{0}\\in\\mathcal{B}(\\mathbf{x}_{loc},\\delta)\\cap\\mathcal{D},\nOPT can output 𝐱l​o​csubscript𝐱𝑙𝑜𝑐\\mathbf{x}_{loc} as a result.\nIf the discrepancy parameter is set to 00,\nthen b(1)​(a)subscript𝑏1𝑎b_{(1)}(a) and b(B)​(a)subscript𝑏𝐵𝑎b_{(B)}(a) converge almost surely to l^​(a)^𝑙𝑎\\hat{l}(a) and h^​(a)^ℎ𝑎\\hat{h}(a) for B→∞→𝐵B\\to\\infty, respectively. This result suggests that the modified algorithm with OPT can converge to the optimal solution with higher rate,\nand the convergence speed can be accelerated compared to the original algorithm.\nIntuitively, the optimization procedure OPT makes the probability Fbsubscript𝐹𝑏F_{b} concentrate at local optima rather than spread over the whole feasible domain.\nAdditionally, optimization procedures can usually work effectively at local regions and guarantee local optimality for sufficiently close initial guesses.\nTherefore, the assumption on OPT is not so strict. MAB.\nWe now consider the first transfer learning task for |𝒜|<∞𝒜|\\mathcal{A}|<\\infty.\nThe algorithm proceeds as follows:\nfirst, we remove any arm a𝑎a for which h​(a)<maxi∈𝒜⁡l​(i)ℎ𝑎subscript𝑖𝒜𝑙𝑖h(a)<\\max_{i\\in\\mathcal{A}}l(i).\nNext, we truncate the upper confidence bound Ua​(t)subscript𝑈𝑎𝑡U_{a}(t) for the remaining actions using their respective causal bounds.\nSpecifically, we define the truncated upper confidence bound as U^a​(t)=min⁡{Ua​(t),h​(a)}subscript^𝑈𝑎𝑡subscript𝑈𝑎𝑡ℎ𝑎\\hat{U}_{a}(t)=\\min\\{U_{a}(t),h(a)\\}.\nThe algorithm then chooses the action with the highest truncated UCB and update the average reward of the chosen action. Consider a |𝒜|𝒜|\\mathcal{A}|-MAB problem with rewards bounded in  01 .\nFor each arm a∈𝒜𝑎𝒜a\\in\\mathcal{A}, its expected reward μasubscript𝜇𝑎\\mu_{a} is bounded by [l​(a),h​(a)]𝑙𝑎ℎ𝑎[l(a),h(a)].\nThen in the Algorithm 2, the number of draws 𝔼​[Na​(T)]𝔼delimited-[]subscript𝑁𝑎𝑇\\mathbb{E}[N_{a}(T)] for any sub-optimal arm is upper bounded as: As a simple corollary, the minimax upper bound is 𝒪​(|𝒜∗~|​T​log⁡T)𝒪~superscript𝒜𝑇𝑇\\mathcal{O}(\\sqrt{|\\widetilde{\\mathcal{A}^{*}}|T\\log T}),\nwhich also matches the lower bound Ω​(|𝒜∗~|​T)Ω~superscript𝒜𝑇\\Omega(\\sqrt{|\\widetilde{\\mathcal{A}^{*}}|T}) in transfer learning setting (see more details in appendix).\n\nReference markers found: [4]\n\nContext windows for each [ref]:\n[ref]4: We first consider an off-policy learning problem between an contextual bandit expert and a MAB agent.\nContextual bandits   [ref]4 is a variation of MAB where the expert can observe extra contextual information W𝑊W associated with the reward signal Y𝑌Y.\nThe expert follows certain policy and summarizes its experiences as\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 4,\n            \"titles\": [\n                \"Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning\",\n                \"Contextual bandit learning with predictable rewards\",\n                \"Bandits with unobserved confounders: A causal approach\",\n                \"Transfer Learning for Contextual Multi-armed Bandits\",\n                \"Contextual bandits with linear payoff functions\",\n                \"An automated approach to causal inference in discrete settings\",\n                \"Minimax theorems\",\n                \"Beyond ucb: Optimal and efficient contextual bandits with regression oracles\",\n                \"Practical Contextual Bandits with Regression Oracles\",\n                \"Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective\",\n                \"Dual Instrumental Method for Confounded Kernelized Bandits\",\n                \"Provably efficient offline reinforcement learning for partially observable Markov decision processes\",\n                \"Provably Efficient Reinforcement Learning with Linear Function Approximation\",\n                \"Instrument-armed bandits\",\n                \"Causal Bandits: Learning Good Interventions via Causal Inference\",\n                \"Bandit algorithms\",\n                \"Sequential transfer in multi-armed bandit with finite set of models\",\n                \"Probabilities of causation with nonbinary treatment and effect\",\n                \"Bounds on Causal Effects and Application to High Dimensional Data\",\n                \"Unit Selection with Nonbinary Treatment and Effect\",\n                \"Learning probabilities of causation from finite population data\",\n                \"Epsilon-identifiability of causal quantities\",\n                \"A contextual-bandit approach to personalized news article recommendation\",\n                \"Transferable contextual bandit for cross-domain recommendation\",\n                \"Learning without knowing: Unobserved context in continuous transfer reinforcement learning\",\n                \"Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes\",\n                \"On some useful “inefficient” statistics\",\n                \"A regret bound for greedy partially observed stochastic contextual bandits\",\n                \"Analysis of Thompson Sampling for Partially Observable Contextual Multi-Armed Bandits\",\n                \"Causal inference in statistics: An overview\",\n                \"The book of why\",\n                \"A Minimax Learning Approach to Off-Policy Evaluation in Confounded Partially Observable Markov Decision Processes\",\n                \"Scalable Computation of Causal Bounds\",\n                \"Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability\",\n                \"Bandits with Partially Observable Confounded Data\",\n                \"A general identification condition for causal effects\",\n                \"Provably efficient reinforcement learning in partially observable dynamical systems\",\n                \"Provably Efficient Causal Reinforcement Learning with Confounded Observational Data\",\n                \"Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation\",\n                \"Transfer learning in multi-armed bandit: a causal approach\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]4 based on the context and research area of Provably Efficient Learning in Partially Observable Contextual Bandit. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}