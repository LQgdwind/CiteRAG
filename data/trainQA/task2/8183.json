{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: STanH : Parametric Quantization for Variable Rate Learned Image Compression\n\nPaper Abstract: \n\nPaper Section Text:\nLearnable Image Compression models have shown the potential to match or even outperform standardized codecs such as the recent H.266/VVC¬† in RD terms.\nEarly seminal works such as¬†[ref]4  exploited a simple convolutional autoencoder structure with a unique latent representation modeled with a fully factorized distribution among channels modeled either analytically¬† or through an auxiliary neural network, and exploiting Generalized divisive normalization (GDN) activation functions.\nThe scheme was improved by¬†[ref]8 introducing a pyramid-based architecture composed of two nested variational autoencoders. Here, the first is called hyperprior and captures spatial correlation within the image, while the second one models the latent representation, which is supposed to follow a zero-mean Gaussian distribution. [ref]10 ¬†combined previous architecture with a context-based auto-regressive entropy model to capture more local spatial correlation by exploiting already decoded parts of images. Similarly, ¬†improved context modeling through a 3D zigzag scanning order, and improved parallelism entropy decoding with a 3D code dividing technique by partitioning the latent representation in multiple independent groups. In¬† a special non-local operation is proposed to consider global similarity within the context, by introducing U-net-like blocks, while¬† introduced non-local network operations as non-linear transforms in both latent representations.\nIn more recent works other techniques have been exploited to improve RD performance; ¬†replaced simple Gaussian distribution with a mixture of Gaussians and introduced some attention module to enhance entropy estimation, while¬† introduced a more flexible discretized Gaussian-Laplacian-Logistic mixture model for the latent representation.\nIn¬† local attention is exploited to combine the local-aware attention with the global-related feature learning and to the present date is among the best-performing architectures.\nIn the following, we chose this architecture to exemplify how we integrated our quantization method into a typical image compression architecture.\nYet, we will demonstrate the versatility of our method by applying it to other two architectures in the experimental section.\n\nTang¬†et¬†al.¬† proposed a self-attention mechanism based on graphs to improve entropy estimation, and other threads of works tried to enhance some aspects of previous models: ¬†for example extracted better transformation between image and latent features space by exploiting invertible networks, while¬†0 1 replaced convolutional modules with the Swin¬†transformer to achieve better compression efficiency with fewer parameters.\nOn the other hand, 2¬†focused on the optimization of the image decoding through a learned block-based framework, 3¬†tried to adapt the entire structure to single images with an additional model stream to generate the transform parameters at the decoder side, and¬†4 introduced a checkerboard context model to improve efficiency during the decoding stage.\nIn¬†5 hierarchical VAE architecture, originally designed for generative image modeling, is exploited for LIC, redefining their probabilistic model to allow easy quantization and practical entropy coding.\nIn addition to the aforementioned variational autoencoder-based approaches, other approaches have been explored for compressing images using neural networks: 6 7¬†exploited GAN-based architecture to reduce image compression artifacts, especially at extremely low bitrates.\n\nThe common feature shared by all the mentioned works is that a single model targets a single rate, meaning that multiple models must be trained and stored to cover the range of rates required in practical image or video compression scenarios.\nThis issue is critical since each model includes tens of millions of learnable parameters.\nThe issue with variable rate image compression can be also related to the way quantization takes place, as discussed in the following.\n\nReference markers found: [4]\n\nContext windows for each [ref]:\n[ref]4: Learnable Image Compression models have shown the potential to match or even outperform standardized codecs such as the recent H.266/VVC¬† in RD terms.\nEarly seminal works such as¬†[ref]4  exploited a simple convolutional autoencoder structure with a unique latent representation modeled with a fully factorized distribution among channels modeled either analytically¬† or through\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 4,\n            \"titles\": [\n                \"Lossy Image Compression with Compressive Autoencoders\",\n                \"Image and Video Compression with Neural Networks: A Review\",\n                \"Versatile video coding\",\n                \"Theoretical foundations of transform coding\",\n                \"End-to-end Optimized Image Compression\",\n                \"A differentiable entropy model for learned image compression.\",\n                \"Density modeling of images using a generalized normalization transformation\",\n                \"Variational image compression with a scale hyperprior\",\n                \"Joint Autoregressive and Hierarchical Priors for Learned Image Compression\",\n                \"Context-Adaptive Entropy Model for End-to-end optimized Image Compression\",\n                \"Channel-wise Autoregressive Entropy Models for Learned Image Compression\",\n                \"Efficient and effective context-based convolutional entropy modeling for image compression.\",\n                \"Learning Context-Based Non-local Entropy Modeling for Image Compression\",\n                \"End-to-End Learnt Image Compression via Non-Local Attention Optimization and Improved Context Modeling\",\n                \"Learned image compression with discretized gaussian mixture likelihoods and attention modules\",\n                \"Learned Image Compression with Gaussian-Laplacian-Logistic Mixture Model and Concatenated Residual Modules\",\n                \"The Devil Is in the Details: Window-based Attention for Image Compression\",\n                \"Joint Graph Attention and Asymmetric Convolutional Neural Network for Deep Image Compression,\",\n                \"Enhanced Invertible Encoding for Learned Image Compression\",\n                \"ùëñ-RevNet: Deep Invertible Networks\",\n                \"Transformer-based transform coding.\",\n                \"Learned Block-based Hybrid Image Compression\",\n                \"Neural Data-Dependent Transform for Learned Image Compression\",\n                \"Checkerboard Context Model for Efficient Learned Image Compression\",\n                \"Lossy Image Compression with Quantized Hierarchical VAEs\",\n                \"Generative Adversarial Networks for Extreme Learned Image Compression\",\n                \"High-Fidelity Generative Image Compression\",\n                \"Quantization\",\n                \"Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations\",\n                \"Soft then hard: Rethinking the quantization in neural image compression.\",\n                \"Universally Quantized Neural Compression\",\n                \"Deep Task-Based Quantization\",\n                \"Variable Rate Deep Image Compression With a Conditional Autoencoder\",\n                \"Variable Rate Deep Image Compression with Modulated Autoencoder\",\n                \"Efficient Variable Rate Image Compression With Multi-Scale Decomposition Network,\",\n                \"Asymmetric Gained Deep Image Compression With Continuous Rate Adaptation\",\n                \"Selective compression learning of latent representations for variable-rate image compression\",\n                \"Variable bitrate image compression with quality scaling factors\",\n                \"Flexible Neural Image Compression via Code Editing\",\n                \"EVC: Towards Real-Time Neural Image Compression with Mask Decay\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]4 based on the context and research area of STanH : Parametric Quantization for Variable Rate Learned Image Compression. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}