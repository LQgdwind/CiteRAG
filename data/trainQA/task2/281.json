{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Approximate Manifold Defense Against Multiple Adversarial Perturbations\n\nPaper Abstract: \n\nPaper Section Text:\nThe robust accuracy of a defense model is defined by their performance against the strongest adversarial attack within a specified â„“psubscriptâ„“ğ‘\\ell_{p} boundary .\nFor RBF-CNN, we reconstruct images by drawing samples from the RBF filters.\nNoise is injected to the images during this sampling process using an isotropic Gaussian distribution (see Eqn. 4).\nThe RBF filters and the reconstruction step are otherwise differentiable. We apply the following wide range of adversarial attacks to evaluate the robustness of our models: Static Attacks.\nAdversarial examples are generated by removing the non-differentiable noise injection step from the reconstruction layer, i.e ğ’›ğ’‹=ğğ’‹subscriptğ’›ğ’‹subscriptğğ’‹\\bm{z_{j}=\\mu_{j}} in Eqn. 4.\nHere, we consider single-step attack FGSM [ref]2, iterative attacks such as PGD [ref]6, MI-FGSM , CW [ref]38, DAA , EAD . Adaptive Attacks.\nAn adaptive attack is constructed after the defense model has been completely specified, such that the adversary can find the optimal adversarial perturbations within the specified perturbation boundaries . For our RBF-CNN models, we design the adaptive attack as a combination of BPDA and EoT (Expectation of Transformations) , as follows.\nIn the forward propagation, we do not change anything in the network.\nIn the backward propagation, we choose a differentiable approximation for the reconstruction layer, by considering ğ’›ğ’‹=ğğ’‹subscriptğ’›ğ’‹subscriptğğ’‹\\bm{z_{j}=\\mu_{j}} in Eqn. 4 to efficiently compute the gradients.\nSince the RBF filters are already differentiable, we do not need any approximation for the RBF layer.\nWe compute the expected loss using a Monte-Carlo method with 505050 simulations to find the strongest adversarial examples. Black-box Attacks. We also evaluate against gradient-free SPSA attack  to ensure that our framework is not giving any false sense of robustness using gradient-masking .\nWe apply black-box transfer attacks where the adversarial examples are generated by attacking a standard CNN classifier with no defense. â„“âˆsubscriptâ„“\\ell_{\\infty} Bounded Attacks.\nTable I presents the performance of our râ€‹Câ€‹Nâ€‹Nğ‘Ÿğ¶ğ‘ğ‘rCNN and râ€‹Câ€‹Nâ€‹N+ğ‘Ÿğ¶ğ‘subscriptğ‘rCNN_{+} models against â„“âˆsubscriptâ„“\\ell_{\\infty} bounded attacks.\nWe choose the perturbation boundaries of 0.30.30.3 and 0.0310.0310.031 for MNIST and CIFAR-10 respectively.\nWe observe that for both MNIST and CIFAR-10, râ€‹Câ€‹Nâ€‹N+ğ‘Ÿğ¶ğ‘subscriptğ‘rCNN_{+} achieves high robustness under â„“âˆsubscriptâ„“\\ell_{\\infty} perturbations bounds of 0.30.30.3 and 0.0310.0310.031 respectively. One main advantage of RBF-CNN compared to the adversarial training frameworks is that it significantly reduces the training time required.\nHere, we compare the training times of our RBF-CNN models with Madryâ€™s models, that achieved robustness only for â„“âˆsubscriptâ„“\\ell_{\\infty} perturbations [ref]6. Table II shows the training times when executed on a GTX 1080Ti GPU.\nWe observe that the training time of râ€‹Câ€‹Nâ€‹N+ğ‘Ÿğ¶ğ‘subscriptğ‘rCNN_{+} models are more than 4.5Ã—4.5\\times and 7Ã—7\\times faster than the Madry models for MNIST and CIFAR-10 respectively.\nEven as our proposed approach significantly reduces the training time required, our RBF-CNN models are still able to achieve similar performances as Madryâ€™s models for â„“âˆsubscriptâ„“\\ell_{\\infty} perturbations in both MNIST and CIFAR-10 as shown in Table III.\nAlso, our best RBF-CNN model for MNIST significantly outperforms the only successful manifold defense model by Schott et al. (2019) .\nWe also compare with other input transformation  and network randomization  techniques to show that unlike these defenses, our RBF-CNN models remain robust against all type of attacks. Another limitation of adversarial training models is that they provide robustness only within a pre-specified boundary from where the adversarial examples were produced for their training, and offer no robustness guarantee slightly beyond these boundaries.\nTable IV shows that [ref]6 achieve high robust accuracies within their specified â„“âˆsubscriptâ„“\\ell_{\\infty} bounds of 0.30.30.3 and 0.0310.0310.031 for MNIST and CIFAR-10 respectively.\nHowever, their robust accuracies drastically drop beyond those bounds when tested against PGD attack with â„“âˆâ‰¤0.35subscriptâ„“0.35\\ell_{\\infty}\\leq 0.35 for MNIST and at â„“âˆâ‰¤0.05subscriptâ„“0.05\\ell_{\\infty}\\leq 0.05 for CIFAR-10.\nIn contrast, RBF-CNN models achieve significantly high accuracies at these â„“âˆsubscriptâ„“\\ell_{\\infty} boundaries. â„“1subscriptâ„“1\\ell_{1} and â„“2subscriptâ„“2\\ell_{2} Bounded Attacks. \nFor our experiments on â„“1subscriptâ„“1\\ell_{1} and â„“2subscriptâ„“2\\ell_{2} bounded attacks, we choose â„“1subscriptâ„“1\\ell_{1}=15 and â„“2subscriptâ„“2\\ell_{2}=2 for MNIST and â„“1subscriptâ„“1\\ell_{1}=20 and â„“2subscriptâ„“2\\ell_{2}=1 for CIFAR-10.\nFor MNIST, we see that the perception of the adversarial images is changing around these bounds (see Fig. 4).\nNote that, such behavior cannot be observed when attacking a non-robust classifier . Table V presents the performance of RBF-CNNs against different â„“1subscriptâ„“1\\ell_{1} and â„“2subscriptâ„“2\\ell_{2} bounded attacks.\nWe observe that RBF-CNN models significantly outperform Madryâ€™s models, that are trained to provide robustness for â„“âˆsubscriptâ„“\\ell_{\\infty} perturbations. Certification for â„“2subscriptâ„“2\\ell_{2}-perturbations. \nCertification provides a lower bound guarantee of robust accuracy.\nSince the reconstruction process of our RBF-CNN models mitigate any minor perturbations, it also improves the classification performance against random Gaussian perturbed images.\nWe apply the existing certification technique of randomized smoothing defenses  to further demonstrate that our RBF-CNN models also improve the certified robustness for â„“2subscriptâ„“2\\ell_{2} perturbations compared to the baseline models with no defense. For each input ğ’™ğ’™{\\bm{x}}, we sample 50,0005000050,000 noisy samples from an isotropic Gaussian distribution Ïµâˆ¼ğ’©â€‹(0,Ï„2â€‹I)similar-tobold-italic-Ïµğ’©0superscriptğœ2ğ¼{\\bm{\\epsilon}}\\sim\\mathcal{N}(0,\\tau^{2}I).\nFor MNIST, we set Ï„ğœ\\tau to 0.20.20.2 for the baseline and râ€‹Câ€‹Nâ€‹Nğ‘Ÿğ¶ğ‘ğ‘rCNN and 0.30.30.3 for râ€‹Câ€‹Nâ€‹N+ğ‘Ÿğ¶ğ‘subscriptğ‘rCNN_{+}.\nFor CIFAR-10, we set Ï„=0.1ğœ0.1\\tau=0.1 for all the models.\nTable VI shows the certified robust accuracies.\nWe see that for CIFAR-10, the robustness of the CNN classifier is improved by adding RBF and reconstruction layers.\nThe results for râ€‹Câ€‹Nâ€‹N+ğ‘Ÿğ¶ğ‘subscriptğ‘rCNN_{+} suggests that the certified robustness of our models are further improved by augmenting minor noises the images for training.\nTo summarize, our RBF-CNN models improves the empirical robustness against any minor â„“pâ‰¥1subscriptâ„“ğ‘1\\ell_{p\\geq 1} perturbations as well as certified robustness for â„“2subscriptâ„“2\\ell_{2} perturbations.\n\nReference markers found: [2]\n\nContext windows for each [ref]:\n[ref]2: by removing the non-differentiable noise injection step from the reconstruction layer, i.e ğ’›ğ’‹=ğğ’‹subscriptğ’›ğ’‹subscriptğğ’‹\\bm{z_{j}=\\mu_{j}} in Eqn. 4.\nHere, we consider single-step attack FGSM [ref]2, iterative attacks such as PGD [ref]6, MI-FGSM , CW [ref]38, DAA , EAD . Adaptive Attacks.\nAn adaptive attack is constructed after the defense model has been completely specified, such that\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 2,\n            \"titles\": [\n                \"The Limitations of Deep Learning in Adversarial Settings\",\n                \"Intriguing properties of neural networks\",\n                \"Explaining and Harnessing Adversarial Examples\",\n                \"Large-scale malware classification using random projections and neural networks\",\n                \"Ad-versarial: Defeating perceptual ad-blocking\",\n                \"Towards Deep Learning Models Resistant to Adversarial Attacks\",\n                \"Theoretically Principled Trade-off between Robustness and Accuracy\",\n                \"Adversarial Training and Robustness for Multiple Perturbations\",\n                \"Certified Adversarial Robustness via Randomized Smoothing\",\n                \"Certified Adversarial Robustness with Additive Noise\",\n                \"Defense-GAN: Protecting classifiers against adversarial attacks using generative models\",\n                \"Resisting Adversarial Attacks Using Gaussian Mixture Variational Autoencoders\",\n                \"Towards the first adversarially robust neural network model on MNIST\",\n                \"Normal Similarity Network for Generative Modelling\",\n                \"Robustness May Be at Odds with Accuracy\",\n                \"Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\",\n                \"On the connection between adversarial robustness and saliency map interpretability\",\n                \"Adversarial Robustness through Local Linearization\",\n                \"Adversarial training for free!\",\n                \"Feature squeezing: Detecting adversarial examples in deep neural networks\",\n                \"Deflecting Adversarial Attacks with Pixel Deflection\",\n                \"Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser\",\n                \"Certified Robustness to Adversarial Examples with Differential Privacy\",\n                \"Countering Adversarial Images using Input Transformations\",\n                \"Generative adversarial nets\",\n                \"APE-GAN: Adversarial Perturbation Elimination with GAN\",\n                \"Auto-Embedding Generative Adversarial Networks for High Resolution Image Synthesis\",\n                \"PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples\",\n                \"Magnet: A two-pronged defense against adversarial examples\",\n                \"The Robust Manifold Defense: Adversarial Training using Generative Models\",\n                \"Provable robustness against all adversarial lpsubscriptğ‘™ğ‘l_{p}-perturbations for pâ‰¥1ğ‘1p\\\\geq 1\",\n                \"Adversarially Robust Generalization Requires More Data\",\n                \"Using pre-training can improve model robustness and uncertainty\",\n                \"MNIST handwritten digit database\",\n                \"Learning multiple layers of features from tiny images\",\n                \"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\n                \"Boosting Adversarial Attacks with Momentum\",\n                \"Towards evaluating the robustness of neural networks\",\n                \"Distributionally Adversarial Attack\",\n                \"EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]2 based on the context and research area of Approximate Manifold Defense Against Multiple Adversarial Perturbations. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}