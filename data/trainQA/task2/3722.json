{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Explainability in Process Outcome Prediction: Guidelines to Obtain Interpretable and Faithful Models\n\nPaper Abstract: \n\nPaper Section Text:\nPPM relies on the use of historic process data recorded in an (event) log, which is a set of traces representing the enactment of a process for a particular case (e.g. a loan application) within an information system . The occurrence of a single activity in a process (case) is referred to as ‘event’. Such an event may record three different attribute types typically used for a process-based analysis: the control flow attribute (the activity), the event attributes (i.e. dynamic attributes, which may change from one event to another), and the case attributes (i.e. static attributes, which do not change throughout the lifetime of a case). An event is a tuple e=(c,a,t,(s1​…​sms),(d1​…​dmd))𝑒𝑐𝑎𝑡subscript𝑠1…subscript𝑠subscript𝑚𝑠subscript𝑑1…subscript𝑑subscript𝑚𝑑e=(c,a,t,(s_{1}\\dots s_{m_{s}}),(d_{1}\\dots d_{m_{d}})), with e∈ξ𝑒𝜉e\\in\\xi (the event universe), c𝑐c the case ID, a∈A𝑎𝐴a\\in A the activity, and t∈ℝ𝑡ℝt\\in\\mathbb{R} the timestamp. This event records both case attributes S=((s1​…​sms))𝑆subscript𝑠1…subscript𝑠subscript𝑚𝑠S=((s_{1}\\dots s_{m_{s}})) and event attributes D=(d1​…​dmd)𝐷subscript𝑑1…subscript𝑑subscript𝑚𝑑D=(d_{1}\\dots d_{m_{d}}), with mssubscript𝑚𝑠m_{s} and mdsubscript𝑚𝑑m_{d} the number of case and event attributes, respectively. A trace is a sequence of events σc=[e1​…​en]subscript𝜎𝑐delimited-[]subscript𝑒1…subscript𝑒𝑛\\sigma_{c}=[e_{1}\\dots e_{n}] generated by executing activities in a process, sorted based on the timestamps of the events, such that ∀i∈[1​…​|σc|],ei∈ξformulae-sequencefor-all𝑖delimited-[]1…subscript𝜎𝑐subscript𝑒𝑖𝜉\\forall i\\in{[1\\dots|\\sigma_{c}|]},e_{i}\\in\\xi and ∀i1,i2∈[1​…​|σc|]for-allsubscript𝑖1subscript𝑖2delimited-[]1…subscript𝜎𝑐\\forall i_{1},i_{2}\\in{[1\\dots|\\sigma_{c}|]} ei1.c=ei2.cformulae-sequencesubscript𝑒subscript𝑖1𝑐subscript𝑒subscript𝑖2𝑐e_{i_{1}}.c=e_{i_{2}}.c, i.e., all the events in a trace belong to the same case. Consequently, an event eisubscript𝑒𝑖e_{i} in a trace σjsubscript𝜎𝑗\\sigma_{j} of the event log L𝐿L is denoted as ei,j=(cj,ai,j,ti,j,Sj,Di,j)subscript𝑒𝑖𝑗subscript𝑐𝑗subscript𝑎𝑖𝑗subscript𝑡𝑖𝑗subscript𝑆𝑗subscript𝐷𝑖𝑗e_{i,j}=(c_{j},a_{i,j},t_{i,j},S_{j},D_{i,j}). The outcome y𝑦y of a trace in the case of POP is usually a binary attribute 6 and depends on the needs and objectives of the process owner . In order to learn, preferably incrementally over time as traces are sequences over time, from the development of traces, (trace) prefixes are often extracted from the completed cases. To this end, a prefix log 𝕃𝕃\\mathbb{L} is derived, which is the extracted event log L𝐿L that contains all the prefixes of each case in the original event log. Next, trace cutting, i.e. limiting the prefix up to a certain number of events, is typically performed for computational reasons. Additionally, trace cutting is also performed when the class label of the case is dependent on the occurrence of an event, otherwise the label of the class becomes known and irreversible . More information about the labelling of cases is given in Section 5.2.\nNext, the use of an encoding mechanism enables the user to work with a varying amount of attributes, since each trace can have a different length. An often used encoding mechanism is the aggregation encoding technique . First, the categorical static attributes are one-hot encoded, which means that each static attribute that is categorical, results in a number of transformed attributes based on the set of unique attribute values. These values can be found with the use of θ𝜃\\theta, also referred to as the uniqueness operator. The numeric static attributes remain unchanged. Second, the timestamp is transformed into three different dynamic numeric attributes: timesincelastevent, timesincecasestart and timesincemidnight. Next, all the dynamic (i.e. event) numeric attributes are replaced by their summary statistics min, max, mean, sum, and std. The last transformation step relates to the dynamic categorical attributes and the control flow attribute, where the frequency of occurrence of an attribute value in a prefix is the value for the new attribute. By contrast, the use of the above encoding mechanism in step-based models such as recurrent neural networks becomes superfluous given their sequential setup. To exploit this efficiently, a low-dimensional representation of discrete attributes in the form of embeddings is an often performed encoding technique 7. This mapping transforms a categorical attribute to a vector of continuous numbers, similarly to how one-hot encoding works, although the latter ignores the similarity between the obtained vectors. Finally, the tuple (xj,1,xj,2,…,xj,p)subscript𝑥𝑗1subscript𝑥𝑗2…subscript𝑥𝑗𝑝(x_{j,1},x_{j,2},\\dots,x_{j,p}) consists of the resulting attribute values (of all the attributes) of prefix trace σj,j∈[1​…​l]subscript𝜎𝑗𝑗delimited-[]1…𝑙\\sigma_{j},j\\in[1\\dots l] with l𝑙l the number of prefix traces after prefix extraction. Note that the case ID attribute is removed from all the traces. This means that the total number of resulting attributes is p, and p=pa+ps+pd𝑝subscript𝑝𝑎subscript𝑝𝑠subscript𝑝𝑑p=p_{a}+p_{s}+p_{d}, with pasubscript𝑝𝑎p_{a}, pssubscript𝑝𝑠p_{s} and pdsubscript𝑝𝑑p_{d} the number of control flow, case, and event attributes after the data transformation steps respectively.\nThis additionally means that the tuple (xj,1​…​xj,pa)subscript𝑥𝑗1…subscript𝑥𝑗subscript𝑝𝑎(x_{j,1}\\dots x_{j,p_{a}}) denotes the attribute values of prefix trace σjsubscript𝜎𝑗\\sigma_{j} for the attribute type control flow, followed by (xj,pa+1​…​xj,pa+ps)subscript𝑥𝑗subscript𝑝𝑎1…subscript𝑥𝑗subscript𝑝𝑎subscript𝑝𝑠(x_{j,p_{a}+1}\\dots x_{j,p_{a}+p_{s}}) and (xj,pa+ps+1​…​xj,pa+ps+pd)subscript𝑥𝑗subscript𝑝𝑎subscript𝑝𝑠1…subscript𝑥𝑗subscript𝑝𝑎subscript𝑝𝑠subscript𝑝𝑑(x_{j,p_{a}+p_{s}+1}\\dots x_{j,p_{a}+p_{s}+p_{d}}) for the attribute type case and event respectively. Another data transformation step in the context of PPM is referred to as trace bucketing, where traces are divided into different buckets while creating separate models for each of them. This technique is commonly used to support the discovery of heterogeneous segments in the data 8 . Algorithms such as K-Nearest Neighbours 9 or K-Means clustering measure the (dis)similarity between traces depending on the parameter K. The prefix bucketing technique  creates different buckets for the prefixes of different lengths, while the state-based bucketing technique  creates a different model for each different decision point within the process model. Although these bucketing techniques can effectively diminish the runtime performance , they do not necessarily result in an intuitive or interpretable outcome. E.g., clustering techniques can base their grouping on a high number of dimensions that are not interpretable. Nonetheless, the use of clusters is deemed a necessary action when there is a notion of trace similarity . The final transformed event log is split into a train and test event log, where the former is used to create a task model to predict the dependent variable based on independent attributes. Moreover, the prediction for a prefix trace σjsubscript𝜎𝑗\\sigma_{j} is denoted as: y^i=F​(xj,1,xj,2,…,xj,p)subscript^𝑦𝑖𝐹subscript𝑥𝑗1subscript𝑥𝑗2…subscript𝑥𝑗𝑝\\hat{y}_{i}=F(x_{j,1},x_{j,2},\\dots,x_{j,p}). The final step is to interpret the predictions made by the task model F𝐹F. There already exist many model-agnostic methods that try to explain the behaviour of the task model. Moreover, attribution-based methods determine the contribution that each attribute made to the final prediction and use this as an explanation. Techniques such as accumulated local effect plots, surrogate models or permutation feature importance are often used in this context. For example, in [ref]42, they calculate the importance of an attribute by calculating the change in the prediction error of the model F𝐹F before and after permuting the values of an attribute. An attribute is deemed important when permuting the values of this attribute increases the prediction error (in mean squared error (MSE)), and is deemed unimportant when the MSE remains unchanged after permutation (as the model did not use this attribute for the prediction). The use of this technique is intuitive, as it clearly shows how much influence each attribute has. The use of this permutation-based attribute importance technique is described in Algorithm 1. Note that some remarks about the design of this algorithm should be made. First, in the case of ML models, permuting the values for original attributes, that are later transformed through frequency aggregation and/or summary statistics, does not make sense, as, e.g., the mean value of attribute values before and after permutation remains the same. Therefore, the attribute importance is calculated for the attributes after transformation (x1,…,xp)subscript𝑥1…subscript𝑥𝑝(x_{1},\\dots,x_{p}). On the other hand, in the case of DL models the attribute importance is calculated by permuting the original attributes. The attribute weights defined by this permutation attribute importance technique P​I𝑃𝐼PI are defined as: wP​I=wP​I,1,…,wP​I,psubscript𝑤𝑃𝐼subscript𝑤𝑃𝐼1…subscript𝑤𝑃𝐼𝑝w_{PI}=w_{PI,1},...,w_{PI,p}. Likewise, transparent models are able to produce their own attribute importances. As an example, the coefficients in a logistic regression model indicate the importances of the different attributes on the dependent variable. On the other hand, in case of a black box model, the attribute importances can also be measured with the use of a post-hoc explainability model, such as SHAP or LIME, which approximates the attribute weights of the black box model. The attribute importance calculated by the explainability model E𝐸E (can be both the transparent model itself, or a post-hoc explainability model) is denoted as: (wE,1​…​wE,psubscript𝑤𝐸1…subscript𝑤𝐸𝑝w_{E,1}\\dots w_{E,p}).\n\nReference markers found: [42]\n\nContext windows for each [ref]:\n[ref]42: final prediction and use this as an explanation. Techniques such as accumulated local effect plots, surrogate models or permutation feature importance are often used in this context. For example, in [ref]42, they calculate the importance of an attribute by calculating the change in the prediction error of the model F𝐹F before and after permuting the values of an attribute. An attribute is deemed\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 42,\n            \"titles\": [\n                \"Interpretation of Neural Networks is Fragile\",\n                \"On the structure of bottlenecks in processes\",\n                \"Inventory diagnosis for flow improvement—a design science approach\",\n                \"Process Mining - Data Science in Action, Second Edition\",\n                \"Reflections on the evolution of operations management\",\n                \"Outcome-Oriented Predictive Process Monitoring: Review and Benchmark\",\n                \"Machine learning in business process monitoring: a comparison of deep learning and classical approaches used for outcome prediction\",\n                \"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI\",\n                \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\",\n                \"The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies\",\n                \"Evaluating the quality of machine learning explanations: A survey on methods and metrics\",\n                \"Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice\",\n                \"A new hybrid classification algorithm for customer churn prediction based on logistic regression and decision trees\",\n                \"Generalized Linear Rule Models\",\n                \"A Systematic Review and Taxonomy of Explanations in Decision Support and Recommender Systems\",\n                \"Fundamentals of Business Process Management, Second Edition\",\n                \"Quantifying explainability in outcome-oriented predictive process monitoring\",\n                \"Complex symbolic sequence encodings for predictive monitoring of business processes\",\n                \"Intra and inter-case features in predictive process monitoring: A tale of two dimensions\",\n                \"Outcome-oriented predictive process monitoring with attention-based bidirectional lstm neural networks\",\n                \"Process Outcome Prediction: CNN vs. LSTM (with Attention)\",\n                \"Xai in the context of predictive process monitoring: An empirical analysis framework\",\n                \"Exploring Interpretability for Predictive Process Analytics\",\n                \"Explainable predictive process monitoring\",\n                \"Explainable artificial intelligence for process mining: A general overview and application of a novel local explanation approach for predictive process monitoring\",\n                \"Exploring interpretable predictive models for business processes\",\n                \"Explainable predictive business process monitoring using gated graph neural networks\",\n                \"Fox: a neuro-fuzzy model for process outcome prediction and explanation\",\n                \"Bayesian network based predictions of business processes\",\n                \"Towards Quantification of Explainability in Explainable Artificial Intelligence Methods\",\n                \"Quantifying model complexity via functional decomposition for better post-hoc interpretability\",\n                \"Attention is not explanation\",\n                \"Is Attention Interpretable?\",\n                \"Predictive and Causal Implications of using Shapley Value for Model Interpretation\",\n                \"Evaluating fidelity of explainable methods for predictive process analytics\",\n                \"Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges\",\n                \"Encoding resource experience for predictive process monitoring\",\n                \"Predicting Process Behaviour using Deep Learning\",\n                \"Clustering-based predictive process monitoring\",\n                \"Predictive monitoring of business processes\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]42 based on the context and research area of Explainability in Process Outcome Prediction: Guidelines to Obtain Interpretable and Faithful Models. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}