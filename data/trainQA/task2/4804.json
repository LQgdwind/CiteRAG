{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training\n\nPaper Abstract: \n\nPaper Section Text:\nThe following subsections detail FS-DETR’s architecture and main components. Template encoding: Let 𝐓i,j∈ℝHp×Wp×3,i∈{1,…,m},j∈{1,…,k}formulae-sequencesubscript𝐓𝑖𝑗superscriptℝsubscript𝐻𝑝subscript𝑊𝑝3formulae-sequence𝑖1…𝑚𝑗1…𝑘\\mathbf{T}_{i,j}\\in\\mathbb{R}^{H_{p}\\times W_{p}\\times 3},i\\in\\{1,\\dots,m\\},j\\in\\{1,\\dots,k\\}\nbe the template images of the available classes (sampled from Cb​a​s​esubscript𝐶𝑏𝑎𝑠𝑒C_{base} during training) where m𝑚m is the number of classes at the current training iteration (m𝑚m can vary), and k𝑘k is the number of examples per class (i.e. k-shot detection; k𝑘k can also vary). A CNN backbone (e.g. ResNet-50) generates template features 𝐗=CNN​(𝐓),𝐗∈ℝm​k×dformulae-sequence𝐗CNN𝐓𝐗superscriptℝ𝑚𝑘𝑑\\mathbf{X}=\\texttt{CNN}(\\mathbf{T}),\\;\\mathbf{X}\\in\\mathbb{R}^{mk\\times d} using either average or attention pooling (see Sec. 5). Pseudo-class embeddings: We propose to dynamically and randomly associate, at each training iteration, the k𝑘k template prompts in 𝐗𝐗\\mathbf{X} belonging to the i-th class (for that iteration) with a pseudo-class represented by a pseudo-class embedding 𝐜𝐢𝐬∈ℝdsubscriptsuperscript𝐜𝐬𝐢superscriptℝ𝑑\\mathbf{c^{s}_{i}}\\in\\mathbb{R}^{d}, which are added to the templates as follows: where 𝐂s∈ℝm​k×dsuperscript𝐂𝑠superscriptℝ𝑚𝑘𝑑\\mathbf{C}^{s}\\in\\mathbb{R}^{mk\\times d} contains the pseudo-class embeddings for all templates at the current iteration. The pseudo-class embeddings are initialised from a normal distribution and learned during training. They are not determined by the ground-truth categories and are class-agnostic. During each inference step, we arbitrarily associate to a template prompt (belonging to some class) the i​-th𝑖-thi\\textrm{-th} embedding as described by Eq. 1. The goal is to retrieve the pseudo-class i𝑖i. Note that the actual class information is not used. As the assigned embedding changes at every iteration, there is no correlation between the actual classes and the learned embeddings. See also Fig. 1 that exemplifies this process.\nIn the proposed FS-DETR, each decoded object query 𝐨isubscript𝐨𝑖\\mathbf{o}_{i} in 𝐎𝐎\\mathbf{O} will attempt to predict a pseudo-class using a classifier.\nPseudo-class embeddings\nadd a signature to each visual prompt allowing the network to track the template within and dissociate it from the rest of the templates belonging to a different class. As transformers are permutation invariant, this vectors are required in order to track the visual prompt within the model. Templates as visual prompts: We propose to provide the templates 𝐗ssuperscript𝐗𝑠\\mathbf{X}^{s} as visual prompts to the system by prepending them to the sequence of object queries fed to the decoder: As shown below, the templates will induce pseudo-class related information into the object queries via attention. This can be interpreted as a new form of training-aware soft-prompting [ref]30. FS-DETR  encoder: Given a target image 𝐈∈ℝH′×W′×3𝐈superscriptℝsuperscript𝐻′superscript𝑊′3\\mathbf{I}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times 3}, the same CNN backbone used for template feature extraction\nfirst\ngenerates image features 𝐙=CNN​(𝐈),𝐙∈ℝS×d,S=H×Wformulae-sequence𝐙CNN𝐈formulae-sequence𝐙superscriptℝ𝑆𝑑𝑆𝐻𝑊\\mathbf{Z}=\\texttt{CNN}(\\mathbf{I}),\\;\\mathbf{Z}\\in\\mathbb{R}^{S\\times d},\\;S=H\\times W, which are enriched with positional information through positional encodings 𝐙←𝐙+𝐏s,𝐏s∈ℝS×dformulae-sequence←𝐙𝐙subscript𝐏𝑠subscript𝐏𝑠superscriptℝ𝑆𝑑\\mathbf{Z}\\leftarrow\\mathbf{Z}+\\mathbf{P}_{s},\\;\\mathbf{P}_{s}\\in\\mathbb{R}^{S\\times d}. The features 𝐙𝐙\\mathbf{Z} are then processed by FS-DETR’s encoder layers in order to be enriched with global contextual information.\nThe l−limit-from𝑙l-th encoding layer processes the output features of the previous layer 𝐙l−1superscript𝐙𝑙1\\mathbf{Z}^{l-1} using a series of Multi-Head Self-Attention (MHSA), Layer Normalization (LN), and MLP layers (typical in  and [ref]3), as well as a newly proposed Multi-Head Cross-Attention (MHCA) layer as follows 222\nWe follow DETR’s notation\nwhere 𝐎′superscript𝐎′\\mathbf{O^{\\prime}} is added to L​N​(𝐕l−1)𝐿𝑁superscript𝐕𝑙1LN(\\mathbf{V}^{l-1}) and then projected to form the query Q and key K for self-attention.\nHere, the first layer 𝐕l−1superscript𝐕𝑙1\\mathbf{V}^{l-1} is initialised as [Xs,z​e​r​o​s]subscript𝑋𝑠𝑧𝑒𝑟𝑜𝑠[X_{s},zeros] while in DETR with z​e​r​o​s𝑧𝑒𝑟𝑜𝑠zeros.\n: The purpose of the MHCA layer above is to filter and highlight early on, before decoding, the image tokens of interest. We have found that such a layer noticeably increases few-shot accuracy (see also Section 5).\nFS-DETR’s encoder is implemented by stacking L=6𝐿6L=6 blocks, each following Eq. (3)-(5). As image tokens are permutation invariant, we followed [ref]3 and used a fixed positional encoding. For the templates, pseudo-class embeddings serve as positional encodings. FS-DETR  decoder: FS-DETR’s decoder accepts as input the concatenated templates and learnable object queries 𝐎′superscript𝐎′\\mathbf{O}^{\\prime} which are transformed\nby the decoder’s layers through self-attention and cross-attention layers in order to be eventually used for pseudo-class prediction and bounding box regression. The l−limit-from𝑙l-th decoding layer processes the output features of the previous layer 𝐕l−1superscript𝐕𝑙1\\mathbf{V}^{l-1} as follows: where 𝐕0=[𝐗s​zeros​(N,d)]superscript𝐕0delimited-[]superscript𝐗𝑠zeros𝑁𝑑\\mathbf{V}^{0}=\\large[\\mathbf{X}^{s}\\;\\;\\texttt{zeros}(N,d)\\large].\nNotably, different MLPs are used to process the decoder’s features 𝐕=[𝐕𝐗s​𝐕𝐎]𝐕delimited-[]subscript𝐕superscript𝐗𝑠subscript𝐕𝐎\\mathbf{V}=[\\mathbf{V}_{\\mathbf{X}^{s}}\\;\\mathbf{V}_{\\mathbf{O}}] corresponding to the templates 𝐕𝐗ssubscript𝐕superscript𝐗𝑠\\mathbf{V}_{\\mathbf{X}^{s}} and the object queries 𝐕𝐎subscript𝐕𝐎\\mathbf{V}_{\\mathbf{O}}: FS-DETR’s decoder consists of L=6𝐿6L=6 layers implemented using Eqs. (6)-(9). FS-DETR  training and loss functions: For each base class that exists in the target image, we create a template for that class by randomly sampling and cropping an object from that category using a different image (containing an object of the same class) from the train set. After applying image augmentation, the cropped object/template is passed through the CNN backbone of FS-DETR. For each target image and template i𝑖i (depicted in that image), the ground truth is yi=(cis,bi)subscript𝑦𝑖subscriptsuperscript𝑐𝑠𝑖subscript𝑏𝑖y_{i}=(c^{s}_{i},b_{i}), where cissubscriptsuperscript𝑐𝑠𝑖c^{s}_{i} is the target pseudo-class label (up to m𝑚m classes in total) and bi∈[ref]0 subscript𝑏𝑖superscript014b_{i}\\in[ref]0 ^{4} are the normalised bounding box coordinates. To calculate the loss for training FS-DETR, only the N𝑁N transformed object queries 𝐕𝐎L∈ℛN×dsubscriptsuperscript𝐕𝐿𝐎superscriptℛ𝑁𝑑\\mathbf{V}^{L}_{\\mathbf{O}}\\in\\mathcal{R}^{N\\times d} from the output of the last decoding layer are used for pseudo-class classification and bounding box regression (i.e. 𝐕𝐗sLsubscriptsuperscript𝐕𝐿superscript𝐗𝑠\\mathbf{V}^{L}_{\\mathbf{X}^{s}} is not used). To this end, pseudo-class and bounding box prediction heads are used to produce a set of N𝑁N predictions {y^i}i=1Nsuperscriptsubscriptsubscript^𝑦𝑖𝑖1𝑁\\{\\hat{y}_{i}\\}_{i=1}^{N} consisting of the pseudo-class probabilities p^i​(cs)subscript^𝑝𝑖superscript𝑐𝑠\\hat{p}_{i}(c^{s}) and the bounding box coordinates b^isubscript^𝑏𝑖\\hat{b}_{i}. The heads are implemented using a 3-layer MLP and ReLU activations. Similarly to [ref]3, we used an additional special pseudo-class ∅\\varnothing to denote tokens without valid object predictions.\nNote that as the training is done in a class-agnostic way via mapping of the base class templates to pseudo-classes (the actual class information is discarded) the model is capable to generalise to the unseen novel categories. Following [ref]3,\nbipartite matching is used to find an optimal permutation {y^σ​i}i=1Nsuperscriptsubscriptsubscript^𝑦𝜎𝑖𝑖1𝑁\\{\\hat{y}_{\\sigma{i}}\\}_{i=1}^{N}. Finally, the loss is: where IoU is the GIoU loss of  and λisubscript𝜆𝑖\\lambda_{i} are the loss weights. Pre-training: Transformers are generally more data hungry than CNNs due to their lack of inductive bias . Therefore,\nbuilding representations that generalise well to unseen data, and prevent overfitting within the DETR framework, requires larger amounts of data. To this end, we used images from ImageNet-100  and to some extent MSCOCO, for unsupervised pre-training where the classes and the bounding boxes are generated on-the-fly using an object proposal system, without using any labels. Note, that unlike all prior works, we make use of neg. templates as prompts, training the network using our proposed loss. See also the appendix.\n\nReference markers found: [30]\n\nContext windows for each [ref]:\n[ref]30: the decoder: As shown below, the templates will induce pseudo-class related information into the object queries via attention. This can be interpreted as a new form of training-aware soft-prompting [ref]30. FS-DETR  encoder: Given a target image 𝐈∈ℝH′×W′×3𝐈superscriptℝsuperscript𝐻′superscript𝑊′3\\mathbf{I}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times 3}, the same CNN backbone used for\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 30,\n            \"titles\": [\n                \"SSD: Single Shot MultiBox Detector\",\n                \"DETReg: Unsupervised Pretraining with Region Priors for Object Detection\",\n                \"Cascade R-CNN: Delving into High Quality Object Detection\",\n                \"End-to-end object detection with transformers\",\n                \"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\",\n                \"Up-DETR: Unsupervised pre-training for object detection with transformers\",\n                \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n                \"The Pascal Visual Object Classes Challenge: A Retrospective\",\n                \"The pascal visual object classes (VOC) challenge\",\n                \"Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector\",\n                \"Few-shot object detection with model calibration\",\n                \"Generalized few-shot object detection without forgetting\",\n                \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\",\n                \"Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning\",\n                \"Query Adaptive Few-Shot Object Detection with Heterogeneous Graph Convolutional Networks\",\n                \"Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment\",\n                \"Few-Shot Object Detection with Fully Cross-Transformer\",\n                \"Mask R-CNN\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"One-Shot Object Detection with Co-Attention and Co-Excitation\",\n                \"Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection\",\n                \"Visual Prompt Tuning\",\n                \"Few-shot Object Detection via Feature Reweighting\",\n                \"tSF: Transformer-based Semantic Filter for Few-Shot Learning\",\n                \"CornerNet: Detecting Objects as Paired Keypoints\",\n                \"Transformation invariant few-shot object detection\",\n                \"AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration\",\n                \"Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection\",\n                \"Focal Loss for Dense Object Detection\",\n                \"Microsoft COCO: Common Objects in Context\",\n                \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\",\n                \"Swin transformer: Hierarchical vision transformer using shifted windows\",\n                \"Decoupled Weight Decay Regularization\",\n                \"Conditional DETR for Fast Training Convergence\",\n                \"On the momentum term in gradient descent learning algorithms\",\n                \"DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection\",\n                \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\",\n                \"You Only Look Once: Unified, Real-Time Object Detection\",\n                \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\n                \"Generalized intersection over union: A metric and a loss for bounding box regression\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]30 based on the context and research area of FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}