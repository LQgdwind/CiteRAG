{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Sparse Spiking Neural-like Membrane Systems on Graphics Processing Units1footnote 11footnote 1When citing this paper, please use the following: J. HernÃ¡ndez-Tello, M.A. MartÃ­nez-del-Amor, D. Orellana-MartÃ­n, F.G.C. Cabarle, Sparse Spiking Neural-Like Membrane Systems on Graphics Processing Unit. International Journal of Neural Systems 34, 07 (2024), 2450038. https://doi.org/10.1142/S0129065724500382\n\nPaper Abstract: \n\nPaper Section Text:\nLet us briefly review the syntactical ingredients of a Spiking Neural P system with delays of degree qâ‰¥1ğ‘1q\\geq 1. It is a tuple Î =(O,sâ€‹yâ€‹n,Ïƒ1,â€¦,Ïƒq,ioâ€‹uâ€‹t)Î ğ‘‚ğ‘ ğ‘¦ğ‘›subscriptğœ1â€¦subscriptğœğ‘subscriptğ‘–ğ‘œğ‘¢ğ‘¡\\Pi=(O,syn,\\sigma_{1},\\dots,\\sigma_{q},i_{out}), where Oğ‘‚O is the singleton alphabet containing the spike symbol ağ‘a; sâ€‹yâ€‹nğ‘ ğ‘¦ğ‘›syn represents the arcs of a directed graph by means of a set of pairs (i,j)ğ‘–ğ‘—(i,j) (i.e. neuron iğ‘–i has a synapse with jğ‘—j), and reflexive synapses are not allowed, that is, synapses of the form (i, i); Ïƒ1,â€¦,Ïƒqsubscriptğœ1â€¦subscriptğœğ‘\\sigma_{1},\\dots,\\sigma_{q} are neurons of the form Ïƒi=(ni,Ri),1â‰¤iâ‰¤qformulae-sequencesubscriptğœğ‘–subscriptğ‘›ğ‘–subscriptğ‘…ğ‘–1ğ‘–ğ‘\\sigma_{i}=(n_{i},R_{i}),1\\leq i\\leq q, where nisubscriptğ‘›ğ‘–n_{i} is the initial number of spikes in the neuron, and Risubscriptğ‘…ğ‘–R_{i} is the finite set of rules associated to the neuron; ioâ€‹uâ€‹tsubscriptğ‘–ğ‘œğ‘¢ğ‘¡i_{out} is the label of the output neuron. Let us define mğ‘šm as the total amount of rules defined in Î Î \\Pi: m=âˆ‘i=1q|Ri|ğ‘šsuperscriptsubscriptğ‘–1ğ‘subscriptğ‘…ğ‘–m=\\sum_{i=1}^{q}|R_{i}| . Finally, The rules can be of two types: E/acâ†’ap;dâ†’ğ¸superscriptğ‘ğ‘superscriptğ‘ğ‘ğ‘‘E/a^{c}\\rightarrow a^{p};d (firing rule), with Eğ¸E being a regular expression over {a}ğ‘\\{a\\}, câ‰¥pâ‰¥1ğ‘ğ‘1c\\geq p\\geq 1, and dğ‘‘d the delay saying for how many steps the neuron is closed; or asâ†’Î»â†’superscriptğ‘ğ‘ ğœ†a^{s}\\rightarrow\\lambda (forgetting rule), for some sâ‰¥1ğ‘ 1s\\geq 1. More details about such expressions, syntax and semantics of SNP systems, are in   , a recent survey and the handbook[ref]29. Next, it is described the matrix representation for SNP systems with delays in order to enable a linear-algebra-based simulator. More details can be found in 5 9 . Many extensions have followed such as supporting non-determinism , but they are not covered in this work. The general matrix representation of standard SNP systems consists of mainly three structures: a Configuration Vector Cksubscriptğ¶ğ‘˜C_{k} (saying for each transition step kğ‘˜k, the amount of spikes in each neuron), a Spiking Vector Sksubscriptğ‘†ğ‘˜S_{k} (saying for each step kğ‘˜k if a rule is going to fire), and a Spiking Transition Matrix MÎ subscriptğ‘€Î M_{\\Pi} (saying how each rule affects each neuron). Their definitions are developed below, but this first view is enough to define how the configuration of a SNP system transits from step kâˆ’1ğ‘˜1k-1 to kğ‘˜k by simply: Ck=Skâ‹…MÎ +Ckâˆ’1subscriptğ¶ğ‘˜â‹…subscriptğ‘†ğ‘˜subscriptğ‘€Î subscriptğ¶ğ‘˜1C_{k}=S_{k}\\cdot M_{\\Pi}+C_{k-1} 5. Next, our matrix representation employed in parallel simulators for a SNP system with delays is presented. Let us assume a SNP system of degree qğ‘q with mğ‘šm rules. Moreover, for the sake of simplicity, current representation only allows the following three types of regular expressions in the rules: Type one (eâˆ—superscriptğ‘’e^{*}). Rule can be activated at any possible condition. Type two (e+superscriptğ‘’e^{+}). Rule may activate if its corresponding neuron contains at least 1 spike. Type three (ensuperscriptğ‘’ğ‘›e^{n}). Rule may activate if and only if its neuron contains exactly n spikes. Algorithm 1 is the pseudocode to perform one computation of a SNP system based on the matrix representation, and the following data structures: Spiking vector Sksubscriptğ‘†ğ‘˜S_{k} of length mğ‘šm, stores which rules are active at each computation step. Set of Spiking Vectors Sâ€‹Vkğ‘†subscriptğ‘‰ğ‘˜SV_{k}, which stores all spiking vectors that can be computed in a computation step kğ‘˜k. Rule Vector Râ€‹VÎ ğ‘…subscriptğ‘‰Î RV_{\\Pi}, of size mğ‘šm. It contains the information for each rule in the model. Each item contains the following data: The regular expression, which is composed of two elements: The multiplicity (Ensubscriptğ¸ğ‘›E_{n}). It indicates the minimum or the exact multiplicity of required spikes: 0 for type one, 1 for type two and nğ‘›n for type three. The type of regular expression (Eisubscriptğ¸ğ‘–E_{i}). It indicates if the expression is a minimum or an exact operation. 0 for types one and two, 1 for type three. The number of spikes cğ‘c that are consumed in the neuron containing the rule. The number of spikes pğ‘p sent. This element is skipped when the transition matrix contains it, i.e., for sparse and ELL formats (more in Section 4). The delay dğ‘‘d that is applied to the neuron when the rule is applied. The id of the neuron (nâ€‹iâ€‹dğ‘›ğ‘–ğ‘‘nid) that contains it. Neuron-Rule Map Vector NÎ subscriptğ‘Î N_{\\Pi}, which, as the name implies, maps each neuron Ïƒisubscriptğœğ‘–\\sigma_{i} to its rule set Risubscriptğ‘…ğ‘–R_{i}. NÎ â€‹[i]subscriptğ‘Î delimited-[]ğ‘–N_{\\Pi}[i] is the index of the first rule in that set and NÎ â€‹[i+1]âˆ’1subscriptğ‘Î delimited-[]ğ‘–11N_{\\Pi}[i+1]-1 is the last one. The vector contains q+1ğ‘1q+1 elements. It is initialized by adding to the index stored in NÎ â€‹[iâˆ’1]subscriptğ‘Î delimited-[]ğ‘–1N_{\\Pi}[i-1] the total number of rules stored in the neuron Ïƒiâˆ’1subscriptğœğ‘–1\\sigma_{i-1} (i.e NÎ â€‹[i]=NÎ â€‹[iâˆ’1]+|Riâˆ’1|subscriptğ‘Î delimited-[]ğ‘–subscriptğ‘Î delimited-[]ğ‘–1subscriptğ‘…ğ‘–1N_{\\Pi}[i]=N_{\\Pi}[i-1]+|R_{i-1}|, for 2â‰¤iâ‰¤q+12ğ‘–ğ‘12\\leq i\\leq q+1). Configuration vector Cksubscriptğ¶ğ‘˜C_{k}, of size qğ‘q. It stores, for each transition step kğ‘˜k, the number of spikes available to each neuron. C0subscriptğ¶0C_{0} corresponds to the initial configuration; i.e., C0â€‹[i]=nisubscriptğ¶0delimited-[]ğ‘–subscriptğ‘›ğ‘–C_{0}[i]=n_{i} for Ïƒi=(ni,Ri)subscriptğœğ‘–subscriptğ‘›ğ‘–subscriptğ‘…ğ‘–\\sigma_{i}=(n_{i},R_{i}). Transition matrix MÎ subscriptğ‘€Î M_{\\Pi}, of size qÃ—mğ‘ğ‘šq\\times m, in which information about the synapses and rules of the model is stored. Delays Vector Dksubscriptğ·ğ‘˜D_{k}, with qğ‘q elements (one per neuron), indicating the state (open or closed) of each neuron and, if closed, how many transition steps before it to reopens. At the beginning of the computation it will be necessary to initialize (INIT function) the Configuration vector (C0subscriptğ¶0C_{0}), the Transition Matrix (MÎ subscriptğ‘€Î M_{\\Pi}), Rule Vector (Râ€‹VÎ ğ‘…subscriptğ‘‰Î RV_{\\Pi}) and Neuron-Rule Map Vector (NÎ subscriptğ‘Î N_{\\Pi}) with the initial data of the system Î Î \\Pi. Moreover, the first Delays Vector (D0subscriptğ·0D_{0}) is initialized with zeroes. Once this is done, the main simulation loop starts. First, it will be necessary to calculate the set of all possible Spiking Vectors (Sâ€‹Vkğ‘†subscriptğ‘‰ğ‘˜SV_{k}) with the SV_CALC function, by using the rules information (Râ€‹VÎ ğ‘…subscriptğ‘‰Î RV_{\\Pi} and NÎ subscriptğ‘Î N_{\\Pi}) and the current configuration of the system (Cksubscriptğ¶ğ‘˜C_{k} and Dksubscriptğ·ğ‘˜D_{k}). From this set, a spiking vector (Sksubscriptğ‘†ğ‘˜S_{k}) will be randomly selected with the SAMPLE_ONE function, in order to simulate nondeterminism in the computation. Sksubscriptğ‘†ğ‘˜S_{k} will subsequently be used, together with the transition matrix (MÎ subscriptğ‘€Î M_{\\Pi}) and the rule vector (Râ€‹VÎ ğ‘…subscriptğ‘‰Î RV_{\\Pi}), to compute the next configuration vector (Ck+1subscriptğ¶ğ‘˜1C_{k+1}). This computation is carried on with the STEP function, and can be implemented with a single Matrix-Vector operation, that is, Ck+1=Skâ‹…MÎ +Cksubscriptğ¶ğ‘˜1â‹…subscriptğ‘†ğ‘˜subscriptğ‘€Î subscriptğ¶ğ‘˜C_{k+1}=S_{k}\\cdot M_{\\Pi}+C_{k}. Once this step is completed, the delays vector (Dk+1subscriptğ·ğ‘˜1D_{k+1}) gets updated with UPDATE_DELAYS, decreasing the delay counter for each closed neuron to indicate that a computation step has just been completed, or otherwise the delay is increased with the one of the rule executed (according to the Spiking Vector Sksubscriptğ‘†ğ‘˜S_{k}). The simulation will end either when a maximum number of steps (Lğ¿L) has been reached, or when no rule can be applied in the current configuration (the spiking vector is empty). In the case that the model makes use of delays, it will also be necessary to check that all neurons are open (the Delays Vector has only null values), in case some rules are pending to be executed. The list of all configuration vectors Cksubscriptğ¶ğ‘˜C_{k} is returned as output of the simulator, which corresponds to a computation (maybe truncated) of the SNP system.\n\nReference markers found: [29]\n\nContext windows for each [ref]:\n[ref]29: (forgetting rule), for some sâ‰¥1ğ‘ 1s\\geq 1. More details about such expressions, syntax and semantics of SNP systems, are in   , a recent survey and the handbook[ref]29. Next, it is described the matrix representation for SNP systems with delays in order to enable a linear-algebra-based simulator. More details can be found in 5 9 . Many extensions have\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 29,\n            \"titles\": [\n                \"A Systematic Survey of General Sparse Matrix-Matrix Multiplication\",\n                \"Spiking neural P systems\",\n                \"On applications of spiking neural P systems\",\n                \"Spiking neural P systems: theoretical results and applications\",\n                \"Spiking neural P systems with astrocytes\",\n                \"Spiking neural P systems with weights\",\n                \"Dendrite P systems\",\n                \"Spiking neural P systems with delay on synapses\",\n                \"A return to stochasticity and probability in spiking neural P systems\",\n                \"Spiking neural P systems with scheduled synapses\",\n                \"Spiking neural P systems with extended channel rules\",\n                \"Non-linear spiking neural P systems\",\n                \"An interactive timeline of simulators in membrane computing\",\n                \"An overview of hardware implementation of membrane computing models\",\n                \"Simulating P systems on GPU devices: a survey\",\n                \"Matrix representation of spiking neural P systems\",\n                \"Dendrite P systems toolbox: Representation, algorithms and simulators\",\n                \"Matrix representation and simulation algorithm of spiking neural P systems with structural plasticity\",\n                \"On solutions and representations of spiking neural P systems with rules on synapses\",\n                \"CuSNP: Spiking neural P systems simulators in CUDA\",\n                \"Handling non-determinism in spiking neural P systems: Algorithms and simulations\",\n                \"Improving GPU simulations of spiking neural P systems\",\n                \"Programming massively parallel processors: a hands-on approach\",\n                \"Understanding the efficiency of GPU algorithms for matrix-matrix multiplication\",\n                \"Optimizations in CUSNP simulator for spiking neural P systems on cuda GPUs\",\n                \"Improving GPU web simulations of spiking neural P systems\",\n                \"Simulation of spiking neural P systems with sparse matrix-vector operations\",\n                \"Some applications of spiking neural P systems.\",\n                \"Spiking neural P systems: main ideas and results\",\n                \"The Oxford Handbook of Membrane Computing\",\n                \"Performance analysis of sparse matrix-vector multiplication (spmv) on graphics processing units (gpus)\",\n                \"Uniform solutions to SAT and subset sum by spiking neural P systems\",\n                \"Implementing sorting networks with spiking neural P systems\",\n                \"On spiking neural membrane systems with neuron and synapse creation\",\n                \"Normal forms for spiking neural P systems and some of its variants\",\n                \"A framework for evolving spiking neural P systems\",\n                \"GPU implementation of evolving spiking neural P systems\",\n                \"Thinking About Spiking Neural P Systems: Some Theories, Tools, and Research Topics\",\n                \"On the tuning of the computation capability of spiking neural membrane systems with communication on request\",\n                \"Edge detection method based on nonlinear spiking neural systems\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]29 based on the context and research area of Sparse Spiking Neural-like Membrane Systems on Graphics Processing Units1footnote 11footnote 1When citing this paper, please use the following: J. HernÃ¡ndez-Tello, M.A. MartÃ­nez-del-Amor, D. Orellana-MartÃ­n, F.G.C. Cabarle, Sparse Spiking Neural-Like Membrane Systems on Graphics Processing Unit. International Journal of Neural Systems 34, 07 (2024), 2450038. https://doi.org/10.1142/S0129065724500382. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}