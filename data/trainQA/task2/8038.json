{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Sparse Spiking Neural-like Membrane Systems on Graphics Processing Units1footnote 11footnote 1When citing this paper, please use the following: J. Hernández-Tello, M.A. Martínez-del-Amor, D. Orellana-Martín, F.G.C. Cabarle, Sparse Spiking Neural-Like Membrane Systems on Graphics Processing Unit. International Journal of Neural Systems 34, 07 (2024), 2450038. https://doi.org/10.1142/S0129065724500382\n\nPaper Abstract: \n\nPaper Section Text:\nLet us briefly review the syntactical ingredients of a Spiking Neural P system with delays of degree q≥1𝑞1q\\geq 1. It is a tuple Π=(O,s​y​n,σ1,…,σq,io​u​t)Π𝑂𝑠𝑦𝑛subscript𝜎1…subscript𝜎𝑞subscript𝑖𝑜𝑢𝑡\\Pi=(O,syn,\\sigma_{1},\\dots,\\sigma_{q},i_{out}), where O𝑂O is the singleton alphabet containing the spike symbol a𝑎a; s​y​n𝑠𝑦𝑛syn represents the arcs of a directed graph by means of a set of pairs (i,j)𝑖𝑗(i,j) (i.e. neuron i𝑖i has a synapse with j𝑗j), and reflexive synapses are not allowed, that is, synapses of the form (i, i); σ1,…,σqsubscript𝜎1…subscript𝜎𝑞\\sigma_{1},\\dots,\\sigma_{q} are neurons of the form σi=(ni,Ri),1≤i≤qformulae-sequencesubscript𝜎𝑖subscript𝑛𝑖subscript𝑅𝑖1𝑖𝑞\\sigma_{i}=(n_{i},R_{i}),1\\leq i\\leq q, where nisubscript𝑛𝑖n_{i} is the initial number of spikes in the neuron, and Risubscript𝑅𝑖R_{i} is the finite set of rules associated to the neuron; io​u​tsubscript𝑖𝑜𝑢𝑡i_{out} is the label of the output neuron. Let us define m𝑚m as the total amount of rules defined in ΠΠ\\Pi: m=∑i=1q|Ri|𝑚superscriptsubscript𝑖1𝑞subscript𝑅𝑖m=\\sum_{i=1}^{q}|R_{i}| . Finally, The rules can be of two types: E/ac→ap;d→𝐸superscript𝑎𝑐superscript𝑎𝑝𝑑E/a^{c}\\rightarrow a^{p};d (firing rule), with E𝐸E being a regular expression over {a}𝑎\\{a\\}, c≥p≥1𝑐𝑝1c\\geq p\\geq 1, and d𝑑d the delay saying for how many steps the neuron is closed; or as→λ→superscript𝑎𝑠𝜆a^{s}\\rightarrow\\lambda (forgetting rule), for some s≥1𝑠1s\\geq 1. More details about such expressions, syntax and semantics of SNP systems, are in   , a recent survey and the handbook[ref]29. Next, it is described the matrix representation for SNP systems with delays in order to enable a linear-algebra-based simulator. More details can be found in 5 9 . Many extensions have followed such as supporting non-determinism , but they are not covered in this work. The general matrix representation of standard SNP systems consists of mainly three structures: a Configuration Vector Cksubscript𝐶𝑘C_{k} (saying for each transition step k𝑘k, the amount of spikes in each neuron), a Spiking Vector Sksubscript𝑆𝑘S_{k} (saying for each step k𝑘k if a rule is going to fire), and a Spiking Transition Matrix MΠsubscript𝑀ΠM_{\\Pi} (saying how each rule affects each neuron). Their definitions are developed below, but this first view is enough to define how the configuration of a SNP system transits from step k−1𝑘1k-1 to k𝑘k by simply: Ck=Sk⋅MΠ+Ck−1subscript𝐶𝑘⋅subscript𝑆𝑘subscript𝑀Πsubscript𝐶𝑘1C_{k}=S_{k}\\cdot M_{\\Pi}+C_{k-1} 5. Next, our matrix representation employed in parallel simulators for a SNP system with delays is presented. Let us assume a SNP system of degree q𝑞q with m𝑚m rules. Moreover, for the sake of simplicity, current representation only allows the following three types of regular expressions in the rules: Type one (e∗superscript𝑒e^{*}). Rule can be activated at any possible condition. Type two (e+superscript𝑒e^{+}). Rule may activate if its corresponding neuron contains at least 1 spike. Type three (ensuperscript𝑒𝑛e^{n}). Rule may activate if and only if its neuron contains exactly n spikes. Algorithm 1 is the pseudocode to perform one computation of a SNP system based on the matrix representation, and the following data structures: Spiking vector Sksubscript𝑆𝑘S_{k} of length m𝑚m, stores which rules are active at each computation step. Set of Spiking Vectors S​Vk𝑆subscript𝑉𝑘SV_{k}, which stores all spiking vectors that can be computed in a computation step k𝑘k. Rule Vector R​VΠ𝑅subscript𝑉ΠRV_{\\Pi}, of size m𝑚m. It contains the information for each rule in the model. Each item contains the following data: The regular expression, which is composed of two elements: The multiplicity (Ensubscript𝐸𝑛E_{n}). It indicates the minimum or the exact multiplicity of required spikes: 0 for type one, 1 for type two and n𝑛n for type three. The type of regular expression (Eisubscript𝐸𝑖E_{i}). It indicates if the expression is a minimum or an exact operation. 0 for types one and two, 1 for type three. The number of spikes c𝑐c that are consumed in the neuron containing the rule. The number of spikes p𝑝p sent. This element is skipped when the transition matrix contains it, i.e., for sparse and ELL formats (more in Section 4). The delay d𝑑d that is applied to the neuron when the rule is applied. The id of the neuron (n​i​d𝑛𝑖𝑑nid) that contains it. Neuron-Rule Map Vector NΠsubscript𝑁ΠN_{\\Pi}, which, as the name implies, maps each neuron σisubscript𝜎𝑖\\sigma_{i} to its rule set Risubscript𝑅𝑖R_{i}. NΠ​[i]subscript𝑁Πdelimited-[]𝑖N_{\\Pi}[i] is the index of the first rule in that set and NΠ​[i+1]−1subscript𝑁Πdelimited-[]𝑖11N_{\\Pi}[i+1]-1 is the last one. The vector contains q+1𝑞1q+1 elements. It is initialized by adding to the index stored in NΠ​[i−1]subscript𝑁Πdelimited-[]𝑖1N_{\\Pi}[i-1] the total number of rules stored in the neuron σi−1subscript𝜎𝑖1\\sigma_{i-1} (i.e NΠ​[i]=NΠ​[i−1]+|Ri−1|subscript𝑁Πdelimited-[]𝑖subscript𝑁Πdelimited-[]𝑖1subscript𝑅𝑖1N_{\\Pi}[i]=N_{\\Pi}[i-1]+|R_{i-1}|, for 2≤i≤q+12𝑖𝑞12\\leq i\\leq q+1). Configuration vector Cksubscript𝐶𝑘C_{k}, of size q𝑞q. It stores, for each transition step k𝑘k, the number of spikes available to each neuron. C0subscript𝐶0C_{0} corresponds to the initial configuration; i.e., C0​[i]=nisubscript𝐶0delimited-[]𝑖subscript𝑛𝑖C_{0}[i]=n_{i} for σi=(ni,Ri)subscript𝜎𝑖subscript𝑛𝑖subscript𝑅𝑖\\sigma_{i}=(n_{i},R_{i}). Transition matrix MΠsubscript𝑀ΠM_{\\Pi}, of size q×m𝑞𝑚q\\times m, in which information about the synapses and rules of the model is stored. Delays Vector Dksubscript𝐷𝑘D_{k}, with q𝑞q elements (one per neuron), indicating the state (open or closed) of each neuron and, if closed, how many transition steps before it to reopens. At the beginning of the computation it will be necessary to initialize (INIT function) the Configuration vector (C0subscript𝐶0C_{0}), the Transition Matrix (MΠsubscript𝑀ΠM_{\\Pi}), Rule Vector (R​VΠ𝑅subscript𝑉ΠRV_{\\Pi}) and Neuron-Rule Map Vector (NΠsubscript𝑁ΠN_{\\Pi}) with the initial data of the system ΠΠ\\Pi. Moreover, the first Delays Vector (D0subscript𝐷0D_{0}) is initialized with zeroes. Once this is done, the main simulation loop starts. First, it will be necessary to calculate the set of all possible Spiking Vectors (S​Vk𝑆subscript𝑉𝑘SV_{k}) with the SV_CALC function, by using the rules information (R​VΠ𝑅subscript𝑉ΠRV_{\\Pi} and NΠsubscript𝑁ΠN_{\\Pi}) and the current configuration of the system (Cksubscript𝐶𝑘C_{k} and Dksubscript𝐷𝑘D_{k}). From this set, a spiking vector (Sksubscript𝑆𝑘S_{k}) will be randomly selected with the SAMPLE_ONE function, in order to simulate nondeterminism in the computation. Sksubscript𝑆𝑘S_{k} will subsequently be used, together with the transition matrix (MΠsubscript𝑀ΠM_{\\Pi}) and the rule vector (R​VΠ𝑅subscript𝑉ΠRV_{\\Pi}), to compute the next configuration vector (Ck+1subscript𝐶𝑘1C_{k+1}). This computation is carried on with the STEP function, and can be implemented with a single Matrix-Vector operation, that is, Ck+1=Sk⋅MΠ+Cksubscript𝐶𝑘1⋅subscript𝑆𝑘subscript𝑀Πsubscript𝐶𝑘C_{k+1}=S_{k}\\cdot M_{\\Pi}+C_{k}. Once this step is completed, the delays vector (Dk+1subscript𝐷𝑘1D_{k+1}) gets updated with UPDATE_DELAYS, decreasing the delay counter for each closed neuron to indicate that a computation step has just been completed, or otherwise the delay is increased with the one of the rule executed (according to the Spiking Vector Sksubscript𝑆𝑘S_{k}). The simulation will end either when a maximum number of steps (L𝐿L) has been reached, or when no rule can be applied in the current configuration (the spiking vector is empty). In the case that the model makes use of delays, it will also be necessary to check that all neurons are open (the Delays Vector has only null values), in case some rules are pending to be executed. The list of all configuration vectors Cksubscript𝐶𝑘C_{k} is returned as output of the simulator, which corresponds to a computation (maybe truncated) of the SNP system.\n\nReference markers found: [29]\n\nContext windows for each [ref]:\n[ref]29: (forgetting rule), for some s≥1𝑠1s\\geq 1. More details about such expressions, syntax and semantics of SNP systems, are in   , a recent survey and the handbook[ref]29. Next, it is described the matrix representation for SNP systems with delays in order to enable a linear-algebra-based simulator. More details can be found in 5 9 . Many extensions have\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 29,\n            \"titles\": [\n                \"A Systematic Survey of General Sparse Matrix-Matrix Multiplication\",\n                \"Spiking neural P systems\",\n                \"On applications of spiking neural P systems\",\n                \"Spiking neural P systems: theoretical results and applications\",\n                \"Spiking neural P systems with astrocytes\",\n                \"Spiking neural P systems with weights\",\n                \"Dendrite P systems\",\n                \"Spiking neural P systems with delay on synapses\",\n                \"A return to stochasticity and probability in spiking neural P systems\",\n                \"Spiking neural P systems with scheduled synapses\",\n                \"Spiking neural P systems with extended channel rules\",\n                \"Non-linear spiking neural P systems\",\n                \"An interactive timeline of simulators in membrane computing\",\n                \"An overview of hardware implementation of membrane computing models\",\n                \"Simulating P systems on GPU devices: a survey\",\n                \"Matrix representation of spiking neural P systems\",\n                \"Dendrite P systems toolbox: Representation, algorithms and simulators\",\n                \"Matrix representation and simulation algorithm of spiking neural P systems with structural plasticity\",\n                \"On solutions and representations of spiking neural P systems with rules on synapses\",\n                \"CuSNP: Spiking neural P systems simulators in CUDA\",\n                \"Handling non-determinism in spiking neural P systems: Algorithms and simulations\",\n                \"Improving GPU simulations of spiking neural P systems\",\n                \"Programming massively parallel processors: a hands-on approach\",\n                \"Understanding the efficiency of GPU algorithms for matrix-matrix multiplication\",\n                \"Optimizations in CUSNP simulator for spiking neural P systems on cuda GPUs\",\n                \"Improving GPU web simulations of spiking neural P systems\",\n                \"Simulation of spiking neural P systems with sparse matrix-vector operations\",\n                \"Some applications of spiking neural P systems.\",\n                \"Spiking neural P systems: main ideas and results\",\n                \"The Oxford Handbook of Membrane Computing\",\n                \"Performance analysis of sparse matrix-vector multiplication (spmv) on graphics processing units (gpus)\",\n                \"Uniform solutions to SAT and subset sum by spiking neural P systems\",\n                \"Implementing sorting networks with spiking neural P systems\",\n                \"On spiking neural membrane systems with neuron and synapse creation\",\n                \"Normal forms for spiking neural P systems and some of its variants\",\n                \"A framework for evolving spiking neural P systems\",\n                \"GPU implementation of evolving spiking neural P systems\",\n                \"Thinking About Spiking Neural P Systems: Some Theories, Tools, and Research Topics\",\n                \"On the tuning of the computation capability of spiking neural membrane systems with communication on request\",\n                \"Edge detection method based on nonlinear spiking neural systems\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]29 based on the context and research area of Sparse Spiking Neural-like Membrane Systems on Graphics Processing Units1footnote 11footnote 1When citing this paper, please use the following: J. Hernández-Tello, M.A. Martínez-del-Amor, D. Orellana-Martín, F.G.C. Cabarle, Sparse Spiking Neural-Like Membrane Systems on Graphics Processing Unit. International Journal of Neural Systems 34, 07 (2024), 2450038. https://doi.org/10.1142/S0129065724500382. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}