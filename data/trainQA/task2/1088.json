{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: On the Robustness of Noise-Blind Low-Rank Recovery from Rank-One Measurements\n\nPaper Abstract: \n\nPaper Section Text:\nInitiated by the seminal works introducing the idea of compressive sensing   , the problem of recovering structured data from an underdetermined system random linear measurements has been subject of intensive study in mathematics, signal processing, and computer science in recent years. Two classes of structural models that have proven particularly useful are variants of sparsity, where the signal is assumed to have only few significant coefficients in an appropriate basis, and low-rank models, where a matrix-valued signal is assumed to be well-approximated by a matrix of rank much less than the dimension. In both cases, the measurements are of the form where 𝒜𝒜\\mathcal{A} is a (random) linear operator, which maps the unknown structured object X𝑋X to the observed measurements b𝑏b and w𝑤w is a noise. One typically assumes that the number of measurement is much smaller than the signal dimension, which is why such measurement scenarios (especially in combination with a sparsity assumption) are commonly referred to as compressive sensing. It is not difficult to show that for both the sparsity and the low-rank model, the object that is extremal with regards to the underlying structure, i.e., has fewest non-zero coefficients or smallest rank, respectively, is indeed the correct solution in many cases if there is no additive noise. That is, in case of sparse recovery with w=0𝑤0w=0 one can often recover X𝑋X by solving with ∥Z∥ℓ0subscriptdelimited-∥∥𝑍subscriptℓ0\\left\\lVert Z\\right\\rVert_{\\ell_{0}} denoting the number of non-zero entries of Z𝑍Z, and for noiseless low-rank matrix recovery X𝑋X can typically be recovered via the problem Both these problems, however, are non-convex optimization problems and, in general, NP-hard  . However, tractable reconstruction of X𝑋X via appropriate convex relaxations is provably possible  if 𝒜𝒜\\mathcal{A} possesses the null space property  or the restricted isometry property . If X𝑋X is sparse, one can reconstruct it in the noiseless case (w=0𝑤0w=0) as a minimizer of which can be efficiently solved numerically . If X𝑋X is of low rank, one can find it via the second order cone problem where the nuclear norm ∥⋅∥S1\\|\\cdot\\|_{S_{1}} of a matrix is defined to be the sum of its singular values. Subsequent works in the field gave an answer to the questions that followed: Will the reconstruction work if the assumption of sparsity or low rank is only approximately satisfied  ? Can X𝑋X be reconstructed robustly in the presence of noise with known strength as constraints in (2) or (3) are no longer valid  ? What if the strength is unknown or underestimated  ? Can one blindly apply (2) or (3) in the presence of noise and still obtain reasonable reconstruction   ? The answer the to the first two question is “yes”, if 𝒜𝒜\\mathcal{A} possesses the robust null space property , which is known to be fulfilled for many random matrices with high probability, mostly matrices with independent rows. The last two questions, however, cannot be answered affirmatively in general if 𝒜𝒜\\mathcal{A} fulfills only a suitable restricted isometry property   or robust null space property   with respect to the underlying structure. On the other hand, if in addition 𝒜𝒜\\mathcal{A} fulfills the so-called quotient property   (see Section 1.5 below), the last two questions can be answered with “yes” in the case of the sparse recovery problem, i.e., if X𝑋X is a sparse vector. For low-rank matrix recovery answers to question Q3 are only available in limited classes of scenarios and Q4 has hardly been studied in the literature. For sparse recovery, in contrast, these questions are much better understood, in parts also because the quotient property is closely related to well-studied questions in the field of high-dimensional geometry . As the null space property, the quotient property is typically established via random constructions, and the choice of the random distribution of 𝒜𝒜\\mathcal{A} is critical. In contrast to the null space property, however, these constructions often rely on the independence of the columns rather than the rows. The quotient property adapted for eq. 2 has been studied in a number of works over the recent years,\nextending the seminal results for Gaussian matrices  to matrices with i.i.d. subgaussian entries such as Bernoulli , Weibull matrices , log-concave distributions , and, more recently, even more heavy-tailed distributions including Cauchy random vectors . When the random matrix has independent entries (and, consequently, both independent rows to yield the quotient property and independent rows to yield the null space property), these results can be combined with results about the null space property to yield noise-blind recovery guarantees, e.g., for Gaussian  and subgaussian random matrices . As such bounds require a logarithmic number of finite moments , the resulting guarantees for noise-blind sparse recovery require slightly stronger conditions on the tail decay of the random matrix entries than the quotient property by itself. Similar results about null space and quotient properties hold for random measurement operators based on independent matrices with i.i.d. subgaussian entries and can be used to characterize the performance of eq. 3 for low-rank matrix recovery. For most application scenarios\nof sparse recovery,\nthe assumption of measurement operators with independent entries is too restrictive. In more realistic measurement scenarios, one encounters structural constraints imposed by the application. Examples for sparse recovery include Fourier structure [ref]58, combinations of Fourier and wavelet structure  3 as motivated, e.g., by magnetic resonance imaging \nand convolutional structure as motivated, e.g., by channel identification   . Such structured scenarios are also preferred for reasons of reduced computational complexity , but their theoretical analysis pose considerable challenges as compared to unstructured random ones 7 . Structural constraints are even more significant in the low-rank matrix recovery as the matrices that constitute the linear operator 𝒜𝒜\\mathcal{A} of eq. 1 are in many applications not modeled very accurately by random matrices with i.i.d. entries. For example, in recommender systems, it is common to model a user-product matrix as approximately low-rank , which is known only through a subset of its entries, often assumed to be revealed at random locations 1 7. This corresponds to a low-rank matrix recovery problem with an operator 𝒜𝒜\\mathcal{A} whose constituent matrices do not have i.i.d. entries, but are outer products of random standard basis vectors. Other relevant structured measurement scenarios for low-rank matrix recovery include symmetric rank-one measurements (see also Section 1.2 below), as related to phase retrieval , often in combination with additional structure such as coded diffraction patterns 0 or non-symmetric rank-one and low-rank measurements as encountered in blind deconvolution  and blind demixing [ref]38, and various structural models related to quantum state tomography  8 . Despite this large variety of relevant structured measurement scenarios, the study of the quotient property with structure is only in its beginnings. To our knowledge, there are two main contributions to mention here:  has provided such an estimate for sparse recovery with Fourier structure, and  observed for low-rank matrix recovery from Pauli measurements the quotient property directly follows from orthogonality. One should mention though that solving (2) is not the only way to attempt noise-blind recovery. For measurement systems satisfying the restricted isometry property, recovery guarantees have been established for various greedy and greedy-type methods such as orthogonal matching pursuit 2 and compressive sampling matching pursuit , which do not require a prior knowledge of the noise level, but instead require an upper bound on the sparsity level, which may also not be available in all cases. Furthermore, the square root lasso, a variant of (2) given by the minimization problem has been shown to yield recovery with a parameter λ𝜆\\lambda independent of the noise level under restricted eigenvalue conditions , and also under the robust null space property . For the low-rank matrix recovery problem, the restricted isometry property has mainly been established for unstructured measurement systems (with the notable exception of Pauli measurements as relevant in quantum information theory ), so the guarantees for greedy and greedy-type methods have limited applicability. While an approach analogous to the square root lasso also allows for noise-blind recovery when the noise is random  – for example for matrix completion – no analysis based on the robust null space property analogous to  is available yet.\nFor the phase retrieval problem, where the solution is known to be positive semidefinite (see below for more details), it has been shown that noise-blind recovery at near-optimal rate can be achieved by completely ignoring the nuclear norm objective of eq. 3 and just optimizing the data fidelity over the positive semidefinite cone 3  . Despite the availability of alternative approaches for a number of scenarios, however, we feel that understanding the quotient property of structured random measurement systems is of interest both from the mathematical point of view – given the fundamental role of algorithms (2) and (3) as a benchmark – and from the viewpoint of applications, given that these approaches form the basis of competitive solution approaches such as Iteratively Reweighted Least Squares 2 9 6 5, and hence their understanding sheds light on these methods as well.\n\nReference markers found: [58]\n\nContext windows for each [ref]:\n[ref]58: entries is too restrictive. In more realistic measurement scenarios, one encounters structural constraints imposed by the application. Examples for sparse recovery include Fourier structure [ref]58, combinations of Fourier and wavelet structure  3 as motivated, e.g., by magnetic resonance imaging \nand convolutional structure as motivated, e.g., by channel identification   . Such\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 58,\n            \"titles\": [\n                \"Regularization-free estimation in trace regressionwith symmetric positive semidefinite matrices\",\n                \"Breaking the coherence barrier: A new theory for compressed sensing\",\n                \"Blind deconvolution using convex programming\",\n                \"Compressive wideband power spectrum estimation\",\n                \"Square-root lasso: pivotal recovery of sparse signals via conic programming\",\n                \"Matrix Analysis, volume 169 of Graduate Texts in Mathematics\",\n                \"Robustness to unknown error in sparse regularization\",\n                \"The restricted isometry property and its implications for compressed sensing\",\n                \"Phase Retrieval via Matrix Completion\",\n                \"Solving quadratic equations via phaselift when there are about as many equations as unknowns\",\n                \"Phase retrieval from coded diffraction patterns\",\n                \"Matrix completion with noise\",\n                \"Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements\",\n                \"Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information\",\n                \"Stable signal recovery from incomplete and inaccurate measurements\",\n                \"Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming\",\n                \"Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?\",\n                \"The power of convex relaxation: Near-optimal matrix completion\",\n                \"Exact and stable covariance estimation from quadratic sampling via convex programming\",\n                \"The distribution of quadratic forms in a normal system, with applications to the analysis of covariance\",\n                \"Compressed sensing and best k-term approximation\",\n                \"Asymptotic shape of a random polytope in a convex body\",\n                \"Iteratively reweighted least squares minimization for sparse recovery\",\n                \"Stable optimizationless recovery from phaseless linear measurements\",\n                \"Instance-optimality in probability with an ℓ1subscriptℓ1\\\\ell_{1}-minimization decoder\",\n                \"Compressed sensing\",\n                \"Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization\",\n                \"Matrix Rank Minimization with Applications\",\n                \"Quantum tomography via compressed sensing: error bounds, sample complexity and efficient estimators\",\n                \"Low-rank matrix recovery via iteratively reweighted least squares minimization\",\n                \"Stability and robustness of ℓ1subscriptℓ1\\\\ell_{1}-minimizations with weibull matrices and redundant dictionaries\",\n                \"A mathematical introduction to compressive sensing\",\n                \"High dimensional matrix estimation with unknown variance of the noise\",\n                \"Extremal Properties of Orthogonal Parallelepipeds and Their Applications to the Geometry of Banach Spaces\",\n                \"CVX: Matlab Software for Disciplined Convex Programming, Version 2.2\",\n                \"Quantum state tomography via compressed sensing\",\n                \"On the geometry of polytopes generated by heavy-tailed random vectors\",\n                \"Toeplitz compressed sensing matrices with applications to sparse channel estimation\",\n                \"Blind demixing and deconvolution at near-optimal rate\",\n                \"Stable low-rank matrix recovery via null space properties\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]58 based on the context and research area of On the Robustness of Noise-Blind Low-Rank Recovery from Rank-One Measurements. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}