{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images\n\nPaper Abstract: \n\nPaper Section Text:\nOur observations of the transfer performance dependency on the pre-training model and data scale and on source and target domain alignment suggest that both intra- and inter-domain transfer benefit from larger pre-training scale. The effect of pre-training scale depends however on transfer conditions, revealing a differentiated picture of when larger scale may lead to transfer improvement. Larger pre-training scale improves intra-domain transfer We obtain evidence that both natural-natural domain transfer and medical-medical domain transfer are improved when increasing model and data size during pre-training (Tab. 2; see also Suppl. material). For natural-natural transfer scenario, the improvement is evident for both full and few-shot regime. Increasing data size by using ImageNet-21k instead of ImageNet-1k or increasing network model size by using ResNet-152x4 instead of smaller ResNet-50x1 creates strong, consistent boost in transfer performance across all natural target datasets, which is in line with previous observations  . Improvement is especially pronounced in few-shot regime (e.g, Fig. 1(a), see also Suppl. material), also adding evidence for more data efficient transfer due to larger scale. The picture is more differentiated for medical-medical transfer scenario. For full transfer regime, the improvement due to larger pre-training scale is clear and consistent across different targets (Tab. 2). In few-shot transfer however, in contrast to natural-natural scenario (Fig. 1(a)), there are no benefits due to larger scale (Fig. 1(b), 2(b), 2(d), 2(f)). Here we have to keep in mind that both absolute data size and increase in data scale we obtain by going from ImageNet-1k to ImageNet-21k (14M samples) in natural pre-training is much stronger than what we achieve in medical pre-training, going from one of X-Ray datasets to the largest combined X-Ray superset that still has much smaller data volume (≈\\approx 870k samples) than ImageNet-21k. This difference in data scale may also explain the observed differences in few-shot regime, while we also cannot rule out that domain type (natural or medical) could as well play an important role in determining how transfer is affected by pre-training scale. Larger pre-training scale improves inter-domain transfer for larger targets. In contrast to intra-domain transfer, where natural-natural or medical-medical source and target datasets are closely related, in natural-medical inter-domain transfer the source and target are much further apart. It is therefore not trivial that effect of pre-training scale brings similar or any improvement in this case as well. Strong discrepancy between source and target may render transfer ineffective, as it was indeed observed in previous studies on natural-medical transfer done on smaller scales . In contrast to these studies, we do find here significant positive effect of larger pre-training scale on inter-domain natural-medical full shot transfer for larger medical targets (Tab. 2; Fig. 1(b), 2(a), see also Suppl. material). The transfer improvement is clearly expressed when increasing both model and natural image data pre-training scale across all large X-Ray targets. Interestingly, the effect of scale is still clearly expressed even when conducting inter-domain transfer in a strongly unfavorable setting, pre-training on narrow domain-specific X-Ray data and transferring to generic natural images (grayed out area in Tab. 2). This indicates that scale can still have beneficial effect even in situations where transfer scenario itself is ill-posed. Another remarkable finding arises when further comparing performance of intra-domain medical-medical and inter-domain natural-medical transfer. The largest ResNet-152x4 pre-trained on the largest generic natural ImageNet-21k turns out to be as good or in many cases better than any network pre-trained on the largest available medical domain specific X-Ray superset data when performing full shot transfer to large X-Ray targets (Tab. 2, Figs. 1(b), 2(a), 2(b)). The finding indicates that by substantially increasing model and generic source natural image data scale during pre-training, we can obtain models for transfer to medical domain-specific X-Ray images that match or even outperform models pre-trained with large amounts of domain-specific X-Ray data, which may be often not available in practice. The finding also strengthens previously inconclusive evidence for benefits of large-scale pre-training on generic natural images when transferring to medical targets , providing direct comparison between models pre-trained either on out-of-domain natural images or on in-domain medical X-Ray chest imaging data when transferring to medical image targets. The observation expands the previous evidence that strongly increasing model and data pre-training scale improves transfer also for larger out-of-domain targets - scale of generic ImageNet-21k (14M samples) is more than order of magnitude larger than scale of the largest medical X-Ray superset we have constructed for this study (≈\\approx 0.87M samples). For few-shot inter-domain transfer or for transfer on smaller targets, we do not see consistent positive effect of larger pre-training scale (Figs. 1(b), 2). Thus, we again obtain differential picture of scaling benefits depending on transfer scenario. Further scaling up of model and data during pre-training may homogenize this picture and make scaling benefit look more consistent through different conditions, showing improvement for smaller targets and for few-shot transfer. For instance, we could not use substantially larger datasets like JFT-300M or JFT-3B [ref]29  [ref]30 which are proprietary and not available publicly. Additionally, computational budget was here not enough to experiment with networks larger than ResNet-152x4. However, there may be also fundamental limitations prohibiting transfer improvement no matter how large pre-training scale may become that are due to strong incompatibility between source and target domains. There is some evidence from language modeling studies that hints on such fundamental limitations for transfer improvement on target datasets far from source when doing straightforward scaling without further changes in model architecture. For instance, the work by Hendrycks et al.  finds no improvement of transfer when increasing size of large Transformer networks pre-trained on very large conventional text datasets and fine-tuning those on a specific target dataset (mathematical text tasks of advanced difficulty) far apart from the source. Limitations of the current study. There are several limitations of the current study that impede more general conclusions about effect of pre-training scale on intra- and inter-domain transfer from the observations made in this study. In the conducted transfer experiments, we made use of a heuristic hyper-parameter selection rule - BiT-HyperRule, as introduced in  - that determines pre-training hyper-parameters directly from target datasets on which transfer is to be performed. This rule may be heavily biased towards transfer on natural image datasets, as those were the targets used in the original study. If modifying the rule to take also target domain - natural or medical - into account, the derived hyper-parameters may serve a much better basis for fine-tuning during transfer. In general, performing hyper-parameter tuning for training procedure can strongly boost performance , and this is no different for transfer procedure. Therefore, it cannot be excluded that performing hyper-parameter tuning for each transfer task would alter the effect of larger pre-training scale on transfer. Hyper-parameter tuning would however also impose further cost on transfer that is avoided by employing the hyper-rule. We also have not explored other backbone network architectures except the standard ResNet. Although ResNet has proven itself a versatile network architecture for dealing with various vision tasks, it cannot be ruled out that while for instance the inductive bias inherent to its convolutional design is well suited for working on natural image statistics with strong local spatial correlations, it may be less suited for providing good basis for generalization when dealing with other types of image signals. Scaling up ResNet architecture may thus be a viable strategy to improve generalization capability on natural image data, while other, more generic architectures, may be required to benefit from scaling in the same way across more diverse data types. We also did not experiment with larger datasets than ImageNet-21k - as those are still mostly proprietary and were not publicly available, as it is the case for JFT-300M [ref]29  or JFT-3B [ref]30. For medical imaging domain, we could not experiment with increasing data scale substantially, as the amount of openly available X-Ray chest data is currently still limited. Finally, we studied the dependence of transfer improvement on pre-training scale exclusively in supervised classification problem setting. Promising work is also done on pre-training in unsupervised fashion with unlabeled data  [ref]49 , where benefits of scaling up pre-training for transfer may turn out to be as well substantial. Conclusion and outlook. Here, we presented here evidence that substantially increasing model and data scale in the pre-training provides benefits for both intra- and inter-domain transfer across various target datasets from natural and medical X-Ray image domain. The effect of pre-training scale on transfer performance depends on transfer scenario. Transfer improvement due to larger pre-training scale was found to be substantial in natural-natural or medical-medical, intra-domain transfer scenarios where source and target datasets were closely related, being especially strongly pronounced in the few-shot transfer regime for natural-natural case and concentrated in full-shot scenario in medical-medical case. For natural-medical inter-domain transfer, clear positive effect of larger pre-training scale was found for full shot transfer on large X-Ray targets. On small X-Ray targets and for few-shot transfer regime, no clear inter-domain transfer improvements were observed. Remarkably, the largest ResNet-152x4 network pre-trained on very large generic natural ImageNet-21k matched or even outperformed networks pre-trained on largest medical domain-specific X-Ray superset data combined for this study when performing full shot transfer to large X-Ray targets. This is relevant for the practice, as large amount of medical domain-specific data is often not available for pre-training. Here we show that high quality models for large X-Ray targets can be also obtained by substantially increasing pre-training model and generic natural image source data scale instead, obliterating need for large domain-specific data. The study offers different follow-up directions. One of these are experiments with larger scale both for network and data size, for instance going beyond ImageNet-21k, or combining in the pre-training different source datasets that may contain both natural and medical images. This may also include experiments with scaling up other architectures than ResNet. Another direction to study effect of scale is to employ various unsupervised learning strategies for pre-training  [ref]49  instead of supervised learning. Yet another fruitful path is to provide a measure of source and target domain similarity and to experiment with more than two distinct domains, varying systematically relatedness between different source and target data. First steps in this direction for language modeling was already undertaken in . Following these directions would pave the path towards scaling laws for transfer in the image domain, taking into account different pre-training regimes and affinity between source and target domains, to enable systematic prediction of transfer performance and improvement due to increase of pre-training scale.\n\nReference markers found: [29]\n\nContext windows for each [ref]:\n[ref]29: more consistent through different conditions, showing improvement for smaller targets and for few-shot transfer. For instance, we could not use substantially larger datasets like JFT-300M or JFT-3B [ref]29  [ref]30 which are proprietary and not available publicly. Additionally, computational budget was here not enough to experiment with networks larger than ResNet-152x4. However, there may be\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 29,\n            \"titles\": [\n                \"Scaling Vision Transformers\",\n                \"Direct transfer of learned information among neural networks\",\n                \"A survey on transfer learning\",\n                \"CNN Features off-the-shelf: an Astounding Baseline for Recognition\",\n                \"Factors of Transferability for a Generic ConvNet Representation\",\n                \"ImageNet classification with deep convolutional neural networks\",\n                \"Overfeat: Integrated recognition, localization and detection using convolutional networks\",\n                \"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\n                \"Imagenet: A large-scale hierarchical image database\",\n                \"ImageNet Large Scale Visual Recognition Challenge\",\n                \"A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark\",\n                \"Factors of influence for transfer learning across diverse appearance domains and task types\",\n                \"Scaling Laws for Neural Language Models\",\n                \"Language Models are Few-Shot Learners\",\n                \"Big Transfer (BiT): General Visual Representation Learning\",\n                \"Imagenet-21k pretraining for the masses\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison\",\n                \"Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports\",\n                \"PadChest: A large chest x-ray image dataset with multi-label annotated reports\",\n                \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases\",\n                \"JUWELS Booster Supercomputer, 2020.\",\n                \"Transfusion: Understanding Transfer Learning for Medical Imaging\",\n                \"Scaling Laws for Transfer\",\n                \"Scaling Laws for Autoregressive Generative Modeling\",\n                \"Reconciling modern machine-learning practice and the classical bias-variance trade-off\",\n                \"Deep Double Descent: Where Bigger Models and More Data Hurt\",\n                \"Triple descent and the Two Kinds of Overfitting: Where & Why do they Appear?\",\n                \"Yfcc100m: The new data in multimedia research\",\n                \"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era\",\n                \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples\",\n                \"On the limits of cross-domain generalization in automated X-ray prediction\",\n                \"CheXtransfer: Performance and Parameter Efficiency of ImageNet Models for Chest X-Ray Interpretation\",\n                \"Supervised transfer learning at scale for medical imaging\",\n                \"Learning multiple layers of features from tiny images\",\n                \"Automated flower classification over a large number of classes\",\n                \"Cats and dogs\",\n                \"Two public chest x-ray datasets for computer-aided screening of pulmonary diseases\",\n                \"COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images\",\n                \"High-performance large-scale image recognition without normalization\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]29 based on the context and research area of Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}