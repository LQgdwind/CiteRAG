{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Zeroth-Order Actor-Critic\n\nPaper Abstract: \n\nPaper Section Text:\nReinforcement learning (RL) has achieved great success in a wide range of challenging domains, including video games , robotic control , autonomous driving , etc. The majority of RL methods formulate the environment as a Markov decision process (MDP) and leverage the temporal structure to design learning algorithms such as Q-learning and policy gradient . Actor-critic methods are among the most popular RL algorithms, which usually introduce two function approximators, one for value function estimation (critic) and another for optimal policy approximation (actor), and optimize these two approximators by alternating between policy evaluation (PEV) and policy improvement (PIM). On-policy actor-critic methods, e.g., A3C  and PPO , often use critics to construct advantage functions and substitute them for the Monte Carlo return used in vanilla policy gradient , which significantly reduces the variance of gradient estimation and improve learning speed and stability. Among existing actor-critic algorithms, a common choice is to use deep neural networks as the function approximators and conduct both PEV and PIM using first-order optimization techniques. An alternative approach for RL, though less popular, is to ignore the underlying MDP structures and regard RL problems as black-box optimization, and to directly search for the optimal policy in a zeroth-order way, i.e., without using the first-order gradient information. Recent researches have shown that zeroth-order optimization (ZOO) methods, e.g., ES [ref]34, ARS   and GA  , are competitive on common RL benchmarks, even when applied to deep neural network with millions of parameters or with complex heterogeneous architectures like world models . ZOO has several advantages compared to first-order MDP-based RL methods [ref]37   [ref]16 : (1) ZOO is not restricted to differentiable policies  ; (2) ZOO perturbs the policy in parameter space rather than in action space, which leads to state-dependent and temporally-extended exploration [ref]34 [ref]16; (3) Zeroth-order population-based optimization possesses robustness-seeking property and diverse policy behaviors . Despite these attractive advantages, the main limitation of ZOO is its high sample complexity and high variance of the parameter update process, especially in high-dimensional problems. Recent researches have proposed various techniques to improve ZOO, e.g., using orthogonal or antithetic sampling methods [ref]37 [ref]34  , identifying a low-dimensional search subspace   , or subtracting a baseline for variance reduction [ref]37 .\nOne of the major reasons for the sample inefficiency of ZOO is its ignorance of the MDP temporal structures.\nMany recent researches have tried to combine ZOO and first-order MDP-based RL into hybrid methods, e.g., run evolutionary algorithms in parallel with off-policy RL algorithms and optimize the population of policies with information from both sides [ref]16  , or inject parameter noise into existing RL algorithms for efficient exploration  . However, existing hybrid methods still conduct first-order gradient-based policy improvement (at least as a part), which reimposes a differentiable requirement on the policy.\nA more detailed discussion of the related works is attached in Appendix A. In this paper, we propose a novel Zeroth-Order Actor-Critic algorithm (ZOAC), which makes several contributions more beneficial than prior arts. Firstly, ZOAC unifies first-order and zeroth-order RL methods into an actor-critic architecture by conducting first-order policy evaluation (PEV) to update the critic and zeroth-order policy improvement (PIM) to update the actor. In such a way, complementary advantages of both methods are preserved, such as wide versatility to policy parameterization, robustness seeking property, state-dependent and temporally-extended exploration. Secondly, we modify the rollouts collection strategy from an episode-wise perturbation as in traditional zeroth-order methods to a timestep-wise perturbation for better exploration, and derive the zeroth-order policy gradient under this setting. We point out that a critic network can be introduced to estimate the state-value function and trade-off between bias and variance. We then propose a practical algorithm that utilizes several parallelized rollout workers and alternates between first-order PEV and zeroth-order PIM based on generated experiences in each iteration. Thirdly, we extensively evaluate ZOAC on a wide range of challenging continuous control benchmarks from OpenAI gym , using different types of policies, including linear policies and neural networks with full or compact architectures. Experiment results show that ZOAC outperforms zeroth-order and first-order baseline algorithms in sample efficiency, final performance, and the robustness of the learned policies. Furthermore, we conduct ablation studies to demonstrate the indispensable contribution of the modified rollouts collection strategy and the introduced critic network to ZOAC.\n\nReference markers found: [34]\n\nContext windows for each [ref]:\n[ref]34: search for the optimal policy in a zeroth-order way, i.e., without using the first-order gradient information. Recent researches have shown that zeroth-order optimization (ZOO) methods, e.g., ES [ref]34, ARS   and GA  , are competitive on common RL benchmarks, even when applied to deep neural network with millions of parameters or with complex heterogeneous architectures like world models .\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 34,\n            \"titles\": [\n                \"High-Dimensional Continuous Control Using Generalized Advantage Estimation\",\n                \"What matters for on-policy deep actor-critic methods? a large-scale study\",\n                \"Layer normalization\",\n                \"A theoretical and empirical comparison of gradient approximations in derivative-free optimization\",\n                \"Proximal distilled evolutionary reinforcement learning\",\n                \"Openai gym\",\n                \"Structured evolution with compact architectures for scalable policy optimization\",\n                \"From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization\",\n                \"Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents\",\n                \"Noisy Networks for Exploration\",\n                \"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation\",\n                \"World Models\",\n                \"Averaging Weights Leads to Wider Optima and Better Generalization\",\n                \"From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion\",\n                \"Learning to drive in a day\",\n                \"On large-batch training for deep learning: Generalization gap and sharp minima\",\n                \"Evolution-Guided Policy Gradient in Reinforcement Learning\",\n                \"ES Is More Than Just a Traditional Finite-Difference Approximator\",\n                \"Non-Differentiable Supervised Learning with Evolution Strategies and Hybrid Methods\",\n                \"RLlib: Abstractions for Distributed Reinforcement Learning\",\n                \"Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving\",\n                \"Continuous control with deep reinforcement learning\",\n                \"Guided evolutionary strategies: Augmenting random search with surrogate gradients\",\n                \"Simple random search of static linear policies is competitive for reinforcement learning\",\n                \"Human-level control through deep reinforcement learning\",\n                \"Asynchronous methods for deep reinforcement learning\",\n                \"Ray: A distributed framework for emerging {{AI}} applications\",\n                \"Random gradient-free minimization of convex functions\",\n                \"Parameter Space Noise for Exploration\",\n                \"CEM-RL: Combining evolutionary and gradient-based methods for policy search\",\n                \"Importance mixing: Improving sample reuse in evolutionary policy search methods\",\n                \"Derivative-free reinforcement learning: A review\",\n                \"Stable baselines3\",\n                \"Deep Neuroevolution of Recurrent and Discrete World Models\",\n                \"Evolution strategies as a scalable alternative to reinforcement learning\",\n                \"Proximal policy optimization algorithms\",\n                \"Parameter-exploring policy gradients\",\n                \"Learning to Guide Random Search\",\n                \"Policy Search in Continuous Action Domains: an Overview\",\n                \"Deterministic policy gradient algorithms\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]34 based on the context and research area of Zeroth-Order Actor-Critic. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}