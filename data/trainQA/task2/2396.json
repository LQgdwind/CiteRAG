{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Intersectional Bias in Causal Language Models\n\nPaper Abstract: \n\nPaper Section Text:\nTo test for intersectional bias, we choose a combination of three categories: gender, religion and disability. Gender bias has been widely studied, and its inclusion here allows for comparison with prior work. Religion and disability bias has received some attention  , but not in relation to the same language models, nor as intersectional categories. All three categories have relatively unambiguous category markers, though some common identity-first disability qualifiers, such as ‘disabled’, ‘deaf’ and ‘blind’, are frequently, and often contentiously, used in contexts outside of social identity categories. For example, ‘disabled’ has often been used in technical literature to mean turned or switched off. While a feature of transformer models has been their ability to disambiguate homophones (such as varied meanings of ‘like’), performance on metaphoric or analogic modifiers varies. Our selection of terms is based on what is generally considered common use, as discussed further in relation to each of the three categories below. This aims to mirror commonality of these terms in the language models themselves, which are weighted towards media and social media text samples, rather than academic, advocacy or other literature. We note that this approach means, on the one hand, that norms and preferences around terminology and identity of particular disability groups may not always be reflected in selection of terms and, on the other, more extreme examples of biased language use may also not be generated and identified. We select five categories of religion, following the World Religion Paradigm . Though this paradigm is challenged in the literature, it likely reflects common religious terms of use in language model training sets. We add ‘atheism’ as a marker of explicit non-religious orientation, and an empty string (‘’) as a null marker. With studies of coreference bias  3, grammatical and social categories necessarily align, and gender is treated as binary- or ternary-valued. For auto-completion tasks, this constraint can be relaxed. We add ‘transgender person’ to ‘woman’ and ‘man’, and include ‘person’ as an un-gendered marker. Disability terms come in various forms, with the two most commonly accepted versions being people-first and identity-first. Terms chosen for this analysis are not exhaustive, and also not universally accepted within the disability community. As with gender and religious categories, term selection does not imply endorsement, but rather represents a compromise between contemporary linguistic norms and likely term frequency within language model data sets, which reflect historical use. We follow  in determining which form is preferred, since these recent journalistic recommendations represent a reasonable balance between scientific and colloquial language use, and are also informed by disability community advocacy. We select two kinds of disability from five major categories of sensory, physical, neurodivergent, cognitive and psycho-social disability. Sensory: blind, deaf Physical: with quadriplegia, who uses a wheelchair Neurodiverse: autistic Cognitive: with Down Syndrome Psycho-social: with OCD, with schizophrenia We also include ‘disabled’ as a general marker of disability, and an empty string (‘’) to signify a null marker. Each category is used to generate part of a prompt of the following form: ‘A/An’ Tp​r​eD​i​s​a​b​i​l​i​t​y​TR​e​l​i​g​i​o​n​TG​e​n​d​e​r​Tp​o​s​tD​i​s​a​b​i​l​i​t​ysubscriptsuperscript𝑇𝐷𝑖𝑠𝑎𝑏𝑖𝑙𝑖𝑡𝑦𝑝𝑟𝑒superscript𝑇𝑅𝑒𝑙𝑖𝑔𝑖𝑜𝑛superscript𝑇𝐺𝑒𝑛𝑑𝑒𝑟subscriptsuperscript𝑇𝐷𝑖𝑠𝑎𝑏𝑖𝑙𝑖𝑡𝑦𝑝𝑜𝑠𝑡T^{Disability}_{pre}\\;T^{Religion}\\;T^{Gender}\\;T^{Disability}_{post} Examples include: ‘An autistic Muslim man’ ‘An atheist woman with OCD’ Prompts constructed from combinations of terms (7 religion * 4 gender * 10 disability markers = 280 combinations) are used to generate samples of 100 sentences from publicly available causal language models. Following 0  , we apply sentiment analysis to the generated sentences. During initial testing, we examined a total of five sentiment analysis metrics to classify sentences. First, we applied the default sentiment analyser supplied by the HuggingFace transformer library, DistilBERT base uncased finetuned SST-2, a lightweight but high quality 7 binary classifier derived from BERT . We have not modified the classifier’s configuration from the defaults. The softmax activation means sentences are coarsely classified, producing confounds (e.g. ‘neutral’ sentences that are classified as highly negative). To counteract this, we also tested NLTK’s sentiment classifier , which is more discriminatory but human review determined to be less accurate. We also analyse generated sentences with and without the prompt itself, since sentiment analysers may introduce bias of their own. We take the softmax scores in the range (0,1)01(0,1), since these reduce the divergence from central values. Finally, we applied a sigmoid rather than softmax activation to sentiment logit scores. This metric appeared higher quality than NLTK, and offers more discrimination compared with softmax values (it is less likely to produce values that converge to 00 or 111). Softmax results are reported, since they are consistent with common classifier use, but sigmoid results have also been checked for consistency. We apply tests to two publicly available transformer model families designed for auto-completion: four GPT-2 models (124M, 355M, 774M and 1.5B) models; and four GPT-NEO models (125M, 350M, 1.3B and 2.7B) released in March and April 2021, an open source implementation of the GPT-3 architecture. This produces eight models in total. GPT-NEO is trained on ‘The Pile’ , which has been designed and evaluated in part to address gender and religious bias. Sentences are generated from zero-shot unconditional prompts through the Huggingface interface to both models, using parameters suggested by 5: 50 tokens; top-k sampling , to limit next-word distribution to a defined number of words, also set to 505050; and top-p nucleus sampling [ref]39, set to 0.95, to sample only words that contribute to that probability distribution. These parameters are likely to be used in many real-world text generation settings, e.g. for story-telling or chatbots, and are for that reason adopted here. We define bias and its converse, fairness, as the difference or similarity between sentiment scores for prompts that are distinguished by categories of gender, religion, disability or other markers of social distinction. Following  0, in order to be considered fair or unbiased, a prompt containing a description of an individual specified by any one or more of these markers should produce a sentence with, on average, the same sentiment score as any other prompt containing different, or absent, markers from the same category sets. As with 0, ‘demographic parity’ is met under the following conditions. For any category such as gender, a set of values is defined, e.g. 𝒜={M​a​l​e,F​e​m​a​l​e,T​r​a​n​s​g​e​n​d​e​r,U​n​s​p​e​c​i​f​i​e​d}𝒜𝑀𝑎𝑙𝑒𝐹𝑒𝑚𝑎𝑙𝑒𝑇𝑟𝑎𝑛𝑠𝑔𝑒𝑛𝑑𝑒𝑟𝑈𝑛𝑠𝑝𝑒𝑐𝑖𝑓𝑖𝑒𝑑\\mathcal{A}=\\{Male,Female,Transgender,Unspecified\\}, and A=a𝐴𝑎A=a denotes a random variable with a value of a∈𝒜𝑎𝒜a\\in\\mathcal{A}. Again following 0, another value a~~𝑎\\tilde{a} is chosen from 𝒜∖a𝒜𝑎\\mathcal{A}\\setminus a. A sentiment classifier, fssubscript𝑓𝑠f_{s}, produces outputs in the interval (0,1)01(0,1), and a language model, L​M𝐿𝑀LM, generates sentences L​M​(𝐱)𝐿𝑀𝐱LM(\\mathbf{x}) and L​M​(𝐱~)𝐿𝑀~𝐱LM(\\mathbf{\\tilde{x}}), where 𝐱𝐱\\mathbf{x} and 𝐱~~𝐱\\mathbf{\\tilde{x}} are prompts containing a𝑎a and a~~𝑎\\tilde{a} respectively. Ps​(𝐱)subscript𝑃𝑠𝐱P_{s}(\\mathbf{x}) and Ps​(𝐱~)subscript𝑃𝑠~𝐱P_{s}(\\mathbf{\\tilde{x}}) represent distributions of fs​(L​M​(𝐱))subscript𝑓𝑠𝐿𝑀𝐱f_{s}(LM(\\mathbf{x})) and fs​(L​M​(𝐱~))subscript𝑓𝑠𝐿𝑀~𝐱f_{s}(LM(\\mathbf{\\tilde{x}})); parity is met when these distributions are equivalent, given a margin of error, ϵitalic-ϵ\\epsilon. Intersectional bias is defined in the same way, but for up to three variables, A=a𝐴𝑎A=a, B=b𝐵𝑏B=b and C=c𝐶𝑐C=c, where a𝑎a, b𝑏b and c𝑐c belong to sets of gender, disability and religious categories, and each is included in prompt 𝐱𝐱\\mathbf{x}. Disparity, or bias, occurs when the distribution differences exceed ϵitalic-ϵ\\epsilon. We utilise simplified measures of bias, where ϵitalic-ϵ\\epsilon values are derived via standard tests of distribution difference, ANOVA and t𝑡t-tests. In particular, if t𝑡t-test comparing distributions Ps​(𝐱)subscript𝑃𝑠𝐱P_{s}(\\mathbf{x}) and Ps​(𝐱~)subscript𝑃𝑠~𝐱P_{s}(\\mathbf{\\tilde{x}}) produces a large enough t𝑡t-value <.0absent.0<.0 (p<.001𝑝.001p<.001), then L​M𝐿𝑀LM is negatively biased towards 𝐱𝐱\\mathbf{x}. In addition, we use exploratory topic modelling and regression to examine what semantic associations and category variables contribute to bias. We also explore several prompt variations, including few-shot cases. We test for four hypotheses: Language models exhibit individual bias across all categories (gender, disability, religion), consistent with other studies (e.g.  ). Language models exhibit intersectional bias. Not all examples of intersectional bias can be inferred from individual bias. Overall, model size should produce more balanced predictions across all categories, and show reduced bias. For example, GPT-2’s 1.5B parameter model should produce less bias than its 125M parameter model. However we also hypothesise that some categories and intersectional combinations may receive lower scores in larger models, in ways that may be systematic, and reflect a more true bias in the training data set, due to the elimination of noise. This hypothesis follows 9 suggestion that larger models do not necessarily eliminate bias, and indeed may introduce more subtle forms. Diversity and weighting of data sets should reduce effects of some (but not necessarily all) bias in comparably sized models. For example, GPT-NEO’s 1.3B model should produce less biased results than GPT-2’s 1.5B model. Similar to the preceding hypothesis, we also anticipate some cases where categories score substantially lower, even with more carefully curated data. Since training data is a major difference between GPT-2 and GPT-NEO model families, we measure this expectation through analysis of model type.\n\nReference markers found: [39]\n\nContext windows for each [ref]:\n[ref]39: interface to both models, using parameters suggested by 5: 50 tokens; top-k sampling , to limit next-word distribution to a defined number of words, also set to 505050; and top-p nucleus sampling [ref]39, set to 0.95, to sample only words that contribute to that probability distribution. These parameters are likely to be used in many real-world text generation settings, e.g. for story-telling\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 39,\n            \"titles\": [\n                \"Reducing Sentiment Bias in Language Models via Counterfactual Evaluation\",\n                \"Mapping the Margins: Intersectionality, Identity Politics, and Violence against Women of Color\",\n                \"NLTK: The Natural Language Toolkit\",\n                \"Convention on the Rights of Persons with Disabilities\",\n                \"The World Religions Paradigm: Time for a Change\",\n                \"Fairness through Awareness\",\n                \"Efficient Estimation of Word Representations in Vector Space\",\n                \"Glove: Global Vectors for Word Representation\",\n                \"The Black Box Society\",\n                \"Postcolonial Reproductions: Disability, Indigeneity and the Formation of the White Masculine Settler State of Australia\",\n                \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\",\n                \"The Social Impact of Natural Language Processing\",\n                \"Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy\",\n                \"The Trouble with Bias\",\n                \"Attention Is All You Need\",\n                \"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification\",\n                \"Hierarchical Neural Story Generation\",\n                \"Disability Language Style Guide\",\n                \"Algorithms of Oppression: How Search Engines Reinforce Racism\",\n                \"Deep contextualized word representations\",\n                \"Improving Language Understanding by Generative Pre-training\",\n                \"Gender Bias in Coreference Resolution\",\n                \"AI Fairness for People with Disabilities: Point of View\",\n                \"Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods\",\n                \"Evaluating the Underlying Gender Bias in Contextualized Word Embeddings\",\n                \"Race After Technology: Abolitionist Tools for the New Jim Code\",\n                \"Identifying and Reducing Gender Bias in Word-Level Language Models\",\n                \"Toward Fairness in AI for People with Disabilities: A Research Roadmap\",\n                \"Gender-preserving Debiasing for Pre-trained Word Embeddings\",\n                \"Fine-grained Sentiment Classification using BERT\",\n                \"My Algorithms Have Determined You’re Not Human: AI-ML, Reverse Turing-Tests, and the Disability Experience\",\n                \"Language Models are Unsupervised Multitask Learners\",\n                \"Evaluating Gender Bias in Machine Translation\",\n                \"Mitigating Gender Bias in Natural Language Processing: Literature Review\",\n                \"Disability, Bias, and AI\",\n                \"Gender Bias in Contextualized Word Embeddings\",\n                \"Language Models are Few-Shot Learners\",\n                \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\",\n                \"A Robot Wrote this Entire Article. Are You Scared Yet, Human?\",\n                \"The Curious Case of Neural Text Degeneration\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]39 based on the context and research area of Intersectional Bias in Causal Language Models. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}