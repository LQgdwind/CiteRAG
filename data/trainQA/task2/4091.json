{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: AritPIM: High-Throughput In-Memory Arithmetic\n\nPaper Abstract: \n\nPaper Section Text:\nEmerging processing-in-memory (PIM) systems attempt to overcome the memory-wall bottleneck by rethinking one of the core principles of computing systems: the separation of storage and logic units. This separation has been followed since the introduction of the von Neumann architecture in the 1940s, when computing systems were primarily utilized for serial program execution. Yet, the recent emergence of data-intensive applications requires parallel high-throughput execution, causing the separation to become a massive bottleneck known as the memory wall¬†. Therefore, PIM integrates logic within the memory itself to bypass the bandwidth-limited memory interface and enable massive in-memory computational parallelism¬†. PIM architectures supplement the traditional read/write memory interface with logic¬†. This enables the CPU to request that the memory perform vectored logic on data stored within the memory without transferring the data through the interface, thereby significantly reducing the load on CPU-memory communication. Early proposals for PIM¬† involved integrating logic circuits near the memory (e.g., in the same chip), yet this still requires a fundamental need for data-transfer between an area dedicated for computation and an area dedicated for storage¬†. Conversely, recent proposals¬†    [ref]8  0 perform digital logic using the same physical devices that store binary information. By performing the logic exactly where the information is stored, data-transfer is effectively dwarfed¬†. These include works that exploit content-addressable-memories (CAMs)¬† [ref]8 0 to selectively apply write operations according to data stored in the memory (serving as inputs), and works that design logic gates from the underlying circuits connecting the memory devices¬†   . Several proposals for PIM architectures, such as memristive (Figure¬†1(a,b))¬†  1 2 3 4 and DRAM-based (Figure¬†1(c,d))¬†5  , have essentially converged to a single abstract computational model that presents highly unique algorithmic capabilities (Figure¬†1(e)). First, consider the memory as a collection of mùëöm binary matrices, called arrays, each of dimension r√ócùëüùëêr\\times c. A bitwise operation can be performed on columns of an array, and in parallel across all arrays, in a single cycle (O‚Äã(1)ùëÇ1O(1) latency). For example, the bit-wise NOR¬† of any two columns can be computed and stored in a third column, all in a single cycle. This is possible as the logic is performed in a distributed fashion amongst the physical elements within arrays (with shared instructions), so there is no centralized computing unit that may cause a bottleneck. This attains massive parallelism for bitwise operations that bypass the memory interface. Expanding the massive bitwise parallelism to large-scale applications requires a strong foundation for fundamental arithmetic operations (addition, subtraction, multiplication, and division), for both fixed-point and floating-point numbers. While theoretically a functionally-complete set of logic gates can perform any function, the data layout plays a crucial role in the efficient utilization of PIM. Figure¬†2(a) presents an overview of the approaches developed in recent years; we focus without loss of generality on a single array (as computation is in parallel across all arrays regardless). We describe the approaches through NùëÅN-bit integer vector addition, where xisuperscriptùë•ùëñx^{i} refers to the it‚Äãhsuperscriptùëñùë°‚Ñéi^{th} element in the vector: Bit-Serial Element-Serial: Two inputs, x1superscriptùë•1x^{1} and y1superscriptùë¶1y^{1}, are stored within a single row of an array, and basic logic gates (e.g., NOR) serially construct an NùëÅN-bit adder within that row (utilizing intermediate cells for temporary results). This has low throughput as only one addition is performed per array and high latency as the gates run serially; thus, this approach is typically not used. Note that column parallelism (constant-time column operations) is not utilized. Bit-Parallel Element-Serial¬†6 7 8 3 9: mùëöm rows are utilized as intermediate space to perform multiple parallel gates for the same NùëÅN-bit adder by utilizing column parallelism, thereby enabling r/mùëüùëör/m adders per array. This provides low latency when the function possesses parallelism among the gates (typically best applicable to multiplication¬†7), yet possesses moderate throughput as several rows perform a single addition. Furthermore, the relative area overhead can be high due to the intermediate cells¬†7, and the data-transfer between rows requires additional support for inter-row logic. Bit-Serial Element-Parallel¬† 0 1   1 2 3  4 3: This approach performs the operations of the bit-serial element-serial approach in parallel across all rows ‚Äì with the exact same latency ‚Äì by exploiting column parallelism. That provides high throughput as rùëür adders per array are performed simultaneously with identical latency; however, the latency remains rather high as gates are performed serially (a single gate per cycle per NùëÅN-bit adder). Bit-Parallel Element-Parallel (parallel single-row)¬†5 6: This recent approach gains both higher throughput and low latency by introducing partitions¬†1 7 (see Section¬†5). The partitions dynamically divide the rows to enable multiple concurrent column operations. The adder is still performed within a single row (and in parallel across all rows), yet multiple gates are performed within each row concurrently. The potential drawback is that partitions may introduce additional physical overhead; however, a recent work has proposed a low-overhead design¬†7. We focus on the element-parallel approaches as PIM is best suited for data-intensive applications which require high throughput. Figure¬†2(b) summarizes our contributions for 16 variants of arithmetic functions, establishing a state-of-the-art foundation for arithmetic in PIM. We propose the first known general-purpose digital PIM algorithm for a majority of the combinations (including cases previously considered impossible, such as floating-point addition¬†8), while also presenting minor and major (>5√ó>5\\times) improvements for others. We accomplish this via a combination of three methods: Variable Shift: We develop a novel algorithm for element-parallel variable shifting: each row iùëñi starts with numbers xisuperscriptùë•ùëñx^{i} and tisuperscriptùë°ùëñt^{i}, and the output is xi‚â™timuch-less-thansuperscriptùë•ùëñsuperscriptùë°ùëñx^{i}\\ll t^{i}. While this was previously considered impossible (as each row can have a different shift¬†8), we attain this efficiently for the first time due to the combination of in-memory multiplexers and a logarithmic-shifter approach (without any custom periphery). We then tackle variable normalization: each row iùëñi starts with number xisuperscriptùë•ùëñx^{i}, and the output is xisuperscriptùë•ùëñx^{i} left-shifted until the MSB is one. This is far more difficult as the shift-amount is unknown, and yet we attain this with latency nearly identical to variable shift due to a technique inspired by a binary search. Partition Toolbox: We exploit a unique algorithmic topology enabled by partitions¬†7 towards an efficient toolbox of general-purpose routines. These include both generalizations of routines proposed in MultPIM¬†6, and two novel routines: reduction ‚Äì reducing (e.g., AND, OR) bits of multiple partitions to a single bit, and prefixing ‚Äì each partition receives the reduction of bits in partitions before it. Arithmetic Theory: We provide a new perspective on historical, lesser-known, algorithms in computer arithmetic, demonstrating their effectiveness with element-parallel in-memory computing for the first time. For example, we utilize Karatsuba¬†9 0 for bit-serial multiplication, parallel-prefix adders¬†1 0 for bit-parallel addition, and carry-lookahead in division¬†2 3 0 for bit-parallel division. Interestingly, some of these algorithms are not effective in traditional systems¬†0 3, yet unique considerations of PIM lead to their effectiveness here. This paper is organized as follows. Section¬†2 provides background on a wide variety of PIM technologies and their compatibility with the abstract model. We start in Section¬†3 with bit-serial fixed-point arithmetic to establish the state-of-the-art approaches and our improvements, and continue in Section¬†4 with bit-serial floating-point arithmetic. Section¬†5 then shifts to the bit-parallel fixed-point approach, introducing bit-parallel addition/subtraction/division for the first time and improving bit-parallel multiplication. We combine bit-serial floating-point and bit-parallel fixed-point in Section¬†6 to establish bit-parallel floating-point algorithms. Section¬†7 evaluates AritPIM through a case study of memristive PIM implemented on a publicly-available cycle-accurate simulator, and Section¬†8 concludes this paper. Throughout, we discuss abstract logic gates (e.g., AND, XOR, full-adder) and latency complexity (e.g., O‚Äã(N2)ùëÇsuperscriptùëÅ2O(N^{2}) where NùëÅN is the representation size) for generality and concise explanations, with Section¬†7 reducing these to the underlying gates supported (e.g., NOR) and providing full implementations that prove correct results (e.g., matching the IEEE round-to-nearest ties-to-even exactly). We refer to steps rather than cycles, where each step performs a single abstract logic gate. Without loss of generality, as we focus on the element-parallel approaches, we often discuss gates performed within a single-row, as the generalization to all rows and arrays is the trivial repetition of the gates. For fixed-point, we discuss unsigned numbers (for simplicity) yet the algorithms can extended to signed. Lastly, visuperscriptùë£ùëñv^{i} refers to the it‚Äãhsuperscriptùëñùë°‚Ñéi^{th} element of vector vùë£v, xisubscriptùë•ùëñx_{i} refers to bit iùëñi of number xùë•x (0 is the LSB), xi:jsubscriptùë•:ùëñùëóx_{i:j} is bits iùëñi (inclusive) through jùëój (exclusive), and (x|y)conditionalùë•ùë¶(x|y) concatenates xùë•x (higher bits) and yùë¶y (lower bits).\n\nReference markers found: [8]\n\nContext windows for each [ref]:\n[ref]8: (e.g., in the same chip), yet this still requires a fundamental need for data-transfer between an area dedicated for computation and an area dedicated for storage¬†. Conversely, recent proposals¬†    [ref]8  0 perform digital logic using the same physical devices that store binary information. By performing the logic exactly where the information is stored, data-transfer is effectively dwarfed¬†.\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 8,\n            \"titles\": [\n                \"Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±\",\n                \"Computing‚Äôs energy problem (and what we can do about it)\",\n                \"In-memory stateful logic computing using memristors: Gate, calculation, and application\",\n                \"A case for intelligent RAM\",\n                \"Logic design within memristive memories using memristor-aided logic (MAGIC)\",\n                \"RACER: Bit-pipelined processing using resistive memory\",\n                \"SIMDRAM: A framework for bit-serial SIMD processing using DRAM\",\n                \"ComputeDRAM: In-memory compute using off-the-shelf DRAMs\",\n                \"Associative computing: a programming paradigm for massively parallel computers\",\n                \"A 28-nm compute SRAM with bit-serial logic/arithmetic operations for programmable in-memory vector computing\",\n                \"FELIX: Fast and energy-efficient logic in memory\",\n                \"'Memristive' switches enable 'stateful' logic operations via material implication\",\n                \"Efficient in-memory processing using spintronics\",\n                \"Using spin-hall MTJs to build an energy-efficient in-memory computation platform\",\n                \"Ambit: In-memory accelerator for bulk bitwise operations using commodity DRAM technology\",\n                \"Ultra-efficient processing in-memory for data intensive applications\",\n                \"A novel in-memory wallace tree multiplier architecture using majority logic\",\n                \"Simple magic: Synthesis and in-memory mapping of logic execution for memristor-aided logic\",\n                \"In-memory processing on the spintronic CRAM: From hardware design to application mapping\",\n                \"Efficient algorithms for in-memory fixed point multiplication using MAGIC\",\n                \"SIMPLER MAGIC: Synthesis and mapping of in-memory logic executed in a single row to improve throughput\",\n                \"FloatPIM: In-memory acceleration of deep neural network training with high precision\",\n                \"Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks\",\n                \"General structure for computational random access memory (CRAM)\",\n                \"RIME: A scalable and energy-efficient processing-in-memory architecture for floating-point operations\",\n                \"MultPIM: Fast Stateful Multiplication for Processing-in-Memory\",\n                \"PartitionPIM: Practical Memristive Partitions for Fast Processing-in-Memory\",\n                \"DRISA: A DRAM-based reconfigurable in-situ accelerator\",\n                \"Multiplication of many-digital numbers by automatic computers\",\n                \"Computer Arithmetic: Algorithms and Hardware Designs\",\n                \"A regular layout for parallel adders\",\n                \"An augmented iterative array for high-speed binary division\",\n                \"Arithmetic and logic in computer systems\",\n                \"Logic computing with stateful neural networks of resistive switches\",\n                \"Compute caches\",\n                \"Pinatubo: A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories\",\n                \"Ferroelectric FET technology and applications: From devices to systems\",\n                \"Computing in memory with FeFETs\",\n                \"Accelerating force-directed graph layout with processing-in-memory architecture\",\n                \"Practical integer division with Karatsuba complexity\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]8 based on the context and research area of AritPIM: High-Throughput In-Memory Arithmetic. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}