{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Evaluating the Social Impact of Generative AI Systems in Systems and Society\n\nPaper Abstract: \n\nPaper Section Text:\nGenerative AI systems can embed and amplify harmful biases that are most detrimental to marginalized peoples. Categories of bias, from system to human to statistical, interact with each other and are intertwined 1. For bias evaluations that do not narrowly capture biases as they occur in Generative AI systems, it is necessary to consider work outside of the field of question. For instance, for natural language processing, bias evaluations must seriously engage with the relationship between the modality (i.e. language) and social hierarchies [ref]33. When thinking about representational harms , it is also important to consider the extent to which any representation could confer harm (see 4.2.2.2 Long-term Amplifying Marginalization by Exclusion (and Inclusion)). Although bias evaluations in data have been subject to a large body of research, bias is not only a “data problem.” Biases are not only introduced in the data pipeline but throughout the entire machine learning pipeline . The overall level of harm is also impacted by modeling choice . These can include choices about many stages of the optimization process  ; privacy constraints , widely used compression techniques  [ref]15  and the choice hardware  have all been found to amplify harm on underrepresented protected attributes . The geographic location, demographic makeup, and team structures of researcher and developer organizations can also introduce biases. While the degree of harm depends on many factors from type of output to the cultural context of training and deployment, focus on bias evaluations has centered on protected classes as defined by United States  and United Nations 9 guidelines. These guidelines are non-exhaustive and harms exist outside of their proposed categories but can be evaluated by adding categories. For instance, for generative AI systems developed on data from the South Asian subcontinent, it may also be useful to include considerations of caste bias .\nAdditional harmful biases include misrepresentations of humans generally, such as associated humans or a group of humans with other animals . Popular evaluations for biases use association tests  or examine stereotypes   , correlations and co-occurrences , and sentiment analysis . In language, these evaluations can occur at the word or sentence level. For images, additional tools such as captioning systems can be used. For certain modalities, such as language, biases can be represented differently . Across modalities, biases can be evaluated using intrinsic and extrinsic methods , where the former seeks to evaluate biases within model weights and the latter evaluates the expression of biases in the outputs for downstream tasks (e.g. captioning). Evaluations can also be specific to a certain function of a modality, such as question-answering in language . There are often legal obstacles around collecting certain protected attributes, which leads to selection bias in the availability of protected features annotations. Moverover, as geographic and cultural contexts shift, so do the meaning of different categories. Annotators often have different perceptions of concepts like race or are influenced by their own lived experience when categorizing protected categories. Due to its contextual and evolving nature , bias evaluation cannot be fully standardized and static . Protected class categorization itself cannot be exhaustive and can be inherently harmful. By framing work within such considerations, it is possible to delineate which qualities that are evaluated for. Precisely identifying which framing is used for bias evaluation and mitigation can help delineate the particular areas where robust evaluation has been done, where developers expect biases to arise, and which groups for whom they believe biases are unlikely to arise, or bias evaluations have not been as rigorous, e.g., due to a lack of bias evaluation resources.\nCertain protected classes, such as race and gender, are often more represented in publications and publication venues around biases of (generative) systems. Many evaluations focus on distinct or binary groups, due to the complexity of operationalising intersectionality  ; in many cases, assumptions used to simplify for the sake of mathematical notation and interpretation result in obscuring the very phenomena they seek to describe . Obtaining data for bias evaluations is not straightforward, as there are often legal obstacles around collecting data about\nprotected attributes, which leads to selection bias in the availability of protected features annotations  .\nMoverover, as geographic and cultural contexts shift, so do the meaning of different categories   and must be interpreted according to their local meaning.\nAnnotators often have different perceptions of concepts like race or are influenced by their own lived experience  when categorizing protected categories . When conducting association tests, although based in human associations, one should remain aware that general societal attitudes do not always represent subgroups of people and cultures. Evaluations for stereotype detection can raise false positives and can flag relatively neutral associations based in fact (e.g. population x has a high proportion of lactose intolerant people) . Whenever additional tooling is used to aid in identifying biases, e.g., the use of an image captioning system in addition to the base system, tool added introduces its own biases, similarly introduced in each step of developing the tool, which are embedded into the ecosystem of the biases of the system under study.\n\nReference markers found: [33]\n\nContext windows for each [ref]:\n[ref]33: of the field of question. For instance, for natural language processing, bias evaluations must seriously engage with the relationship between the modality (i.e. language) and social hierarchies [ref]33. When thinking about representational harms , it is also important to consider the extent to which any representation could confer harm (see 4.2.2.2 Long-term Amplifying Marginalization by\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 33,\n            \"titles\": [\n                \"인공지능 기반 공격 그래프 생성\",\n                \"Analyse and automate every message\",\n                \"Global Comprehensive Privacy Law Mapping Chart\",\n                \"Methodologies and Tools to Evaluate Climate Change Impacts and Adaptation\",\n                \"California Consumer Privacy Act of 2018\",\n                \"AB-5 Worker status: employees and independent contractors.\",\n                \"DOD Adopts Ethical Principles for Artificial Intelligence\",\n                \"COM (2021) 762: Proposal for a DIRECTIVE OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL on improving working conditions in platform work\",\n                \"Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS\",\n                \"Americans’ Views on Generative Artificial Intelligence, Hate and Harassment\",\n                \"Cyber violence is a growing threat, especially for women and girls\",\n                \"REAIM 2023\",\n                \"A Systematic Review of the Empirical Literature Evaluating IRBs: What We Know and What We Still Need to Learn\",\n                \"Mirages: On Anthropomorphism in Dialogue Systems, May 2023\",\n                \"The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation\",\n                \"Behavioural consequences of child abuse\",\n                \"Protecting One’s Own Privacy in a Big Data Economy\",\n                \"The Limitations of Online Surveys\",\n                \"”What We Can’t Measure, We Can’t Understand”: Challenges to Demographic Data Procurement in the Pursuit of Fairness\",\n                \"The Labor Market Impacts of Technological Change: From Unbridled Enthusiasm to Qualified Optimism to Vast Uncertainty, May 2022\",\n                \"Marginalization: Conceptualizing patient vulnerabilities in the framework of social determinants of health – An integrative review\",\n                \"Differential Privacy Has Disparate Impact on Model Accuracy\",\n                \"T. Telleen-Lawton\",\n                \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\",\n                \"State of AI Report 2022\",\n                \"“It’s Complicated”: Negotiating Accessibility and (Mis)Representation in Image Descriptions of Race, Gender, and Disability\",\n                \"Digital Labour Platforms and the Future of Work: Towards Decent Work in the Online World\",\n                \"Bloom: A 176b-parameter open-access multilingual language model, 2023\",\n                \"Power to the People? Opportunities and Challenges for Participatory AI\",\n                \"The Forgotten Margins of AI Ethics\",\n                \"Language (Technology) is Power: A Critical Survey of “Bias” in NLP\",\n                \"On the Opportunities and Risks of Foundation Models, 2022-07-12\",\n                \"Sorting Things Out Classification and Its Consequences\",\n                \"Beyond ethnography: engagement and reciprocity as foundations for design research out here\",\n                \"F. Mireshghallah\",\n                \"Language Models are Few-Shot Learners, 2020-07-22\",\n                \"Truth, Lies, and Automation, May 2021\",\n                \"Do Opt-Outs Really Opt Me Out?\",\n                \"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification\",\n                \"Human Rights and National Security: The Strategic Correlation\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]33 based on the context and research area of Evaluating the Social Impact of Generative AI Systems in Systems and Society. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}