{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic Segmentation\n\nPaper Abstract: \n\nPaper Section Text:\nFigure¬†4 shows our results, which analyze the embedding dimension hyperparameter of various unsupervised semantic segmentation techniques. As previously noted, we use three methods to reduce the dimensionality of the DINO features: the segmentation head of STEGO, PCA, and RP. In the latter two cases, we swap out STEGO‚Äôs segmentation head and train the linear and cluster probes on the PCA- or RP-transformed output directly while keeping the rest of the training and validation pipeline fixed. We observe several interesting results. For linear probing in Figure¬†4¬†(a‚Äìc), STEGO can reduce the dimensionality with little loss in performance up to approximately DSTEGOsubscriptùê∑STEGOD_{\\textrm{STEGO}}. This means that STEGO is an efficient non-linear dimensionality reduction technique. However, through an ablation across multiple dimensions, we confirm our experimental result from Tab.¬†2, indicating that STEGO provides little, if any, performance benefit over the DINO baseline for linear probe-style evaluation. By analyzing STEGO‚Äôs performance at the unreduced DINO dimension, i.e., DSTEGO=DViTsubscriptùê∑STEGOsubscriptùê∑ViTD_{\\textrm{STEGO}}=D_{\\textrm{ViT}}, we can isolate the segmentation head‚Äôs dimensionality reduction from its non-linear projection. Notably, features produced by the STEGO segmentation head at the unreduced dimension are less or equally linearly separable than DINO features. Hence, the segmentation head does not project DINO‚Äôs features in a useful way for linear probing. However, it should be noted that the hyperparameters of the STEGO training strategy are not fine-tuned and thus are strictly only compatible with the original output dimension DSTEGOsubscriptùê∑STEGOD_{\\textrm{STEGO}}. Hence, the reduced performance at the full dimension could also be due to hyperparameter instability.\nFinally, STEGO performs better or at least on par with the linear dimensionality reduction techniques on the linear probing task. Figure¬†4¬†(d, e) shows that STEGO consistently outperforms PCA, RP, and plain DINO across all embedding dimensions in the unsupervised clustering task. This demonstrates that STEGO can robustly project ViT features into a wide range of target dimensions ‚Äì even into much lower ones than suggested in the original paper¬†[ref]20. Hence, the segmentation head preserves semantic information for the clustering task while simultaneously reducing the number of embedding dimensions. However, STEGO‚Äôs superior performance for unsupervised clustering cannot only be attributed to its dimensionality-reducing aspect since we also see a performance boost in¬†Figure¬†4¬†(d, e) at the original dimension DSTEGO=DViT=768subscriptùê∑STEGOsubscriptùê∑ViT768D_{\\textrm{STEGO}}=D_{\\textrm{ViT}}=768. This observation could be evidence of Hamilton et al.‚Äôs argument of more compact and distinct clusters in features produced by STEGO that could facilitate better kùëòk-means convergence. However, STEGO‚Äôs improvement over DINO may also be due to the adaptation of the segmentation head to the new training data distribution. Training the segmentation head or Transformer adapter layers¬†[ref]21 using a simpler loss (e.g., the DINO loss) could potentially achieve comparable performance. Nevertheless, we conclude for the unsupervised downstream task: Besides efficiently reducing the dimension, STEGO also projects DINO‚Äôs output features to representations that are more suited for kùëòk-means clustering. In contrast, we did not observe a benefit of STEGO for linear probing over the DINO baseline at an unreduced dimension. Figure¬†4¬†(d, e) also demonstrates that PCA approximately retains the unsupervised performance of DINO up to an embedding dimension of D=48ùê∑48D=48 ‚Äì a significant reduction by a factor of 16 compared to the original dimension DViT=768subscriptùê∑ViT768D_{\\textrm{ViT}}=768 of the DINO baseline. Simultaneously, in Figure¬†4¬†(a‚Äìc), the performance of the PCA-transformed features on the linear probing downstream task approximately exponentially decreases with lower embedding dimensions (note that the xùë•x-axis is in log-scale).\nFrom Figure¬†3, we know that the first 48 components account for roughly 40‚Äâ% of variance explained for Cityscapes and 25‚Äâ% for Cocostuff. Therefore, lower performance of the PCA-projected features on the linear probing task is expected since the transformed features contain much less information. Surprisingly, the unsupervised clustering performance does not worsen until D=48ùê∑48D=48 for Cocostuff and Cityscapes, even though significantly less information is available. Hence, another factor is at play, improving performance for lower embedding dimensions. This factor likely is related to our previous argument that the kùëòk-means algorithm converges better in lower dimensions. Interestingly, all STEGO results in Figure¬†4 follow a similar trend, where the cluster and linear performance peaks at a dimension DSTEGO‚â™DViTmuch-less-thansubscriptùê∑STEGOsubscriptùê∑ViTD_{\\textrm{STEGO}}\\ll D_{\\textrm{ViT}}. Furthermore, Figure¬†4¬†(d, e) shows a similar trend for the PCA cluster performance, which also has a slight upwards trend at approximately DSTEGOsubscriptùê∑STEGOD_{\\textrm{STEGO}}. These results suggest some optimal configuration for the kùëòk-means algorithm, where both effects ‚Äì lower dimensions mean less information content, but a better kùëòk-means convergence ‚Äì balance out, again indicating that the dimensionality reduction is a critical contribution to STEGO‚Äôs performance in the unsupervised clustering case. Another explanation for the peak STEGO performance could be that the hyperparameters, which Hamilton et al.¬†[ref]20 identified, work best for DSTEGOsubscriptùê∑STEGOD_{\\textrm{STEGO}} and that hyperparameter tuning influences the results at other dimensions. As shown in Figure¬†4¬†(a, b, d, e), we are able to reproduce the originally reported performance of STEGO without CRF for Cocostuff and Cityscapes. However, the Potsdam results in Figure¬†4¬†(f) behave quite differently than those on the more diverse and larger datasets. While in our previous analysis, we found that all STEGO cluster probe results show a conceptually similar trend (i.e., performance peaks at a dimension D‚â™DViTmuch-less-thanùê∑subscriptùê∑ViTD\\ll D_{\\textrm{ViT}} and they robustly outperform the baselines), the entire Potsdam curve is shifted to the left and does not meet the expected performance of the pre-trained STEGO model. This vastly different behavior is likely due to two reasons. First, the Potsdam aerial imagery dataset is out of distribution input for the ImageNet-pre-trained DINO backbone, likely leading DINO to produce lower-quality embeddings. Second, the segmentation head is trained on significantly fewer data for Potsdam than Cityscapes and Cocostuff, making it more challenging for the module to compensate for lower-quality DINO features. Note that Potsdam is inherently smaller than Cityscapes and Cocostuff containing only roughly 4.5k training images as opposed to 15k and 490k pre-processed training images for Cityscapes and Cocostuff, respectively.\nNevertheless, it is impressive that STEGO achieves over 60% mIoU using only 6-dimensional embeddings in Figure¬†4¬†(f). A likely reason for much better relative performance at lower dimensions is the considerably easier segmentation task having only 3 classes for Potsdam, as opposed to 27 for Cocostuff and Cityscapes. Separating these three classes using the cluster probe in very high dimensions is challenging, again supporting our argument that STEGO is an efficient dimensionality reduction technique. Finally, due to computational efficiency, we cache the precomputed DINO features using the Squirrel library¬†[ref]38 for PCA and RP. Therefore, we apply a slightly different shuffling than the one used in the case of STEGO. The shuffling styles can influence the performance of the mini-batch kùëòk-means variant and the linear probe results, which might explain why the PCA and RP results in Figure¬†4 sometimes do not match the DINO performance at DViTsubscriptùê∑ViTD_{\\textrm{ViT}} exactly.\nNote that for all STEGO results shown in Figure¬†4, we run the original code with the unaltered shuffling strategy. Minor results in Figure¬†4 include that all other dimensionality reduction techniques in the linear and cluster probe downstream task outperform RP. This is somewhat surprising as RP approximately preserves pair-wise distances. A possible explanation might be the use of a cosine similarity-based kùëòk-means clustering. Investigating different unsupervised clusterings, such as a standard Euclidian distance-based kùëòk-means or more advanced clustering approaches, is an interesting direction for future investigations. However, fitting RP is exceptionally fast and compute-efficient. Finally, CRF improves performance by a few percent, as indicated in the STEGO paper¬†[ref]20 and other works in the field.\n\nReference markers found: [20]\n\nContext windows for each [ref]:\n[ref]20: unsupervised clustering task. This demonstrates that STEGO can robustly project ViT features into a wide range of target dimensions ‚Äì even into much lower ones than suggested in the original paper¬†[ref]20. Hence, the segmentation head preserves semantic information for the clustering task while simultaneously reducing the number of embedding dimensions. However, STEGO‚Äôs superior performance\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 20,\n            \"titles\": [\n                \"Parameter-Efficient Transfer Learning for NLP\",\n                \"Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation\",\n                \"Monitoring deforestation in Jordan using deep semantic segmentation with satellite imagery\",\n                \"Deep ViT Features as Dense Visual Descriptors\",\n                \"AgriSegNet: Deep aerial semantic segmentation framework for IoT-assisted precision agriculture\",\n                \"Random projection in dimensionality reduction: Applications to image and text data\",\n                \"Random projections for kùëòk-means clustering\",\n                \"COCO-Stuff: Thing and Stuff Classes in Context\",\n                \"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\",\n                \"Emerging properties in self-supervised vision transformers\",\n                \"A Simple Framework for Contrastive Learning of Visual Representations\",\n                \"PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering\",\n                \"Deep Feature Factorization For Concept Discovery\",\n                \"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\n                \"Experiments with random projection\",\n                \"The MNIST database of handwritten digit images for machine learning research [Best of the Web]\",\n                \"An image is worth 16√ó\\\\times16 words: Transformers for image recognition at scale\",\n                \"Deep sparse rectifier neural networks\",\n                \"Bootstrap Your Own Latent A New Approach to Self-Supervised Learning\",\n                \"GA-Nav: Efficient Terrain Segmentation for Robot Navigation in Unstructured Outdoor Environments\",\n                \"Unsupervised Semantic Segmentation by Distilling Feature Correspondences\",\n                \"Invariant Information Clustering for Unsupervised Image Classification and Segmentation\",\n                \"Extensions of Lipschitz maps into a Hilbert space\",\n                \"Efficient inference in fully connected CRFs with Gaussian edge potentials\",\n                \"Learning multiple layers of features from tiny images\",\n                \"The Hungarian method for the assignment problem\",\n                \"Semi-supervised Semantic Segmentation with Directional Context-aware Consistency\",\n                \"Some methods for classification and analysis of multivariate observations\",\n                \"Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization\",\n                \"Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation\",\n                \"On lines and planes of closest fit to systems of points in space\",\n                \"U-Net: Convolutional Networks for Biomedical Image Segmentation\",\n                \"ImageNet Large Scale Visual Recognition Challenge\",\n                \"Her2Net: A deep framework for semantic segmentation and classification of cell membranes and nuclei in breast cancer evaluation\",\n                \"Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction\",\n                \"A Survey on Label-efficient Deep Image Segmentation: Bridging the Gap between Weak Supervision and Dense Prediction\",\n                \"Normalized cuts and image segmentation\",\n                \"Squirrel: A Python library that enables ML teams to share, load, and transform data in a collaborative, flexible, and efficient way\",\n                \"Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations\",\n                \"Unsupervised semantic segmentation by contrasting object mask proposals\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]20 based on the context and research area of Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic Segmentation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}