{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Evaluating the Social Impact of Generative AI Systems in Systems and Society\n\nPaper Abstract: \n\nPaper Section Text:\nHuman trust in systems, institutions, and people represented by system outputs evolves as generative AI systems are increasingly embedded in daily life. WIth the increased ease of access to creating machine generated content, which produce misinformation  as a product, distinguishing between human and machine generated content, verified and misinformation, will become increasingly difficult and poses a series of threats to trust in media and what we can experience with our own hearing and vision. \n\nHigh capability generative AI systems create believable outputs across modalities and level of risk depends on use case. From impersonation spurring spamming to disinformation campaigns, the spread of misinformation online can be perpetuated by reinforcement and volume; people are more likely to believe false information when they see it more than once, for example if it has been shared by multiple people in their network . This can have devastating real world impacts, from attempting dangerous COVID-19 treatments , to inciting violence [ref]146, and the loss of trust in mainstream news [ref]95. The increasing sophistication of generative AI in recent years has expanded the possibilities of misinformation and disinformation campaigns, and made it harder for people to know when they should trust what they see or hear . Surveying trust can apply to trust in AI systems   to output factual information, trust in researchers, developers, and organizations developing and deploying AI , mitigation and detection measures , and trust in overall media and how it is distributed . Trust can be evaluated in the category of information, such as information about democratic and policy institutions . Evaluations and countermeasures of false and misleading information remain challenging. There is no universal agreement about what constitutes misinformation and much of the research on intervention remains siloed . Furthermore, current research efforts towards watermarking text remain brittle and the area of developing watermarks for machine generated outputs is an active research area . Interventions on technical systems include encouraging people to shift their attention to the accuracy of posts they might share , using crowd-sourced fact checking , and using digital forensics to detect AI-generated content . However, technical tools such as detection are less accurate as AI systems become more powerful . Emerging legal and regulatory approaches around the world include the EU AI Act, which requires labeling AI-generated content, and certain U.S. state laws that criminalize non-consensual deepfake pornography and deepfake content that interferes with elections , where lessons can be extrapolated to generated AI outputs. Policymakers and developers can also ban use cases where false outputs have highest risks. \n\nOverreliance on automation in general is a long-studied problem [ref]174, and carries over in novel and important ways to AI-generated content . People are prone to overestimate and put a higher degree of trust in AI generated content, especially when outputs appear authoritative or when people are in time-sensitive situations . This can be dangerous because many organizations are pursuing the use of large language models to help analyze information despite persistent flaws and limitations, which can lead to the spread of biased and inaccurate information . The study of human-generative AI relationships is nascent, but growing, and highlights that the anthropomorphism  of these technologies may contribute to unfounded trust and reliance  . Improving the trustworthiness of AI systems is an important ongoing effort across sectors  . Persistent security vulnerabilities in large language models and other generative AI systems are another reason why overreliance can be dangerous. For example, data poisoning, backdoor attacks, and prompt injection attacks can all trick large language models into providing inaccurate information in specific instances . For language, in the case of AI chatbots specifically, the conversational interface can additionally elicit trust and other strong emotions from people, even when they understand the limitations of the technology . Overreliance on such tools can not only make people prone to believe inaccurate information, but can also be abused to subtly change or manipulate people’s behaviors, for example to make them more likely to purchase particular products or even encourage self-harm . For language models trained on code and code generative systems, inaccurate outputs  can nullify potential benefits. Code generative systems can be evaluated for their limitations  and hazards , from alignment questions like producing bugs and harmful biases, to economic and environmental impacts (see Section 4.1 Impacts: The Technical Base System). There are few protections against these risks. Vulnerability disclosure, bug bounties, and AI incident databases can help report the vulnerabilities and limitations of generative AI systems. Several components of the EU AI Act may also be helpful, for example requiring labeling of AI-generated content, and prohibiting certain kinds of manipulation. For example, Section 5.2.2 of the 2021 proposal prohibits \"practices that have a significant potential to manipulate persons through subliminal techniques beyond their consciousness or exploit vulnerabilities of specific vulnerable groups such as children or persons with disabilities in order to materially distort their behavior in a manner that is likely to cause them or another person psychological or physical harm.” The proposal also notes, “Other manipulative or exploitative practices affecting adults that might be facilitated by AI systems could be covered by the existing data protection, consumer protection and digital service legislation that guarantee that natural persons are properly informed and have free choice not to be subject to profiling or other practices that might affect their behavior.”  \n\nPrivacy is linked with autonomy; to have privacy is to have control over information related to oneself. Privacy can protect both powerful and vulnerable peoples and is interpreted and protected differently by culture and social classes throughout history . Personal and private information has many legal definitions and protections globally  and when violated, can be distinct from harm  and refer to content that is shared, seen, or experienced outside of the sphere a person has consented to. As seen in the Technical Base System section on 4.1.4 Privacy and Data Protection, privacy can be evaluated in a system as well as its impacts on society. Impacts 30 and harms  from the loss and violation of privacy are difficult to enumerate and evaluate, such as loss of opportunity or reputational damage. Harms can lead to shifts in power differentials and less respect or influence in an affected environment, in addition to personal changes in expectations of privacy  and autonomy. The type of private information violated, such as medical information, can trigger different impacts and responses. Mitigation first should determine who is responsible for an individual’s privacy, while recognizing that all individuals may not have the same level of technical or data literacy. Robustly protecting privacy and autonomy requires both individual and collective action; an individual must be data-conscious in addition to technical and policy privacy protection provisions . Outside of an individualistic framework, certain rights such as refusal  and inclusion also requires consideration of individual self-determination: establishing how an individual wants to interact with technology. Technical methods to preserve privacy in a generative AI system, as seen in privacy-preserving approaches to language modeling , cannot guarantee full protection. Upholding privacy regulations requires engagement from multiple affected parties 9 and can protect individuals but fail at loopholes, as seen with tracking continuing when an individual opts-out  from data collection . Improving common practices and better global regulation for collecting training data can help. Opt-in approaches can protect individuals but are often not practiced due to economic incentives that stem from collecting data 44. Privacy options for users should ease accessibility 63, such as standardized form factors when users visit a website requesting privacy permissions.\n\nReference markers found: [146]\n\nContext windows for each [ref]:\n[ref]146: than once, for example if it has been shared by multiple people in their network . This can have devastating real world impacts, from attempting dangerous COVID-19 treatments , to inciting violence [ref]146, and the loss of trust in mainstream news [ref]95. The increasing sophistication of generative AI in recent years has expanded the possibilities of misinformation and disinformation\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 146,\n            \"titles\": [\n                \"Between Subjectivity and Imposition: Power Dynamics in Data Annotation for Computer Vision\",\n                \"Analyse and automate every message\",\n                \"Global Comprehensive Privacy Law Mapping Chart\",\n                \"Methodologies and Tools to Evaluate Climate Change Impacts and Adaptation\",\n                \"California Consumer Privacy Act of 2018\",\n                \"AB-5 Worker status: employees and independent contractors.\",\n                \"DOD Adopts Ethical Principles for Artificial Intelligence\",\n                \"COM (2021) 762: Proposal for a DIRECTIVE OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL on improving working conditions in platform work\",\n                \"Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS\",\n                \"Americans’ Views on Generative Artificial Intelligence, Hate and Harassment\",\n                \"Cyber violence is a growing threat, especially for women and girls\",\n                \"REAIM 2023\",\n                \"A Systematic Review of the Empirical Literature Evaluating IRBs: What We Know and What We Still Need to Learn\",\n                \"Mirages: On Anthropomorphism in Dialogue Systems, May 2023\",\n                \"인공지능 기반 공격 그래프 생성\",\n                \"The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation\",\n                \"Behavioural consequences of child abuse\",\n                \"Protecting One’s Own Privacy in a Big Data Economy\",\n                \"The Limitations of Online Surveys\",\n                \"”What We Can’t Measure, We Can’t Understand”: Challenges to Demographic Data Procurement in the Pursuit of Fairness\",\n                \"The Labor Market Impacts of Technological Change: From Unbridled Enthusiasm to Qualified Optimism to Vast Uncertainty, May 2022\",\n                \"Marginalization: Conceptualizing patient vulnerabilities in the framework of social determinants of health – An integrative review\",\n                \"Differential Privacy Has Disparate Impact on Model Accuracy\",\n                \"T. Telleen-Lawton\",\n                \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\",\n                \"State of AI Report 2022\",\n                \"“It’s Complicated”: Negotiating Accessibility and (Mis)Representation in Image Descriptions of Race, Gender, and Disability\",\n                \"Digital Labour Platforms and the Future of Work: Towards Decent Work in the Online World\",\n                \"Bloom: A 176b-parameter open-access multilingual language model, 2023\",\n                \"Power to the People? Opportunities and Challenges for Participatory AI\",\n                \"The Forgotten Margins of AI Ethics\",\n                \"Language (Technology) is Power: A Critical Survey of “Bias” in NLP\",\n                \"On the Opportunities and Risks of Foundation Models, 2022-07-12\",\n                \"Sorting Things Out Classification and Its Consequences\",\n                \"Beyond ethnography: engagement and reciprocity as foundations for design research out here\",\n                \"F. Mireshghallah\",\n                \"Language Models are Few-Shot Learners, 2020-07-22\",\n                \"Truth, Lies, and Automation, May 2021\",\n                \"Do Opt-Outs Really Opt Me Out?\",\n                \"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]146 based on the context and research area of Evaluating the Social Impact of Generative AI Systems in Systems and Society. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}