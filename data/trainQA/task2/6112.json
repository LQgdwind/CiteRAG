{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Logic for Explainable AI This work has been partially supported by NSF grant #ISS-1910317.\n\nPaper Abstract: \n\nPaper Section Text:\nWe assume in this section that our goal is to undermine the current decision, without targeting a particular alternate decision.\nThis aim can be achieved using the following notion. Suppose instance ‚Ñê‚Ñê{\\cal I} is in class cisubscriptùëêùëñc_{i}.\nThe necessary reasons (NRs) for the decision on instance ‚Ñê‚Ñê{\\cal I} are the prime implicates of the complete reason ‚àÄ‚Ñê‚ãÖŒîifor-all‚ãÖ‚ÑêsubscriptŒîùëñ\\forall{\\cal I}\\cdot\\Delta_{i}. Each necessary reason is a clause ‚Ñì1+‚Ä¶+‚Ñìnsubscript‚Ñì1‚Ä¶subscript‚Ñìùëõ\\ell_{1}+\\ldots+\\ell_{n} where each literal ‚Ñìisubscript‚Ñìùëñ\\ell_{i} is simple (a state) and appears in instance ‚Ñê‚Ñê{\\cal I}.888Since\nthe complete reason is simple and fixated on the instance. Hence, each necessary reason is satisfied by the instance.\nHowever, if we change the instance in a way that violates a necessary reason, the decision is no longer guaranteed to be sustained.\nSuppose state ‚Ñìisubscript‚Ñìùëñ\\ell_{i} is for feature XisubscriptùëãùëñX_{i}. To violate the necessary reason ‚Ñì1+‚Ä¶+‚Ñìnsubscript‚Ñì1‚Ä¶subscript‚Ñìùëõ\\ell_{1}+\\ldots+\\ell_{n} we must change the state of each\nfeature XisubscriptùëãùëñX_{i} in the instance. The guarantee that comes with a necessary reason is that at least one such change will flip the decision,\nand no strict subset of this change will flip the decision. The notion of a necessary reason was first formalized in¬† using a different but equivalent definition,\nunder the name of a contrastive explanation¬†.\nNecessary reasons are sometimes also called counterfactual explanations¬†.999There is an\nextensive body of work in philosophy, social science and AI that discusses contrastive explanations and counterfactual\nexplanations; see, e.g.,¬†    [ref]19    [ref]23 .\nWhile the definitions of these notions are sometimes variations or refinements on one another, they are not always compatible.\nThe term ‚Äúnecessary reason‚Äù was first used in¬†\nwhich showed that contrastive explanations are the prime implicates of the complete reason and provided the semantics of\nnecessity discussed earlier (a property that must be preserved if the decision is to be preserved). Let us now illustrate necessary reasons\nusing a concrete example. We are back to patient Lara given in¬†(1) who was decided as yes by the classifier in Fig.¬†3.\nThe class formula is given in¬†(2) and\nthe complete reason for the decision is given in¬†(3).\nThis complete reason has two prime implicates,\n(Diabetes‚Äã=‚Äãy)Diabetesy({{\\footnotesize\\textsc{Diabetes}}}\\mathord{=}\\mathord{{\\footnotesize\\textsc{y}}}) and (Weight‚Äã=‚Äãover)+(Btype‚Äã=‚ÄãA)WeightoverBtypeA({{\\footnotesize\\textsc{Weight}}}\\mathord{=}\\mathord{{\\footnotesize\\textsc{over}}})+({{\\footnotesize\\textsc{Btype}}}\\mathord{=}\\mathord{{\\footnotesize\\textsc{A}}}).\nThere is only one way to violate the first reason by setting Diabetes to n, which will indeed flip the decision to no\nas can be verified in Fig.¬†3. The second necessary reason can be violated in six different ways,\nby setting Weight to a state in {under,norm}undernorm\\{{\\footnotesize\\textsc{under}},{\\footnotesize\\textsc{norm}}\\} and setting Btype to a state in {B,AB,O}BABO\\{{\\footnotesize\\textsc{B}},{\\footnotesize\\textsc{AB}},{\\footnotesize\\textsc{O}}\\}.\nNot all of these changes/violations will flip the decision but at least one will.\nThe\nchange to (Weight‚Äã=‚Äãunder)‚ãÖ(Btype‚Äã=‚ÄãO)‚ãÖWeightunderBtypeO({{\\footnotesize\\textsc{Weight}}}\\mathord{=}\\mathord{{\\footnotesize\\textsc{under}}})\\cdot({{\\footnotesize\\textsc{Btype}}}\\mathord{=}\\mathord{{\\footnotesize\\textsc{O}}}) does flip the decision but the\nchange to (Weight‚Äã=‚Äãunder)‚ãÖ(Btype‚Äã=‚ÄãAB)‚ãÖWeightunderBtypeAB({{\\footnotesize\\textsc{Weight}}}\\mathord{=}\\mathord{{\\footnotesize\\textsc{under}}})\\cdot({{\\footnotesize\\textsc{Btype}}}\\mathord{=}\\mathord{{\\footnotesize\\textsc{AB}}}) does not (always assuming that Diabetes is left unchanged).\nWe stress again that no strict subset of the first change will flip the decision since the changes suggested by\nnecessary reasons are minimal. We will now see that we can do better than this if we employ the general reasons of decisions\nas suggested recently in¬†[ref]2. Suppose instance ‚Ñê‚Ñê{\\cal I} is in class cisubscriptùëêùëñc_{i}.\nThe general necessary reasons (GNRs) for the decision on ‚Ñê‚Ñê{\\cal I} are the variable-minimal prime implicates\nof general reason ‚àÄ¬Ø‚Äã‚Ñê‚ãÖŒîi‚ãÖ¬Øfor-all‚ÑêsubscriptŒîùëñ\\overline{\\forall}\\,{\\cal I}\\cdot\\Delta_{i}.101010This\ndefinition is a proposition in¬†[ref]2 which provided a different definition for general necessary reasons\nthat does not reference general reasons. Like necessary reasons, violating a general necessary reason will undermine the decision. However, we now\nhave an additional guarantee: every violation of a general necessary reason will flip the decision.\nThis guarantee will not hold if we do not insist on variable-minimal prime implicates, and if some prime implicate\nthat is not variable minimal satisfies this guarantee, then some changes it suggests cannot be minimal (i.e., we\ncan flip the decision with fewer changes to the instance).\nMoreover, any change\nthat flips the decision and is suggested by some necessary reason will also be suggested by some\ngeneral necessary reason. That is, the latter reasons subsume the former and convey more information\nabout decisions and the underlying classifiers.111111The general necessary\nreasons are fixated (Def.¬†7) on instance ‚Ñê‚Ñê{\\cal I}.\nLet us look at a concrete example. The general reason for the decision on Lara is given in¬†(6)\nand has three prime implicates All are variable-minimal so they are all general necessary reasons.\nThe first can be violated in only one way by setting Diabetes to n which will flip the decision.\nThe second can be violated in two ways by setting Weight to a state in {under,norm}undernorm\\{{\\footnotesize\\textsc{under}},{\\footnotesize\\textsc{norm}}\\}\nand setting Btype to state O. Both violations will flip the decision as can be verified using\nthe classifier in Fig.¬†3.\nThe third general necessary reason can be violated in two ways by setting Weight to state norm\nand setting Btype to a state in {AB,O}ABO\\{{\\footnotesize\\textsc{AB}},{\\footnotesize\\textsc{O}}\\}. Again, both violations will flip the decision.\nHence, every violation of these general necessary reasons is guaranteed to flip the decision.\nMoreover, every change suggested by a necessary reason and which flips the decision is also suggested by\nsome general necessary reason. We conclude this section by the following remarks.\nBoolean resolution derives the clause Œ±+Œ≤ùõºùõΩ\\alpha+\\beta from clauses x+Œ±ùë•ùõºx+\\alpha and x¬Ø+Œ≤¬Øùë•ùõΩ\\overline{x}+\\beta.\nMoreover, a classical method for computing prime implicates of Boolean CNFs is to close the\nCNF under resolution and remove subsumed clauses after each resolution step; see, e.g.,¬† .\nAs recently shown in¬†[ref]2,\nthis can also be done for discrete CNFs but using a generalized\nresolution rule that derives clause ‚Ñì‚ãÖ‚Ñì‚Ä≤+Œ±+Œ≤‚ãÖ‚Ñìsuperscript‚Ñì‚Ä≤ùõºùõΩ\\ell\\cdot\\ell^{\\prime}+\\alpha+\\beta from clauses ‚Ñì+Œ±‚Ñìùõº\\ell+\\alpha and ‚Ñì‚Ä≤+Œ≤superscript‚Ñì‚Ä≤ùõΩ\\ell^{\\prime}+\\beta\nwhere ‚Ñì‚Ñì\\ell and ‚Ñì‚Ä≤superscript‚Ñì‚Ä≤\\ell^{\\prime} are literals for the same variable. What is particularly\nstriking is this. If the CNF is locally fixated (Def.¬†7), as is the case for general reasons,\nthen one can compute the general necessary reasons by simply discarding clauses that are\nnot variable-minimal after each resolution step¬†[ref]2.\n\nReference markers found: [19]\n\nContext windows for each [ref]:\n[ref]19: called counterfactual explanations¬†.999There is an\nextensive body of work in philosophy, social science and AI that discusses contrastive explanations and counterfactual\nexplanations; see, e.g.,¬†    [ref]19    [ref]23 .\nWhile the definitions of these notions are sometimes variations or refinements on one another, they are not always compatible.\nThe term ‚Äúnecessary reason‚Äù was first used\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 19,\n            \"titles\": [\n                \"Explanation in Artificial Intelligence: Insights from the Social Sciences\",\n                \"Reasoning about Bayesian network classifiers\",\n                \"A new class of explanations for classifiers with non-binary variables\",\n                \"On Symbolically Encoding the Behavior of Random Forests\",\n                \"On the Computation of Necessary and Sufficient Explanations\",\n                \"On the reasons behind decisions\",\n                \"Graph-based algorithms for Boolean function manipulation\",\n                \"On Quantifying Literals in Boolean Logic and Its Applications to Explainable AI\",\n                \"Decomposable negation normal form\",\n                \"A Symbolic Approach to Explaining Bayesian Network Classifiers\",\n                \"Abduction-based explanations for machine learning models\",\n                \"On Tractable Representations of Binary Neural Networks\",\n                \"From contrastive to abductive explanations and back again\",\n                \"Contrastive explanation\",\n                \"On tractable XAI queries based on compiled representations\",\n                \"Forms of explanation: Rethinking the questions in social theory\",\n                \"Causal explanation\",\n                \"The contrast theory of why-questions\",\n                \"Counterfactual explanations without opening the black box: Automated decisions and the GDPR\",\n                \"Contrastive explanations with local foil trees\",\n                \"Explaining explanations in AI\",\n                \"Counterfactual visual explanations\",\n                \"Counterfactual explanations and algorithmic recourses for machine learning: A review\",\n                \"Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations\",\n                \"On generating the irredundant conjunctive and disjunctive normal forms of monotone Boolean functions\",\n                \"Boolean functions - theory, algorithms, and applications\",\n                \"Three Modern Roles for Logic in AI\",\n                \"A knowledge compilation map\",\n                \"Reasoning about bayesian network classifiers\",\n                \"Compiling bayesian network classifiers into decision graphs\",\n                \"Verifying Properties of Binarized Deep Neural Networks\",\n                \"In search for a sat-friendly binarized neural network architecture\",\n                \"Assessing heuristic machine learning explanations with model counting\",\n                \"Quantitative Verification of Neural Networks And Its Security Applications\",\n                \"Verifying binarized neural networks by angluin-style learning\",\n                \"Efficient Exact Verification of Binarized Neural Networks\",\n                \"BDD4BNN: A BDD-based Quantitative Analysis Framework for Binarized Neural Networks\",\n                \"How many bits does it take to quantize your neural network?\",\n                \"Scalable verification of quantized neural networks\",\n                \"QVIP: An ILP-based Formal Verification Approach for Quantized Neural Networks\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]19 based on the context and research area of Logic for Explainable AI This work has been partially supported by NSF grant #ISS-1910317.. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}