{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models\n\nPaper Abstract: \n\nPaper Section Text:\nModel-based iterative reconstruction (MBIR). \n\nConsider a linear forward model for an imaging system (e.g. CT, MRI) where 𝒚∈ℝm𝒚superscriptℝ𝑚{\\bm{y}}\\in{\\mathbb{R}}^{m} is the measurement (i.e. sinogram, k-space), 𝒙∈ℝn𝒙superscriptℝ𝑛{\\bm{x}}\\in{\\mathbb{R}}^{n} is the image that we wish to reconstruct, 𝑨∈ℝm×n𝑨superscriptℝ𝑚𝑛{\\bm{A}}\\in{\\mathbb{R}}^{m\\times n} is the discrete transform matrix (i.e. Radon, Fourier111While we denote real-valued transforms and measurements for the simplicity of exposition, the discrete Fourier transform (DFT) matrix, and the corresponding measurement are complex-valued.), and 𝒏𝒏{\\bm{n}} is the measurement noise in the system. As the problem is ill-posed, a standard approach for the inverse problem\nthat estimates the unknown image 𝒙𝒙{\\bm{x}} from the measurement 𝒚𝒚{\\bm{y}} is to perform the following regularized reconstruction: where R𝑅R is the suitable regularization for 𝒙𝒙{\\bm{x}}, for instance, sparsity in some transformed domain. One widely used function is the TV penalty, R​(𝒙)=‖𝑫​𝒙‖2,1𝑅𝒙subscriptnorm𝑫𝒙21R({\\bm{x}})=\\|{\\bm{D}}{\\bm{x}}\\|_{2,1}, where 𝑫:=[𝑫x,𝑫y,𝑫z]Tassign𝑫superscriptsubscript𝑫𝑥subscript𝑫𝑦subscript𝑫𝑧𝑇{\\bm{D}}:=[{\\bm{D}}_{x},{\\bm{D}}_{y},{\\bm{D}}_{z}]^{T} computes the finite difference in each axis. Minimization of (2) can be performed with robust optimization algorithms, such as fast iterative soft thresholding algorithm (FISTA)  or ADMM. Score-based diffusion models. \n\nA score-based diffusion model is a generative model that defines the generative process as the reverse of the data noising process. Specifically,\nconsider the stochastic process {𝒙​(t)≜𝒙t},t∈ ≜𝒙𝑡subscript𝒙𝑡𝑡01\\{{\\bm{x}}(t)\\triangleq{\\bm{x}}_{t}\\},\\,t\\in , where we introduce the time variable t𝑡t to represent the evolution of the random variable. Particularly, we define p​(𝒙0)≜pdata​(𝒙)≜𝑝subscript𝒙0subscript𝑝data𝒙p({\\bm{x}}_{0})\\triangleq p_{\\rm{data}}({\\bm{x}}), i.e. the data distribution, and p​(𝒙T)𝑝subscript𝒙𝑇p({\\bm{x}}_{T}) to approximately a Gaussian distribution. The evolution can be formalized with the following stochastic differential equation where 𝒇​(𝒙,t):ℝn×1↦ℝn:𝒇𝒙𝑡maps-tosuperscriptℝ𝑛1superscriptℝ𝑛{\\bm{f}}({\\bm{x}},t):{\\mathbb{R}}^{n\\times 1}\\mapsto{\\mathbb{R}}^{n} is the drift function, g​(t):ℝ↦ℝ:𝑔𝑡maps-toℝℝg(t):{\\mathbb{R}}\\mapsto{\\mathbb{R}} is the scalar diffusion function, and 𝒘𝒘{\\bm{w}} is the n−limit-from𝑛n-dimensional standard Brownian motion 7. Let 𝒇​(𝒙,t)=0,g​(t)=d​[σ2​(t)]d​tformulae-sequence𝒇𝒙𝑡0𝑔𝑡𝑑delimited-[]superscript𝜎2𝑡𝑑𝑡{\\bm{f}}({\\bm{x}},t)=0,g(t)=\\sqrt{\\frac{d[\\sigma^{2}(t)]}{dt}}. Then, the SDE simplifies to the following Brownian motion in which the mean remains the same across the evolution, while Gaussian noise will be continuously added to 𝒙𝒙{\\bm{x}}, eventually approaching pure Gaussian noise as the noise term dominates. This is the so called variance-exploding SDE (VE-SDE) in the literature , and as we construct all our methods on the VE-SDE, we derive what follows from (4). Directly applying Anderson’s theorem   leads to the following reverse SDE where d​t,d​𝒘¯𝑑𝑡𝑑¯𝒘dt,d\\bar{{\\bm{w}}} are the reverse time differential, and the reverse standard n−limit-from𝑛n-dimensional Brownian motion. (5) defines the generative process of the diffusion model, where the equation can be solved by numerical integration. Notably, the key workhorse in the integration step is the score function ∇𝒙tlog⁡p​(𝒙t)subscript∇subscript𝒙𝑡𝑝subscript𝒙𝑡\\nabla_{{\\bm{x}}_{t}}\\log p({\\bm{x}}_{t}), that can be trained with denoising score matching (DSM) [ref]34 where 𝒔θ​(𝒙​(t),t):ℝn×1↦ℝn:subscript𝒔𝜃𝒙𝑡𝑡maps-tosuperscriptℝ𝑛1superscriptℝ𝑛{\\bm{s}}_{\\theta}({\\bm{x}}(t),t):{\\mathbb{R}}^{n\\times 1}\\mapsto{\\mathbb{R}}^{n} is a time-dependent neural network, and λ​(t)𝜆𝑡\\lambda(t) is the weighting scheme. Since ∇𝒙tlog⁡p​(𝒙​(t)|𝒙​(0))subscript∇subscript𝒙𝑡𝑝conditional𝒙𝑡𝒙0\\nabla_{{\\bm{x}}_{t}}\\log p({\\bm{x}}(t)|{\\bm{x}}(0)) is simply the residual noise added to 𝒙​(t)𝒙𝑡{\\bm{x}}(t) from 𝒙​(0)𝒙0{\\bm{x}}(0) scaled with noise variance, optimizing for (6) amounts to training a residual denoiser across multiple noise scales - a fairly robust training scheme. While the training is robust, the equivalence between DSM and explicit score matching (ESM) can be established [ref]34 in the optimization sense, and hence sθ∗​(𝒙​(t),t)≃∇𝒙tlog⁡p​(𝒙t)similar-to-or-equalssubscript𝑠superscript𝜃𝒙𝑡𝑡subscript∇subscript𝒙𝑡𝑝subscript𝒙𝑡s_{\\theta^{*}}({\\bm{x}}(t),t)\\simeq\\nabla_{{\\bm{x}}_{t}}\\log p({\\bm{x}}_{t}) can be used as a plug-in approximate in practice, i.e. One can solve (7) with, e.g. the predictor-corrector (PC) sampler  by discretization of the time interval  01  to N𝑁N bins. 3D diffusion. \n\nThe generative process (i.e. reverse diffusion), explicitly represented by (7), runs in the full data dimension ℝnsuperscriptℝ𝑛{\\mathbb{R}}^{n}. It is widely known that the voxel representation of 3D data is heavy, and scaling the data size over 643superscript64364^{3} requires excessive GPU memory 9. For example, a recent work that utilizes diffusion models for 3D shape reconstruction  uses the dataset of size 643superscript64364^{3}. Other works that use diffusion models for 3D generative modeling typically focus on the more efficient point cloud representation 0  1, where the number of point clouds remain less than a few thousand (e.g. 2048≪643much-less-than2048superscript6432048\\ll 64^{3} in 0 ). Naturally, point cloud representations are efficient but extremely sparse, certainly not suitable for the problem of tomographic reconstruction, where we require accurate estimation of the interior. There is one concurrent workshop paper that aims for designing a diffusion model that can model the 3D voxel representation 5. In 5, the authors train a score function that can model 160×224×160160224160160\\times 224\\times 160 volumes, by training a latent diffusion model 6, where the latent dimension is relatively small (i.e. 20×28×2020282020\\times 28\\times 20). However, the model requires 1000 synthetic 3D volumes for the training dataset. More importantly, using latent diffusion models for solving inverse problems is not straightforward, and has never been reported in literature. Solving inverse problems with diffusion. \n\nSolving the reverse SDE with the approximated score function (7) amounts to sampling from the prior distribution p​(𝒙)𝑝𝒙p({\\bm{x}}). For the case of solving inverse problem, we desire to sample from the posterior distribution p​(𝒙|𝒚)𝑝conditional𝒙𝒚p({\\bm{x}}|{\\bm{y}}), where the relationship between the two can be formulated by the Bayes’ rule p​(𝒙|𝒚)=p​(𝒙)​p​(𝒚|𝒙)/p​(𝒚)𝑝conditional𝒙𝒚𝑝𝒙𝑝conditional𝒚𝒙𝑝𝒚p({\\bm{x}}|{\\bm{y}})=p({\\bm{x}})p({\\bm{y}}|{\\bm{x}})/p({\\bm{y}}), leading to Here, the likelihood term enforces the data consistency, and thereby inducing samples that satisfy 𝒚=𝑨​𝒙𝒚𝑨𝒙{\\bm{y}}={\\bm{A}}{\\bm{x}}. Two approaches to incorporating (8) exist in the literature. First, one can split the update step into the 1) prior update (i.e. denoising), and then 2) projection in to the measurement subspace  . Formally, in the discrete setting, where SolveSolve{\\rm Solve} denotes a general numerical solver that can solve the reverse-SDE in (7), and 𝒫Csubscript𝒫𝐶{\\mathcal{P}}_{C} denotes the projection operator to the set C𝐶C. Specifically, when using the Euler-Maruyama discretization, the equation reads222For all equations and algorithms that are presented, we refer to the sampled random Gaussian noise as ϵ∼𝒩​(0,𝑰)similar-tobold-italic-ϵ𝒩0𝑰\\bm{\\epsilon}\\sim{\\mathcal{N}}(0,{\\bm{I}}), unless specified otherwise. Note that the stochasticity of Solve​(⋅)Solve⋅{\\rm Solve(\\cdot)} is implicitly defined.\nIt was shown in  that using the PC solver, which alternates between the numerical SDE solver and monte carlo markov chain (MCMC) steps leads to superior performance. Throughout the manuscript, we refer to a single step of PC sampler as Solve​(⋅)Solve⋅{\\rm Solve(\\cdot)} unless specified otherwise.\nAlternatively, one can try to explicitly approximate the gradient of the log likelihood and take the update in a single step .\n\nReference markers found: [34]\n\nContext windows for each [ref]:\n[ref]34: in the integration step is the score function ∇𝒙tlog⁡p​(𝒙t)subscript∇subscript𝒙𝑡𝑝subscript𝒙𝑡\\nabla_{{\\bm{x}}_{t}}\\log p({\\bm{x}}_{t}), that can be trained with denoising score matching (DSM) [ref]34 where 𝒔θ​(𝒙​(t),t):ℝn×1↦ℝn:subscript𝒔𝜃𝒙𝑡𝑡maps-tosuperscriptℝ𝑛1superscriptℝ𝑛{\\bm{s}}_{\\theta}({\\bm{x}}(t),t):{\\mathbb{R}}^{n\\times 1}\\mapsto{\\mathbb{R}}^{n} is a time-dependent neural network,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 34,\n            \"titles\": [\n                \"A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images\",\n                \"Reverse-time diffusion equation models\",\n                \"A fast iterative shrinkage-thresholding algorithm for linear inverse problems\",\n                \"Distributed optimization and statistical learning via the alternating direction method of multipliers\",\n                \"ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models\",\n                \"Improving Diffusion Models for Inverse Problems using Manifold Constraints\",\n                \"Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction\",\n                \"Score-based diffusion models for accelerated MRI\",\n                \"Diffusion Models Beat GANs on Image Synthesis\",\n                \"Denoising Diffusion Probabilistic Models\",\n                \"Robust Compressed Sensing MRI with Deep Generative Priors\",\n                \"Deep Convolutional Neural Network for Inverse Problems in Imaging\",\n                \"k-t focuss: a general compressed sensing framework for high resolution dynamic mri\",\n                \"A Deep Convolutional Neural Network using Directional Wavelets for Low-dose X-ray CT Reconstruction\",\n                \"Model-based iterative reconstruction technique for radiation dose reduction in chest ct: comparison with the adaptive statistical iterative reconstruction technique\",\n                \"Denoising Diffusion Restoration Models\",\n                \"Sparse-view Cone Beam CT Reconstruction using Data-consistent Supervised and Adversarial Learning from Scarce Training Data\",\n                \"Model-based iterative reconstruction: a promising algorithm for today’s computed tomography imaging\",\n                \"Adaptive-weighted total variation minimization for sparse data toward low-dose x-ray computed tomography image reconstruction\",\n                \"Point-voxel cnn for efficient 3d deep learning\",\n                \"Diffusion Probabilistic Models for 3D Point Cloud Generation\",\n                \"A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion\",\n                \"The multimodal brain tumor image segmentation benchmark (brats)\",\n                \"Single-slice rebinning method for helical cone-beam ct\",\n                \"Proximal algorithms\",\n                \"Brain Imaging Generation with Latent Diffusion Models\",\n                \"High-Resolution Image Synthesis with Latent Diffusion Models\",\n                \"Applied stochastic differential equations, volume 10\",\n                \"Image reconstruction in circular cone-beam computed tomography by constrained, total-variation minimization\",\n                \"Maximum Likelihood Training of Score-Based Diffusion Models\",\n                \"Solving inverse problems in medical imaging with score-based generative models\",\n                \"Score-Based Generative Modeling through Stochastic Differential Equations\",\n                \"Exploiting structural redundancy in q-space for improved eap reconstruction from highly undersampled (k, q)-space in dmri\",\n                \"A connection between score matching and denoising autoencoders\",\n                \"fastMRI: An Open Dataset and Benchmarks for Accelerated MRI\",\n                \"Image prediction for limited-angle tomography via deep learning with convolutional neural network\",\n                \"DuDoRNet: Learning a Dual-Domain Recurrent Network for Fast MRI Reconstruction with Deep T1 Prior\",\n                \"3d shape generation and completion through point-voxel diffusion\",\n                \"Generated Reference Title 38 for [ref]34\",\n                \"Generated Reference Title 39 for [ref]34\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]34 based on the context and research area of Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}