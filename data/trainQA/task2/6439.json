{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Framework for dynamically meeting performance objectives on a service mesh\n\nPaper Abstract: \n\nPaper Section Text:\nThe work described in this paper falls into several broad areas of traditional and recent research in resource management, namely self-adaptive systems (surveys ), autonomic computing and networking (e.g., ), self-driving systems (e.g., ) and intent-based networking (see e.g., ). The solutions our framework gives rise to are self-adaptive since the agent policy dynamically adapts the system to a changing environment; they are autonomous because this adaptation is performed without human intervention; they follow self-driving principles in the sense that they navigate the system towards a specific objective; and they are intent-based because they implement an end-to-end management objective. Our work specifically focuses on adaptive resource management for cloud services using reinforcement learning. There are several survey papers covering this topic, for instance, [ref]27 and . (The main difference between them is that [ref]27 reviews work that uses virtual machines (VMs) while  discusses papers that use containers instead). The objectives considered in the cited papers are similar to our work. They contain variables like resource utilization, quality of service, and cost. The control functions used to meet the objectives include scaling, scheduling, and migration. A common problem these investigations face stems from the fact that reinforcement learning often requires long training times, which we discuss in detail below. Resource allocation is investigated in many papers as the problem of designing a single control function, such as horizontal scaling     [ref]33      , application-layer routing , or placement and migration of the microservice instances. [ref]31. When designing such a function, the authors associate a performance objective, for example meeting a delay constraint for an application while minimizing the CPU allocation , or meeting SLO constraints while minimizing energy consumption . Note that we take a different approach with our framework. We start with an end-to-end performance objective and then direct the available resource control functions to jointly meet the objective. The RL agent takes a combination of actions, whereby one type of action corresponds to one resource management function. In the scenarios studied in this paper, the management objective can be achieved through any combination of three functions, request routing, blocking, and scaling. (If desired, additional control functions like placement or horizontal scaling can be organically added to our framework.) Therefore, our approach can be described as top-down, and it is more general than virtually all related works we are aware of, which create bottom-up solutions that are use-case specific. In the literature resource management problems are generally formulated as optimization problems with constraints. Among the works studying adaptive resource management on microservice-based applications, we find these problems formulated using mathematical frameworks like control theory , decision theory , or scheduling theory . Approaches to solve these optimization problems include reinforcement learning [ref]33[ref]31, dynamic programming , contextual bandit, and Bayesian optimization . Reinforcement learning has become a popular approach in recent research, because it allows controlling complex systems with little knowledge about their structure and internal workings and is therefore well suited for cloud-based applications. The main challenge of applying reinforcement learning to resource management is the long training time of the agent, which originates from the large number of interactions of the agent with the environment that are needed to optimally solve a resource allocation problem. Several strategies are pursued in the literature to address this issue, often in combination. The first strategy is called model-based reinforcement learning [ref]31. It includes creating a mathematical model of the environment, for instance using queuing theory  or caching methods . Alternatively, it uses supervised learning to obtain a system model . This is the approach we are pursuing in this paper. The second strategy is to control only a small part of the target system, for instance a single microservice, in a single control action [ref]33. This significantly reduces the complexity of the control problem and requires a much smaller number of interactions of the agent with the target system. This strategy is often used with reactive schemes that resolve performance bottlenecks. A third approach consists of discretizing the action space and sometimes the state space as well, thus restricting the number of possible actions and states to small numbers. This limits the number of possible state-action pairs that need to be considered by the training algorithm . In our work, we limit the number of possible control actions for this reason (see Section II). The downside of this approach is that the management objective is achieved with respect to the discretized control and state spaces, which can result in suboptimal control. The fourth strategy to reduce training time entails training the agent on a simulator instead of on the target system. Note that this method does not reduce the number of interactions of the agent with the environment but takes advantage of the fact that the interaction time on a simulator is typically orders of magnitude smaller than on the target system. For this reason, we have included a simulator in our framework. Orchestration platforms like Kubernetes include automated resource management functions. For example, Kubernetes Horizontal Pod Autoscaling (HPA)  performs horizontal scaling based on thresholds that the user can set. The system then performs scaling actions to keep the CPU utilization below the given threshold. This autoscaling function can be included in our framework and expand the control capabilities we have implemented in our testbed. However, note that this function alone is not an alternative to our framework, since it only uses CPU consumption thresholds to trigger autoscaling [ref]33 and cannot in a meaningful way implement complex management objectives like those discussed in this paper . Important related works on adaptive resource management for microservice-based applications using reinforcement learning are listed in Table III. All use reinforcement learning to find the optimal control policy. All works (except ours) address a very specific use case and do not develop a general approach like we introduce in this paper.  is of interest because it uses external traces from an operational environment to initialize the policy. [ref]31 does not consider a system model but instead solves an optimal placing problem using reinforcement learning.\nThe authors of [ref]33 train an RL agent on the target system in an online fashion. They perform resource management in a reactive manner as follows. Once an SLO violation is detected, a microservice component is identified as a possible cause. Then, the allocation of resources to this particular microservice is adjusted using reinforcement learning.\nThe authors in  follow the same approach as we do by training the agent in a simulation environment and evaluating the learned policy on the target system. They study resource management for scientific workflows, whereby resources are allocated to microservices that process workflow tasks in order to minimize the average processing time for a task.\n\nReference markers found: [27]\n\nContext windows for each [ref]:\n[ref]27: objective. Our work specifically focuses on adaptive resource management for cloud services using reinforcement learning. There are several survey papers covering this topic, for instance, [ref]27 and . (The main difference between them is that [ref]27 reviews work that uses virtual machines (VMs) while  discusses papers that use containers instead). The objectives considered in the\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 27,\n            \"titles\": [\n                \"Reinforcement Learning-based Application Autoscaling in the Cloud: A Survey\",\n                \"“Simplify observability\",\n                \"Simplify observability, traffic management, security, and policy with the leading service mesh\",\n                \"Production-grade container orchestration\",\n                \"Dynamically meeting performance objectives for multiple services on a service mesh\",\n                \"Markov decision processes: discrete stochastic dynamic programming\",\n                \"Reinforcement learning: An introduction\",\n                \"Feedback control of dynamic systems\",\n                \"Reinforcement learning and optimal control\",\n                \"Random Forests\",\n                \"sklearn.ensemble.RandomForestClassifier\",\n                \"Feedback control of computing systems\",\n                \"A Closer Look at Invalid Action Masking in Policy Gradient Algorithms\",\n                \"Maskable ppo\",\n                \"인공지능 기반 공격 그래프 생성\",\n                \"Python\",\n                \"Flask\",\n                \"Pods\",\n                \"Proximal policy optimization algorithms\",\n                \"Ppo\",\n                \"Towards goal-oriented development of self-adaptive systems\",\n                \"A survey of formal methods in self-adaptive systems\",\n                \"A survey on engineering approaches for self-adaptive systems\",\n                \"The vision of autonomic computing\",\n                \"Autonomic computing: An overview\",\n                \"Self-driving network and service coordination using deep reinforcement learning\",\n                \"Intent-based networking with proactive load distribution in data center using ibn manager and smart path manager\",\n                \"Intent based network operations\",\n                \"Self-adaptive container deployment in the fog: A survey\",\n                \"Coscal: Multi-faceted scaling of microservices with reinforcement learning\",\n                \"Self-learning multi-objective service coordination using deep reinforcement learning\",\n                \"Heuristic and Reinforcement Learning Algorithms for Dynamic Service Placement on Mobile Edge Cloud\",\n                \"Self-adaptive threshold-based policy for microservices elasticity\",\n                \"FIRM: An Intelligent Fine-Grained Resource Management Framework for SLO-Oriented Microservices\",\n                \"Miras: Model-based reinforcement learning for microservice resource allocation over scientific workflows\",\n                \"Graf: A graph neural network based proactive resource allocation framework for slo-oriented microservices\",\n                \"Horizontal scaling in cloud using contextual bandits\",\n                \"A lightweight workload-aware microservices autoscaling with qos assurance\",\n                \"Microscaler: Cost-effective scaling for microservice applications in the cloud with an online learning approach\",\n                \"Throughput-aware partitioning and placement of applications in fog computing\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]27 based on the context and research area of A Framework for dynamically meeting performance objectives on a service mesh. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}