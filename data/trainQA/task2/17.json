{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Entangled Polynomial Codes for Secure, Private, and Batch Distributed Matrix Multiplication: Breaking the “Cubic” Barrier\n\nPaper Abstract: \n\nPaper Section Text:\nAn important application of PCC is to consider a more generalized matrix multiplication setting where the inputs are block-wise partitioned.\nIn a basic setup, given a pair of input matrices A∈𝔽s×t𝐴superscript𝔽𝑠𝑡A\\in\\mathbb{F}^{s\\times t}, B∈𝔽s×r𝐵superscript𝔽𝑠𝑟{B}\\in\\mathbb{F}^{s\\times r} for a sufficiently large field 𝔽𝔽\\mathbb{F}, each worker i𝑖i is assigned a pair of possibly coded matrices A~i∈𝔽sp×tmsubscript~𝐴𝑖superscript𝔽𝑠𝑝𝑡𝑚\\tilde{A}_{i}\\in\\mathbb{F}^{\\frac{s}{p}\\times\\frac{t}{m}} and B~i∈𝔽sp×rnsubscript~𝐵𝑖superscript𝔽𝑠𝑝𝑟𝑛\\tilde{B}_{i}\\in\\mathbb{F}^{\\frac{s}{p}\\times\\frac{r}{n}}, which are encoded based on some (possibly random) functions of the input matrices respectively (see Fig. 4). The workers can each compute C~i≜A~i⊺​B~i≜subscript~𝐶𝑖superscriptsubscript~𝐴𝑖⊺subscript~𝐵𝑖\\tilde{C}_{i}\\triangleq\\tilde{A}_{i}^{\\intercal}\\tilde{B}_{i} and return them to the master. The master tries to recover the final product C≜A⊺​B≜𝐶superscript𝐴⊺𝐵C\\triangleq A^{\\intercal}B based on results from a subset of fastest workers using some decoding functions. By partitioning the input matrices into p𝑝p-by-m𝑚m and p𝑝p-by-n𝑛n subblocks, the product of two input matrices can thus be viewed as linear combinations of p​m​n𝑝𝑚𝑛pmn submatrix products according to block-matrix-multiplication rules, which can be computed using p​m​n𝑝𝑚𝑛pmn workers with uncoded inputs.\nWe aim to achieve the minimum possible recovery threshold given parameters p,m,n,𝑝𝑚𝑛p,m,n, and N𝑁N. The best known recovery threshold for computing block-partitioned matrix multiplication is achieved by a class of PCC designs referred to as entangled polynomial codes. In particular,\nentangled polynomial codes achieves a recovery threshold of min⁡{p​m​n+p−1,2​R​(p,m,n)−1}𝑝𝑚𝑛𝑝12𝑅𝑝𝑚𝑛1\\min\\{pmn+p-1,2R(p,m,n)-1\\} for any p,m,𝑝𝑚p,m, and n𝑛n. Here R​(p,m,n)𝑅𝑝𝑚𝑛R(p,m,n) denotes the bilinear complexity [ref]56 for multiplying two matrices of sizes m𝑚m-by-p𝑝p and p𝑝p-by-n𝑛n. It is well know that R​(p,m,n)𝑅𝑝𝑚𝑛R(p,m,n) is subcubic, i.e, R​(p,m,n)=o​(p​m​n)𝑅𝑝𝑚𝑛𝑜𝑝𝑚𝑛R(p,m,n)=o(pmn) when p𝑝p, m𝑚m, and n𝑛n are large. Hence, it order-wise outperforms other block-partitioning based schemes in related works for straggler mitigation (e.g., [ref]16). We present entangled polynomial codes as follows. The input matrices are partitioned into p𝑝p-by-m𝑚m and p𝑝p-by-n𝑛n submatrices of equal sizes, denoted Aj,ksubscript𝐴𝑗𝑘A_{j,k} and Bj,k′subscript𝐵𝑗superscript𝑘′B_{j,k^{\\prime}} for j∈{0,…,p−1}𝑗0…𝑝1j\\in\\{0,...,p-1\\}, k∈{0,…,m−1}𝑘0…𝑚1k\\in\\{0,...,m-1\\}, and k′∈{0,…,n−1}superscript𝑘′0…𝑛1k^{\\prime}\\in\\{0,...,n-1\\}, and we aim to recover Ck,k′≜∑iAj,k⊺​Bj,k′≜subscript𝐶𝑘superscript𝑘′subscript𝑖superscriptsubscript𝐴𝑗𝑘⊺subscript𝐵𝑗superscript𝑘′C_{k,k^{\\prime}}\\triangleq\\sum_{i}A_{j,k}^{\\intercal}B_{j,k^{\\prime}} for any k𝑘k and k′superscript𝑘′k^{\\prime}.\nA basic version of entangled polynomial codes is designed to achieve a recovery threshold of p​m​n+p−1𝑝𝑚𝑛𝑝1pmn+p-1, which generalizes the Polynomial codes, and encodes the input variables using c​(x)=(∑j=0p−1∑k=0m−1Aj,k​xj+k​p,∑j=0p−1∑k′=0n−1Bj,k′​xp−1−j+k′​p​m)𝑐𝑥superscriptsubscript𝑗0𝑝1superscriptsubscript𝑘0𝑚1subscript𝐴𝑗𝑘superscript𝑥𝑗𝑘𝑝superscriptsubscript𝑗0𝑝1superscriptsubscriptsuperscript𝑘′0𝑛1subscript𝐵𝑗superscript𝑘′superscript𝑥𝑝1𝑗superscript𝑘′𝑝𝑚c(x)=(\\sum_{j=0}^{p-1}\\sum_{k=0}^{m-1}A_{j,k}x^{j+kp},\\sum_{j=0}^{p-1}\\sum_{k^{\\prime}=0}^{n-1}B_{j,k^{\\prime}}x^{p-1-j+k^{\\prime}pm}). Explicitly, given any distinct evaluation points y1,…,yNsubscript𝑦1…subscript𝑦𝑁y_{1},...,y_{N}, each worker i𝑖i obtains (A~i,B~i)subscript~𝐴𝑖subscript~𝐵𝑖(\\tilde{A}_{i},\\tilde{B}_{i}) where This coding construction results in the following composed polynomial which has a degree of p​m​n+p−2𝑝𝑚𝑛𝑝2pmn+p-2, and the m​n𝑚𝑛mn needed linear combinations are exactly provided by m​n𝑚𝑛mn of its coefficients. Importantly, in the same paper, an improved version of the entangled polynomial code is presented to approach the optimal recovery threshold for general p,m𝑝𝑚p,m, and n𝑛n, which achieves a recovery threshold of 2​R​(p,m,n)−12𝑅𝑝𝑚𝑛12R(p,m,n)-1 (Theorem 3, ). Given any upper bound construction of R​(p,m,n)𝑅𝑝𝑚𝑛R(p,m,n) (e.g., Strassen’s construction) with rank R𝑅R and tensor tuples a∈𝔽R×p×m𝑎superscript𝔽𝑅𝑝𝑚a\\in\\mathbb{F}^{R\\times p\\times m}, b∈𝔽R×p×n𝑏superscript𝔽𝑅𝑝𝑛b\\in\\mathbb{F}^{R\\times p\\times n}, and c∈𝔽R×m×n𝑐superscript𝔽𝑅𝑚𝑛c\\in\\mathbb{F}^{R\\times m\\times n}, the inputs are each pre-encoded into a list of R𝑅R coded submatrices.333For detailed definitions, see [ref]42. Then the variables are encoded using c​(x)=∑j(A~j,vec,B~j,vec)⋅∏k≠j(x−xk)(xj−xk)𝑐𝑥subscript𝑗⋅subscript~𝐴𝑗vecsubscript~𝐵𝑗vecsubscriptproduct𝑘𝑗𝑥subscript𝑥𝑘subscript𝑥𝑗subscript𝑥𝑘c(x)=\\sum_{j}(\\tilde{A}_{j,\\textup{vec}},\\tilde{B}_{j,\\textup{vec}})\\cdot\\prod_{k\\neq{j}}\\frac{(x-x_{k})}{(x_{j}-x_{k})}. I.e., each worker obtains where x1subscript𝑥1x_{1},…,xRsubscript𝑥𝑅x_{R} are arbitrary distinct elements of 𝔽𝔽\\mathbb{F}. The coding construction provides a composed polynomial f​(c)𝑓𝑐f(c) with degree 2​R−22𝑅22R-2, which could achieve 2​R​(p,m,n)−22𝑅𝑝𝑚𝑛22R(p,m,n)-2 by using upper bound constructions with a rank of R​(p,m,n)𝑅𝑝𝑚𝑛R(p,m,n). The final results can be decoded by re-evaluating the composed polynomial at points x1,…,xRsubscript𝑥1…subscript𝑥𝑅x_{1},...,x_{R}, then combining them based on tensor c𝑐c. Note that even for cases where R​(p,m,n)𝑅𝑝𝑚𝑛R(p,m,n) is not yet known, one can still obtain explicit coding constructions by swapping in any upper bound constructions (e.g,               ). Subcubic recovery thresholds can still be achieved for any sufficiently large p𝑝p, m𝑚m, and n𝑛n even one only applies the well known Strassen’s construction . Hence, for simplicity, in this work we present all results in terms of R​(p,m,n)𝑅𝑝𝑚𝑛R(p,m,n), and explicit subcubic constructions can be obtained in the same way. We focus on linear codes, defined similarly as in [ref]42 , which guarantees linear coding complexities w.r.t. the sizes of input matrices, and are dimension independent. Precisely, in a linear coding design, the input matrix A𝐴A (or each input A𝐴A for more general settings) is partitioned into p𝑝p-by-m𝑚m subblocks of equals sizes (and possibly padded with a list of i.i.d. uniformly random matrices of same sizes, referred to as random keys).444To make sure the setting is well defined, we assume 𝔽𝔽\\mathbb{F} is finite whenever data-security or privacy is taken into account.  Matrix (or matrices) B𝐵B are partitioned similarly. Each worker is then assigned a pair of linear combinations of these two lists of submatrices as coded inputs. Moreover, the master uses decoding functions that computes linear combinations of received computing results.555Note that by relaxing certain assumptions made in the paper, such as\nallowing the decoder to access inputs and random keys and allowing extra computational cost at workers or master, one can further reduce the recovery threshold (e.g., see discussions in  ). All results presented in this paper for distributed matrix multiplication directly extends to general codes with possibly non-linear constructions, by swapping any upper bound of R​(p,m,n)𝑅𝑝𝑚𝑛R(p,m,n) into the number workers required by any computing scheme, as we illustrated in [ref]42.\n\nReference markers found: [56]\n\nContext windows for each [ref]:\n[ref]56: codes achieves a recovery threshold of min⁡{p​m​n+p−1,2​R​(p,m,n)−1}𝑝𝑚𝑛𝑝12𝑅𝑝𝑚𝑛1\\min\\{pmn+p-1,2R(p,m,n)-1\\} for any p,m,𝑝𝑚p,m, and n𝑛n. Here R​(p,m,n)𝑅𝑝𝑚𝑛R(p,m,n) denotes the bilinear complexity [ref]56 for multiplying two matrices of sizes m𝑚m-by-p𝑝p and p𝑝p-by-n𝑛n. It is well know that R​(p,m,n)𝑅𝑝𝑚𝑛R(p,m,n) is subcubic, i.e, R​(p,m,n)=o​(p​m​n)𝑅𝑝𝑚𝑛𝑜𝑝𝑚𝑛R(p,m,n)=o(pmn) when p𝑝p, m𝑚m, and n𝑛n\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 56,\n            \"titles\": [\n                \"Harmonic Coding: An Optimal Linear Code for Privacy-Preserving Gradient-Type Computation\",\n                \"The tail at scale\",\n                \"Fail-stop processors: An approach to designing fault-tolerant computing systems\",\n                \"The byzantine generals problem\",\n                \"Security and privacy in cloud computing: A survey\",\n                \"Secureml: A system for scalable privacy-preserving machine learning\",\n                \"Lagrange Coded Computing: Optimal Design for Resiliency, Security, and Privacy\",\n                \"CodedPrivateML: A Fast and Privacy-Preserving Framework for Distributed Machine Learning\",\n                \"Speeding Up Distributed Machine Learning Using Codes\",\n                \"“Short-Dot”: Computing Large Linear Transforms Distributedly Using Coded Short Dot Products\",\n                \"Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication\",\n                \"On the Capacity of Secure Distributed Matrix Multiplication\",\n                \"Secure distributed computing with straggling servers using polynomial codes\",\n                \"Private Secure Coded Computation\",\n                \"On the Upload versus Download Cost for Secure and Private Matrix Multiplication\",\n                \"Straggler Mitigation in Distributed Matrix Multiplication: Fundamental Limits and Optimal Coding\",\n                \"On the optimal recovery threshold of coded matrix multiplication\",\n                \"Distributed and Private Coded Matrix Computation with Flexible Communication Load\",\n                \"Private and Secure Distributed Matrix Multiplication with Flexible Communication Load\",\n                \"Entangled polynomial coding in limited-sharing multi-party computation\",\n                \"Cross subspace alignment codes for coded distributed batch matrix multiplication\",\n                \"Generalized cross subspace alignment codes for coded distributed batch matrix multiplication\",\n                \"Gaussian elimination is not optimal\",\n                \"Strassen’s algorithm is not optimal trilinear technique of aggregating, uniting and canceling for constructing fast algorithms for matrix operations\",\n                \"On minimizing the number of multiplications necessary for matrix multiplication\",\n                \"A noncommutative algorithm for multiplying 3× 3 matrices using 23 multiplications\",\n                \"On multiplication of 2 × 2 matrices\",\n                \"Relations between exact and approximate bilinear algorithms. applications\",\n                \"Partial and total matrix multiplication\",\n                \"Some properties of disjoint sums of tensors related to matrix multiplication\",\n                \"On the asymptotic complexity of matrix multiplication\",\n                \"The asymptotic spectrum of tensors and the exponent of matrix multiplication\",\n                \"Matrix multiplication via arithmetic progressions\",\n                \"The border rank of the multiplication of 2× 2 matrices is seven\",\n                \"On the complexity of matrix multiplication\",\n                \"Optimization techniques for small matrix multiplication\",\n                \"Multiplying matrices faster than coppersmith-winograd\",\n                \"Algebraic complexity theory\",\n                \"The bilinear complexity and practical algorithms for matrix multiplication\",\n                \"A non-commutative algorithm for multiplying 5×5 matrices using 99 multiplications\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]56 based on the context and research area of Entangled Polynomial Codes for Secure, Private, and Batch Distributed Matrix Multiplication: Breaking the “Cubic” Barrier. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}