{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: EGRC-Net: Embedding-induced Graph Refinement Clustering Network\n\nPaper Abstract: \n\nPaper Section Text:\nClustering is a typical yet challenging machine learning topic with a series of real-world applications, including object detection   [ref]3 [ref]4 , social network analysis    , and face recognition 0 1 2 3. Clustering aims to partition data into different groups based on their intrinsic patterns, such that similar samples are clustered together while dissimilar samples are separated from each other. Due to the powerful embedding learning capability of the convolutional neural network, deep embedding clustering has been widely studied in recent decades. For example, Hinton et al. 4 proposed a deep auto-encoder network (DAE) to focus on the node attribute information for clustering assignments. Xie et al. 5 jointly learned feature representations and clustering assignments to present the deep embedded clustering network (DEC). Guo et al. 6 improved DEC via a reconstruction loss. Although these DAE-based networks have shown impressive embedding learning capability, they neglect the topology structure information of the input, limiting the clustering performance. To this end, the graph convolution network (GCN) is utilized to conduct the deep embedding clustering by propagating spatial relations with the representations of its neighbors, namely the graph clustering network. For example, Kipf and Welling 7 proposed the graph auto-encoder to learn the graph structure feature. Pan et al. 2 injected an adversarial regularizer into the GCN framework. Zhang et al. 3 used a relaxed K-means 4 to improve the representation learning capability of the GCN-based clustering method. Recently, numerous works have combined DAE and GCN to simultaneously consider node attribute and topology structure information, achieving significant improvement in clustering performance. For instance, Bo et al. 9 integrated the DAE and GCN features via the structural deep clustering network. Peng et al. 0 employed two feature fusion modules to merge the DAE and GCN features.\nHe et al. 1 conducted the feature fusion and graph reconstruction layer by layer.\nNevertheless, all these GCN-based graph clustering networks adopt a predefined graph as a fixed input and may fail to correctly distinguish the samples if the initial graph cannot truly and precisely reflect their topology structures on the embedding space, which is referred to as the fixed graph issue. Thus, it is expected that the clustering performance can be boosted by performing graph refinement based on the guidance from the learned embedding representation. In this paper, we propose a novel embedding-induced graph refinement clustering network (EGRC-Net) framework to adaptively use the learned embedding for evolving the initial graph, aiming to boost the embedding learning capability of the graph clustering network. Specifically, we first use the vanilla DAE and GCN modules to conduct embedding learning. Subsequently, we explore the local geometric structure information on the embedding space to construct an adjacency matrix. Then, we utilize multilayer perceptron layers and a series of normalization terms to dynamically fuse the constructed graph with the initial one, seeking to preserve the intrinsic semantic structure information. Finally, we minimize the Jeffreys divergence of multiple derived distributions in an unsupervised manner to jointly conduct embedding learning and graph refinement. Furthermore, we advance EGRC-Net in a scalable manner to handle the practical large-graph problem 5 3 6. Specifically, the weak scalability of EGRC-Net ascribes to the utilization of GCN, since the explicit message-passing of GCN leads to an expensive neighborhood expansion 7. Thus, we utilize the approximate personalized propagation of neural predictions (APPNP) 8 instead of multi-layer graph convolution to complete the transmission and aggregation of node information on the graph. However, it is still required to predefine a heuristic threshold that determines the teleport probability. Besides, the predefined value also limits its availability since, in real-world clustering tasks, there has little guideline to set a suitable threshold. Thus, we empirically initialize the threshold using a random generator to sample from a uniform distribution on the interval [0,1)01[0,1) and learn that variable during the network training. To this end, we can achieve information propagation by using the personalized PageRank with a learned threshold value without relying on a predefined heuristic value, namely improved APPNP (IAPPNP). In this way, we obtain the scalable EGRC-Net. In summary, the contributions of our work are as follows: We handle the fixed graph issue by dynamically fusing an embedding-induced graph with the initial one, aiming to preserve the intrinsic semantic structure information. We design a unified learning framework by minimizing the Jeffreys divergence of multiple derived distributions to jointly conduct embedding learning and graph refinement, mutually benefiting both components. We investigate the scalability of the proposed method by utilizing an improved approximate personalized propagation of neural predictions instead of multi-layer graph convolution to complete the transmission and aggregation of node information on the graph. Extensive experiments on nine commonly used benchmark datasets demonstrate that the proposed methods consistently outperform several state-of-the-art approaches. In particular, as shown in Figure 1, EGRC-Net improves the ARI by more than 11.99% over the best baseline on DBLP, and the scalable one has a 10.73% gain in ARI while saving 33.73% in memory and 19.71% in running time. The rest of this paper is organized as follows. We briefly review the related works in Section II and introduce the proposed network in Section III. Afterward, we give the experimental results and analyses in Section IV and conclude this paper in Section V. Throughout this paper, matrices are denoted by bold upper case letters, vectors by bold lower case letters, and scalars by italic lower case letters, respectively. Given a matrix 𝐁∈ℝm×n𝐁superscriptℝ𝑚𝑛\\mathbf{B}\\in\\mathbb{R}^{m\\times n}, ‖𝐁‖Fsubscriptnorm𝐁𝐹\\left\\|\\mathbf{B}\\right\\|_{F} denotes the Frobenius norm of 𝐁𝐁\\mathbf{B}, i.e., ‖𝐁‖F=∑i=1m∑j=1n|bi,j|2subscriptnorm𝐁𝐹superscriptsubscript𝑖1𝑚superscriptsubscript𝑗1𝑛superscriptsubscript𝑏𝑖𝑗2\\left\\|\\mathbf{B}\\right\\|_{F}=\\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}{\\left|b_{i,j}\\right|}^{2}}. Let 𝐗∈ℝn×d𝐗superscriptℝ𝑛𝑑\\mathbf{X}\\in\\mathbb{R}^{n\\times d} be the raw data, 𝐀∈ℝn×n𝐀superscriptℝ𝑛𝑛\\mathbf{A}\\in\\mathbb{R}^{n\\times n} the adjacency matrix of the graph, we summarize the main notations in Table I.\n\nReference markers found: [3]\n\nContext windows for each [ref]:\n[ref]3: Clustering is a typical yet challenging machine learning topic with a series of real-world applications, including object detection   [ref]3 [ref]4 , social network analysis    , and face recognition 0 1 2 3. Clustering aims to partition data into different groups based on their intrinsic patterns, such that similar samples are\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 3,\n            \"titles\": [\n                \"Adaptive Attribute and Structure Subspace Clustering Network\",\n                \"“Multiview consensus graph clustering\",\n                \"Multiview consensus graph clustering\",\n                \"Deep fair clustering for visual learning\",\n                \"Semi-Supervised Subspace Clustering via Tensor Low-Rank Representation\",\n                \"Graph embedding contrastive multi-modal representation learning for clustering\",\n                \"Learning graphs for knowledge transfer with limited labels\",\n                \"Decorrelated Clustering with Data Selection Bias\",\n                \"Attributed graph clustering with dual redundancy reduction\",\n                \"Attributed graph clustering with multi-task embedding learning\",\n                \"Deep embedded non-redundant clustering\",\n                \"Maximum Entropy Subspace Clustering Network\",\n                \"Clustering Ensemble Meets Low-rank Tensor Approximation\",\n                \"SAIL: Self-Augmented Graph Contrastive Learning\",\n                \"Reducing the dimensionality of data with neural networks\",\n                \"Unsupervised Deep Embedding for Clustering Analysis\",\n                \"Improved deep embedded clustering with local structure preservation\",\n                \"Variational graph auto-encoders\",\n                \"Attributed Graph Clustering: A Deep Attentional Embedding Approach\",\n                \"Structural Deep Clustering Network\",\n                \"Attention-driven Graph Clustering Network\",\n                \"Parallelly adaptive graph convolutional clustering model\",\n                \"Adversarially Regularized Graph Autoencoder for Graph Embedding\",\n                \"Embedding Graph Auto-Encoder for Graph Clustering\",\n                \"Some methods for classification and analysis of multivariate observations\",\n                \"Graph Convolutional Neural Networks for Web-Scale Recommender Systems\",\n                \"CGC: Contrastive Graph Clustering for Community Detection and Tracking\",\n                \"Scaling Graph Neural Networks with Approximate PageRank\",\n                \"Predict then propagate: Graph neural networks meet personalized pagerank\",\n                \"Objective-based hierarchical clustering of deep embedding vectors.\",\n                \"Mixture Model Auto-Encoders: Deep Clustering Through Dictionary Learning\",\n                \"SPICE: Semantic Pseudo-Labeling for Image Clustering\",\n                \"Rectifier nonlinearities improve neural network acoustic models\",\n                \"Multi-view Contrastive Graph Clustering\",\n                \"Multi-stream attention-aware graph convolution network for video salient object detection\",\n                \"Non-graph data clustering via 𝒪​(n)𝒪𝑛\\\\mathcal{O}(n) bipartite graph convolution\",\n                \"Bag of Tricks for Training Deeper Graph Neural Networks: A Comprehensive Benchmark Study\",\n                \"Graph convolution rpca with adaptive graph\",\n                \"Correntropy-induced wasserstein gcn: Learning graph embedding via domain adaptation\",\n                \"Towards projected clustering with aggregated mapping\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]3 based on the context and research area of EGRC-Net: Embedding-induced Graph Refinement Clustering Network. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}