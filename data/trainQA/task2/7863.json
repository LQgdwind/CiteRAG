{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: CPLIP: Zero-Shot Learning for Histopathology with Comprehensive Vision-Language Alignment\n\nPaper Abstract: \n\nPaper Section Text:\n1. Zero-shot performance comparison using single vs. merged prompts (Table 6).\nIn this experiment, we compared the zero-shot classification performance of the proposed CPLIP algorithm using single prompts vs. merged prompts at the inference step (see Table 6).\nFor a fair comparison with earlier works [ref]17 [ref]29 , we have used the same set of merged prompts as employed by CONCH [ref]28.\nOn all five datasets for tile-based zero-shot classification, significant performance improvement is observed which is in line with the previous studies [ref]17 [ref]28.\n\n 2. Zero-shot performance comparison using different image-text encoders (Table 7).\nIn this experiment, we compared the performance of our proposed CPLIP algorithm in terms of initializing different image-text encoders including CLIP (out-of-domain pre-trained encoders), PLIP (in-domain pre-trained encoders), CTransPath (in-domain pre-trained image encoder), BioclinicalBert and PubMedBERT (out-of-domain pre-trained text encoders) as shown in Table 7.\nThe best results on five datasets are reported using CTransPath as an image encoder and BioClinicalBert to initialize the text encoder.\nThis is because CTransPath is pre-trained on unlabeled larger histology images and BioClinicalBert is trained on 2M clinical notes in the MIMIC-III v1.4 database .\nThe in-domain CPLIP variants also showed comparable performance compared to the best-performing CPLIP (out-of-domain) variant.\n\n 3. Zero-shot performance comparison using different sizes of bags (Table 8).\nExperiments are also performed by varying the sizes of both bags.\nIn the textual bag (Btsuperscriptùêµùë°B^{t}), rank-1 best-matching textual description with the input image, rank-5, rank-10, rank-15, and all 17 textual descriptions are considered.\nThe corresponding visual bags (Bvsuperscriptùêµùë£B^{v}) also contain 1, 5, 10, 15, and 21 images.\nAs the bag sizes increase, continuous improvements in performance are observed until bag size 15, as shown in Table 8.\nA further increase has caused a slight decrease in performance due to noisy textual descriptions.\n\n 4. Many-to-Many Vs. One-to-One Contrastive Learning Approach (Table 9).\nThe many-to-many learning approach has two main advantages: (1) it better reflects actual medical practice.\nPathologists evaluate WSIs using not just visual morphology, but also patient symptoms and medical knowledge about disease causes.\nBy integrating these multiple data sources, the approach allows for more comprehensive clinical integration compared to previous one-to-one methods; (2) the approach enhances visual representations through augmented slide image inputs, capturing phenotypic diversity and improving model generalization.\nThis methodology constructs comprehensive textual descriptions and corresponding visual concepts to enable VLMs to handle the complexity of pathology images and text.\nThe approach is akin to multi-task learning as learning joint representations across related tasks can promote generalization - analogous to how multi-task learning leads to more robust models.\nTo assess its efficacy, an ablation study (Table 9) was conducted, revealing that this novel strategy significantly boosts zero-shot learning performance on four different datasets.\nAdditionally, when compared to four SOTA vision-language models fine-tuned on the same datasets (Table 10), the many-to-many contrastive learning-based CPLIP model demonstrated superior accuracy, highlighting the robustness of this new technique.\n\n 5. Comparing CPLIP with in-domain SSL single modality CNNs and ViTs:\nTable 11 compares the classification performance of CPLIP against domain-specific SSL CNNs and ViTs, namely DinoSSLPath  and MoCo v2 .\nThe study spanned four datasets at both WSI and tile levels, using zero-shot, linear evaluation, and full fine-tuning evaluation protocols.\nNotably, zero-shot evaluations using CPLIP with merged prompts surpassed the performance of DinoSSLPath and MoCo v2, which did not employ zero-shot settings.\nIn linear and fine-tuning settings, CPLIP achieved significant performance gains, maintaining consistency with the protocols established by DinoSSLPath. \n\n\n\n\nDatasets\nTCGA-NSCLC\nTCGA-RCC\nWSSS4LUAD\nDigestPath\n\nCLIP\n0.488\n0.291\n0.541\n0.137\n\nBiomedCLIP\n0.733\n0.714\n0.571\n0.671\n\nPLIP\n0.744\n0.751\n0.761\n0.842\n\nMI-Zero\n0.811\n0.815\n0.762\n0.823\n\nCPLIP\n0.835\n0.855\n0.791\n0.856\n\n\n \n\n\n\n\nDatasets\nEvaluation\nDinoSSLPath\nMoCo v2\nCPLIP\n\nCAM16\nZero-shot\n√ó\\times\n√ó\\times\n0.632\n\nLinear\n0.618\n0.592\n0.663\n\n(WSI-level)\nFine-tune\n0.722\n0.678\n0.746\n\nWSSS4LUAD\nZero-shot\n√ó\\times\n√ó\\times\n0.882\n\nLinear\n0.878\n0.881\n0.924\n\n(Tile-based)\nFine-tune\n0.951\n0.944\n0.976\n\nCRC100K\nZero-shot\n√ó\\times\n√ó\\times\n0.844\n\nLinear\n0.862\n0.853\n0.894\n\n(Tile-based)\nFine-tune\n0.945\n0.911\n0.964\n\nSICAP\nZero-shot\n√ó\\times\n√ó\\times\n0.511\n\nLinear\n0.502\n0.466\n0.554\n\n(Tile-based)\nFine-tune\n0.604\n0.547\n0.626\n\n\n \n\n\n\n\nSingle Prompt\nCRC100K\nDigestPath\nSICAP\nWSSS4LUAD\nPanNuke\n\nCLIP baseline \n0.234|||0.185|||0.727\n0.11|||0.030|||0.203\n0.231|||0.139|||0.201\n0.451|||0.481|||0.705\n0.322|||0.352|||0.683\n\nBiomedCLIP \n0.422|||0.372|||0.859\n0.591|||0.622|||0.781\n0.381|||0.361|||0.506\n0.466|||0.495|||0.698\n0.522|||0.572|||0.711\n\nPLIP [ref]17\n0.520|||0.517|||0.879\n0.815|||0.832|||0.901\n0.319|||0.255|||0.603\n0.702|||0.734|||0.822\n0.629|||0.656|||0.805\n\nMI-Zero [ref]29\n0.544|||0.536|||0.872\n0.822|||0.811|||0.911\n0.308|||0.251|||0.605\n0.722|||0.742|||0.805\n0.659|||0.688|||0.755\n\nCONCH [ref]28\n0.566|||0.542|||0.901\n-\n0.349|||0.245|||-\n0.598‚àó|||0.590‚àó|||0.795‚àó\n-\n\nProposed CPLIP\n0.701|||0.681|||0.922\n0.835|||0.856|||0.933\n0.366|||0.388|||0.711\n0.778|||0.791|||0.836\n0.681|||0.757|||0.835\n\nMerged Prompts\nCRC100K\nDigestPath\nSICAP\nWSSS4LUAD\nPanNuke\n\nCLIP baseline \n0.271|||0.247|||0.781\n0.188|||0.210|||0.280\n0.283|||0.191|||0.205\n0.501|||0.544|||0.791\n0.385|||0.412|||0.744\n\nBiomedCLIP \n0.553|||0.533|||0.924\n0.644|||0.671|||0.831\n0.483|||0.439|||0.605\n0.511|||0.533|||0.764\n0.631|||0.651|||0.802\n\nPLIP [ref]17\n0.674|||0.687|||0.944\n0.865|||0.871|||0.931\n0.355|||0.315|||0.656\n0.751|||0.791|||0.833\n0.719|||0.744|||0.874\n\nMI-Zero [ref]29\n0.721|||0.755|||0.956\n0.844|||0.866|||0.941\n0.341|||0.306|||0.641\n0.741|||0.781|||0.846\n0.744|||0.759|||0.901\n\nCONCH [ref]28\n0.791|||0.803|||0.979\n-\n0.624|||0.424|||-\n0.719‚àó|||0.705‚àó|||0.877‚àó\n-\n\nProposed CPLIP\n0.823|||0.844|||0.980\n0.871|||0.903|||0.971\n0.498|||0.511|||0.716\n0.851|||0.882|||0.903\n0.795|||0.811|||0.936\n\n\n \n\n\n\n\nModels (Single prompts)\nImage encoder pretraining\nText encoder pretraining\nCAM16\nTCGA-BRCA\nTCGA-RCC\nTCGA-NSCLC\n\nCLIP baseline \nViT-B/16-224\nGPT-2/77\n0.134|||0.175|||0.325\n0.512|||0.328|||0.551\n0.321|||0.178|||0.578\n0.496|||0.358|||0.536\n\nBiomedCLIP \nViT-B/16-224\nPMB/256\n0.311|||0.377|||0.545\n0.527|||0.422|||0.761\n0.677|||0.646|||0.872\n0.699|||0.684|||0.851\n\nPLIP [ref]17\nViT-B/32-224\nGPT/347\n0.399|||0.416|||0.681\n0.451|||0.331|||0.611\n0.726|||0.739|||0.915\n0.676|||0.666|||0.781\n\nMI-Zero [ref]29\nCTransPath/224\nBioClinicalBert/512\n0.456|||0.461|||0.755\n0.781|||0.723|||0.856\n0.805|||0.782|||0.881\n0.802|||0.792|||0.866\n\nCONCH [ref]28\nViT-B/16-256\nHistPathGPT/512\n-\n0.643|||0.600|||0.873\n0.796|||0.797|||0.961\n0.807|||0.803|||0.915\n\nCPLIP1 (Ours)\nViT-B/16-224 (OoD)\nGPT-2/77 (OoD)\n0.502|||0.477|||0.705\n0.500|||0.544|||0.722\n0.754|||0.749|||0.865\n0.761|||0.788|||0.821\n\nCPLIP2 (Ours)\nPLIP-ViT-B/32-224 (InD)\nPLIP-GPT/347 (InD)\n0.591|||0.587|||0.827\n0.824|||0.786|||0.889\n0.844|||0.855|||0.926\n0.854|||0.835|||0.936\n\nModels (Merged Prompts)\nImage encoder pretraining\nText encoder pretraining\nCAM16\nTCGA-BRCA\nTCGA-RCC\nTCGA-NSCLC\n\nCLIP baseline \nViT-B/16-224\nGPT-2/77\n0.151|||0.198|||0.331\n0.534|||0.346|||0.623\n0.367|||0.219|||0.651\n0.567|||0.431|||0.598\n\nBiomedCLIP \nViT-B/16-224\nPMB/256\n0.337|||0.402|||0.564\n0.532|||0.441|||0.837\n0.807|||0.773|||0.903\n0.777|||0.761|||0.861\n\nPLIP [ref]17\nViT-B/32-224\nGPT/347\n0.446|||0.442|||0.711\n0.487|||0.364|||0.655\n0.794|||0.772|||0.935\n0.768|||0.805|||0.819\n\nMI-Zero [ref]29\nCTransPath/224\nBioClinicalBert/512\n0.499|||0.521|||0.821\n0.833|||0.821|||0.905\n0.871|||0.855|||0.933\n0.881|||0.871|||0.944\n\nCONCH [ref]28\nViT-B/16-256\nHistPathGPT/512\n-\n0.840|||0.839|||0.932\n0.893|||0.895|||0.973\n0.900|||0.900|||0.964\n\nCPLIP1 (Ours)\nViT-B/16-224 (OoD)\nGPT-2/77 (OoD)\n0.578|||0.551|||0.751\n0.557|||0.588|||0.783\n0.834|||0.805|||0.921\n0.811|||0.856|||0.833\n\nCPLIP2 (Ours)\nPLIP-ViT-B/32-224 (InD)\nPLIP-GPT/347 (InD)\n0.661|||0.632|||0.886\n0.887|||0.871|||0.963\n0.941|||0.937|||0.978\n0.931|||0.951|||0.981\n\n\n \n\n\n\n\nModels (Single prompts)\nImage encoder pretraining\nText encoder pretraining\nSICAP\nDigestPath\n\nCLIP baseline \nViT-B/16-224\nGPT-2/77\n0.367|||0.599|||0.605\n0.367|||0.492|||0.511\n\nBiomedCLIP \nViT-B/16-224\nPMB/256\n0.484|||0.536|||0.557\n0.446|||0.581|||0.601\n\nPLIP [ref]17\nViT-B/32-224\nGPT/347\n0.549|||0.605|||0.644\n0.426|||0.526|||0.541\n\nMI-Zero [ref]29\nCTransPath/224\nBioClinicalBert/512\n0.587|||0.651|||0.726\n0.599|||0.648|||0.691\n\nCONCH [ref]28\nViT-B/16-256\nHistPathGPT/512\n0.601|||0.672|||0.751\n0.615|||0.663|||0.709\n\nCPLIP1 (Ours)\nViT-B/16-224 (OoD)\nGPT-2/77 (OoD)\n0.591|||0.661|||0.681\n0.491|||0.581|||0.602\n\nCPLIP2 (Ours)\nPLIP-ViT-B/32-224 (InD)\nPLIP-GPT/347 (InD)\n0.654|||0.704|||0.803\n0.685|||0.719|||0.754\n\nCPLIP3 (Ours)\nCTransPath/224 (InD)\nBioClinicalBert/512 (OoD)\n0.633|||0.702|||0.791\n0.665|||0.711|||0.744\n\nCPLIP4 (Ours)\nCTransPath/224 (InD)\nPLIP-GPT/347 (InD)\n0.651|||0.715|||0.806\n0.687|||0.722|||0.761\n\nReference markers found: [17]\n\nContext windows for each [ref]:\n[ref]17: the zero-shot classification performance of the proposed CPLIP algorithm using single prompts vs. merged prompts at the inference step (see Table 6).\nFor a fair comparison with earlier works [ref]17 [ref]29 , we have used the same set of merged prompts as employed by CONCH [ref]28.\nOn all five datasets for tile-based zero-shot classification, significant performance improvement is\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 17,\n            \"titles\": [\n                \"Attention-based Deep Multiple Instance Learning\",\n                \"Histopathology Glossary\",\n                \"Cancer Glossary\",\n                \"Publicly Available Clinical BERT Embeddings\",\n                \"Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer\",\n                \"Language Models are Few-Shot Learners\",\n                \"Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning\",\n                \"Improved Baselines with Momentum Contrastive Learning\",\n                \"Semi-Supervised and Unsupervised Deep Visual Learning: A Survey\",\n                \"Digestpath: A benchmark dataset with challenge review for the pathological detection and segmentation of digestive-system\",\n                \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n                \"Learning to prompt for open-vocabulary object detection with vision-language model\",\n                \"Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles\",\n                \"Pannuke: an open pan-cancer histology dataset for nuclei instance segmentation and classification\",\n                \"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\",\n                \"Visual programming: Compositional visual reasoning without training\",\n                \"WSSS4LUAD: Grand Challenge on Weakly-supervised Tissue Semantic Segmentation for Lung Adenocarcinoma\",\n                \"A visual‚Äìlanguage foundation model for pathology image analysis using medical twitter\",\n                \"Mimic-iii, a freely accessible critical care database\",\n                \"Benchmarking self-supervised learning on diverse pathology datasets\",\n                \"Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multicenter study\",\n                \"Self-Path: Self-supervision for Classification of Pathology Images with Limited Annotations\",\n                \"Clipath: Fine-tune clip with visual feature fusion for pathology image analysis towards minimizing data collection efforts\",\n                \"CiteTracker: Correlating Image and Text for Visual Tracking\",\n                \"Decoupled Weight Decay Regularization\",\n                \"Data-efficient and weakly supervised computational pathology on whole-slide images\",\n                \"Harnessing medical twitter data for pathology ai\",\n                \"Towards a visual-language foundation model for computational pathology\",\n                \"Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images\",\n                \"End-to-End Learning of Visual Representations from Uncurated Instructional Videos\",\n                \"Language models are unsupervised multitask learners\",\n                \"Learning Transferable Visual Models From Natural Language Supervision\",\n                \"TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification\",\n                \"Prompting large language models with answer heuristics for knowledge-based visual question answering\",\n                \"Self-learning for weakly supervised Gleason grading of local patterns\",\n                \"Deep neural network models for computational histopathology: A survey\",\n                \"Review the cancer genome atlas (tcga): an immeasurable source of knowledge\",\n                \"Handcrafted histological transformer (h2t): Unsupervised representation of whole slide images\",\n                \"Transformer-based unsupervised contrastive learning for histopathological image classification\",\n                \"A Survey on ChatGPT: AI‚ÄìGenerated Contents, Challenges, and Solutions\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]17 based on the context and research area of CPLIP: Zero-Shot Learning for Histopathology with Comprehensive Vision-Language Alignment. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}