{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A parallel structured divide-and-conquer algorithm for symmetric tridiagonal eigenvalue problems\n\nPaper Abstract: \n\nPaper Section Text:\nAll the experimental results are obtained on the Tianhe-2 supercomputerÂ  ,\nlocated in Guangzhou, China. Each compute node is equipped with two 12-cores Intel Xeon E5-2692 v2 CPUs and our experiments only use CPU cores.\nThe details of the test platform and environment of compute nodes are shown in TableÂ I.\nFor all these numerical experiments, we only used plain MPI, run 242424 MPI processes per node in principle, and\none process per core.\nFor example, we used 171171171 compute nodes for testing 409640964096 processes. Example 1. Assume that Aâˆˆâ„nÃ—nğ´superscriptâ„ğ‘›ğ‘›A\\in\\mathbb{R}^{n\\times n} is a random matrix and\nBğµB is defined as Biâ€‹j=uiâ€‹vjdiâˆ’wjsubscriptğµğ‘–ğ‘—subscriptğ‘¢ğ‘–subscriptğ‘£ğ‘—subscriptğ‘‘ğ‘–subscriptğ‘¤ğ‘—B_{ij}=\\frac{u_{i}v_{j}}{d_{i}-w_{j}},\nwhere uisubscriptğ‘¢ğ‘–u_{i} and vjsubscriptğ‘£ğ‘—v_{j} are random numbers, di=iâ‹…bâˆ’ansubscriptğ‘‘ğ‘–â‹…ğ‘–ğ‘ğ‘ğ‘›d_{i}=i\\cdotp\\frac{b-a}{n}, wj=dj+bâˆ’a2âˆ—nsubscriptğ‘¤ğ‘—subscriptğ‘‘ğ‘—ğ‘ğ‘2ğ‘›w_{j}=d_{j}+\\frac{b-a}{2*n},\na=1.0,b=9.0formulae-sequenceğ‘1.0ğ‘9.0a=1.0,b=9.0, for i,j=1,â‹¯,n.formulae-sequenceğ‘–ğ‘—1â‹¯ğ‘›i,j=1,\\cdots,n. It is known that BğµB is a rank-structured\nCauchy-like matrix, seeÂ  .\nWe compute C=AÃ—Bğ¶ğ´ğµC=A\\times B. Let n=8192ğ‘›8192n=8192, 163841638416384 and 327683276832768, respectively, and choose\ndifferent number of processes, NP=16subscriptğ‘ğ‘ƒ16N_{P}=16, 646464, 256256256, 102410241024 and 409640964096, to compare\nPDGEMM with PSMMA.\nTo avoid performance variance during multiple executions,\nwe evaluated the performance of PDGEMM and PSMMA twice in the same program and\ncalled that program three times, and chose the best results among these six executions.\nOur codes will be released on Github (available at https://github.com/shengguolsg/PSMMA). It is well-known that the performance of PDGEMM depends on the block size NBsubscriptğ‘ğµN_{B}.\nWe tested the performances of PDGEMM by choosing NB=64subscriptğ‘ğµ64N_{B}=64, 128128128 and 256256256, and\nwe found that their differences are very small.\nBut they are better than choosing NBâ‰¤32subscriptğ‘ğµ32N_{B}\\leq 32.\nTherefore, we chose NB=64subscriptğ‘ğµ64N_{B}=64 and n/NPğ‘›subscriptğ‘ğ‘ƒn/\\sqrt{N_{P}},\nwhich corresponds to the BCDD form and BDD form, respectively.\nNote that a large NBsubscriptğ‘ğµN_{B} is better for PSMMA since the\nranks of off-diagonal blocks after permutations may be smaller,\nsee TableÂ II. As shown in sectionÂ 3.1.1, we can redistribute matrix Ağ´A from BCDD to\nBDD to exploit the off-diagonal low-rank property of matrix BğµB.\nIn this example, we tested four versions of PSMMA, which are PSMMA_BCDD: NB=64subscriptğ‘ğµ64N_{B}=64 and with low-rank approximation; PSMMA_BDD: NB=n/NPsubscriptğ‘ğµğ‘›subscriptğ‘ğ‘ƒN_{B}=n/\\sqrt{N_{P}} and with low-rank approximation; PSMMA_WRedist: NB=64subscriptğ‘ğµ64N_{B}=64 and with data redistribution and low-rank approximation\n(matrix Ağ´A is transformed from BCDD to BDD with NB=n/NPsubscriptğ‘ğµğ‘›subscriptğ‘ğ‘ƒN_{B}=n/\\sqrt{N_{P}} and then back); PSMMA_NLowrank: NB=64subscriptğ‘ğµ64N_{B}=64 and without low-rank approximation; The speedups of PSMMA over PDGEMM are shown in Fig.Â 5(a),\n5(b) and 5(c) with dimensions n=8192ğ‘›8192n=8192, 163841638416384, 327683276832768,\nrespectively.\nFrom the results, we can see that PSMMA_BCDD is always faster than PDGEMM\nexcept for NP=16subscriptğ‘ğ‘ƒ16N_{P}=16.\nIt is because the ranks of BğµB blocks are very large when\nusing the BCDD form, and most of the time is spent in\ncomputing the low-rank approximations.\nTableÂ II shows the ranks of the off-diagonal blocks\nin the first block column, see Fig.Â 3 for the structure of\nmatrix BğµB.\nWithout using low-rank approximation, PSMMA can be faster than PDGEMM.\nIt is interesting to see that PSMMA_NLowrank is always faster than\nPDGEMM for these three matrices.\nNote that PSMMA_NLowrank requires more floating point operations than PDGEMM\nsince it needs to construct the local BğµB submatrices.\nHowever, PSMMA_NLowrank requires fewer communications than PDGEMM. PSMMA_WRedist is only slower than PDGEMM when\nthe size of the matrix is small (n=8192)ğ‘›8192(n=8192) and the number of processes is large.\nFrom the last row of TableÂ II, we can see that\nthe ranks of off-diagonal submatrices in BDD form are much smaller than the sizes (4096)4096(4096) of submatrices.\nPSMMA_WRedist is generally faster than both PSMMA_BCDD and PSMMA_NLowrank.\nThe disadvantage of PSMMA_WRedist is that it requires to perform data redistribution (communication).\nWhen the number of processes is large, the data redistribution represents a large portion of the overall time.\nFig.Â 6 shows the percentages of transforming matrix Ağ´A from BCDD to BDD\nand transforming it back.\nIt takes over 70%percent7070\\% of total time when nğ‘›n is small and NPsubscriptğ‘ğ‘ƒN_{P} is large. PSMMA_BDD is always the best and can be more than ten times faster than PDGEMM.\nThis is because it assumes that matrix Ağ´A is initially stored in BDD form and\nthe low-rank property of matrix BğµB is not destroyed either. It is difficult to know exactly why the speedups of PSMMA_WRedist and PSMMA_BDD\nover PDGEMM firstly increase and then decrease.\nIt depends on the properties of these two algorithms.\nComparing with PDGEMM, PSMMA_BDD and PSMMA_WRedist save both computations and communications.\nWhen the number of processes increases, these two contributions make the speedups first increase.\nAs the number of processes increases, the size of the local submatrix located on each process decreases,\nand therefore the percentage of floating point operations\nsaved from using low-rank approximations decreases.\nSince PSMMA_BDD does not save many floating point operations compared to PDGEMM,\nthe speedups decrease when using more processes.\nFor PSMMA_WRedist, the cost of data redistribution also increases as the number\nof processes increases.\nMeanwhile, as the number of processes grows, the percentage of saved\ncommunication by PSMMA_BDD and PSMMA_WRedist increases.\nTherefore, PSMMA_BDD can be always faster than PDGEMM since it not only reduces\ncomputations but also communications.\nIt is better to use PSMMA_NLowrank instead of PSMMA_WRedist when\nthe number of processes is very large. Remark 4.â€„We can adaptively choose a particular PSMMA algorithm based on the size of matrix and the number\nof processes.\nIt is better to use PSMMA_WRedist when the number of processes is small, and\nuse PSMMA_BCDD or PSMMA_NLowrank when the number of processes is large and/or\nthe size of matrix is small.\nIt is always best to use PSMMA_BDD when it is available and matrix BğµB is off-diagonally\nlow rank. Example 2. We use some â€™difficultâ€™ matricesÂ  for the DC algorithm,\nfor which few or no eigenvalues are deflated. Examples include\nthe Clement-type, Hermite-type and Toeplitz-type matrices, which are defined as follows. The Clement-type matrixÂ  is given by where the off-diagonal entries are iâ€‹(n+1âˆ’i),i=1,â€¦,nformulae-sequenceğ‘–ğ‘›1ğ‘–ğ‘–1â€¦ğ‘›\\sqrt{i(n+1-i)},i=1,\\ldots,n. The Hermite-type matrix is given asÂ , The Toeplitz-type matrix is defined asÂ , For the results of strong scaling, we let the dimension nğ‘›n be 30,0003000030,000, and use\nrank-structured techniques only when the size of the secular equation is larger than K=20,000ğ¾20000K=20,000.\nWe used PSMMA_WRedist and chose NB=64.subscriptğ‘ğµ64N_{B}=64.\nThe results for strong scaling of PSDC are shown in Fig.Â 7(a).\nThe speedups of PSDC over ScaLAPACK are reported in Fig.Â 7(b).\nWe can see that PSDC is about 1.41.41.4xâ€“1.61.61.6x times faster than PDSTEDC in ScaLAPACK for all cases.\nBecause of deflations, the performances of these three matrices can be\ndifferent even though they have the same dimensions. The orthogonality of the eigenvectors computed by PSDC is\nin the same order as those by ScaLAPACK,\nas shown in TableÂ III.\nThe orthogonality of matrix Qğ‘„Q is defined as â€–Iâˆ’Qâ€‹QTâ€–maxsubscriptnormğ¼ğ‘„superscriptğ‘„ğ‘‡\\|I-QQ^{T}\\|_{\\max},\nwhere âˆ¥â‹…âˆ¥max\\|\\cdot\\|_{\\max} is the maximum absolute value of entries of (â‹…)â‹…(\\cdot).\nWe confirm that the residuals of eigenpairs computed by PSDC are in the same order\nas those computed by ScaLAPACK though the results are not included here. Furthermore, we compare PSDC with PHDC which was introduced inÂ [ref]16\nand used STRUMPACK to accelerate the matrix-matrix multiplications.\nThe results are shown in Fig.Â 8.\nFor these three matrices, PSDC is much faster than PHDC when using many processes.\nIt is better to use STRUMPACK when using few processes since HSS-based multiplications\ncan save more floating point operations than BLR-based multiplications.\n\nReference markers found: [16]\n\nContext windows for each [ref]:\n[ref]16: residuals of eigenpairs computed by PSDC are in the same order\nas those computed by ScaLAPACK though the results are not included here. Furthermore, we compare PSDC with PHDC which was introduced inÂ [ref]16\nand used STRUMPACK to accelerate the matrix-matrix multiplications.\nThe results are shown in Fig.Â 8.\nFor these three matrices, PSDC is much faster than PHDC when using many processes.\nIt is\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 16,\n            \"titles\": [\n                \"A distributed-memory package for dense Hierarchically Semi-Separable matrix computations using randomization\",\n                \"A divide and conquer method for the symmetric tridiagonal eigenproblem\",\n                \"A divide-and-conquer algorithm for the symmetric tridiagonal eigenproblem\",\n                \"Matrix Computations, 3rd ed.\",\n                \"Multiple representations to compute orthogonal eigenvectors of symmetric tridiagonal matrices\",\n                \"High-performance solvers for dense Hermitian eigenproblems\",\n                \"LAPACK Usersâ€™ Guide, 3rd ed.\",\n                \"Scalapack: A portable linear algebra library for distributed memory computers-design issues and performance\",\n                \"A parallel divide and conquer algorithm for the symmetric eigenvalue problem on distributed memory architectures\",\n                \"Minimizing communication in numerical linear algebra\",\n                \"Communication-optimal parallel and sequential QR and LU factorizations\",\n                \"Fast and stable algorithms for hierarchically semi-separable representations\",\n                \"A fast solver for HSS representations via sparse matrices\",\n                \"New fast divide-and-conquer algorithms for the symmetric tridiagonal eigenvalue problem\",\n                \"An accelerated divide-and-conquer algorithm for the bidiagonal SVD problem\",\n                \"An improved divide-and-conquer algorithm for the banded matrices with narrow bandwidths\",\n                \"An efficient hybrid tridiagonal divide-and-conquer algorithm on distributed memory architectures\",\n                \"Improving multifrontal methods by means of block low-rank representations\",\n                \"A sparse matrix arithmetic based on â„‹-matrices. Part I: Introduction to â„‹-matrices\",\n                \"On â„‹2-matrices\",\n                \"Distributed-memory lattice â„‹-matrix factorization\",\n                \"Parallel solution of partial symmetric eigenvalue problems from electronic structure calculations\",\n                \"A cellular computer to implement the kalman filter algorithm\",\n                \"Matrix algorithms on a hypercube I: matrix multiplication\",\n                \"Pumma: Parallel universal matrix multiplication algorithms on distributed memory concurrent computers\",\n                \"A multi-structured superfast Toeplitz solver\",\n                \"SUMMA: scalable universal matrix mltiplication algorithm\",\n                \"A stable and efficient algorithm for the rank-one modification of the symmetric eigenproblem\",\n                \"Fast Gaussian elimination with partial pivoting for matrices with displacement structure\",\n                \"Stable and efficient algorithms for structured systems of linear equations\",\n                \"On the existence and computation of rank revealing LU factorizations\",\n                \"Communication-optimal parallel 2.5d matrix multiplication and lu factorization algorithms\",\n                \"Communication-optimal parallel recursive rectangular matrix multiplication\",\n                \"Cyclops tensor framework: Reducing communication and eliminating load imbalance in massively parallel contractions\",\n                \"Red-Blue Pebbling Revisited: Near Optimal Parallel Matrix-Matrix Multiplication\",\n                \"Structured Matrices and polynomials, unified superfast algorithms\",\n                \"A new parallel matrix multiplication algorithm on distributed-memory concurrent computers\",\n                \"Rank one modification of the symmetric eigenproblem\",\n                \"Studies in numerical linear algebra\",\n                \"Milkyway-2 supercomputer: System and application\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]16 based on the context and research area of A parallel structured divide-and-conquer algorithm for symmetric tridiagonal eigenvalue problems. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}