{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Do Differentiable Simulators Give Better Policy Gradients?\n\nPaper Abstract: \n\nPaper Section Text:\nConsider the problem of minimizing a stochastic objective, At the heart of many algorithms for reinforcement learning (RL) lies zeroth-order estimation of the gradient ‚àáF‚àáùêπ\\nabla F  . Yet, in domains that deal with structured systems, such as linear control, physical simulation, or robotics, it is possible to obtain exact gradients of fùëìf, which can also be used to construct a first-order estimate of ‚àáF‚àáùêπ\\nabla F. The availability of both options begs the question: given access to exact gradients of fùëìf, which estimator should we prefer? In stochastic optimization, the theoretical benefits of using first-order estimates of ‚àáF‚àáùêπ\\nabla F over zeroth-order ones have mainly been understood through the lens of variance and convergence rates 9 : the first-order estimator often (not always) results in much less variance compared to the zeroth-order one, which leads to faster convergence rates to a local minima of general nonconvex smooth objective functions. However, the landscape of RL objectives that involve long-horizon sequential decision making (e.g. policy optimization) is challenging to analyze, and convergence properties in these landscapes are relatively poorly understood, except for structured settings such as finite-state MDPs   or linear control 6 . In particular, physical systems with contact, as we show in Figure¬†1, can display complex characteristics including nonlinearities, non-smoothness, and discontinuities   6. Nevertheless, lessons from convergence rate analysis tell us that there may be benefits to using the exact gradients even for these complex physical systems. Such ideas have been championed through the term ‚Äúdifferentiable simulation‚Äù, where forward simulation of physics is programmed in a manner that is consistent with automatic differentiation 7  8  8 , or computation of analytic derivatives . These methods have shown promising results in decreasing computation time compared to zeroth-order methods  7  1 0 . Existing literature in differentiable simulation mainly focuses on the use of exact gradients for deterministic optimization. However, 6  show that using exact gradients for a deterministic objective can lead to suboptimal behavior of certain systems due to their landscapes. In these systems, stochasticity can be used to regularize the landscapes with randomized smoothing 3. We illustrate how the landscapes change upon injecting noise (Figure¬†1), and list some benefits of considering a surrogate stochastic objective. Stochasticity smooths local minima. As noted in 6 , stochasticity can alleviate some of the high-frequency local minima that deterministic gradients will be stuck on. For instance, the small discontinuity on the right side of Figure 1.B is filtered by Gaussian smoothing. Stochasticity alleviates flat regions. In systems of Figure 1, the gradients in some of the regions can be completely flat. This stalls progress of gradient descent. The stochastic objective, however, still has non-zero gradient as some samples escape the flat regions and provide an informative direction of improvement. Stochasticity encodes robustness. In Figure 1.C, following the gradient to increase the transferred momentum causes the ball to miss the pivot and land in a high-cost region. In contrast, the stochastic objective has a local minimum within the safe region, as the samples provide information about missing the pivot. Thus, our work attempts to compare two versions of gradient estimators in the stochastic setting: the first-order estimator and the zeroth-order one. This setting rules out the case that zeroth-order estimates perform better simply because of stochasticity, and sets equal footing for the two methods. When fùëìf is continuous, these quantities both converge to the same quantity (‚àáF‚àáùêπ\\nabla F) in expectation. We first show that even with continuous fùëìf, the first-order gradient estimate can result in more variance than the zeroth-order one due to the stiffness of dynamics or due to compounding of gradients in chaotic systems [ref]38 . In addition, we show that the assumption of continuous fùëìf can be violated in many relevant physical systems that are nearly/strictly discontinuous in the underlying landscape. These discontinuities are commonly caused by contact and geometrical constraints. We provide minimal examples to highlight specific challenges in Figure 1. These are not mere pathologies, but abstractions of more complicated examples that are rich with contact, such as robotic manipulation. We show that the presence of such discontinuities causes the first-order gradient estimator to be biased, while the zeroth-order one still remains unbiased under discontinuities. Furthermore, we show that stiff continuous approximations of discontinuities, even if asymptotically unbiased, can still suffer from what we call empirical bias under finite-sample settings. This results in a bias-variance tradeoff between the biased first-order estimator and the often high-variance, yet unbiased zeroth-order estimator. Intriguingly, we find that the bias-variance tradeoff in this setting manifests itself not through convergence rates, but through different local minima. This shows that the two estimators may fundamentally operate on different landscapes implicitly. The presence of discontinuities need not indicate that we need to commit ourselves to uniformly using one of the estimators. Many physical systems are hybrid by nature ; they consist of smooth regions that are separated by manifolds of non-smoothness or discontinuities. This suggests that we may be able to utilize the first-order estimates far away from these manifolds to obtain benefits of convergence rates, while switching to zeroth-order ones in the vicinity of discontinuities to obtain unbiased estimates. For this purpose, we further attempt to answer the question: how can we then correctly utilize exact gradients of fùëìf for variance reduction when we know the objective is nearly discontinuous? Previous works show that the two estimators can be combined by interpolating based on empirical variance [ref]38  . However, we show that in the presence of near-discontinuities, selecting based on empirical variance alone can lead to highly inaccurate estimates of ‚àáF‚àáùêπ\\nabla F, and propose a robustness constraint on the accuracy of the interpolated estimate to remedy this effect. Contributions. We 1) shed light on some of the inherent problems of RL using differentiable simulators, and answer which gradient estimator can be more useful under different characteristics of underlying systems such as discontinuities, stiffness, and chaos; and 2) present the Œ±ùõº\\alpha-order gradient estimator, a robust interpolation strategy between the two gradient estimators that utilizes exact gradients without falling into the identified pitfalls of the previous methods. We hope both contributions inspire algorithms for policy optimization using differentiable simulators, as well as design guidelines for new and existing simulators.\n\nReference markers found: [38]\n\nContext windows for each [ref]:\n[ref]38: with continuous fùëìf, the first-order gradient estimate can result in more variance than the zeroth-order one due to the stiffness of dynamics or due to compounding of gradients in chaotic systems [ref]38 . In addition, we show that the assumption of continuous fùëìf can be violated in many relevant physical systems that are nearly/strictly discontinuous in the underlying landscape. These\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\n                \"On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift\",\n                \"Systematically differentiating parametric discontinuities\",\n                \"A theoretical and empirical comparison of gradient approximations in derivative-free optimization\",\n                \"Global Optimality Guarantees For Policy Gradient Methods\",\n                \"Convex optimization\",\n                \"The pinocchio c++ library : A fast and flexible implementation of rigid body dynamics algorithms and their analytical derivatives\",\n                \"A Transition-Aware Method for the Simulation of Compliant Contact with Regularized Friction\",\n                \"Probability and stochastics, volume 261\",\n                \"Pybullet, a python module for physics simulation for games, robotics and machine learning\",\n                \"End-to-end differentiable physics for learning and control\",\n                \"D3{pg}: Deep differentiable deterministic policy gradients\",\n                \"Randomized smoothing for stochastic optimization\",\n                \"Optimal rates for zero-order convex optimization: The power of two function evaluations\",\n                \"A pressure field model for fast, robust approximation of net contact force and moment between nominally rigid objects\",\n                \"Theory and practice of finite elements, volume 159\",\n                \"Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator\",\n                \"Brax - a differentiable physics engine for large scale rigid body simulation\",\n                \"ADD: Analytically Differentiable Dynamics for Multi-Body Systems with Frictional Contact\",\n                \"Stochastic first- and zeroth-order methods for nonconvex stochastic programming\",\n                \"Deluca ‚Äì A Differentiable Control Library: Environments, Methods, and Benchmarking\",\n                \"Dojo: A differentiable simulator for robotics\",\n                \"DiffTaichi: Differentiable Programming for Physical Simulation\",\n                \"PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics\",\n                \"Coefficient of Restitution Interpreted as Damping in Vibroimpact\",\n                \"On the sample complexity of reinforcement learning\",\n                \"Variational Dropout and the Local Reparameterization Trick\",\n                \"Chaos, Fractals, and Noise: Stochastic Aspects of Dynamics\",\n                \"Leveraging Randomized Smoothing for Optimal Control of Nonsmooth Dynamical Systems\",\n                \"Unified particle physics for real-time applications\",\n                \"Monte carlo gradient estimation in machine learning\",\n                \"Mechanics of Robotic Manipulation\",\n                \"Empirical bernstein bounds and sample variance penalization\",\n                \"Understanding and correcting pathologies in the training of learned optimizers\",\n                \"Gradients are Not All You Need\",\n                \"Impulse-Based Dynamic Simulation of Rigid Body Systems\",\n                \"Pods: Policy optimization via differentiable simulation\",\n                \"A convex quasistatic time-stepping scheme for rigid multibody systems with contact and friction\",\n                \"PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos\",\n                \"Principles of mathematical analysis, volume 3\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]38 based on the context and research area of Do Differentiable Simulators Give Better Policy Gradients?. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}