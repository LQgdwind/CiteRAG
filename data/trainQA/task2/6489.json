{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Diffusion-Jump GNNs: Homophiliation via Learnable Metric Filters\n\nPaper Abstract: \n\nPaper Section Text:\nFor the sake of clarity, in this appendix we develop the key concepts of the theorems stated in the paper instead of providing detailed proofs. Our emphasis here is on the practical implications of each result. For more details, we refer the reader to the cited papers. The Trace Ratio Problem.\nMinâ€‹ğ”Tâ€‹â–³â€‹ğ”Minsuperscriptğ”ğ‘‡â–³ğ”\\text{Min}\\;\\mathbf{U}^{T}\\triangle\\mathbf{U} is achieved by ğ•nÃ—psubscriptğ•ğ‘›ğ‘\\mathbf{V}_{n\\times p} whose pğ‘p columns are given by the eigenvectors of the pğ‘p smallest eigenvalues Î»1â‰¤Î»2â‰¤â€¦â‰¤Î»psubscriptğœ†1subscriptğœ†2â€¦subscriptğœ†ğ‘\\lambda_{1}\\leq\\lambda_{2}\\leq\\ldots\\leq\\lambda_{p} of the Laplacian â–³â–³\\triangle. Then, Trâ€‹[ğ•Tâ€‹â–³â€‹ğ•]=Î»2+â€¦+Î»pTrdelimited-[]superscriptğ•ğ‘‡â–³ğ•subscriptğœ†2â€¦subscriptğœ†ğ‘\\text{Tr}[\\mathbf{V}^{T}\\triangle\\mathbf{V}]=\\lambda_{2}+\\ldots+\\lambda_{p}, when the graph G=(V,E)ğºğ‘‰ğ¸G=(V,E) with adjacency matrix ğ€ğ€\\mathbf{A} is connected. However, ğ•ğ•\\mathbf{V} does not necessarily minimize Trâ€‹[ğ•Tâ€‹ğƒğ•]Trdelimited-[]superscriptğ•ğ‘‡ğƒğ•\\text{Tr}[\\mathbf{V}^{T}\\mathbf{D}\\mathbf{V}]. As a result, we have that where d1â‰¤d2â‰¤â€¦â‰¤dnsubscriptğ‘‘1subscriptğ‘‘2â€¦subscriptğ‘‘ğ‘›d_{1}\\leq d_{2}\\leq\\ldots\\leq d_{n} are the sorted degrees. As a result, we have the following bounds: The definition of Ïâˆ—superscriptğœŒâˆ—\\rho^{\\ast} plays a key role in the original trace-ratio optimization. FollowingÂ [ref]28, such a problem is formulated in scalar terms, i.e. in terms of finding Actually, for Ïâˆ—superscriptğœŒâˆ—\\rho^{\\ast} we have have that Therefore, the trace-ratio problem can be solved by alternating two updating steps: : Given ÏğœŒ\\rho, apply the Lanczos method to obtain the pğ‘p largest eigenvalues of the transition matrix ğâˆ’Ïâ€‹ğƒğğœŒğƒ\\mathbf{P}-\\rho\\mathbf{D} (the smallest of â–³âˆ’Ïâ€‹ğƒâ–³ğœŒğƒ\\triangle-\\rho\\mathbf{D}) and their associated eigenvectors ğ”ğ”\\mathbf{U}. : Given the current eigenvectors ğ”ğ”\\mathbf{U}, update Ï=Trâ€‹[ğ”Tâ€‹â–³â€‹ğ”]Trâ€‹[ğ”Tâ€‹ğƒğ”]ğœŒTrdelimited-[]superscriptğ”ğ‘‡â–³ğ”Trdelimited-[]superscriptğ”ğ‘‡ğƒğ”\\rho=\\frac{\\text{Tr}[\\mathbf{U}^{T}\\triangle\\mathbf{U}]}{\\text{Tr}[\\mathbf{U}^{T}\\mathbf{D}\\mathbf{U}]} In the above process, the update of ğ”ğ”\\mathbf{U} ensures the orthogonality constraint. The Trace Ratio and SGD.\nHowever, solving the trace-ratio problem through gradient descent drives us to a different solution from the eigenvectors of â–³âˆ’Ïâˆ—â€‹ğƒâ–³superscriptğœŒâˆ—ğƒ\\triangle-\\rho^{\\ast}\\mathbf{D}. For instance, consider the Dirichlet loss â„’D=Trâ€‹[ğ”Tâ€‹â–³â€‹ğ”]Trâ€‹[ğ”Tâ€‹ğƒğ”]subscriptâ„’ğ·Trdelimited-[]superscriptğ”ğ‘‡â–³ğ”Trdelimited-[]superscriptğ”ğ‘‡ğƒğ”{\\cal L}_{D}=\\frac{\\text{Tr}[\\mathbf{U}^{T}\\triangle\\mathbf{U}]}{\\text{Tr}[\\mathbf{U}^{T}\\mathbf{D}\\mathbf{U}]}. Then, its gradient (supposing that the orthogonality is enforced by a complementary loss) is given by: Therefore, âˆ‡â„’D=0âˆ‡subscriptâ„’ğ·0{\\nabla{\\cal L}_{D}}=0 implies â–³â€‹ğ”=Ïâˆ—â€‹ğƒğ”â–³ğ”superscriptğœŒâˆ—ğƒğ”\\triangle\\mathbf{U}=\\rho^{\\ast}\\mathbf{D}\\mathbf{U}, where Ïâˆ—â†’0â†’superscriptğœŒâˆ—0\\rho^{\\ast}\\rightarrow 0 is the asymptotic value of the trace ratio. As a result, we have that the optimal ğ”ğ”\\mathbf{U} satisfy â–³~â€‹ğ”=Ïâˆ—â€‹ğ”~â–³ğ”superscriptğœŒâˆ—ğ”\\tilde{\\triangle}\\mathbf{U}=\\rho^{\\ast}\\mathbf{U}, i.e. the gradient descent converges to the (orthonormal) functions of the normalized Laplacian â–³~~â–³\\tilde{\\triangle} associated with the value Ïâˆ—superscriptğœŒâˆ—\\rho^{\\ast}. However, as Ïâˆ—superscriptğœŒâˆ—\\rho^{\\ast} is not necessarily an eigenvalue of â–³~~â–³\\tilde{\\triangle}, but it is close to the Fiedler value Î»2subscriptğœ†2\\lambda_{2}, we denote ğ”ğ”\\mathbf{U} as a Fiedler environment. It is an environment since the pğ‘p columns ğ®:jsubscriptğ®:absentğ‘—\\mathbf{u}_{:j} are mutually orthonormal and close to the Fiedler vector Ï•2subscriptitalic-Ï•2\\phi_{2} insofar their Dirichlet energies Ïjâˆ—=ğ®:jTâ€‹â–³~â€‹ğ®:jsubscriptsuperscriptğœŒâˆ—ğ‘—superscriptsubscriptğ®:absentğ‘—ğ‘‡~â–³subscriptğ®:absentğ‘—\\rho^{\\ast}_{j}={\\mathbf{u}_{:j}}^{T}\\tilde{\\triangle}\\mathbf{u}_{:j} satisfy |Î»2âˆ’Ïjâˆ—|<Ïµsubscriptğœ†2subscriptsuperscriptğœŒâˆ—ğ‘—italic-Ïµ|\\lambda_{2}-\\rho^{\\ast}_{j}|<\\epsilon with Ïµâ†’0â†’italic-Ïµ0\\epsilon\\rightarrow 0 (TheoremÂ 4.1). In our experiments, we have chosen the trace-ratio formulation because: It leads an implicit normalization of the gradient âˆ‡â„’Dâˆ‡subscriptâ„’ğ·{\\nabla{\\cal L}_{D}}, namely Trâ€‹[ğ”Tâ€‹ğƒğ”]Trdelimited-[]superscriptğ”ğ‘‡ğƒğ”\\text{Tr}[\\mathbf{U}^{T}\\mathbf{D}\\mathbf{U}]. The gradient is more structured when we apply the constrain ğ”=fÎ¸â€‹(ğ€)ğ”subscriptğ‘“ğœƒğ€\\mathbf{U}=f_{\\theta}(\\mathbf{A}), where ğ€ğ€\\mathbf{A} is the adjacency matrix. Regarding a), our implicit normalization alleviates the problem of landing in local minima due to the orthonormalization constraint (that we also enforce in the global loss). As notedÂ , constraints of the form ğ”Tâ€‹ğ”=ğˆsuperscriptğ”ğ‘‡ğ”ğˆ\\mathbf{U}^{T}\\mathbf{U}=\\mathbf{I} define a Riemannian manifold and the trace problem s.t. them is not geodesically convex. InÂ [ref]49 this is addressed by introducing a Riemannian gradient and retraction normalization. However, our main gain in performance is achieved when we address b) via the joint effect of normalization and ğ”=fÎ¸â€‹(ğ€)ğ”subscriptğ‘“ğœƒğ€\\mathbf{U}=f_{\\theta}(\\mathbf{A}). In our preliminary experiments, we compared the gradient when applying the constraint ğ”=fÎ¸â€‹(ğ€)ğ”subscriptğ‘“ğœƒğ€\\mathbf{U}=f_{\\theta}(\\mathbf{A}) vs the one when doing only ğ”=fÎ¸â€‹(ğˆ)ğ”subscriptğ‘“ğœƒğˆ\\mathbf{U}=f_{\\theta}(\\mathbf{I}). Discarding the biases, and the non-linearities in both cases, we have ğ”=ğ€ğ–ğ”ğ€ğ–\\mathbf{U}=\\mathbf{AW} vs ğ”=ğ–ğ”ğ–\\mathbf{U}=\\mathbf{W}. For simplicity, we consider the gradient wrt a single column, i.e. we analyze ğ®=ğ€ğ°ğ®ğ€ğ°\\mathbf{u}=\\mathbf{Aw} vs ğ®=ğ°ğ®ğ°\\mathbf{u}=\\mathbf{w} Given a random initialization of ğ°ğ°\\mathbf{w}, this vector plays the role of a random projector of the rows in ğ€ğ€\\mathbf{A}. Following, the Johnson-Lindenstrauss Lemma, ğ°^=ğ€ğ°^ğ°ğ€ğ°\\hat{\\mathbf{w}}=\\mathbf{Aw} tends to replicate the structure of the adjacency. Actually, if the entries of ğ°iâˆ¼ğ’©â€‹(0,1)similar-tosubscriptğ°ğ‘–ğ’©01\\mathbf{w}_{i}\\sim{\\cal N}(0,1) then, those of the projection satisfy ğ°^iâˆ¼ğ’©â€‹(0,di2)similar-tosubscript^ğ°ğ‘–ğ’©0superscriptsubscriptğ‘‘ğ‘–2\\hat{\\mathbf{w}}_{i}\\sim{\\cal N}(0,d_{i}^{2}), where disubscriptğ‘‘ğ‘–d_{i} is the degree of node iğ‘–i. As a result, if we have cğ‘c well-defined communities in the graph G=(V,E)ğºğ‘‰ğ¸G=(V,E) with adjacency matrix ğ€ğ°ğ€ğ°\\mathbf{Aw}, then the projection ğ°^^ğ°\\hat{\\mathbf{w}} is near piecewise constant (actually the norm of the iâˆ’limit-fromğ‘–i-th row is preserved: â€–ğ€i:â€–â‰ˆâ€–ğ°^â€–normsubscriptğ€:ğ‘–absentnorm^ğ°\\|\\mathbf{A}_{i:}\\|\\approx\\|\\hat{\\mathbf{\\mathbf{w}}}\\|). As a result, the projection ğ°^^ğ°\\hat{\\mathbf{w}} is more structured than ğ°ğ°\\mathbf{w} and this is propagated and even amplified during gradient descent. In addition, the normalization of âˆ‡â„’D,Î¸â€²âˆ‡subscriptsuperscriptâ„’â€²ğ·ğœƒ{\\nabla{\\cal L}^{\\prime}_{D,\\theta}} is stronger than that of âˆ‡â„’D,Î¸âˆ‡subscriptâ„’ğ·ğœƒ{\\nabla{\\cal L}_{D,\\theta}}. Overall, when evaluating the performance in Squirrel and Chameleon using only ğ”=fÎ¸â€‹(ğˆ)ğ”subscriptğ‘“ğœƒğˆ\\mathbf{U}=f_{\\theta}(\\mathbf{I}) (i.e. using âˆ‡â„’D,Î¸âˆ‡subscriptâ„’ğ·ğœƒ{\\nabla{\\cal L}_{D,\\theta}}) we only obtain 41.38Â±2.98plus-or-minus41.382.9841.38\\pm{2.98} and 58.48Â±4.69plus-or-minus58.484.6958.48\\pm{4.69}. However, using ğ”=fÎ¸â€‹(ğ€)ğ”subscriptğ‘“ğœƒğ€\\mathbf{U}=f_{\\theta}(\\mathbf{A}) (gradient\nâˆ‡â„’D,Î¸â€²âˆ‡subscriptsuperscriptâ„’â€²ğ·ğœƒ{\\nabla{\\cal L}^{\\prime}_{D,\\theta}}) leads to 73.48Â±1.59plus-or-minus73.481.5973.48\\pm{1.59} and 80.48Â±1.46plus-or-minus80.481.4680.48\\pm{1.46} respectively. Finally, a detailed impact of the two above formulations in the variance of the SGD as inÂ  is beyond the scope of this paper. Assymptotic Diffusions. As explained above, optimizing the Dirichlet loss leads to Fiedler environments, i.e. the rows ğ”i:subscriptğ”:ğ‘–absent\\mathbf{U}_{i:} contain the pğ‘p nearly orthogonal eigenvectors with eigenvalues Î³1=1>Î³2â‰¥â€¦â‰¥Î³psubscriptğ›¾11subscriptğ›¾2â€¦subscriptğ›¾ğ‘\\gamma_{1}=1>\\gamma_{2}\\geq\\ldots\\geq\\gamma_{p}. FollowingÂ , the diffusion distance Djâ€‹(i,j)subscriptğ·ğ‘—ğ‘–ğ‘—D_{j}(i,j) at time tğ‘¡t between two nodes iğ‘–i and jğ‘—j is defined spectrally as: where Ï€(k):=dk/vol(G)\\pi(k)\\mathrel{\\mathop{:}}=d_{k}/vol(G) are the components from the stationary probability distribution limtâ†’âˆpâ€‹(j,t|i)=Ï€subscriptâ†’ğ‘¡ğ‘ğ‘—conditionalğ‘¡ğ‘–ğœ‹\\lim_{t\\rightarrow\\infty}p(j,t|i)={\\mathbf{\\pi}}, i.e. the eigenvector ğ”:1âˆ—subscriptsuperscriptğ”âˆ—:absent1\\mathbf{U}^{\\ast}_{:1} corresponding to Î³1=1subscriptğ›¾11\\gamma_{1}=1. In the above equation, ğ”âˆ—superscriptğ”âˆ—\\mathbf{U}^{\\ast} denote the true eigenvectors of the transition matrix ğğ\\mathbf{P}, and Î“:=diag(Î³1,Î³2,â€¦,Î³p)\\Gamma\\mathrel{\\mathop{:}}=\\text{diag}(\\gamma_{1},\\gamma_{2},\\ldots,\\gamma_{p}) is the diagonal matrix with the corresponding eigenvalues. Thus, Eq.Â 11 can be explained in the following terms: Dt2â€‹(i,j)subscriptsuperscriptğ·2ğ‘¡ğ‘–ğ‘—D^{2}_{t}(i,j) compare the probabilities that two random walks (one starting in iğ‘–i and another one in jğ‘—j) reach any other node kğ‘˜k in time tğ‘¡t. The spectral interpretation relies on the spectral theorem applied to the transition matrix ğ=ğ”âˆ—â€‹Î›â€‹ğ”âˆ—Tğsuperscriptğ”âˆ—Î›superscriptsuperscriptğ”âˆ—ğ‘‡\\mathbf{P}=\\mathbf{U}^{\\ast}\\Lambda{\\mathbf{U}^{\\ast}}^{T}. As a result, ğt=ğ”âˆ—â€‹Î“tâ€‹ğ”âˆ—T=âˆ‘r=1nÎ³rtâ€‹ğ”:râˆ—â€‹ğ”:râˆ—Tsuperscriptğğ‘¡superscriptğ”âˆ—superscriptÎ“ğ‘¡superscriptsuperscriptğ”âˆ—ğ‘‡superscriptsubscriptğ‘Ÿ1ğ‘›superscriptsubscriptğ›¾ğ‘Ÿğ‘¡subscriptsuperscriptğ”âˆ—:absentğ‘Ÿsuperscriptsubscriptsuperscriptğ”âˆ—:absentğ‘Ÿğ‘‡\\mathbf{P}^{t}=\\mathbf{U}^{\\ast}{\\Gamma}^{t}{\\mathbf{U}^{\\ast}}^{T}=\\sum_{r=1}^{n}\\gamma_{r}^{t}\\mathbf{U}^{\\ast}_{:r}{\\mathbf{U}^{\\ast}_{:r}}^{T}, with n=|V|ğ‘›ğ‘‰n=|V|. However, since determining what is the correct diffusion time is very hard (it is usually a hyperparameter in some GNNs), we are interested in the asymptotic diffusion distance Dtâ†’âˆ2subscriptsuperscriptğ·2â†’ğ‘¡D^{2}_{t\\rightarrow\\infty}. Qiu and HancockÂ  determined that i.e. eigenvalues {1âˆ’Î³r}1subscriptğ›¾ğ‘Ÿ\\{1-\\gamma_{r}\\} of the normalized Laplacian â–³~~â–³\\tilde{\\triangle} are used instead of those of ğğ\\mathbf{P}. Actually, the right side of the above equation is the well-known commute timesÂ  distance CTâ€‹(i,j)CTğ‘–ğ‘—\\text{CT}(i,j). Note that such a distance is dominated by the Fiedler value and vector: Î»2=(1âˆ’Î³2)subscriptğœ†21subscriptğ›¾2\\lambda_{2}=(1-\\gamma_{2}) and ğ”:2âˆ—subscriptsuperscriptğ”âˆ—:absent2\\mathbf{U}^{\\ast}_{:2}, respectively. This fact simplifies the interpretation of our approximate diffusion distance as follows: Our approximated eigenvectors, contained in the pğ‘p columns of ğ”ğ”\\mathbf{U} have eigenvalues (Dirichlet energies) Ïrâˆ—subscriptsuperscriptğœŒâˆ—ğ‘Ÿ\\rho^{\\ast}_{r} close to Ïâˆ—superscriptğœŒâˆ—\\rho^{\\ast} (the optimal trace ratio achieved by the Dirichlet loss). Theorethically, we have that the smallest Ïrminâˆ—subscriptsuperscriptğœŒâˆ—subscriptğ‘Ÿmin\\rho^{\\ast}_{r_{\\text{min}}} satisfies Î»2â‰¤Ïrminâˆ—subscriptğœ†2subscriptsuperscriptğœŒâˆ—subscriptğ‘Ÿmin\\lambda_{2}\\leq\\rho^{\\ast}_{r_{\\text{min}}}. Therefore, if we order Ïrâˆ—subscriptsuperscriptğœŒâˆ—ğ‘Ÿ\\rho^{\\ast}_{r} in ascending order, then we obtain However, in the heterophilic regime (where the labels break the structure) we usually have Î»2â‰ªÏrminâˆ—much-less-thansubscriptğœ†2subscriptsuperscriptğœŒâˆ—subscriptğ‘Ÿmin\\lambda_{2}\\ll\\rho^{\\ast}_{r_{\\text{min}}}. See for instance the Fiedler environments obtained for SBMs in FigureÂ 4 and the discussion below (Appendix B). As a result, in practice we have This proofs CorollaryÂ 4.2. Escape Probabilities. Approximating commute times distances is very convenient for our jump-based analysis, since it is well known that the escape probability is related to the commute times distanceÂ : peâ€‹sâ€‹c=1CTâ€‹(i,j)subscriptğ‘ğ‘’ğ‘ ğ‘1CTğ‘–ğ‘—p_{esc}=\\frac{1}{\\text{CT}(i,j)}. Escape probabilities are actually dependent on the spectral gap (approximated by the Fiedler value Î»2subscriptğœ†2\\lambda_{2}). This is illustrated in the very first Figure of this paper (FigureÂ 1), where a random walker tries to escape from the community AÂ¯Â¯ğ´\\bar{A}. A classic resultÂ  shows that the probability that a random walk started in its asymptotic (tâ†’âˆâ†’ğ‘¡t\\rightarrow\\infty) distribution Ï€ğœ‹\\pi is transitioning from iâˆˆAÂ¯ğ‘–Â¯ğ´i\\in\\bar{A} to jâˆˆAğ‘—ğ´j\\in A in one step is peâ€‹sâ€‹câ€‹(AÂ¯,A)=cutâ€‹(AÂ¯,A)volâ€‹(AÂ¯)subscriptğ‘ğ‘’ğ‘ ğ‘Â¯ğ´ğ´cutÂ¯ğ´ğ´volÂ¯ğ´p_{esc}(\\bar{A},A)=\\frac{\\text{cut}(\\bar{A},A)}{\\text{vol}(\\bar{A})}, where cutâ€‹(AÂ¯,A)=âˆ‘iâˆˆAÂ¯,jâˆˆAeiâ€‹jcutÂ¯ğ´ğ´subscriptformulae-sequenceğ‘–Â¯ğ´ğ‘—ğ´subscriptğ‘’ğ‘–ğ‘—\\text{cut}(\\bar{A},A)=\\sum_{i\\in\\bar{A},j\\in A}e_{ij} (in the Figure we have that cutâ€‹(AÂ¯,A)=1cutÂ¯ğ´ğ´1\\text{cut}(\\bar{A},A)=1). Therefore, as dtâ†’âˆâ€‹(i,j)=Î±â€‹CTâ€‹(i,j)superscriptğ‘‘â†’ğ‘¡ğ‘–ğ‘—ğ›¼CTğ‘–ğ‘—d^{t\\rightarrow\\infty}(i,j)=\\alpha\\text{CT}(i,j), with Î±â‰ª1much-less-thanğ›¼1\\alpha\\ll 1 our jump-hierarchy is closer to that of the escape probability than choosing CTâ€‹(i,j)CTğ‘–ğ‘—\\text{CT}(i,j) as asymptotic diffusion distance. In addition, we are sensitive to the spectral gap since the Fiedler environment contains approximations of the Fiedler vector, and the spectral gap is approximated in turn by the Dirichlet energy of the Fiedler vector. Clustering and Metastable States. Minimizing the Dirichlet loss in conjunction with the classification loss (see EqÂ 3) leads to a trade-off between two clustering problems. On the one hand, we infer a piecewise-smooth latent space. On the other hand, we simultaneously try to preserve the structure of the input graph as much as possible. In both cases, we try to find metastable states. A metastable state is a concept borrowed from dynamical systems but basically, it is an equilibrium state in a random process (for instance the one defined by a random walk that tries to escape from a community in FigureÂ 1). Metastable states are also characterized by wells in potential functions Uâ€‹(ğ±)ğ‘ˆğ±U(\\mathbf{x}), where ğ±ğ±\\mathbf{x} is a state and its probability is given by the Boltzmann distribution pâ€‹(ğ±)=eâˆ’Uâ€‹(ğ±)ğ‘ğ±superscriptğ‘’ğ‘ˆğ±p(\\mathbf{x})=e^{-U(\\mathbf{x})}.\nThen, the characteristic relaxation processes and time scales of a given space are usually described by a Stochastic Diffusion Equation (SDE): where ğ°ğ°\\mathbf{w} denotes Brownian motion. In the above equation, we have a drag term (the gradient) that drives the process to a deep well, and a random term (the Brownian motion) that allows us to escape from local minima. During this process, we find different time scales: fast scales while we are moving through a given well, and slow scales when we try to escape from it. For instance, escaping from the right community in FigureÂ 1 takes a long time which depends on the difference between the potential at the well Uâ€‹(ğ±mâ€‹iâ€‹n)ğ‘ˆsubscriptğ±ğ‘šğ‘–ğ‘›U(\\mathbf{x}_{min}) and that of the saddle point Uâ€‹(ğ±mâ€‹aâ€‹x)ğ‘ˆsubscriptğ±ğ‘šğ‘ğ‘¥U(\\mathbf{x}_{max})Â . This time is in turn the inverse of the spectral gap, i.e. there is a spectral interpretation of the SDE. Such interpretation comes from the analysis of the Fokker-Planck equation: This equation leads to the pdf of the SDE and it has a spectral interpretation. More precisely, the eigenvectors of ğğ\\mathbf{P} converge to the eigenfunctions Î¨â€‹(ğ±)Î¨ğ±\\Psi(\\mathbf{x}) of the Fokker-Planck equation as followsÂ [ref]53: where â–³=âˆ‡â‹…âˆ‡â–³â‹…âˆ‡âˆ‡\\triangle=\\nabla\\cdot\\nabla is the Laplacian and Î¼ğœ‡\\mu the eigenstates (eigenvalues). As a result, we may use the Fiedler vector to characterize the separation between two clusters. The steepest the Fiedler vector, the better the separation (TheoremÂ 4.4). Interestingly, the third eigenvector âˆ‡~~âˆ‡\\tilde{\\nabla} may not work well as a state separator when we have different spatial scales, as we will see in Appendix B. Dirichlet Label Propagation. As we mentioned in the main paper, our Dirichlet formulation is inspired by classical graph-based semi-supervised methods. The work inÂ  poses the problem of propagating known labels â„“â€‹(B)â„“ğµ\\ell(B) to unknown nodes uâˆˆUğ‘¢ğ‘ˆu\\in U. Let ğ˜ğ˜\\mathbf{Y} be a nÃ—cğ‘›ğ‘n\\times c matrix where ğ˜â€‹(i,j)=1ğ˜ğ‘–ğ‘—1\\mathbf{Y}(i,j)=1 means that node iâˆˆBğ‘–ğµi\\in B has label jğ‘—j and ğ˜â€‹(i,j)=0ğ˜ğ‘–ğ‘—0\\mathbf{Y}(i,j)=0 otherwise. Then, the optimal label of each node iğ‘–i is given by ğ²â€‹(i)=argâ¡maxjâ‰¤câ¡ğ…â€‹(i,j)ğ²ğ‘–subscriptğ‘—ğ‘ğ…ğ‘–ğ‘—\\mathbf{y}(i)=\\arg\\max_{j\\leq c}\\mathbf{F}(i,j), where being ğğ\\mathbf{P} the transition matrix and Î±+Î²=1ğ›¼ğ›½1\\alpha+\\beta=1. The nÃ—cğ‘›ğ‘n\\times c matrix ğ…ğ…\\mathbf{F} works as a basic node representation (not exactly a latent space) since each of is cğ‘c rows is stochastic. However, its construction exploits the powers of ğğ\\mathbf{P} as follows: and In addition, we also noted that ğ…ğ…\\mathbf{F} is also the solution of the Dirichlet problem: where Î¼>0ğœ‡0\\mu>0 is a regularization parameter satisfying Î±=1/(1+Î¼)ğ›¼11ğœ‡\\alpha=1/(1+\\mu). The proof is obtained by setting the gradient to zero: which leads to Eq.Â 19. Concerning the relationship of this formulation with absorbing random walksÂ ,\nthe main idea is to extend the nÃ—nğ‘›ğ‘›n\\times n transition matrix ğğ\\mathbf{P} so that: We include an upper block with the pÃ—pğ‘ğ‘p\\times p identity matrix ğˆğˆ\\mathbf{I}. This block represents the pğ‘p absorbing states, where p=|â„“â€‹(B)|ğ‘â„“ğµp=|\\ell(B)|. Then the nÃ—pğ‘›ğ‘n\\times p block ğ‘ğ‘\\mathbf{R} encodes the prior probabilities of reaching an absorbing state from a non-absorbing one. The absorbing probabilities are given by ğ:=(ğˆnÃ—nâˆ’ğ)âˆ’1ğ‘\\mathbf{B}\\mathrel{\\mathop{:}}=(\\mathbf{I}_{n\\times n}-\\mathbf{P})^{-1}\\mathbf{R}â€„. Finally, the random-walker versionÂ  is quite similar to the above one, but reorganizes the Laplacian matrix (TheoremÂ 4.3).\n\nReference markers found: [28]\n\nContext windows for each [ref]:\n[ref]28: d_{n} are the sorted degrees. As a result, we have the following bounds: The definition of Ïâˆ—superscriptğœŒâˆ—\\rho^{\\ast} plays a key role in the original trace-ratio optimization. FollowingÂ [ref]28, such a problem is formulated in scalar terms, i.e. in terms of finding Actually, for Ïâˆ—superscriptğœŒâˆ—\\rho^{\\ast} we have have that Therefore, the trace-ratio problem can be solved by\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 28,\n            \"titles\": [\n                \"Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods\",\n                \"Semi-Supervised Classification with Graph Convolutional Networks\",\n                \"Inductive Representation Learning on Large Graphs\",\n                \"Graph Attention Networks\",\n                \"Beyond homophily in graph neural networks: Current limitations and effective designs\",\n                \"Non-Local Graph Neural Networks\",\n                \"Graph Neural Networks with Heterophily\",\n                \"Simple and Deep Graph Convolutional Networks\",\n                \"Make Heterophily Graphs Better Fit GNN: A Graph Rewiring Approach\",\n                \"DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound\",\n                \"Understanding over-squashing and bottlenecks on graphs via curvature\",\n                \"On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology\",\n                \"Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks\",\n                \"Shortest Path Networks for Graph Property Prediction\",\n                \"Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing\",\n                \"MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing\",\n                \"Improving Graph Neural Networks with Simple Architecture Design\",\n                \"Not all neighbors are friendly: Learning to choose hop features to improve node classification\",\n                \"SIGN: Scalable Inception Graph Neural Networks\",\n                \"Simplified Graph Convolution with Heterophily\",\n                \"Adaptive Universal Generalized PageRank Graph Neural Network\",\n                \"Phase transitions and metastability in markovian and molecular systems\",\n                \"Eigenvalues of the fokkerâ€“planck operator and the approach to equilibrium for diffusions in potential fields\",\n                \"Reaction-rate theory: fifty years after kramers\",\n                \"Fundamental limitations of spectral clustering\",\n                \"Diffusion maps, spectral clustering and eigenfunctions of fokker-planck operators\",\n                \"Trace ratio vs. ratio trace for dimensionality reduction\",\n                \"Trace ratio problem revisited\",\n                \"The trace ratio optimization problem\",\n                \"Graph U-Nets\",\n                \"Differentiable top-k with optimal transport\",\n                \"Learning with local and global consistency\",\n                \"Random walks and electric networks\",\n                \"Random walks for image segmentation\",\n                \"A random walks view of spectral segmentation\",\n                \"Collective classification in network data\",\n                \"Geom-GCN: Geometric Graph Convolutional Networks\",\n                \"Multi-scale attributed node embedding\",\n                \"Graph neural networks as gradient flows: understanding graph convolutions via energy\",\n                \"PairNorm: Tackling Oversmoothing in GNNs\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]28 based on the context and research area of Diffusion-Jump GNNs: Homophiliation via Learnable Metric Filters. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}