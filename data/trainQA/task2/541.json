{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Benchmarking Learned Indexes\n\nPaper Abstract: \n\nPaper Section Text:\nA primary concern of index structures is lookup performance: given a query, how quickly can the correct record be fetched? However, size is also important: with no limits, one could simply store a lookup table and retrieve the correct record with only a single cache miss. Such a lookup table would be prohibitively large in many cases, such as 64-bit keys. Thus, we consider the performance / size tradeoff provided by each index structure, plotted in Figure¬†7. For each index structure, we selected ten configurations ranging from minimum to maximum size. While different applications may weigh performance and size differently, all applications almost surely desire a Pareto optimal index: an index for which no alternative has both a smaller size and improved performance. For the amzn and wiki datasets, learned structures are Pareto optimal up to a size of 100MB, at which point the RBS lookup table becomes effective. For face, learned structures are Pareto optimal throughout. Poor performance on osm. Both traditional and learned index structures fail to outperform RBS on the osm dataset for nearly any size. The poor performance of learned index structures can be attributed to the osm‚Äôs dataset lack of local structure: even small pieces of the CDF exhibit difficult-to-model erratic behavior. This is an artifact of the technique used to project the Earth into one-dimensional space (a Hilbert curve). In Section¬†4.3, we confirm this intuition by analyzing the errors of the learned models; all three learned structures required significantly more storage to achieve errors comparable to those observed on the other datasets. Simply put, learned structures perform poorly on osm because osm is difficult to learn. Because osm is a one-dimensional projection of multi-dimensional data, a multi-dimensional learned index¬† may yield improvements. Performance of PGM. In¬†[ref]13, the authors showed that ‚Äúthe PGM-index dominates RMI,‚Äù contradicting our previous experience that the time spent on searches between the layers of the index outweighed the benefits of having a lower error. Indeed, in our experimental evaluation we found that the PGM index performs significantly worse than RMI on 3 out of the 4 datasets and slightly worse on osm.\nAfter contacting the authors of¬†[ref]13, we found that their RMI implementation was missing several key optimizations: their RMI only used linear models rather than tuning different type of models as proposed in¬† , and omitted some optimizations for RMIs with only linear models.222We shared our RMI implementation with Ferragina and Vinciguerra before the publication of¬†[ref]13, but since¬†[ref]13 was already undergoing revision, they elected to continue with their own RMI implementation instead, without note. All PGM results in this paper are based on Ferragina and Vinciguerra‚Äôs tuned PGM code as of May 18th, 2020. This highlights how implementation details can affect experimental results, and the importance of having a common benchmark with strong implementations. We stress that our results are the first to compare RMI and PGM implementations tuned by their respective authors. Performance of RBS. RBS exhibits substantially degraded performance on face compared to other datasets. This is due to a small number (‚âà100absent100\\approx 100) of outliers in the face dataset: most keys fall within (0,250)0superscript250(0,2^{50}), but the outliers fall in (259,264‚àí1)superscript259superscript2641(2^{59},2^{64}-1). These outliers cause the first 16 prefix bits of the RBS lookup table to be nearly useless.\nOne could adjust RBS to handle this simple case (when all outliers are at one end of the dataset), but in general such large jumps in values represents a severe weakness of RBS. ART¬†[ref]20 can be viewed as a generalization of RBS to handle this type of skew regardless of where it occurs in the dataset. On other datasets, RBS is surprisingly competitive, often outperforming other indexes. This is partially explained by the low inference time required by RBS: getting a search bound requires only a bit shift and an array lookup. When the prefixes of keys are distributed uniformly across a range, an RBS with a radix table of size 2bsuperscript2ùëè2^{b} provides equally accurate bounds as a binary search tree with bùëèb levels, but requires only a single cache miss. When the keys are heavily skewed (as is the case with face), the radix table is nearly useless. Tree structures are non-monotonic. All tree structures tested (ART, BTree, IBTree, and FAST) become less effective after a certain size. For example, the largest ART index for the amzn data occupies nearly 1GB of space, but has worse lookup performance than an ART index occupying only 100MB of space. This is because, at a certain point, performing a binary search on a small densely-packed array becomes more efficient than traversing a tree. As a result, tree structures show non-monotonic behavior in Figure¬†7. Indexes slower than binary search? At extremely small or large sizes, some index structures perform worse than binary search. In both cases, this is because some index structures are unable to provide sufficiently small search bounds to make up for the inference time required. For example, on the osm dataset, very small RMIs barely narrow down the search range at all. Because this small RMIs fit is so poor (analyzed later, Figure¬†12), the time required to execute the RMI model and produce the search bound is comparatively worse than executing a binary search on the entire dataset. Structures for strings. Many recent works on index structures have focused on indexing keys of arbitrary length (e.g., strings)¬† .\nFor completeness, we evaluated two structures designed for string keys ‚Äì FST and Wormhole ‚Äì in Figure¬†8. Unsurprisingly, neither performed as well as binary search. These string indexes contain optimizations that assume that comparing two keys is expensive. These optimizations translate to overhead when considering only integer keys, which can be compared in a single instruction. ART, an index designed for both string and integer data, does so by indexing one key-byte per radix tree level. Hashing. Hashing provides O‚Äã(1)ùëÇ1O(1) time point lookups. However, hashing differs from both traditional and learned indexes in a number of ways: first, hashing generally does not support lower bound lookups.333Wormhole, evaluated in Figure¬†8, is a hash-based technique that provides ordering, but is primarily optimized for strings. Second, hash tables generally have a large footprint, as they store every key.\nWe evaluate two hashing techniques ‚Äì a Cuckoo hash table¬† and a Robinhood hash table¬†.\nWe found that a load factor of 0.99 and 0.25 (respectively) maximized lookup performance. Table¬†2 lists the size and lookup performance of the best-performing (and thus often largest) variant of each index structure and both hashing techniques for a 32-bit version444The SIMD Cuckoo implementation only supports 32-bit keys. of the amzn dataset (results similar for others). Unsurprisingly, both hashing techniques offer superior point-lookup latency compared to traditional and learned index structures. This decreased latency comes at the cost of a larger in-memory footprint. For example, CuckooMap provides a 114ns lookup time compared to the 180ns provided by the RMI, but CuckooMap uses over 1GB of memory, whereas the RMI uses only 48MB. When range lookups and memory footprint are not concerns, hashing is a clear choice.\n\nReference markers found: [13]\n\nContext windows for each [ref]:\n[ref]13: on osm because osm is difficult to learn. Because osm is a one-dimensional projection of multi-dimensional data, a multi-dimensional learned index¬† may yield improvements. Performance of PGM. In¬†[ref]13, the authors showed that ‚Äúthe PGM-index dominates RMI,‚Äù contradicting our previous experience that the time spent on searches between the layers of the index outweighed the benefits of having\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 13,\n            \"titles\": [\n                \"FITing-Tree: A Data-aware Index Structure\",\n                \"MIT RMI\",\n                \"C++ lower_bound\",\n                \"RobinMap\",\n                \"RocksDB\",\n                \"Searching on sorted data benchmark\",\n                \"SIMD Cuckoo Hash\",\n                \"STX B+ Tree\",\n                \"Efficient parallel lists intersection and index compression algorithms using graphics processing units\",\n                \"An almost optimal algorithm for unbounded searching\",\n                \"HOT: A height optimized trie index for main-memory database systems\",\n                \"ALEX: An Updatable Adaptive Learned Index\",\n                \"Learned data structures\",\n                \"The PGM-index: A fully-dynamic compressed learned index with provable worst-case bounds\",\n                \"B-tree indexes, interpolation search, and skew\",\n                \"FAST: Fast architecture sensitive tree search on modern CPUs and GPUs\",\n                \"SOSD: A Benchmark for Learned Indexes\",\n                \"RadixSpline: A Single-Pass Learned Index\",\n                \"The Case for Learned Index Structures\",\n                \"The adaptive radix tree: ARTful indexing for main-memory databases\",\n                \"LSM-based Storage Techniques: A Survey\",\n                \"CDFShop: Exploring and Optimizing Learned Index Structures\",\n                \"Peephole optimization\",\n                \"Learning Multi-dimensional Indexing\",\n                \"Smooth interpolating histograms with error guarantees\",\n                \"Don‚Äôt Throw Out Your Algorithms Book Just Yet: Classical Data Structures That Can Outperform Learned Indexes (blog post)\",\n                \"The Case for B-Tree Index Structures (blog post)\",\n                \"A seven-dimensional analysis of hashing methods and its implications on query processing\",\n                \"An eight-dimensional systematic evaluation of optimized search algorithms on modern processors\",\n                \"Efficiently Searching In-Memory Sorted Arrays: Revenge of the Interpolation Search?\",\n                \"Wormhole: A Fast Ordered Index for In-memory Data Management\",\n                \"Maximum error-bounded Piecewise Linear Representation for online stream approximation\",\n                \"SuRF: Practical Range Query Filtering with Fast Succinct Tries\",\n                \"Generated Reference Title 33 for [ref]13\",\n                \"Generated Reference Title 34 for [ref]13\",\n                \"Generated Reference Title 35 for [ref]13\",\n                \"Generated Reference Title 36 for [ref]13\",\n                \"Generated Reference Title 37 for [ref]13\",\n                \"Generated Reference Title 38 for [ref]13\",\n                \"Generated Reference Title 39 for [ref]13\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]13 based on the context and research area of Benchmarking Learned Indexes. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}