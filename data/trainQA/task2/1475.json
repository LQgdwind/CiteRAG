{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Chapter 1 Model-Based Machine Learning for Communications\n\nPaper Abstract: \n\nPaper Section Text:\nDNN-aided hybrid algorithms combine domain knowledge in the form of a model-based inference algorithm for the problem at hand. This strategy allows the design of model-based ML systems with varying levels of domain knowledge, in which deep learning is used to robustify and remove model-dependence of specific components of the algorithm. In the following we first review the rationale when designing DNN-aided symbol detectors, after which we detail three concrete examples arising from different symbol detection algorithms. DNN-aided algorithms aim to carry out model-based inference methods in a data-driven fashion. These hybrid systems thus utilize deep learning not for the overall inference task, but for robustifying and relaxing the model-dependence of established model-based inference algorithms. Consequently, the design of DNN-aided hybrid systems consists of the following steps: First, a proper inference algorithm is chosen. In particular, the domain knowledge is encapsulated in the selection of the algorithm that is learned from data. For example, the Viterbi algorithm is a natural candidate for symbol detection over finite-memory channels when seeking a symbol detector capable of operating in real-time, or alternatively, the BCJR scheme is the suitable choice for carrying out MAP inference over such channels. When designing receivers for flat MIMO channels, interference cancellation methods may be the preferable algorithmic approach for symbol detection. We show how these methods are converted into DNN-aided algorithms in the sequel. Once a model-based algorithm is selected, we identify its model-specific computations, and replace them with dedicated compact DNNs. The resulting DNNs can be either trained individually, or the overall system can be trained in an end-to-end manner. Since the implementation of DNN-aided algorithms highly varies with the selection of the learned model-based method, we next present three concrete examples in the context of symbol detection: ViterbiNet, which learns to carry out Viterbi detection ; BCJRNet, which implements the BCJR algorithm of  in a data-driven fashion; and DeepSIC, which is based on the soft iterative interference cancellation methods for MIMO symbol detection . ViterbiNet proposed in [ref]11 is a data-driven implementation of the Viterbi detection algorithm , which is one of the most common workhorses in digital communications. This DNN-aided symbol detection algorithm is suitable for finite-memory channels of the form (1), without requiring prior knowledge of the channel conditional distributions p𝒀i|{𝑺l}l=i−L+1isubscript𝑝conditionalsubscript𝒀𝑖superscriptsubscriptsubscript𝑺𝑙𝑙𝑖𝐿1𝑖p_{{{{\\boldsymbol{Y}}}_{i}|\\{{{\\boldsymbol{S}}}_{l}\\}_{l=i-L+1}^{i}}}. For simplicity, we assume in our description that the input cardinality K𝐾K is K=1𝐾1K=1. As a preliminary step to presenting ViterbiNet, we now briefly review conventional model-based Viterbi detection. The Viterbi algorithm recovers the maximum likelihood sequence detector, i.e., Using (1) the optimization problem (12) becomes To proceed, we define a state variable 𝑺¯i:=[𝑺i−L+1,…,𝑺i]∈𝒮Lassignsubscript¯𝑺𝑖subscript𝑺𝑖𝐿1…subscript𝑺𝑖superscript𝒮𝐿\\bar{{{\\boldsymbol{S}}}}_{i}:=[{{\\boldsymbol{S}}}_{i-L+1},\\ldots,{{\\boldsymbol{S}}}_{i}]\\in\\mathcal{S}^{L}. Since the symbols are i.i.d. and uniformly distributed, it follows that p𝑺¯i|𝑺¯i−1​(𝒔¯i|𝒔¯i−1)=M−1subscript𝑝conditionalsubscript¯𝑺𝑖subscript¯𝑺𝑖1conditionalsubscript¯𝒔𝑖subscript¯𝒔𝑖1superscript𝑀1p_{{\\bar{{{\\boldsymbol{S}}}}_{i}|\\bar{{{\\boldsymbol{S}}}}_{i-1}}}(\\bar{{\\boldsymbol{s}}}_{i}|\\bar{{\\boldsymbol{s}}}_{i-1})=M^{-1} when 𝒔¯isubscript¯𝒔𝑖\\bar{{\\boldsymbol{s}}}_{i} is a shifted version of 𝒔¯i−1subscript¯𝒔𝑖1\\bar{{\\boldsymbol{s}}}_{i-1}, i.e., the first L−1𝐿1L-1 entries of 𝒔¯isubscript¯𝒔𝑖\\bar{{\\boldsymbol{s}}}_{i} are the last L−1𝐿1L-1 entries of 𝒔¯i−1subscript¯𝒔𝑖1\\bar{{\\boldsymbol{s}}}_{i-1}, and zero otherwise. We can now write (13) as The optimization problem (14) can be solved recursively using dynamic programming, by iteratively updating a path cost ci​(𝒔¯)subscript𝑐𝑖¯𝒔c_{i}(\\bar{{\\boldsymbol{s}}}) for each state 𝒔¯∈𝒮L¯𝒔superscript𝒮𝐿\\bar{{\\boldsymbol{s}}}\\in\\mathcal{S}^{L}. The resulting scheme, known as the Viterbi algorithm, is given below as Algorithm 1, and illustrated in Fig. 6(a). The Viterbi algorithm has two major advantages: 1)1) It solves (12) at a computational complexity that is linear in the blocklength T𝑇T. For comparison, the computational complexity of solving (12) directly grows exponentially with T𝑇T; 2)2) The algorithm produces estimates sequentially during run-time. In particular, while in (12) the estimated output 𝒔^Tsuperscript^𝒔𝑇\\hat{{\\boldsymbol{s}}}^{T} is computed using the entire received block 𝒚Tsuperscript𝒚𝑇{\\boldsymbol{y}}^{T}, Algorithm 1 computes 𝒔^isubscript^𝒔𝑖\\hat{{\\boldsymbol{s}}}_{i} once 𝒚i+L−1subscript𝒚𝑖𝐿1{\\boldsymbol{y}}_{i+L-1} is received. ViterbiNet proposed in [ref]11 learns to implement the Viterbi algorithm from data in a model-agnostic manner. Following the rationale of DNN-aided algorithms, this is achieved by identifying the model-based components of the algorithm, which for Algorithm 1 boils down to the computation of the log-likelihood function log⁡p𝒀i|𝑺¯i​(𝒚i|𝒔¯)subscript𝑝conditionalsubscript𝒀𝑖subscript¯𝑺𝑖conditionalsubscript𝒚𝑖¯𝒔\\log p_{{{{\\boldsymbol{Y}}}_{i}|\\bar{{{\\boldsymbol{S}}}}_{i}}}\\left({\\boldsymbol{y}}_{i}|\\bar{{\\boldsymbol{s}}}\\right).\nOnce this quantity is computed for each 𝒔¯∈𝒮L¯𝒔superscript𝒮𝐿\\bar{{\\boldsymbol{s}}}\\in\\mathcal{S}^{L}, the Viterbi algorithm only requires knowledge of the memory length L𝐿L. This requirement is much easier to satisfy compared to full CSI. Since the channel is stationary, it holds that the log-likelihood function depends only on the realizations of 𝒚isubscript𝒚𝑖{\\boldsymbol{y}}_{i} and of 𝒔¯¯𝒔\\bar{{\\boldsymbol{s}}}, and not on the time index i𝑖i.\nTherefore, to implement Algorithm 1 in a data-driven fashion, ViterbiNet replaces the explicit computation of the log-likelihoods with an ML-based system that learns to evaluate this function from training data. In this case, the input of the system is the channel output realization 𝒚isubscript𝒚𝑖{\\boldsymbol{y}}_{i} and the output is an estimate of log⁡p𝒀i|𝑺¯i​(𝒚i|𝒔¯)subscript𝑝conditionalsubscript𝒀𝑖subscript¯𝑺𝑖conditionalsubscript𝒚𝑖¯𝒔\\log p_{{{{\\boldsymbol{Y}}}_{i}|\\bar{{{\\boldsymbol{S}}}}_{i}}}\\left({\\boldsymbol{y}}_{i}|\\bar{{\\boldsymbol{s}}}\\right) for each 𝒔¯∈𝒮L¯𝒔superscript𝒮𝐿\\bar{{\\boldsymbol{s}}}\\in\\mathcal{S}^{L}. The rest of the Viterbi algorithm remains intact, and the detector implements Algorithm 1 using the learned log-likelihoods. The proposed architecture is illustrated in Fig. 6(b). Two candidate architectures are considered for learning to compute the log likelihood, one based on classification networks and one using density estimation networks.\n\nLearned likelihood using classification networks: Since 𝒚isubscript𝒚𝑖{\\boldsymbol{y}}_{i} is given and may take continuous values while the desired variables take discrete values, a natural approach to evaluate p𝒀i|𝑺¯i​(𝒚i|𝒔¯)subscript𝑝conditionalsubscript𝒀𝑖subscript¯𝑺𝑖conditionalsubscript𝒚𝑖¯𝒔p_{{{{\\boldsymbol{Y}}}_{i}|\\bar{{{\\boldsymbol{S}}}}_{i}}}\\left({\\boldsymbol{y}}_{i}|\\bar{{\\boldsymbol{s}}}\\right) for each 𝒔¯∈𝒮L¯𝒔superscript𝒮𝐿\\bar{{\\boldsymbol{s}}}\\in\\mathcal{S}^{L} is to estimate p𝑺¯i|𝒀i​(𝒔¯|𝒚i)subscript𝑝conditionalsubscript¯𝑺𝑖subscript𝒀𝑖conditional¯𝒔subscript𝒚𝑖p_{{\\bar{{{\\boldsymbol{S}}}}_{i}|{{\\boldsymbol{Y}}}_{i}}}\\left(\\bar{{\\boldsymbol{s}}}|{\\boldsymbol{y}}_{i}\\right) and then use Bayes rule to obtain A parametric estimate of p𝑺¯i|𝒀i​(𝒔¯|𝒚i)subscript𝑝conditionalsubscript¯𝑺𝑖subscript𝒀𝑖conditional¯𝒔subscript𝒚𝑖p_{{\\bar{{{\\boldsymbol{S}}}}_{i}|{{\\boldsymbol{Y}}}_{i}}}\\left(\\bar{{\\boldsymbol{s}}}|{\\boldsymbol{y}}_{i}\\right), denoted P^𝜽​(𝒔¯|𝒚i)subscript^𝑃𝜽conditional¯𝒔subscript𝒚𝑖\\hat{P}_{{\\boldsymbol{\\theta}}}(\\bar{{\\boldsymbol{s}}}|{\\boldsymbol{y}}_{i}), is obtained for each 𝒔¯∈𝒮L¯𝒔superscript𝒮𝐿\\bar{{\\boldsymbol{s}}}\\in\\mathcal{S}^{L} by training classification networks with softmax output layers to minimize the cross entropy loss.\nHere, for a labeled set {𝒔t,𝒚t}t=1ntsuperscriptsubscriptsubscript𝒔𝑡subscript𝒚𝑡𝑡1subscript𝑛𝑡\\{{\\boldsymbol{s}}_{t},{\\boldsymbol{y}}_{t}\\}_{t=1}^{n_{t}}, the loss function is In general, the marginal PDF of 𝒀isubscript𝒀𝑖{{\\boldsymbol{Y}}}_{i} can be estimated from the training data using mixture density estimation via, e.g.,  expectation minimization (EM) [36, Ch. 2], or any other finite mixture model fitting method. However, obtaining an accurate density estimation becomes challenging when 𝒀isubscript𝒀𝑖{{\\boldsymbol{Y}}}_{i} is high-dimensional.\nSince p𝒀i​(𝒚i)subscript𝑝subscript𝒀𝑖subscript𝒚𝑖p_{{{{\\boldsymbol{Y}}}_{i}}}\\left({\\boldsymbol{y}}_{i}\\right) does not depend on the variable 𝒔¯¯𝒔\\bar{{\\boldsymbol{s}}}, setting p𝒀i​(𝒚i)≡1subscript𝑝subscript𝒀𝑖subscript𝒚𝑖1p_{{{{\\boldsymbol{Y}}}_{i}}}\\left({\\boldsymbol{y}}_{i}\\right)\\equiv 1 does not affect the decisions in Algorithm 1 due to the arg⁡min\\arg\\min arguments, which are invariant to scaling the conditional distribution by a term that does not depend on 𝒔¯¯𝒔\\bar{{\\boldsymbol{s}}}.\nThe resulting structure is illustrated in Fig.  7(a).\n\nLearned likelihood using conditional density networks: An additional strategy is to directly estimate the conditional p𝒀i|𝑺¯i​(𝒚i|𝒔¯)subscript𝑝conditionalsubscript𝒀𝑖subscript¯𝑺𝑖conditionalsubscript𝒚𝑖¯𝒔p_{{{{\\boldsymbol{Y}}}_{i}|\\bar{{{\\boldsymbol{S}}}}_{i}}}\\left({\\boldsymbol{y}}_{i}|\\bar{{\\boldsymbol{s}}}\\right) from data. This can be achieved using conditional density estimation networks   that are specifically designed to learn such PDFs, or alternatively, using normalizing flow networks to learn complex densities .\nFor example, mixture density networks  model the conditional PDF p𝒀i|𝑺¯i​(𝒚i|𝒔¯)subscript𝑝conditionalsubscript𝒀𝑖subscript¯𝑺𝑖conditionalsubscript𝒚𝑖¯𝒔p_{{{{\\boldsymbol{Y}}}_{i}|\\bar{{{\\boldsymbol{S}}}}_{i}}}\\left({\\boldsymbol{y}}_{i}|\\bar{{\\boldsymbol{s}}}\\right) as a Gaussian mixture, and train a DNN to estimate its mixing parameters, mean values, and covariances, denoted wk,𝜽​(𝒔¯)subscript𝑤𝑘𝜽¯𝒔w_{k,{\\boldsymbol{\\theta}}}(\\bar{{\\boldsymbol{s}}}), 𝝁k,𝜽​(𝒔¯)subscript𝝁𝑘𝜽¯𝒔{\\boldsymbol{\\mu}}_{k,{\\boldsymbol{\\theta}}}(\\bar{{\\boldsymbol{s}}}) and Σk,𝜽​(𝒔¯)subscriptsans-serif-Σ𝑘𝜽¯𝒔{\\mathsf{\\Sigma}}_{k,{\\boldsymbol{\\theta}}}(\\bar{{\\boldsymbol{s}}}), respectively, by maximizing the likelihood P^𝜽​(𝒚i|𝒔¯)=∑kwk,𝜽​(𝒔¯)​𝒩​(𝒚i|𝝁k,𝜽​(𝒔¯),Σk,𝜽​(𝒔¯))subscript^𝑃𝜽conditionalsubscript𝒚𝑖¯𝒔subscript𝑘subscript𝑤𝑘𝜽¯𝒔𝒩conditionalsubscript𝒚𝑖subscript𝝁𝑘𝜽¯𝒔subscriptsans-serif-Σ𝑘𝜽¯𝒔\\hat{P}_{{\\boldsymbol{\\theta}}}\\left({\\boldsymbol{y}}_{i}|\\bar{{\\boldsymbol{s}}}\\right)=\\sum_{k}w_{k,{\\boldsymbol{\\theta}}}(\\bar{{\\boldsymbol{s}}})\\mathcal{N}\\big{(}{\\boldsymbol{y}}_{i}|{\\boldsymbol{\\mu}}_{k,{\\boldsymbol{\\theta}}}(\\bar{{\\boldsymbol{s}}}),{\\mathsf{\\Sigma}}_{k,{\\boldsymbol{\\theta}}}(\\bar{{\\boldsymbol{s}}})\\big{)},\nas illustrated in Fig. 7(b). Both approaches can be utilized for learning to compute the likelihood in ViterbiNet. When the channel outputs are high-dimensional, i.e., N𝑁N is large, directly learning the conditional density is difficult and likely to be inaccurate. In such cases, the classification-based architecture, which avoids the need to explicitly learn the density, may be preferable. When the state cardinality MLsuperscript𝑀𝐿M^{L} is large, conditional density networks are expected to be more reliable. However, the Viterbi algorithm becomes computationally infeasible when MLsuperscript𝑀𝐿M^{L} grows, regardless of whether it is implemented in a model-based or data-driven fashion, making ViterbiNet non-suitable for such setups. Factor graph methods, such as the sum-product algorithm, exploit the factorization of a joint distribution to efficiently compute a desired quantity . In particular, the application of the sum-product algorithm for the joint input-output distribution of finite-memory channels allows for computing the MAP rule, an operation whose burden typically grows exponentially with the block size, with complexity that only grows linearly with T𝑇T. This instance of the sum-product algorithm is exactly the BCJR detector proposed . In the following we show how the BCJR method can be extended into the DNN-aided BCJRNet. As in our description of ViterbiNet, we again focus on the case of K=1𝐾1K=1, and begin by presenting the model-based BCJR algorithm. The BCJR algorithm computes the MAP rule in (2) for finite-memory channels with complexity that grows linearly with the block size. To formulate this method, we recall the definition of 𝑺¯isubscript¯𝑺𝑖\\bar{{{\\boldsymbol{S}}}}_{i}, and define the function Combining (19) and (1), we obtain a factorizable expression of the joint distribution p𝒀T,𝑺T​(⋅)subscript𝑝superscript𝒀𝑇superscript𝑺𝑇⋅p_{{{\\boldsymbol{Y}}^{T},{\\boldsymbol{S}}^{T}}}(\\cdot), given by The factorizable expression of the joint distribution (20) implies that it can be represented as a factor graph with T𝑇T function nodes {f​(𝒚i,𝒔¯i,𝒔¯i−1)}𝑓subscript𝒚𝑖subscript¯𝒔𝑖subscript¯𝒔𝑖1\\{f\\left({\\boldsymbol{y}}_{i},\\bar{{\\boldsymbol{s}}}_{i},\\bar{{\\boldsymbol{s}}}_{i-1}\\right)\\}, in which {𝒔¯i}i=2T−1superscriptsubscriptsubscript¯𝒔𝑖𝑖2𝑇1\\{\\bar{{\\boldsymbol{s}}}_{i}\\}_{i=2}^{T-1} are edges while the remaining variables are half-edges222Here we use Forney style factor graphs , where variables are represented as edges or half-edges. However, it is also possible to represent variables as variables notes..\n Using its factor graph representation, one can compute the joint distribution of 𝑺Tsuperscript𝑺𝑇{\\boldsymbol{S}}^{T} and 𝒀Tsuperscript𝒀𝑇{\\boldsymbol{Y}}^{T} by recursive message passing along this factor graph. In particular, where the forward path messages satisfy for i=1,2,…,k𝑖12…𝑘i=1,2,\\ldots,k. Similarly, the backward messages are for i=T−1,T−2,…,k+1𝑖𝑇1𝑇2…𝑘1i=T-1,T-2,\\ldots,k+1. This message passing is illustrated in Fig. 8. The ability to compute the joint distribution in (21) via message passing results in the MAP detector in (2), an operation whose burden typically grows exponentially with the block size, with complexity that only grows linearly with T𝑇T. This is achieved by noting that the MAP estimate satisfies for each i∈𝒯𝑖𝒯i\\in\\mathcal{T}, where the summands can be computed recursively. When the block size T𝑇T is large, the messages may tend to zero, and are thus commonly scaled [ref]42, e.g., μ←𝑺¯i​(𝒔¯)subscript←𝜇subscript¯𝑺𝑖¯𝒔\\overleftarrow{\\mu}_{\\bar{{{\\boldsymbol{S}}}}_{i}}(\\bar{{\\boldsymbol{s}}}) is replaced with γi​μ←𝑺¯i​(𝒔¯)subscript𝛾𝑖subscript←𝜇subscript¯𝑺𝑖¯𝒔\\gamma_{i}\\overleftarrow{\\mu}_{\\bar{{{\\boldsymbol{S}}}}_{i}}(\\bar{{\\boldsymbol{s}}}) for some scale factor that does not depend on 𝒔¯¯𝒔\\bar{{\\boldsymbol{s}}}, and thus does not affect the MAP rule. The BCJR algorithm is summarized as Algorithm 2. BCJRNet is a receiver method that learns to implement MAP detection from labeled data.\nBCJRNet exploits the fact that in order to implement Algorithm 2, one must be able to specify the factor graph representing the underlying distribution. In particular, the stationarity assumption implies that the complete factor graph is encapsulated in the single function f​(⋅)𝑓⋅f(\\cdot) (19) regardless of the block size T𝑇T. Building upon this insight, BCJRNet utilizes DNNs to learn the mapping carried out at the function node separately from the inference task. The resulting learned stationary factor graph is then used to recover {𝑺i}subscript𝑺𝑖\\{{{\\boldsymbol{S}}}_{i}\\} by message passing, as illustrated in Fig. 9.\nAs learning a single function node is expected to be a simpler task compared to learning the overall inference method for recovering 𝑺Tsuperscript𝑺𝑇{\\boldsymbol{S}}^{T} from 𝒀Tsuperscript𝒀𝑇{\\boldsymbol{Y}}^{T}, this approach allows using relatively compact DNNs, which in turn can be learned from a relatively small set of labeled data. Furthermore, the learned function node describes the factor graph for different values of T𝑇T.\nWhen the learned function node is an accurate estimate of the true one, BCJRNet effectively implements the MAP detection rule (2), and thus approaches the minimal probability of error. The function node that encapsulates the factor graph of stationary finite-memory channels is given in (19). The formulation in (19) implies that it can be estimated by training an ML-based system to evaluate p𝒀i|𝑺¯i​(⋅)subscript𝑝conditionalsubscript𝒀𝑖subscript¯𝑺𝑖⋅p_{{{{\\boldsymbol{Y}}}_{i}|\\bar{{{\\boldsymbol{S}}}}_{i}}}(\\cdot) from which the corresponding function node value is obtained via (19).\nOnce the factor graph representing the channel is learned, symbol recovery is carried out using Algorithm 2.\nAs the mapping of Algorithm 2 is invariant to scaling f​(𝒚i,𝒔¯i,𝒔¯i−1)𝑓subscript𝒚𝑖subscript¯𝒔𝑖subscript¯𝒔𝑖1f({\\boldsymbol{y}}_{i},\\bar{{\\boldsymbol{s}}}_{i},\\bar{{\\boldsymbol{s}}}_{i-1}) with some factor that does not depend on the states, it follows that a parametric estimate of the function f​(⋅)𝑓⋅f(\\cdot), denoted f^𝜽​(⋅)subscript^𝑓𝜽⋅\\hat{f}_{{\\boldsymbol{\\theta}}}(\\cdot), can be obtained using the same networks utilized for learning the log-likelihood in ViterbiNet. Specifically, the learned log-likelihood is used in (19) to obtain f^𝜽​(⋅)subscript^𝑓𝜽⋅\\hat{f}_{{\\boldsymbol{\\theta}}}(\\cdot).\nThe resulting receiver, referred to as BCJRNet, thus implements BCJR detection in a data-driven manner. The same DNN architecture can be applied, once trained, to carry out multiple inference algorithms in a hybrid model-based/data-driven manner, including the BCJR scheme (as BCJRNet) as well as the Viterbi algorithm (via ViterbiNet). Since both BCJRNet and ViterbiNet utilize the same learned models, one can decide which inference system to apply, i.e., BCJRNet or ViterbiNet, by considering the differences in the algorithms from which they are derived, i.e., the BCJR algorithm and the Viterbi algorithm, respectively.\nThe main advantages of Algorithm 1 over Algorithm 2, and thus of ViterbiNet over BCJRNet, are its reduced complexity and real-time operation.\nIn particular, both algorithms implement recursive computations, involving |𝒮|Lsuperscript𝒮𝐿|\\mathcal{S}|^{L} evaluations for each sample, and thus the complexity of both algorithms grows linearly with the block size T𝑇T. Nonetheless, the Viterbi scheme computes only a forward recursion and can thus provide its estimations in real time within a given delay from each incoming observation, while the BCJR scheme implements both forward and backward recursions, and can thus infer only once the complete block is observed, while involving twice the computations carried out by the Viterbi detector. The main advantage of Algorithm 2 over Algorithm 1, i.e., of BCJRNet over ViterbiNet, stems from the fact that it implements the MAP rule (2), which minimizes the error probability.\nThe Viterbi algorithm is designed to compute the maximum likelihood sequence detector, i.e., which is not equivalent to the symbol-level MAP rule (2). To see this, we focus on the case where the symbols are equiprobable, as in such scenarios the sequence-wise maximum likelihood rule coincides with the sequence-wise MAP detector. Here, the decision rule implemented by the Viterbi algorithm (25) can be written as For a given realization 𝒀T=𝒚Tsuperscript𝒀𝑇superscript𝒚𝑇{\\boldsymbol{Y}}^{T}={\\boldsymbol{y}}^{T}, the function p𝑺T|𝒀T​(𝒔T|𝒚T)subscript𝑝conditionalsuperscript𝑺𝑇superscript𝒀𝑇conditionalsuperscript𝒔𝑇superscript𝒚𝑇p_{{{\\boldsymbol{S}}^{T}|{\\boldsymbol{Y}}^{T}}}\\left({{\\boldsymbol{s}}^{T}|{\\boldsymbol{y}}^{T}}\\right) maximized by the sequence-wise detector in (26) is a joint distribution measure. The functions {p𝑺i|𝒀T​(𝒔i|𝒚T)}i=1Tsuperscriptsubscriptsubscript𝑝conditionalsubscript𝑺𝑖superscript𝒀𝑇conditionalsubscript𝒔𝑖superscript𝒚𝑇𝑖1𝑇\\{p_{{{\\boldsymbol{S}}_{i}|{\\boldsymbol{Y}}^{T}}}\\left({{\\boldsymbol{s}}_{i}|{\\boldsymbol{y}}^{T}}\\right)\\}_{i=1}^{T}, which are the individually maximized by the symbol-wise MAP rule computed by the BCJR scheme (2), are the marginals of the aforementioned joint distribution. Furthermore, given 𝒀T=𝒚Tsuperscript𝒀𝑇superscript𝒚𝑇{\\boldsymbol{Y}}^{T}={\\boldsymbol{y}}^{T} the elements of 𝑺Tsuperscript𝑺𝑇{\\boldsymbol{S}}^{T} are statistically dependent in finite-memory channels. As a a result, the maxima of the joint distribution p𝑺T|𝒀Tsubscript𝑝conditionalsuperscript𝑺𝑇superscript𝒀𝑇p_{{{\\boldsymbol{S}}^{T}|{\\boldsymbol{Y}}^{T}}} is not necessarily the individual maximas of each of its marginals, i.e., the elements of the vector 𝒔^T​(𝒚T)superscript^𝒔𝑇superscript𝒚𝑇\\hat{{\\boldsymbol{s}}}^{T}\\left({\\boldsymbol{y}}^{T}\\right) in (25) are not necessarily the symbol-wise MAP estimates {𝒔^i​(𝒚T)}i=1Tsuperscriptsubscriptsubscript^𝒔𝑖superscript𝒚𝑇𝑖1𝑇\\{\\hat{{\\boldsymbol{s}}}_{i}\\left({\\boldsymbol{y}}^{T}\\right)\\}_{i=1}^{T} in (2). To conclude, the Viterbi algorithm does not implement the symbol-wise MAP even in the presence of equal priors. This explains the difference in their performance, since, unlike the BCJR scheme, the Viterbi algorithm does not minimize the error probability. DeepSIC proposed in  is a DNN-aided hybrid algorithm that is based on the iterative  soft interference cancellation (SIC) method  for symbol detection in flat MIMO channels. However, unlike its model-based counterpart, and alternative deep MIMO receivers such as DetNet, it is not tailored for linear Gaussian channels of the form (6). The only assumption required is that the channel is memoryless, i.e., L=1𝐿1L=1, and thus we drop the time index subscript in this example.\nAs in our previous DNN-aided examples, we first review iterative SIC, after which we present its DNN-aided implementation. The iterative SIC algorithm proposed in  is a MIMO detection method that combines multi-stage interference cancellation with soft decisions.\nThe detector operates in an iterative fashion where, in each iteration, an estimate of the conditional PMF of Sksubscript𝑆𝑘S_{k}, which is the k𝑘kth entry of 𝑺𝑺{{\\boldsymbol{S}}}, given the observed 𝒀=𝒚𝒀𝒚{{\\boldsymbol{Y}}}={\\boldsymbol{y}}, is generated for every symbol k∈{1,2,…,K}:=𝒦𝑘12…𝐾assign𝒦k\\in\\{1,2,\\ldots,K\\}:=\\mathcal{K} using the corresponding estimates of the interfering symbols {Sl}l≠ksubscriptsubscript𝑆𝑙𝑙𝑘\\{S_{l}\\}_{l\\neq k} obtained in the previous iteration. Iteratively repeating this procedure refines the conditional distribution estimates, allowing the detector to accurately recover each symbol from the output of the last iteration. This iterative procedure is illustrated in Fig. 10. To formulate the algorithm, we consider the flat Gaussian MIMO channel in (6). Iterative SIC consists of Q𝑄Q iterations. Each iteration indexed q∈{1,2,…,Q}≜𝒬𝑞12…𝑄≜𝒬q\\in\\{1,2,\\ldots,Q\\}\\triangleq\\mathcal{Q} generates K𝐾K distribution vectors 𝒑^k(q)superscriptsubscript^𝒑𝑘𝑞\\hat{{\\boldsymbol{p}}}_{k}^{(q)} of size M×1𝑀1M\\times 1, where k∈𝒦𝑘𝒦k\\in\\mathcal{K}. These vectors are computed from the channel output 𝒚𝒚{\\boldsymbol{y}} as well as the distribution vectors obtained at the previous iteration, {𝒑^k(q−1)}k=1Ksuperscriptsubscriptsuperscriptsubscript^𝒑𝑘𝑞1𝑘1𝐾\\{\\hat{{\\boldsymbol{p}}}_{k}^{(q-1)}\\}_{k=1}^{K}. The entries of 𝒑^k(q)superscriptsubscript^𝒑𝑘𝑞\\hat{{\\boldsymbol{p}}}_{k}^{(q)} are estimates of the distribution of Sksubscript𝑆𝑘S_{k} for each possible symbol in 𝒮𝒮\\mathcal{S}, given the channel output 𝒀=𝒚𝒀𝒚{{\\boldsymbol{Y}}}={\\boldsymbol{y}} and assuming that the interfering symbols {Sl}l≠ksubscriptsubscript𝑆𝑙𝑙𝑘\\{S_{l}\\}_{l\\neq k} are distributed via {𝒑^l(q−1)}l≠ksubscriptsuperscriptsubscript^𝒑𝑙𝑞1𝑙𝑘\\{\\hat{{\\boldsymbol{p}}}_{l}^{(q-1)}\\}_{l\\neq k}. Every iteration consists of two steps, carried out in parallel for each user: Interference cancellation and soft decoding.\nFocusing on the k𝑘kth user and the q𝑞qth iteration, the interference cancellation stage first computes the expected values and variances of {Sl}l≠ksubscriptsubscript𝑆𝑙𝑙𝑘\\{S_{l}\\}_{l\\neq k} based on {𝒑^l(q−1)}l≠ksubscriptsuperscriptsubscript^𝒑𝑙𝑞1𝑙𝑘\\{\\hat{{\\boldsymbol{p}}}_{l}^{(q-1)}\\}_{l\\neq k}. Letting {αj}j=1Msuperscriptsubscriptsubscript𝛼𝑗𝑗1𝑀\\{\\alpha_{j}\\}_{j=1}^{M} be the indexed elements of the constellation set 𝒮𝒮\\mathcal{S}, the expected values and variances are computed via\nel(q−1)=∑αj∈𝒮αj​(𝒑^l(q−1))jsuperscriptsubscript𝑒𝑙𝑞1subscriptsubscript𝛼𝑗𝒮subscript𝛼𝑗subscriptsuperscriptsubscript^𝒑𝑙𝑞1𝑗e_{l}^{(q-1)}=\\sum_{\\alpha_{j}\\in\\mathcal{S}}\\alpha_{j}\\big{(}\\hat{{\\boldsymbol{p}}}_{l}^{(q-1)}\\big{)}_{j},\nand\nvl(q−1)=∑αj∈𝒮(αj−el(q−1))2​(𝒑^l(q−1))jsuperscriptsubscript𝑣𝑙𝑞1subscriptsubscript𝛼𝑗𝒮superscriptsubscript𝛼𝑗superscriptsubscript𝑒𝑙𝑞12subscriptsuperscriptsubscript^𝒑𝑙𝑞1𝑗v_{l}^{(q-1)}=\\sum_{\\alpha_{j}\\in\\mathcal{S}}\\big{(}\\alpha_{j}-e_{l}^{(q-1)}\\big{)}^{2}\\big{(}\\hat{{\\boldsymbol{p}}}_{l}^{(q-1)}\\big{)}_{j},\nrespectively.\nThe contribution of the interfering symbols from 𝒚𝒚{\\boldsymbol{y}} is then canceled by replacing them with {el(q−1)}superscriptsubscript𝑒𝑙𝑞1\\{e_{l}^{(q-1)}\\} and subtracting their resulting term. Letting 𝒉lsubscript𝒉𝑙{\\boldsymbol{h}}_{l} be the l𝑙lth column of 𝖧𝖧{\\mathsf{H}}, the interference canceled channel output is given by Substituting the channel output 𝒚𝒚{\\boldsymbol{y}} into (27), the realization of the interference canceled 𝒁k(q)superscriptsubscript𝒁𝑘𝑞{\\boldsymbol{Z}}_{k}^{(q)}, denoted 𝒛k(q)superscriptsubscript𝒛𝑘𝑞{\\boldsymbol{z}}_{k}^{(q)}, is obtained. To implement soft decoding, it is assumed that W~k(q)≜∑l≠k𝒉l​(Sl−el(q−1))+𝑾≜superscriptsubscript~𝑊𝑘𝑞subscript𝑙𝑘subscript𝒉𝑙subscript𝑆𝑙superscriptsubscript𝑒𝑙𝑞1𝑾\\tilde{W}_{k}^{(q)}\\triangleq\\sum_{l\\neq k}{\\boldsymbol{h}}_{l}(S_{l}\\!-\\!e_{l}^{(q\\!-\\!1)})+{\\boldsymbol{W}} obeys a zero-mean Gaussian distribution, independent of Sksubscript𝑆𝑘S_{k}, and that its covariance is given by\nΣW~k(q)=σw2​𝖨K+∑l≠kvl(q−1)​𝒉l​𝒉lTsubscriptsans-serif-Σsuperscriptsubscript~𝑊𝑘𝑞superscriptsubscript𝜎𝑤2subscript𝖨𝐾subscript𝑙𝑘superscriptsubscript𝑣𝑙𝑞1subscript𝒉𝑙superscriptsubscript𝒉𝑙𝑇{\\mathsf{\\Sigma}}_{\\tilde{W}_{k}^{(q)}}=\\sigma_{w}^{2}{{\\mathsf{I}}}_{K}+\\sum_{l\\neq k}v_{l}^{(q-1)}{\\boldsymbol{h}}_{l}{\\boldsymbol{h}}_{l}^{T}, where σw2superscriptsubscript𝜎𝑤2\\sigma_{w}^{2} is the noise variance.\nCombining this assumption with (27), the conditional distribution of 𝒁k(q)superscriptsubscript𝒁𝑘𝑞{\\boldsymbol{Z}}_{k}^{(q)} given Sk=αjsubscript𝑆𝑘subscript𝛼𝑗S_{k}=\\alpha_{j} is multivariate Gaussian with mean value 𝒉k​αjsubscript𝒉𝑘subscript𝛼𝑗{\\boldsymbol{h}}_{k}\\alpha_{j} and covariance ΣW~k(q)subscriptsans-serif-Σsuperscriptsubscript~𝑊𝑘𝑞{\\mathsf{\\Sigma}}_{\\tilde{W}_{k}^{(q)}}. Since 𝒁k(q)superscriptsubscript𝒁𝑘𝑞{\\boldsymbol{Z}}_{k}^{(q)} is given by a bijective transformation of 𝒀𝒀{\\boldsymbol{Y}}, it holds that pSk|𝒀​(αj|𝒚)=pSk|𝒁k(q)​(αj|𝒛k(q))subscript𝑝conditionalsubscript𝑆𝑘𝒀conditionalsubscript𝛼𝑗𝒚subscript𝑝conditionalsubscript𝑆𝑘superscriptsubscript𝒁𝑘𝑞conditionalsubscript𝛼𝑗superscriptsubscript𝒛𝑘𝑞p_{{S_{k}|{{\\boldsymbol{Y}}}}}(\\alpha_{j}|{\\boldsymbol{y}})=p_{{S_{k}|{\\boldsymbol{Z}}_{k}^{(q)}}}(\\alpha_{j}|{\\boldsymbol{z}}_{k}^{(q)}) for each αj∈𝒮subscript𝛼𝑗𝒮\\alpha_{j}\\in\\mathcal{S} under the above assumptions. Consequently, the conditional distribution of Sksubscript𝑆𝑘S_{k} given 𝒀𝒀{{\\boldsymbol{Y}}} is approximated from the conditional distribution of 𝒁k(q)superscriptsubscript𝒁𝑘𝑞{\\boldsymbol{Z}}_{k}^{(q)} given Sksubscript𝑆𝑘S_{k} via Bayes theorem. Since the symbols are equiprobable, this estimated conditional distribution is computed as After the final iteration, the symbols are decoded by taking the symbol that maximizes the estimated conditional distribution for each user, i.e., The overall joint detection scheme is summarized as Algorithm 3.\nThe initial estimates {𝒑^k(0)}k=1Ksuperscriptsubscriptsuperscriptsubscript^𝒑𝑘0𝑘1𝐾\\{\\hat{{\\boldsymbol{p}}}_{k}^{(0)}\\}_{k=1}^{K} can be arbitrarily set. For example, these may be chosen based on a linear separate estimation of each symbol for 𝒚𝒚{\\boldsymbol{y}}, as proposed in . Iterative SIC has several advantages compared to both joint decoding as well as separate decoding: In terms of computational complexity, it replaces the joint exhaustive search over 𝒮Ksuperscript𝒮𝐾\\mathcal{S}^{K}, required by the MAP decoder, with a set of computations carried out separately for each user. Hence, its computational complexity only grows linearly with the number of users [ref]16, making it feasible with large values of K𝐾K. Unlike conventional separate decoding, in which the symbol of each user is recovered individually while treating the interference as noise, the iterative procedure refines the separate estimates sequentially, and the usage of soft values mitigates the effect of error propagation. Algorithm 3 is thus capable of approaching the performance of the MAP detector, which is only feasible for small values of K𝐾K. Iterative SIC is specifically designed for linear channels of the form (6). In particular, the interference cancellation in Step 3 of Algorithm 3 requires the contribution of the interfering symbols to be additive. Furthermore, it requires accurate CSI. To circumvent these limitations in the model-based approach, the DNN-aided DeepSIC learns to implement the iterative SIC from data. Architecture:\nDeepSIC builds upon the observation that iterative SIC can be viewed as a set of interconnected basic building blocks, each implementing the two stages of interference cancellation and soft decoding, i.e., Steps 3-3 of Algorithm 3. While the high level architecture in Fig. 10 is ignorant of the underlying channel model, the basic building blocks are channel-model-dependent. In particular, interference cancellation requires the contribution of the interference to be additive, i.e., a linear model channel as in (6), as well as full CSI, in order to cancel the contribution of the interference. Soft decoding requires complete knowledge of the channel input-output relationship in order to estimate the conditional probabilities via (28). Although each of these basic building blocks consists of two sequential procedures that are completely channel-model-based, we note that the purpose of these computations is to carry out a classification task. In particular, the k𝑘kth building block of the q𝑞qth iteration, k∈𝒦𝑘𝒦k\\in\\mathcal{K}, q∈𝒬𝑞𝒬q\\in\\mathcal{Q}, produces 𝒑^k(q)superscriptsubscript^𝒑𝑘𝑞\\hat{{\\boldsymbol{p}}}_{k}^{(q)}, which is an estimate of the conditional PMF of Sksubscript𝑆𝑘S_{k} given 𝒀=𝒚𝒀𝒚{{\\boldsymbol{Y}}}={\\boldsymbol{y}} based on {𝒑^l(q−1)}l≠ksubscriptsuperscriptsubscript^𝒑𝑙𝑞1𝑙𝑘\\{\\hat{{\\boldsymbol{p}}}_{l}^{(q-1)}\\}_{l\\neq k}. Such computations are naturally implemented by classification DNNs, e.g., fully-connected networks with a softmax output layer. Embedding these ML-based conditional PMF computations into the iterative SIC block diagram in Fig. 10 yields the overall receiver architecture depicted in Fig. 11. The initial estimates {𝒑^k(0)}k=1Ksuperscriptsubscriptsuperscriptsubscript^𝒑𝑘0𝑘1𝐾\\{\\hat{{\\boldsymbol{p}}}_{k}^{(0)}\\}_{k=1}^{K} can be set to represent a uniform distribution, i.e., (𝒑^k(0))j≡1Msubscriptsuperscriptsubscript^𝒑𝑘0𝑗1𝑀\\big{(}\\hat{{\\boldsymbol{p}}}_{k}^{(0)}\\big{)}_{j}\\equiv\\frac{1}{M}. A major advantage of using classification DNNs as the basic building blocks in Fig. 11 stems from the fact that such ML-based methods are capable of accurately computing conditional distributions in complex non-linear setups without requiring a-priori knowledge of the channel model and its parameters. Consequently, when these building blocks are trained to properly implement their classification task, the receiver essentially realizes iterative SIC for arbitrary channel models in a data-driven fashion. Training Methods:\nIn order for the DNN-aided receiver structure of Fig. 11 to reliably implement joint decoding, its building block classification DNNs must be properly trained. Here, we consider two possible approaches to train the receiver based on a set of ntsubscript𝑛𝑡n_{t} pairs of channel inputs and their corresponding outputs, denoted {𝒔t,𝒚t}t=1ntsuperscriptsubscriptsubscript𝒔𝑡subscript𝒚𝑡𝑡1subscript𝑛𝑡\\{{\\boldsymbol{s}}_{t},{\\boldsymbol{y}}_{t}\\}_{t=1}^{n_{t}}: End-to-end training, and sequential training. End-to-end training: The first approach jointly trains the entire network, i.e., all the building block DNNs. Since the output of the deep network is the set of conditional distributions {𝒑^k(Q)}k=1Ksuperscriptsubscriptsuperscriptsubscript^𝒑𝑘𝑄𝑘1𝐾\\{\\hat{{\\boldsymbol{p}}}_{k}^{(Q)}\\}_{k=1}^{K}, where each 𝒑^k(Q)superscriptsubscript^𝒑𝑘𝑄\\hat{{\\boldsymbol{p}}}_{k}^{(Q)} is used to estimate Sksubscript𝑆𝑘S_{k}, we use the sum cross entropy as the training objective. Let\n𝜽𝜽{\\boldsymbol{\\theta}} be the network parameters, and\n𝒑^k(Q)​(𝒚,α;𝜽)superscriptsubscript^𝒑𝑘𝑄𝒚𝛼𝜽\\hat{{\\boldsymbol{p}}}_{k}^{(Q)}({\\boldsymbol{y}},\\alpha;{\\boldsymbol{\\theta}}) be the entry of 𝒑^k(Q)superscriptsubscript^𝒑𝑘𝑄\\hat{{\\boldsymbol{p}}}_{k}^{(Q)} corresponding to Sk=αsubscript𝑆𝑘𝛼S_{k}=\\alpha when the input to the network parameterizd by 𝜽𝜽{\\boldsymbol{\\theta}} is 𝒚𝒚{\\boldsymbol{y}}.\nThe sum cross entropy loss is Training the receiver in Fig. 11 in an end-to-end manner based on the loss (30) jointly updates the coefficients of all the K⋅Q⋅𝐾𝑄K\\cdot Q building block DNNs. For a large number of users, training so many parameters simultaneously is expected to require a large labeled set. Sequential training: To allow the network to be trained with a reduced number of training samples, we note that the goal of each building block DNN does not depend on the iteration index: The k𝑘kth building block of the q𝑞qth iteration outputs a soft estimate of Sksubscript𝑆𝑘S_{k} for each q∈𝒬𝑞𝒬q\\in\\mathcal{Q}. Therefore, each building block DNN can be trained individually, by minimizing the conventional cross entropy loss. To formulate this objective, let\n𝜽k(q)superscriptsubscript𝜽𝑘𝑞{\\boldsymbol{\\theta}}_{k}^{(q)} represent the parameters of the k𝑘kth DNN at iteration q𝑞q, and write\n𝒑^k(q)​(𝒚,{𝒑^l(q−1)}l≠k,α;𝜽k(q))superscriptsubscript^𝒑𝑘𝑞𝒚subscriptsuperscriptsubscript^𝒑𝑙𝑞1𝑙𝑘𝛼superscriptsubscript𝜽𝑘𝑞\\hat{{\\boldsymbol{p}}}_{k}^{(q)}\\big{(}{\\boldsymbol{y}},\\{\\hat{{\\boldsymbol{p}}}_{l}^{(q-1)}\\}_{l\\neq k},\\alpha;{\\boldsymbol{\\theta}}_{k}^{(q)}\\big{)} as the entry of 𝒑^k(q)superscriptsubscript^𝒑𝑘𝑞\\hat{{\\boldsymbol{p}}}_{k}^{(q)} corresponding to Sk=αsubscript𝑆𝑘𝛼S_{k}=\\alpha when the DNN parameters are 𝜽k(q)superscriptsubscript𝜽𝑘𝑞{\\boldsymbol{\\theta}}_{k}^{(q)} and its inputs are 𝒚𝒚{\\boldsymbol{y}} and {𝒑^l(q−1)}l≠ksubscriptsuperscriptsubscript^𝒑𝑙𝑞1𝑙𝑘\\{\\hat{{\\boldsymbol{p}}}_{l}^{(q-1)}\\}_{l\\neq k}. The cross entropy loss is given by where {𝒑^t,l(q−1)}superscriptsubscript^𝒑𝑡𝑙𝑞1\\{\\hat{{\\boldsymbol{p}}}_{t,l}^{(q-1)}\\} represent the estimated probabilities associated with 𝒚isubscript𝒚𝑖{\\boldsymbol{y}}_{i} computed at the previous iteration.\nThe problem with training each DNN individually is that the soft estimates {𝒑^t,l(q−1)}superscriptsubscript^𝒑𝑡𝑙𝑞1\\{\\hat{{\\boldsymbol{p}}}_{t,l}^{(q-1)}\\} are not provided as part of the training set. This challenge can be tackled by training the DNNs corresponding to each layer in a sequential manner, where for each layer the outputs of the trained previous iterations are used as the soft estimates fed as training samples. Sequential training uses the ntsubscript𝑛𝑡n_{t} input-output pairs to train each DNN individually. Compared to the end-to-end training that utilizes the training samples to learn the complete set of parameters, which can be quite large, sequential training uses the same data set to learn a significantly smaller number of parameters, reduced by a factor of K⋅Q⋅𝐾𝑄K\\cdot Q, multiple times. Consequently, this approach is expected to require much fewer training samples, at the cost of a longer learning procedure for a given training set, due to its sequential operation, and possible performance degradation as the building blocks are not jointly trained. DNN-aided algorithms implement hybrid model-based/data-driven inference by integrating ML into established model-based methods. As such, it is particularly suitable for digital communications setups, in which a multitude of reliable model-based algorithms exist, each tailored to a specific structure. The implementation of these techniques in a data-driven fashion thus has three main advantages as a model-based ML strategy: First, when properly trained, the resulting system effectively implements the model-based algorithm from which it originated, thus benefiting from its proven performance and controllable complexity, while being robust to CSI uncertainty and operable in complex environments, due to the usage of DNNs. This behavior is numerically illustrated in the simulation study detailed in Section 3.5. Second, the fact that DNN-aided algorithms use ML tools as intermediate components in the overall end-to-end inference tasks allows the use of relatively compact networks which can be trained with small training sets. Even when the overall system consists of a large set of DNNs, as is the case in DeepSIC, their interpretable operation which follows from the model-based method facilitates their training with small data sets, e.g., via sequential training techniques. Finally, DNN-aided algorithms can utilize different levels of domain knowledge, depending on what prior information one has on the problem at hand. For example, BCJRNet requires only prior knowledge that the channel has finite memory to learn to carry out MAP detection from data. When additional domain knowledge is available, such as an underlying stationarity or some partial CSI, it can be incorporated into the number and structure of the learned function nodes, further reducing the number of training data required to tune the receiver. The resulting ability of DNN-aided symbol detectors to adapt with small training sets can be exploited to facilitate channel tracking via periodic re-training using existing pilots and other forms of structures present in digital communications protocols, as demonstrated in [ref]11 .\n\nReference markers found: [11]\n\nContext windows for each [ref]:\n[ref]11: implements the BCJR algorithm of  in a data-driven fashion; and DeepSIC, which is based on the soft iterative interference cancellation methods for MIMO symbol detection . ViterbiNet proposed in [ref]11 is a data-driven implementation of the Viterbi detection algorithm , which is one of the most common workhorses in digital communications. This DNN-aided symbol detection algorithm is\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 11,\n            \"titles\": [\n                \"Data-Driven Factor Graphs for Deep Symbol Detection\",\n                \"Deep Learning\",\n                \"Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups\",\n                \"Learning deep architectures for AI\",\n                \"Learning fast approximations of sparse coding\",\n                \"Radar and communication coexistence: An overview: A review of recent methods\",\n                \"Effect of non-linear power amplifiers on future wireless communication networks\",\n                \"Model-based deep learning\",\n                \"Deep unfolding: Model-based inspiration of novel deep architectures\",\n                \"Algorithm Unrolling: Interpretable, Efficient Deep Learning for Signal and Image Processing\",\n                \"Deep Unfolding for Communications Systems: A Survey and Some New Directions\",\n                \"ViterbiNet: A Deep Learning Based Viterbi Algorithm for Symbol Detection\",\n                \"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm\",\n                \"Optimal decoding of linear codes for minimizing symbol error rate\",\n                \"Interference cancellation for cellular systems: a contemporary overview\",\n                \"Deep Learning for Image Denoising: A Survey\",\n                \"Deep Learning for Image Super-resolution: A Survey\",\n                \"Image Segmentation Using Deep Learning: A Survey\",\n                \"Towards end-to-end speech recognition with recurrent neural networks\",\n                \"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\",\n                \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n                \"Learning phrase representations using RNN encoder-decoder for statistical machine translation\",\n                \"A survey of deep learning techniques for neural machine translation\",\n                \"Protein Secondary Structure Prediction Using Cascaded Convolutional and Recurrent Neural Networks\",\n                \"A survey of data mining and deep learning in bioinformatics\",\n                \"Neural Network Detection of Data Sequences in Communication Systems\",\n                \"Bidirectional recurrent neural networks\",\n                \"End-to-end optimized transmission over dispersive intensity-modulated channels using bidirectional recurrent neural networks\",\n                \"Experimental Investigation of Deep Learning for Digital Signal Processing in Short Reach Optical Fiber Communications\",\n                \"Learning to Detect\",\n                \"A Model-Driven Deep Learning Network for MIMO Detection\",\n                \"Model-driven deep learning for joint MIMO channel estimation and signal detection\",\n                \"Iterative soft interference cancellation for multiple antenna systems.\",\n                \"Finite mixture models\",\n                \"Mixture density networks\",\n                \"Conditional density estimation with neural networks: Best practices and benchmarks\",\n                \"Normalizing flows: Introduction and ideas\",\n                \"Factor graphs and the sum-product algorithm\",\n                \"Codes on graphs: Normal realizations\",\n                \"An introduction to factor graphs\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]11 based on the context and research area of Chapter 1 Model-Based Machine Learning for Communications. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}