{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Layer-wise Feedback Propagation\n\nPaper Abstract: \n\nPaper Section Text:\nThe LFP-0-rule described in Section 2.1 captures the fundamental idea behind LFP, however, it is difficult to use in practice (comparable to the LRP-0-rule ), due to several undesirable properties: For instance, it becomes numerically unstable for denominators close to zero, and tends to cause backpropagated feedback to explode (although this is not exclusive to LFP, i.e., gradients can also explode) in deeper models if |zi​j|≫|zj|much-greater-thansubscript𝑧𝑖𝑗subscript𝑧𝑗|z_{ij}|\\gg|z_{j}| (cf. Equation 3), which easily occurs when a neuron has both positive and negative contributions zi​jsubscript𝑧𝑖𝑗z_{ij}. To resolve these problems, we extend LFP by three rules in the following, similar to what is done to render LRP numerically stable or adapt it to specific types of layers  [ref]31. For a detailed description of each rule and its potential benefits, please refer to Appendix A.3: LFP-ε𝜀\\boldsymbol{\\varepsilon}\nFor this rule, a normalizing constant ε𝜀\\varepsilon is introduced for numerical stability, similar to LRP-ε𝜀\\varepsilon : LFP-α​β𝛼𝛽\\boldsymbol{\\alpha\\beta}\nTreating positive and negative contributions separately can alleviate the issue of exploding feedback. With zj+=∑imax​(zi​j,0)superscriptsubscript𝑧𝑗subscript𝑖maxsubscript𝑧𝑖𝑗0z_{j}^{+}=\\sum_{i}\\text{max}(z_{ij},0) and zj−=∑imin​(zi​j,0)superscriptsubscript𝑧𝑗subscript𝑖minsubscript𝑧𝑖𝑗0z_{j}^{-}=\\sum_{i}\\text{min}(z_{ij},0), we can formulate an LFP-α​β𝛼𝛽\\alpha\\beta-rule as subject to α−β=1𝛼𝛽1\\alpha-\\beta=1 to ensure the sum of feedback being conserved across layers in estimation. We will investigate the behavior of LFP-α​β𝛼𝛽\\alpha\\beta in more detail in the experimental part of this section.\nIn practice we use α=2𝛼2\\alpha=2 and β=1𝛽1\\beta=1 (called LFP-α​2​β​1𝛼2𝛽1\\alpha 2\\beta 1 in the following), as this is a recommendation prevalent in LRP-literature , with the caveat that the values that work well for LRP may not carry over to LFP in the same manner. However, in-depth exploration of this parameter choice is beyond the scope of this paper. LFP-z+​z−superscript𝑧superscript𝑧\\boldsymbol{z^{+}z^{-}} In order to avoid exploding feedback and to adapt the general idea of the α​β𝛼𝛽\\alpha\\beta-rule to updating parameters instead of explaining models, we further propose the LFP-z+​z−superscript𝑧superscript𝑧z^{+}z^{-}-rule, which also treats positive and negative contributions separately, but relaxes the α−β=1𝛼𝛽1\\alpha-\\beta=1 constraint: where the above formula is derived from Equation (A.13). In practice, the stabilizing constant ε𝜀\\varepsilon from the previous paragraph is also applied to LFP-α​β𝛼𝛽\\alpha\\beta and LFP-z+​z−superscript𝑧superscript𝑧z^{+}z^{-} for numerical stability, but left out from the equations for readability. All of the above rules can be applied in a layer-wise fashion and combined into composites [ref]35, meaning that different rules are applied to different layers or types of layers. A description of the composites utilized in the following experiments can be found in Appendix A.6.2. Effects of LFP-Rules After establishing that LFP is able to achieve comparable performances to gradient backpropagation in Section 2.3, we will now examine the effects of the different LFP-Rules introduced above, as well as gradient descent as a baseline. In a slighly more complex setting than for the previous experiments, we compare the different methods on a VGG-16  model learning to classify a 20-class subset of the ImageNet  dataset. Both random initialization and pretraining on the original (1000-class) ImageNet dataset are utilized, with the latter aiming at evaluation of each method’s ability to retain and utilize existing information. More details on the training setup can be found in Appendix A.6.2. Figure 3 shows the results of that experiment. Considering the randomly initialized setting first (III), we find that LFP-ε𝜀\\varepsilon is on par with gradient backpropagation in terms of performance, confirming the results of the previous section. However, all other composites perform significantly worse, with the mixed ones that employ LFP-ε𝜀\\varepsilon for dense layers leading to better accuracies than LFP-z+​z−superscript𝑧superscript𝑧z^{+}z^{-}, and LFP-α​2​β​1𝛼2𝛽1\\alpha 2\\beta 1 only achieving random performance. In the pretrained setting (I), only LFP-α​2​β​1𝛼2𝛽1\\alpha 2\\beta 1 performs significantly lower than the other composites. The reason for this may be that while LFP-α​2​β​1𝛼2𝛽1\\alpha 2\\beta 1 imposes a bound on the exploding feedback, it also introduces an average increase (assuming equal distribution of positively and negatively contributing connections) in feedback magnitude of 1.5 with each layer. For deep models, this may impact training more than for any other rules. Figure A.6 confirms through the MSE-difference between parameters at consecutive epochs that LFP-α​2​β​1𝛼2𝛽1\\alpha 2\\beta 1 on average indeed leads to the largest updates by magnitude, supporting above interpretation. These results showcase that while LFP utilizes the basic formulation of LRP to measure parameter and neuron contribution, when it comes to specific rules and parameter choices, some adaptations need to be made, due to the diverging goals of explaining vs. learning between the two methods. Comparing the number of changing parameter signs between methods, we observe that gradient backpropagation seems to change them most significantly, followed by LFP-ε𝜀\\varepsilon and the combined composites using LFP-ε𝜀\\varepsilon. LFP-α​2​β​1𝛼2𝛽1\\alpha 2\\beta 1 changes parameters the least. In fact, for the randomly initialized setting (IV), changing parameter signs more significantly seems to lead to better performance overall, which is consistent with the fact that the random model is not adapted to the classification task at all yet. However, in the finetuning setting (II), where the internal representations are already well-adapted to the task from initialization, composites that make less changes to parameter signs (excluding LFP-α​2​β​1𝛼2𝛽1\\alpha 2\\beta 1 which behaves unstably assumedly due to exploding feedback) perform just as well as the gradient baseline. This observation is most significant for LFP-z+​z−superscript𝑧superscript𝑧z^{+}z^{-}. As the sign of a parameter can be interpreted as the qualitative meaning of the corresponding connection (i.e., inhibitory or excitatory), not changing parameter signs during learning is interesting, as it implies a task-solving subnet being extracted and only finetuned in terms of quantitative contributions, not qualitative meaning, i.e., more existing knowledge is utilized, similar to what the lottery ticket hypothesis [ref]8  proposes. We will further investigate the LFP-z+​z−superscript𝑧superscript𝑧z^{+}z^{-} rule in that context in Section 3.2.\n\nReference markers found: [31]\n\nContext windows for each [ref]:\n[ref]31: zi​jsubscript𝑧𝑖𝑗z_{ij}. To resolve these problems, we extend LFP by three rules in the following, similar to what is done to render LRP numerically stable or adapt it to specific types of layers  [ref]31. For a detailed description of each rule and its potential benefits, please refer to Appendix A.3: LFP-ε𝜀\\boldsymbol{\\varepsilon}\nFor this rule, a normalizing constant ε𝜀\\varepsilon is\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 31,\n            \"titles\": [\n                \"signSGD: Compressed Optimisation for Non-Convex Problems\",\n                \"Learning internal representations by error propagation\",\n                \"Building high-level features using large scale unsupervised learning\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Transformer in Transformer\",\n                \"The shattered gradients problem: If resnets are the answer, then what is the question?\",\n                \"Networks of spiking neurons: The third generation of neural network models\",\n                \"Introduction to spiking neural networks: Information processing, learning and applications\",\n                \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\",\n                \"Proving the Lottery Ticket Hypothesis: Pruning is All You Need\",\n                \"Distributed optimization of deeply nested systems\",\n                \"Training Neural Networks Without Gradients: A Scalable ADMM Approach\",\n                \"ADMM for Efficient Deep Learning with Global Convergence\",\n                \"Global Convergence of Block Coordinate Descent in Deep Learning\",\n                \"Accelerated gradient-free neural network training by multi-convex alternating optimization\",\n                \"Evolution strategies as a scalable alternative to reinforcement learning\",\n                \"Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning\",\n                \"RSO: A gradient free sampling based approach for training deep neural networks\",\n                \"Gradient-Free Neural Network Training via Synaptic-Level Reinforcement Learning\",\n                \"The Forward-Forward Algorithm: Some Preliminary Investigations\",\n                \"How to explain individual classification decisions\",\n                \"Deep inside convolutional networks: Visualising image classification models and saliency maps\",\n                \"Striving for Simplicity: The All Convolutional Net\",\n                \"On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation\",\n                \"Axiomatic attribution for deep networks\",\n                \"Pruning by Explaining: A Novel Criterion for Deep Neural Network Pruning\",\n                \"Ecqxx{}^{\\\\mbox{x}}: Explainability-driven quantization for low-bit and sparse dnns\",\n                \"Explain to not forget: Defending against catastrophic forgetting with XAI\",\n                \"Reward is enough\",\n                \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\n                \"Toward Explainable AI for Regression Models\",\n                \"Layer-wise relevance propagation: An overview\",\n                \"Gradient-based learning applied to document recognition\",\n                \"Learning multiple layers of features from tiny images\",\n                \"Towards best practice in explaining neural network decisions with LRP\",\n                \"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\n                \"Imagennet classification with deep convolutional neural networks\",\n                \"Training spiking neural networks using lessons from deep learning\",\n                \"Deep Learning With Spiking Neurons: Opportunities and Challenges\",\n                \"Regression shrinkage and selection via the lasso\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]31 based on the context and research area of Layer-wise Feedback Propagation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}