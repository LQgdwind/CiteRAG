{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Case Study on AI Engineering Practices: Developing an Autonomous Stock Trading System\n\nPaper Abstract: \n\nPaper Section Text:\nFor each practice, we first present how it was applied and then discuss its perceived effectiveness (analysis).\nA summary is presented in Table I. Standardize and automate data quality check procedures:\nWe applied this practice twice.\nFirst, we used it to validate retrieved intraday stock data.\nThe retrieval script waited 12 seconds between requests to avoid API rate limiting.\nData for each stock was split into 24 files, which at first were checked manually.\nWhen this became infeasible, we extended the retrieval script with automatic quality checks.\nThe script checked for missing files and retried the retrieval in this case.\nAdditionally, it checked for files with less than 100 lines, as this indicated erroneous data.\nSuch files were deleted to avoid training with corrupted data.\nBoth types of events were also logged with an error message. Later in the development, we also retrieved aggregated daily stock data.\nTo avoid missing days, we implemented another quality check.\nClosed days like weekends and public holidays needed to be respected, so we compared gaps between the different stocks.\nThis procedure revealed that 17 of the 716 stocks contained real gaps.\nIn total, 137 days were missing.\nSince for many stocks only one or two days were missing, we decided to still use them as training data and only ignored stocks missing more than two consecutive days.\nThe biggest gap we discovered was over a period of 28 days. Analysis:\nSince we received more than 9,000 files, automated procedures were critical to identify data sets that were likely to have a negative effect on ML training.\nThe practice was fairly effective, as it helped us to detect many gaps in the intraday and daily trading data.\nHowever, it would have been very time-consuming to ensure near perfect data quality.\nThis would have required at least a second data source for comparison.\nNonetheless, without this practice, we would have only been able to detect a fraction of the existing data quality issues, and even the most sophisticated ML algorithms perform poorly when trained with bad data .\nApplying this practice definitely improved the development process. Use error validation and categorization:\nAs proposed by Akkiraju et al. , we used a confusion matrix to identify training errors.\nSuch a visualization of correct and incorrect predictions easily shows in which areas a model is not effective.\nFor example, we identified that a decent number of our incorrect predictions were false negatives.\nAlthough this was not ideal, it was also less severe than false positives: false negative predictions would make the system only miss out on good investments, but it would not lose any money.\nAs a result, we categorized these errors as less important than false positives, since these would make the system take unprofitable investments.\nTherefore, we focused on reducing the number of false positives and optimized for precision. Another training error that occurred many times was underfitting, mostly due to using too many features.\nSome initial models were trained with more than 60 features, which led to a poor confusion matrix.\nAdditionally, we detected some minor flaws, e.g., that the model did not consider any market correlation.\nIf the stock price of most car companies is massively decreasing, the system should probably not invest in any car companies during this week.\nSince this was complex to implement, we categorized this issue as too costly to fix. Analysis:\nAkkiraju et al.  proposed that by prioritizing errors based on the severity and business value of fixing them, it is possible to improve the model more efficiently.\nThis was exactly the case in our study.\nBy prioritizing truly costly errors, we avoided wasting time fixing errors that barely impacted system effectiveness.\nTherefore, we were able to develop a more effective system in a more efficient way. Capture the training objective in a metric that is easy to measure and understand:\nWe first defined the training objective for the ML model to predict if a stock increases by at least 1.6% during the next day with at least 90% accuracy.\nHowever, these initial models were ineffective, and we had to redefine our objective.\nThis time, we decided to use the overall system-level objective to invest profitably in stocks.\nWe experimented with multiple corresponding low-level training objectives, e.g., predicting if stock prices increase in the next five days by a certain threshold.\nFurthermore, the model precision required to make the system profitable also varied depending on the approach.\nWe generally defined that the precision had to be high enough for the system to be profitable, even though this is difficult to measure during training and is somewhat in conflict with the proposed practice. The deployed system uses 19 different models.\nFor each model, we defined the training objective as predicting whether a stock price will increase above a specific threshold.\nAdditionally, we simulated how much profit the entire system would make using the predictions of all 19 models and used the results of this simulation to identify whether the general objective of making a profit is achieved. Analysis:\nUsing the practice resulted in helpful reflection about our system objectives and improved communication.\nDuring a meeting with the ML engineer, we could easily explain the system objective and the individual model objectives.\nSimplifying communication during the development would be even more helpful when working in a team with many developers.\nHowever, the training objective may also change during development, and ensuring that all team members are aware of this is important.\nAdditionally, there are cases where it is very difficult to capture the system objective in a metric that is easy to measure.\nWe had to develop a complex simulation to estimate if the objective would be achieved. Use cross-validation:\nWe tested models using holdout cross-validation, which splits the entire data randomly into two mutually exclusive data sets , one for training the model and one for testing it.\nThis ensured more reliable results because the model was tested only on data outside the training set.\nFor illustration, we trained and validated a random forest model for our data once with cross-validation and once without it.\nThe cross-validation variant achieved a precision of 65%, whereas the variant without it achieved over 99%.\nTesting the model on data that it has already seen during training clearly produces results that cannot be trusted.\nFurthermore, we used an efficient implementation of the leave-one-out cross-validation to test a k-nearest-neighbor model.\nThis procedure uses all data points for training except one, which is used for validation .\nThis is then repeated with each data point, which is time-intensive, but allows validating the model on more data.\nWe additionally aimed to identify how much cross-validation affects the test results.\nWe tested three different k-nearest-neighbor models with k=5,k=50formulae-sequence𝑘5𝑘50k=5,k=50, and k=100𝑘100k=100.\nEach model was validated once without cross-validation and once using leave-one-out cross-validation. Analysis:\nIn our study, the effect of using cross-validation varied strongly depending on the model.\nFor the model with k=5𝑘5k=5, not using cross-validation falsely indicated a precision more than 12 percentage points higher, but for the model with k=100𝑘100k=100, it was only 0.29 percentage points.\nNonetheless, we still recommend using it to ensure more generalizable results.\nFurthermore, there are sometimes more efficient techniques to implement cross-validation for specific models (see, e.g., Cheng et al.  and Cawley et al. ).\nIn our case, the more efficient implementation of the leave-one-out validation for the k-nearest-neighbor model was more than 12 times faster.\nFurthermore, cross-validation via random splits always achieves different results.\nIn some of our runs, precision could differ by up to 5 percentage points just by using different training and test data sets.\nAs a consequence, it was critical to train and test the model several times and average the results. Continuously measure model quality, performance, and drift:\nDuring fine-tuning of the ML models, we continuously measured the model prediction quality via precision to identify how the many new versions compared.\nThis was especially important after drastically changing the feature selection.\nMany of the models we used had a strong bias, e.g., due to unbalanced training data.\nOnce we realized the potential effect of new data on prediction quality, we ensured that newly trained models were only incorporated into the system if model quality was better than for the previous version. The final version of the system makes approximately 760 predictions each day before the stock market opens, which took, on average, 39 seconds.\nThis duration was important to know for deciding when the system should start its decision-making process.\nFor example, we could configure the system to start 10 min before the stock market opens.\nHowever, if the ML component is changed or more data is used, this process could take longer, meaning that the system would lack decisions for investing.\nTo avoid this, we continuously measured the prediction duration of the current model. We did not discover any serious model drift during development.\nHowever, we assumed that the state of the stock market would impact model precision.\nTo verify this, we tested one model purely on data from September 2008, since this was the month the stock market crashed.\nThe chosen model predicts if a stock will increase by more than 2.6% during the next day.\nThis model had a precision of 63% when tested on random data, but only scored 59% for September 2008.\nThis indicates that the state of the market can have an impact on prediction quality.\nWhile such crashes are rare, there are other events that also have an impact, e.g., the war in Ukraine, which caused stocks like Rheinmetall to increase and others like the EPAM Systems stock to decrease massively . Analysis:\nIn our case, continuously measuring model quality was essential to select the best versions, measuring prediction duration was critical to ensure system correctness, and measuring model drift was important to ensure that system effectiveness does not unexpectedly decrease.\nIn summary, applying this practice increased observability and improved the development process and resulting system. Review model training scripts:\nAt first, our training script was so short and simple that reviewing it was not necessary.\nHowever, later on, we developed three different training scripts for two ML algorithms, which led to considerably more complexity.\nDuring a review of this code, we identified and implemented an optimization procedure which increased model precision from 59% to 61%.\nHowever, this improvement also introduced two bugs.\nOne of them, an infinite loop bug, was discovered because training took much longer than expected.\nAs a result, we performed another review of the training code, and found and fixed the bug within 5 min.\nAlthough this bug was critical for continuing development, we only conducted the review due to our suspicion, and should have done so much earlier.\nThe second bug, a floating-point error, was only detected because the newly added optimization process did not show any effect.\nPython returns 0.010999​…0.010999…0.010999... as the result for 1.011−11.01111.011-1, which we discovered by debugging the newly added code.\nAfterwards, we reviewed the entire training script again, and discovered several ways to improve understandability, such as renaming variables or extracting methods. The second training script we implemented was a k-nearest-neighbor algorithm, and a first review of it did not reveal any bugs.\nHowever, it again revealed several ways to increase code understandability.\nFurthermore, we conducted a second review with the external ML engineer, who pointed out several ways in which the training script could be further improved, e.g., by using NumPy666https://numpy.org to achieve greater performance and modifying the optimization procedure to achieve a better tuned model.\nWe used his proposal to develop another k-nearest-neighbor training script, which, however, also contained a bug related to element removal based on the list index.\nThis bug was not identified during the conducted review.\nOnly after testing the training script, we finally discovered and removed it. Analysis:\nConducting code reviews is already a well-established practice, as it improves software quality [ref]39.\nOur case exemplifies that this is also valuable for ML training code.\nBy conducting such reviews, we were able to detect and fix bugs and to improve code quality.\nEspecially for complex training scripts, some bugs can be difficult to detect.\nThe review with the ML engineer led to further improvements of the training script.\nWe conclude that conducting such reviews was an important practice for us.\nHowever, one should be aware that one review might not be enough.\nFixing detected bugs can unintentionally lead to new bugs.\nTherefore, we strongly recommend that, after fixing bugs or modifying the training script, another review should be conducted. Test all feature extraction code:\nDuring ML component development, most time was spent on feature engineering code.\nAs this code also changed quickly during exploratory model development, it was difficult to decide when we should write tests for it.\nThis code was also fairly complex, e.g., our first feature extraction script parsed thousands of CSV lines just to create a single data point.\nCreating dummy test data would therefore have been very time-consuming.\nInstead, we conducted manual tests by comparing the extracted features with the corresponding data.\nThe first bug we discovered was caused by two duplicate lines that added new data points, which led to profitable data points having different feature vectors than unprofitable ones and an accuracy of 99.5%.\nOut of suspicion, we decided to test the feature extraction code and added logging, which quickly led to the discovery and fixing of the bug.\nOnce the feature extraction code became more stable, we also added equivalence class testing to ensure a reasonably broad coverage of the training set.\nFor each equivalence class, we compared one of the extracted features with the corresponding stock data, which led to the discovery of another bug for one class. Analysis:\nWe draw several lessons from this experience.\nFirst, testing the feature extraction code is critical to ensure the prediction quality of the system.\nSecond, if we had tested this code more frequently, we would have saved several days of work.\nThird, it should raise suspicions when the model suddenly performs unexpectedly well.\nVerify that this is not caused by any bugs.\nFourth, a bug might only subtly corrupt a fraction of the data.\nTherefore, ensure that the conducted tests cover the majority of the data set, e.g., by using equivalence class testing.\nOverall, this practice was very effective, and we should have used it earlier and more extensively. Automate hyperparameter optimization and model selection:\nAutomating the hyperparameter optimization always requires additional code.\nWhen we only had a small set of values for the hyperparameters, writing this code could take more time than just manually trying out each combination.\nHowever, once the model training took several minutes, it was always more effective to automate this optimization, since it allowed us to simultaneously work on something else.\nFurthermore, there was a trade-off between training duration and model prediction quality, as extensive automated optimization increased both.\nAt some point, we therefore executed the hyperparameter optimization during the night.\nAlthough this required more resources and planning, the precision of the tuned models was always at least 0.5% better (usually more) than with a lightweight optimization taking less than one hour.\nIn most cases, we used a particle swarm optimization algorithm implemented in the tool Optunity777https://optunity.readthedocs.io.\nIn some cases, we also used a conventional grid search algorithm to compare the results. Additionally, we experienced that the hyperparameter optimization duration could be drastically decreased with more efficient model implementations.\nOur final ML component used 19 individual k-nearest-neighbor models, for which the hyperparameter optimization should find the optimal k𝑘k in the range from 5 to 50, i.e., a simple grid search.\nSwitching to a model implementation based on NumPy reduced the optimization process of the combined 19 models from 52 min to only 3 min.\nAlternatively, this increased performance could be used to execute a more extensive optimization process in less or comparable time. Analysis:\nAutomating the hyperparameter optimization definitely had a positive effect on our process, since it improved model precision by more than 4% in some cases.\nEven though manual fine-tuning could be faster for less complex configurations, automating hyperparameter optimization was an essential step to increase our model precision.\nAdditionally, it also allowed parallel development on other tasks.\nStill, extensive optimization requires a lot of time and resources, in our case more than 5 hours, which needs to be planned accordingly.\nMore efficient model implementations can be a way to address this. Log prediction results together with model version and input data:\nSince we used between 20 and 65 features in the beginning, logging predictions was not valuable, as it was difficult to understand why model predictions were incorrect.\nHowever, at some point, we wanted to identify which additional features increase model precision.\nWe therefore enabled logging and analyzed the feature values and precision with charts, which led to a feature set that increased model precision from 62% to 66%.\nHowever, the real value of logging during training became only apparent when we used a set of models instead of only one, which also required adding the model version.\nThe deployed system makes approximately 50,000 predictions every day.\nLogging all of them would create around 12 GB each year.\nA more efficient approach could only log positive predictions that the system actually invests in.\nThis would reduce the log data size to less than 120 MB per year. Analysis:\nDuring the development, logging predictions was an essential step to generate insights into how the model can be further improved.\nTo improve the model after deployment, logging predictions in production can definitely be valuable, too.\nHowever, one should consider the required space, and provision a feasible infrastructure for this.\nMore efficient partial logging can be an alternative. Collaborate with multidisciplinary stakeholders:\nWe conducted three meetings with two external stakeholders.\nThe first meeting was with a professional stockbroker and took 44 min, during which we reviewed the completed system design.\nFurthermore, we clarified domain-specific questions that came up during the requirements analysis and system design.\nDuring the discussion, additional questions emerged, to which the stockbroker provided valuable domain-specific knowledge.\nFor example, one unexpected result was the necessary precision for the ML models.\nWe had expected something around 90%, but the stockbroker explained that, even for the best traders, only 55-60% of investments are profitable.\nThis changed our way of thinking: instead of building a system that rarely makes incorrect predictions, we focused on a precision of at least 60%, but additionally tried to reduce money lost on unprofitable investments and to increase money gained for the inverse case.\nThis changed the system requirements and impacted the system design. The second meeting was held with an ML engineer and took 54 min, with the goal to improve the system design.\nWe received a substantial amount of valuable feedback for different stages of the ML development process, such as model selection, feature selection, hyperparameter optimization, and deployment.\nFor example, he suggested calculating the feature importance for each feature to identify useless features and using Optunity888https://optunity.readthedocs.io to automate the hyperparameter optimization.\nThe last meeting with the same ML engineer took 70 min, during which the finished system prototype was reviewed.\nAgain, several valuable improvements were suggested, such as using NumPy to improve model performance. Analysis:\nApplying this practice highlighted the importance of acquiring missing domain knowledge for ML projects.\nIf developers lack this knowledge, it is essential to collaborate with domain experts.\nFor example, although we were able to develop a model with a precision of 83%, the remaining 17% of unprofitable investments would still result in a net loss for the system.\nBased on the stockbroker feedback, we aborted this misguided strategy and were able to increase system gains.\nAccording to Piorkowski et al. , knowledge in a domain as complex as AI cannot be transferred during a single meeting, as new questions will continuously arise during development.\nWe experienced the same after the meeting with the stock market expert, which highlights the importance of continuous collaboration with domain experts. The technical meetings with the ML engineer improved system performance and effectiveness, i.e., incorporating reviews with an expert more experienced with certain ML aspects was extremely helpful.\nFor example, the engineer pointed out improvements which resulted in more than 20 times faster model performance and 1.5% more model precision.\nOverall, this practice was very effective for our case study.\n\nReference markers found: [39]\n\nContext windows for each [ref]:\n[ref]39: review.\nOnly after testing the training script, we finally discovered and removed it. Analysis:\nConducting code reviews is already a well-established practice, as it improves software quality [ref]39.\nOur case exemplifies that this is also valuable for ML training code.\nBy conducting such reviews, we were able to detect and fix bugs and to improve code quality.\nEspecially for complex\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 39,\n            \"titles\": [\n                \"How AI Developers Overcome Communication Challenges in a Multidisciplinary Team: A Case Study\",\n                \"Steps toward robust artificial intelligence\",\n                \"Software Engineering for AI-Based Systems: A Survey\",\n                \"A taxonomy of software engineering challenges for machine learning systems: An empirical investigation\",\n                \"Engineering safety in machine learning\",\n                \"Software and system reliability engineering for autonomous systems incorporating machine learning\",\n                \"Characterizing Technical Debt and Antipatterns in AI-Based Systems: A Systematic Mapping Study\",\n                \"Adapting Software Architectures to Machine Learning Challenges\",\n                \"Evolving critical systems\",\n                \"Software engineering practices in the us and japan.\",\n                \"A survey of software engineering practices in turkey\",\n                \"Guidelines for quality assurance of machine learning-based artificial intelligence.\",\n                \"Software engineering for machine learning: A case study\",\n                \"Adoption and effects of software engineering best practices in machine learning\",\n                \"Structured techniques: the basis for CASE (revised ed.)\",\n                \"A view of 20th and 21st century software engineering\",\n                \"What is really different in engineering ai-enabled systems?\",\n                \"Engineering AI Systems: A Research Agenda\",\n                \"Office of the director of national intelligence sponsors sei to lead national ai engineering initiative\",\n                \"Ai engineering\",\n                \"Autonomous Systems – An Architectural Characterization\",\n                \"Current trends in the development of intelligent unmanned autonomous systems\",\n                \"A machine learning model for stock market prediction\",\n                \"An empirical study of machine learning algorithms for stock daily trading strategy\",\n                \"Improving stock trading decisions based on pattern recognition using machine learning technology\",\n                \"Characterizing machine learning processes: A maturity framework\",\n                \"A Multiple Case Study of Artificial Intelligent System Development in Industry\",\n                \"Machine learning models and big data tools for evaluating kidney acceptance\",\n                \"Understanding development process of machine learning systems: Challenges and solutions\",\n                \"Machine learning with big data: Challenges and approaches\",\n                \"Guidelines for conducting and reporting case study research in software engineering\",\n                \"Qualitative methods in empirical studies of software engineering\",\n                \"The nyse and nasdaq: How they work\",\n                \"Data collection and quality challenges for deep learning\",\n                \"A study of cross-validation and bootstrap for accuracy estimation and model selection\",\n                \"Limitations of bayesian leave-one-out cross-validation for model selection\",\n                \"Efficient strategies for leave-one-out cross validation for genomic best linear unbiased prediction\",\n                \"Efficient approximate leave-one-out cross-validation for kernel logistic regression\",\n                \"Comprehensive analysis of global stock market reactions to the russia-ukraine war\",\n                \"An empirical study of the impact of modern code review practices on software quality\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]39 based on the context and research area of A Case Study on AI Engineering Practices: Developing an Autonomous Stock Trading System. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}