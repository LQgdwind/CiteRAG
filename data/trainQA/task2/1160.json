{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Steady State Analysis of Episodic Reinforcement Learning\n\nPaper Abstract: \n\nPaper Section Text:\nThis section provides more discussions about the MDP formulation as introduced in Section 2. A faithful implementation of this formulation was used to generate all the results reported in Section 6 and Section F. Our MDP formulation assumes both SSSS\\SS and 𝒜𝒜\\mathcal{A} are countable sets. This is mainly for aligning to the standard markov chain theory, and also for enabling convenient notations like transition matrix M𝑀M and summation ∑s,asubscript𝑠𝑎\\sum_{s,a}. Results in this paper are readily generalizable to uncountable action spaces (which still induce countable transitions after marginalizing over the actions), and may also be generalized to uncountable state spaces based on the theory of general-state-space markov chains . Our MDP formulation also assumes state-based deterministic reward function R𝑅R. While this formulation was used in many previous works [ref]16, some literature  explicitly assign stochastic rewards to transitions, in the form of R​(r|s,a,s′)=ℙ[rt+1=r|st=s,at=a,st+1=s′]𝑅conditional𝑟𝑠𝑎superscript𝑠′ℙsubscript𝑟𝑡1conditional𝑟subscript𝑠𝑡𝑠subscript𝑎𝑡𝑎subscript𝑠𝑡1superscript𝑠′R(r|s,a,s^{\\prime})=\\operatorname*{\\mathbb{P}}[r_{t+1}=r|s_{t}=s,a_{t}=a,s_{t+1}=s^{\\prime}]. Our reward-function formulation has no loss of generality here, as one can think of a “state” in our formulation as a (s,r)𝑠𝑟(s,r) pair in stochastic-reward models, with the transition-dependent stochasticity of the reward captured by the transition function P𝑃P. Our MDP formulation has replaced the (often included) discounting constant γcsubscript𝛾𝑐\\gamma_{c} with the (often excluded) initial state distribution ρ0subscript𝜌0\\rho_{0}. Similar perspective to “downgrade” the discounting constant was discussed in  (Chapter 10.4). As will become evident later in the paper, the discounting constant is neither necessary for the purpose of defining the problem, nor is it necessary for treating the problem. On the other hand, an explicit specification of ρ0subscript𝜌0\\rho_{0} is necessary to define the episode-wise performance Je​p​isubscript𝐽𝑒𝑝𝑖J_{epi} as used in all episodic tasks, and will be also needed to define the terminal states in the formulation of episodic learning process proposed in this paper. A finite-horizon task can have degenerated steady states if it is formulated as an infinite-horizon MDP with an imaginary absorbing state s□subscript𝑠□s_{\\scriptscriptstyle\\Box}. In the absorbing MDP formulation, any finite episode will end up with moving from its terminal state to the absorbing state s□subscript𝑠□s_{\\scriptscriptstyle\\Box}, from there the rollout is trapped in the absorbing state forever without generating effective reward  . As a result, the stationary (and limiting) distribution of the absorbing MDP of any finite-horizon task concentrates fully and only to the absorbing state, making it of limited use for designing and analyzing RL algorithms. The definition of the steady-state performance measure Ja​v​g​(π)≐limT→∞1T​∑t=1TR​(st)=𝔼s∼ρπ[R​(s)]approaches-limitsubscript𝐽𝑎𝑣𝑔𝜋subscript→𝑇1𝑇superscriptsubscript𝑡1𝑇𝑅subscript𝑠𝑡subscript𝔼similar-to𝑠subscript𝜌𝜋𝑅𝑠J_{avg}(\\pi)\\doteq\\lim\\limits_{T\\rightarrow\\infty}\\frac{1}{T}\\sum_{t=1}^{T}R(s_{t})=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\pi}\\end{subarray}}[R(s)], as introduced in Section 2, is based on the following ergodic theorem of markov chain: In ergodic markov chain M𝑀M with stationary distribution ρMsubscript𝜌𝑀\\rho_{\\scriptscriptstyle M}, let f​(s)𝑓𝑠f(s) be any function such that 𝔼s∼ρM[|f​(s)|]<∞subscript𝔼similar-to𝑠subscript𝜌𝑀𝑓𝑠\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\scriptscriptstyle M}\\end{subarray}}[~{}|f(s)|~{}]<\\infty, then the time-average of f𝑓f converges almost surely to the state-average of f𝑓f, i.e., limT→∞ℙ[1T​∑t=1Tf​(st)=𝔼s∼ρM[f​(s)]]=1subscript→𝑇ℙ1𝑇superscriptsubscript𝑡1𝑇𝑓subscript𝑠𝑡subscript𝔼similar-to𝑠subscript𝜌𝑀𝑓𝑠1\\lim\\limits_{T\\rightarrow\\infty}\\operatorname*{\\mathbb{P}}[~{}\\frac{1}{T}\\sum_{t=1}^{T}f(s_{t})=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\scriptscriptstyle M}\\end{subarray}}[f(s)]~{}]=1.\n(, Theorem 74) The value function Q𝑄Q is often more specifically called the action-value function. We called the Q𝑄Q-function just value function because of its symmetric role with policy function π𝜋\\pi. Moreover, in classic literature, the term “action-value function” was often referred specifically to\nQπγc​(s,a)≐𝔼s1∼P​(s,a),{s≥2}∼Mπ[∑t=1∞R​(st)⋅γct−1]approaches-limitsubscriptsuperscript𝑄subscript𝛾𝑐𝜋𝑠𝑎subscript𝔼formulae-sequencesimilar-tosubscript𝑠1𝑃𝑠𝑎similar-tosubscript𝑠absent2subscript𝑀𝜋superscriptsubscript𝑡1⋅𝑅subscript𝑠𝑡superscriptsubscript𝛾𝑐𝑡1Q^{\\scriptscriptstyle\\gamma_{c}}_{\\pi}(s,a)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s_{1}\\sim P(s,a),\\{s_{\\geq 2}\\}\\sim M_{\\pi}\\end{subarray}}[\\sum_{t=1}^{\\infty}R(s_{t})\\cdot\\gamma_{c}^{t-1}]\n, in which the constant 0≤γc<10subscript𝛾𝑐10\\leq\\gamma_{c}<1 is called the discounting factor. The specialized definition of Qπγcsuperscriptsubscript𝑄𝜋subscript𝛾𝑐Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}} entails the specialized version of Bellman equation\nQπγc​(s,a)=𝔼s′∼P​(s,a),a′∼π​(s′)[R​(s′)+γc⋅Qπγc​(s′,a′)],superscriptsubscript𝑄𝜋subscript𝛾𝑐𝑠𝑎subscript𝔼formulae-sequencesimilar-tosuperscript𝑠′𝑃𝑠𝑎similar-tosuperscript𝑎′𝜋superscript𝑠′𝑅superscript𝑠′⋅subscript𝛾𝑐superscriptsubscript𝑄𝜋subscript𝛾𝑐superscript𝑠′superscript𝑎′Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s,a)=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s^{\\prime}\\sim P(s,a),a^{\\prime}\\sim\\pi(s^{\\prime})\\end{subarray}}[R(s^{\\prime})+\\gamma_{c}\\cdot Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s^{\\prime},a^{\\prime})],\nand induces the specialized version of state-value function\nVπγc​(s)≐𝔼a∼π​(s)[Qπγc​(s,a)].approaches-limitsuperscriptsubscript𝑉𝜋subscript𝛾𝑐𝑠subscript𝔼similar-to𝑎𝜋𝑠superscriptsubscript𝑄𝜋subscript𝛾𝑐𝑠𝑎V_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}a\\sim\\pi(s)\\end{subarray}}[Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s,a)]. The product of γ𝛾\\gamma in the general-form value functions (1) was originally proposed as a virtual probabilistic termination in the agent’s mind ,\nbut was latter found useful to account for the real episode-boundaries when the MDP is used to model multi-episode rollouts  .\nOur paper uses product-form value functions for the same purpose as in  (Section 2.1.1) and . However, both  and  used transition-based γ𝛾\\gamma-discounting, while the value function in our treatment uses state-based discounting and is probably closer to the method of Figure 1(c) in , which was considered an sub-optimal design in that paper. In fact,  attributes much of its main technical contribution to the adoption and contrastive analysis of transition-based discounting (against state-based discounting) which concludes that “transition-based discounting is necessary to enable the unified specification of episodic and continuing tasks” ( Section 2.1, see also Section 6 and B). Despite this major technical disparity, we nevertheless think the approach as described in Section 3 of our paper actually aligns with  (Section 2.1.1) and  in terms of the bigger idea that connecting finite episodes into a single infinite-horizon MDP greatly helps with unifying the episodic and continual formalisms.\n\nReference markers found: [16]\n\nContext windows for each [ref]:\n[ref]16: based on the theory of general-state-space markov chains . Our MDP formulation also assumes state-based deterministic reward function R𝑅R. While this formulation was used in many previous works [ref]16, some literature  explicitly assign stochastic rewards to transitions, in the form of\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 16,\n            \"titles\": [\n                \"Equivalence Between Policy Gradients and Soft Q-Learning\",\n                \"Pybullet gymperium.\",\n                \"Dynamic programming and optimal control, volume 1.\",\n                \"Openai gym.\",\n                \"Off-policy actor-critic.\",\n                \"Benchmarking Deep Reinforcement Learning for Continuous Control\",\n                \"Addressing Function Approximation Error in Actor-Critic Methods\",\n                \"Continuous Deep Q-Learning with Model-based Acceleration\",\n                \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\n                \"A natural policy gradient.\",\n                \"Continuous control with deep reinforcement learning.\",\n                \"Simulation-based optimization of markov reward processes.\",\n                \"Markov chains and stochastic stability.\",\n                \"Human-level control through deep reinforcement learning.\",\n                \"Asynchronous methods for deep reinforcement learning.\",\n                \"Time Limits in Reinforcement Learning\",\n                \"Trust Region Policy Optimization\",\n                \"Proximal policy optimization algorithms.\",\n                \"Basics of applied stochastic processes.\",\n                \"Deterministic policy gradient algorithms.\",\n                \"The predictron: End-to-end learning and planning.\",\n                \"A new q (lambda) with interim forward view and monte carlo equivalence.\",\n                \"Td models: modeling the world at a mixture of time scales.\",\n                \"Reinforcement learning: An introduction.\",\n                \"Policy gradient methods for reinforcement learning with function approximation.\",\n                \"Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction.\",\n                \"An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning\",\n                \"Bias in natural actor-critic algorithms.\",\n                \"On average versus discounted reward temporal-difference learning.\",\n                \"Deep Reinforcement Learning with Double Q-learning\",\n                \"Insights in reinforcement rearning: formal analysis and empirical evaluation of temporal-difference learning algorithms.\",\n                \"Learning from delayed rewards.\",\n                \"Unifying Task Specification in Reinforcement Learning\",\n                \"Simple statistical gradient-following algorithms for connectionist reinforcement learning.\",\n                \"On convergence of emphatic temporal-difference learning.\",\n                \"Generalized Off-Policy Actor-Critic\",\n                \"Generated Reference Title 36 for [ref]16\",\n                \"Generated Reference Title 37 for [ref]16\",\n                \"Generated Reference Title 38 for [ref]16\",\n                \"Generated Reference Title 39 for [ref]16\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]16 based on the context and research area of Steady State Analysis of Episodic Reinforcement Learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}