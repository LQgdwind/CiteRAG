{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Unified Out-Of-Distribution Detection: A Model-Specific Perspective\n\nPaper Abstract: \n\nPaper Section Text:\nEquipping a model with the capability to identify “what it does not know” is critical for reliable machine learning. Take image classification as an example. While state-of-the-art neural network models [ref]11    could perform fairly well on “in-distribution (ID)” data that belong to the training distribution, their accuracy often degrades drastically when facing data with covariate shift (e.g., different image domains or styles)  or semantic shift (e.g., novel categories) . It is thus crucial to detect data on which the models cannot perform well and reject them from being classified. Out-of-distribution (OOD) detection [ref]13, which aims to identify test examples drawn from a distribution different from the training distribution, is a promising paradigm toward such a goal. OOD detection has attracted significant attention lately, with a plethora of methods being developed  . However, most of them focus solely on detecting examples with semantic shift, which arguably limits their applicability in uncontrolled environments where other kinds of OOD examples (e.g., with covariate shift) may appear. In this paper, we thus attempt to expand the scope of OOD detection to further include covariate shift. At first glance, one may treat OOD examples with covariate shift (denoted as C-OOD) the same as OOD examples with semantic shift (denoted as S-OOD) — i.e., to detect and reject as many of them as possible. However, unlike S-OOD examples, which the deployed classifier can never classify correctly, C-OOD examples have a chance to be correctly classified, as their label space is covered by the classifier. For instance, if the covariate shift is small (e.g., ImageNet  as ID; ImageNetV2 as C-OOD ), many of the C-OOD examples will likely be correctly classified. Even if the covariate shift is more pronounced (e.g., ImageNet-Sketch ), a robust classifier (e.g., with the CLIP-pre-trained backbone ) is likely to still classify a decent amount of C-OOD examples correctly. Blindly rejecting these correctly classified C-OOD examples would adversely reduce the efficacy of the classifier. Taking this insight into account, we propose a novel, unifying framework — MS-OOD Detection — to study OOD detection from a “Model-Specific” perspective. In MS-OOD Detection, whether an example should be detected and rejected from being classified (denoted by a ground-truth label −11-1) depends on whether the deployed classifier would misclassify it. With this definition, every test example can be deterministically assigned a ground-truth label based on the deployed classifier: +11+1 for correctly classified examples, which should not be rejected; −11-1 for misclassified examples, which should be rejected. This enables us to study different causes of OOD examples in a unifying way. It is worth noting that while C-OOD examples could be assigned different ground-truth labels, all the S-OOD examples are assigned ground-truth labels −11-1. In other words, similar to conventional OOD detection, all the S-OOD examples should be detected and rejected in MS-OOD Detection. Nevertheless, unlike conventional OOD detection, which aims to accept all the ID examples drawn from the training distribution, MS-OOD Detection, according to how it assigns ground-truth labels, aims to detect and reject misclassified ID examples as well. That is, what MS-OOD Detection aims to accept are correctly classified ID and C-OOD examples; what MS-OOD Detection aims to reject are misclassified ID and C-OOD examples, and all the S-OOD examples (which are always misclassified). Such a definition seamlessly unifies and generalizes the two related problems studied in the seminal work by Hendrycks and Gimpel [ref]13 — detecting misclassified and OOD examples — and could better reflect real-world application scenarios. For instance, for end-users who seek to reliably apply the machine learning model in uncontrolled environments, misclassification reveals the limitation of the model or the inherent difficulty of the examples (e.g., hard or ambiguous examples), implying the need for end-user intervention. We conduct an extensive empirical study of MS-OOD Detection. We consider three dimensions: 1) sources of OOD examples, which include both semantic and covariate shift; 2) deployed classifiers, which include different neural network architectures and training strategies; 3) OOD detection methods, which include representative approaches such as Maximum Softmax Probabilities (MSP) [ref]13, Energy Score , Maximum Logit Score (MLS) [ref]42, Virtual-logit Matching (ViM) , and GradNorm . New datasets, classifiers, and OOD methods can easily be incorporated to broaden the scope. This experimental framework not only offers a platform to unify the community but also provides a “manual” to end-users for selecting the appropriate OOD methods in their respective use cases. Along with this study are 1) a list of novel insights into OOD detection and 2) a unifying re-validation of several existing but seemingly isolated insights found in different contexts. For instance, we find that the best detection methods for S-OOD, misclassified C-OOD, and misclassified ID data are not consistent; their effectiveness could be influenced by the paired model, hence “model-specific”.\nSpecifically for C-OOD examples, we find that the more robust the classifier is (i.e., having a higher accuracy in classifying C-OOD examples), the easier the misclassified C-OOD examples can be detected.\nFor S-OOD examples, while in general, we see the same trend as in [ref]42 — the stronger the classifier is (i.e., having a higher accuracy in classifying ID examples), the easier the S-OOD examples can be detected — there are exceptions when we apply particular detection methods and classifiers. This suggests the need for a more thorough study. For misclassified ID examples, we find that they normally have lower scores (e.g., softmax probabilities) than correctly classified ID examples. In other words, one could set a higher threshold to reject more S-OOD examples without sacrificing the true positive rate of accepting ID examples that are correctly classified. Last but not least, we find that the baseline OOD detection method MSP [ref]13 performs favorably in detecting misclassified ID and C-OOD examples, often outperforming other more advanced methods. Contributions. Our contributions are two-folded: We propose a novel framework, MS-OOD Detection, which enables us to study different OOD examples (e.g., covariate shift and semantic shift) in a unifying way. We conduct an extensive study of MS-OOD Detection, which reveals novel insights into OOD detection and unifies existing insights gained from different contexts.\n\nReference markers found: [11]\n\nContext windows for each [ref]:\n[ref]11: a model with the capability to identify “what it does not know” is critical for reliable machine learning. Take image classification as an example. While state-of-the-art neural network models [ref]11    could perform fairly well on “in-distribution (ID)” data that belong to the training distribution, their accuracy often degrades drastically when facing data with covariate shift (e.g.,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 11,\n            \"titles\": [\n                \"The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization\",\n                \"Adversarial Reciprocal Points Learning for Open Set Recognition\",\n                \"Gradual Domain Adaptation without Indexed Intermediate Domains\",\n                \"Describing textures in the wild\",\n                \"Imagenet: A large-scale hierarchical image database\",\n                \"Reducing Network Agnostophobia\",\n                \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n                \"Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)\",\n                \"Exploring the Limits of Out-of-Distribution Detection\",\n                \"Selective Classification for Deep Neural Networks\",\n                \"On Calibration of Modern Neural Networks\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks\",\n                \"Deep Anomaly Detection with Outlier Exposure\",\n                \"Natural Adversarial Examples\",\n                \"Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data\",\n                \"Densely connected convolutional networks\",\n                \"On the Importance of Gradients for Detecting Distributional Shifts in the Wild\",\n                \"MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space\",\n                \"Understanding self-training for gradual domain adaptation\",\n                \"A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks\",\n                \"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks\",\n                \"Energy-based Out-of-distribution Detection\",\n                \"Swin transformer: Hierarchical vision transformer using shifted windows\",\n                \"Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization\",\n                \"Delving into Out-of-Distribution Detection with Vision-Language Representations\",\n                \"On the Impact of Spurious Correlation for Out-of-distribution Detection\",\n                \"Finding Competence Regions in Domain Generalization\",\n                \"Reading digits in natural images with unsupervised feature learning\",\n                \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\n                \"Learning Transferable Visual Models From Natural Language Supervision\",\n                \"Do ImageNet Classifiers Generalize to ImageNet?\",\n                \"Imagenet-21k pretraining for the masses\",\n                \"ImageNet Large Scale Visual Recognition Challenge\",\n                \"A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges\",\n                \"Towards out-of-distribution generalization: A survey\",\n                \"ReAct: Out-of-distribution Detection With Rectified Activations\",\n                \"Measuring robustness to natural distribution shifts in image classification\",\n                \"Exploring covariate and concept shift for detection and calibration of out-of-distribution data\",\n                \"The iNaturalist Species Classification and Detection Dataset\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]11 based on the context and research area of Unified Out-Of-Distribution Detection: A Model-Specific Perspective. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}