{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Residual Gaussian Process: A Tractable Nonparametric Bayesian Emulator for Multi-fidelity Simulations\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section we introduce a novel tractable and scalable structure to model multi-fidelity simulation data.\nRather than imposing a concatenating structure as in NARGP, which forgoes the tractable nature of a GP , we decompose the GP defined over the high-fidelity output into a sum of GPs relating to the differences between successive fidelities.\nSpecifically, we impose the following residual structure where ğ«fâ€‹(ğƒ)=ğ²fâ€‹(ğƒ)âˆ’ğ²fâˆ’1â€‹(ğƒ)superscriptğ«ğ‘“ğƒsuperscriptğ²ğ‘“ğƒsuperscriptğ²ğ‘“1ğƒ{\\bf r}^{f}(\\bm{\\xi})={\\bf y}^{f}(\\bm{\\xi})-{\\bf y}^{f-1}(\\bm{\\xi}), f=2,â€¦,Fğ‘“2â€¦ğ¹f=2,\\ldots,F, are the residual functions between fidelities. The function ğ«1â€‹(ğƒ)superscriptğ«1ğƒ{\\bf r}^{1}(\\bm{\\xi}), on the other hand, is defined as ğ«1â€‹(ğƒ)=ğ²1â€‹(ğƒ)superscriptğ«1ğƒsuperscriptğ²1ğƒ{\\bf r}^{1}(\\bm{\\xi})={\\bf y}^{1}(\\bm{\\xi}). Note that we present the model for the multivariate case. The univariate case can be obtained from the formulae presented below in an obvious manner. The unknown functions ğ«fâ€‹(ğƒ)superscriptğ«ğ‘“ğƒ{\\bf r}^{f}(\\bm{\\xi}) are then treated as random processes that can be approximated using any probabilistic data-driven model.\nWe place an independent, zero-mean GP prior over each residual function ğ«fâ€‹(ğƒ)superscriptğ«ğ‘“ğƒ{\\bf r}^{f}(\\bm{\\xi}), i.e., where ğ›€fâˆˆâ„dÃ—dsuperscriptğ›€ğ‘“superscriptâ„ğ‘‘ğ‘‘\\mathbf{\\Omega}^{f}\\in\\mathbb{R}^{d\\times d} is an unknown coregionalization matrix corresponding to the correlations between the components of ğ«fâ€‹(ğƒ)superscriptğ«ğ‘“ğƒ{\\bf r}^{f}(\\bm{\\xi}), âŠ—tensor-product\\otimes is the Kronecker product and Î´â€‹(ğƒ,ğƒâ€²)ğ›¿ğƒsuperscriptğƒâ€²\\delta(\\bm{\\xi},\\bm{\\xi}^{\\prime}) is the Kronecker-delta function. Without loss of generality, the mean is assumed to be identically zero by centering the observations. We note that this is the multivariate GP model of Conti and Oâ€™Hagan , with a separable covariance structure. That is, the covariance matrices assume the forms ğŠfâŠ—ğ›€ftensor-productsuperscriptğŠğ‘“superscriptğ›€ğ‘“{\\bf K}^{f}\\otimes\\mathbf{\\Omega}^{f}, in which [ğŠf]nâ€‹m=kfâ€‹(ğƒn,ğƒm|ğœ½f)subscriptdelimited-[]superscriptğŠğ‘“ğ‘›ğ‘šsuperscriptğ‘˜ğ‘“subscriptğƒğ‘›conditionalsubscriptğƒğ‘šsuperscriptğœ½ğ‘“[{\\bf K}^{f}]_{nm}=k^{f}(\\bm{\\xi}_{n},\\bm{\\xi}_{m}|\\bm{\\theta}^{f}), ğƒn,ğƒmâˆˆğ’³fsubscriptğƒğ‘›subscriptğƒğ‘šsuperscriptğ’³ğ‘“\\bm{\\xi}_{n},\\bm{\\xi}_{m}\\in\\mathcal{X}^{f}, captures the correlations between values of ğ«fâ€‹(ğƒ)superscriptğ«ğ‘“ğƒ{\\bf r}^{f}(\\bm{\\xi}) at different inputs ğƒğƒ\\bm{\\xi} and ğ›€fsuperscriptğ›€ğ‘“\\mathbf{\\Omega}^{f} captures the spatial correlations, i.e., between different components of ğ«fâ€‹(ğƒ)superscriptğ«ğ‘“ğƒ{\\bf r}^{f}(\\bm{\\xi}). The covariance functions kfâ€‹(ğƒ,ğƒ|ğœ½)superscriptğ‘˜ğ‘“ğƒconditionalğƒğœ½k^{f}(\\bm{\\xi},\\bm{\\xi}|\\bm{\\theta}) contain unknown hyperparameters ğœ½fsuperscriptğœ½ğ‘“\\bm{\\theta}^{f}. The terms Î´â€‹(ğƒ,ğƒâ€²)â€‹Ï„fâ€‹ğˆğ›¿ğƒsuperscriptğƒâ€²superscriptğœğ‘“ğˆ\\delta(\\bm{\\xi},\\bm{\\xi}^{\\prime})\\tau^{f}{\\bf I} are included to account for zero-mean, i.i.d. measurement error (across inputs and spatial coordinates), or, equivalently, as regularization terms to prevent ill-conditioning during training. The Ï„fsuperscriptğœğ‘“\\tau^{f} are treated as hyperparameters or (optionally) fixed to some small values in the regularization interpretationÂ . The distribution (3) is conditioned on the full set of hyperparameters {ğœ½f,Î©f,Ï„f}superscriptğœ½ğ‘“superscriptÎ©ğ‘“superscriptğœğ‘“\\{\\bm{\\theta}^{f},\\Omega^{f},\\tau^{f}\\} but to avoid notational clutter, conditioning on hyperparameters and inputs or observations is not explicitly indicated. In the measurement error interpretation, the targets, i.e., given values of ğ«fâ€‹(ğƒ)superscriptğ«ğ‘“ğƒ{\\bf r}^{f}(\\bm{\\xi}), are considered to be values of a latent function ğ«lfâ€‹(ğƒ)superscriptsubscriptğ«ğ‘™ğ‘“ğƒ{\\bf r}_{l}^{f}(\\bm{\\xi}) corrupted by the i.i.d. noise Ïµfsuperscriptbold-italic-Ïµğ‘“\\boldsymbol{\\epsilon}^{f} with priors ğ«lf(ğƒ)âˆ¼ğ’¢ğ’«(ğ«lf(ğƒ)âˆ£ğŸ,kf(ğƒ,ğƒâ€²|ğœ½f)âŠ—ğ›€f){\\bf r}_{l}^{f}(\\bm{\\xi})\\sim\\mathcal{GP}({\\bf r}_{l}^{f}(\\bm{\\xi})\\mid{\\bf 0},k^{f}(\\bm{\\xi},\\bm{\\xi}^{\\prime}|\\bm{\\theta}^{f})\\otimes\\mathbf{\\Omega}^{f}) and Ïµfâˆ¼ğ’¢â€‹ğ’«â€‹(Ïµfâˆ£ğŸ,Î´â€‹(ğƒ,ğƒâ€²)â€‹Ï„fâ€‹ğˆ)similar-tosuperscriptbold-italic-Ïµğ‘“ğ’¢ğ’«conditionalsuperscriptbold-italic-Ïµğ‘“0ğ›¿ğƒsuperscriptğƒâ€²superscriptğœğ‘“ğˆ\\boldsymbol{\\epsilon}^{f}\\sim\\mathcal{GP}(\\boldsymbol{\\epsilon}^{f}\\mid{\\bf 0},\\delta(\\bm{\\xi},\\bm{\\xi}^{\\prime})\\tau^{f}{\\bf I}). Choosing the right kernel function for a specific application is non-trivial.\nWhen there is no prior knowledge to guide the choice, the automatic relevance determinant (ARD) kernel  with ğœ½f=(Î¸0f,â€¦,Î¸1f)Tsuperscriptğœ½ğ‘“superscriptsuperscriptsubscriptğœƒ0ğ‘“â€¦superscriptsubscriptğœƒ1ğ‘“ğ‘‡\\bm{\\theta}^{f}=(\\theta_{0}^{f},\\ldots,\\theta_{1}^{f})^{T} is often used. The ARD kernel can freely capture the influence of each individual input (coordinate of ğƒğƒ\\bm{\\xi}) on the output.\nThe hyperparameters {Ï„f,ğœ½f}superscriptğœğ‘“superscriptğœ½ğ‘“\\{\\tau^{f},\\bm{\\theta}^{f}\\} can be estimated by maximizing the log-marginal likelihood (see section 3.1). The high-fidelity prior in this model can be written as follows by virtue of the independence assumption. Again, this is the GP over the noisy observations with cumulative i.i.d. noise variance âˆ‘fÏ„fsubscriptğ‘“subscriptğœğ‘“\\sum_{f}\\tau_{f}, and underlying latent function ğ²lF(ğƒ)âˆ¼ğ’¢ğ’«(ğ²lF(ğƒ)âˆ£ğŸ,âˆ‘f=1Fkf(ğƒ,ğƒâ€²|ğœ½f)âŠ—ğ›€f){\\bf y}_{l}^{F}(\\bm{\\xi})\\sim\\mathcal{GP}({\\bf y}^{F}_{l}(\\bm{\\xi})\\mid{\\bf 0},\\sum_{f=1}^{F}k^{f}(\\bm{\\xi},\\bm{\\xi}^{\\prime}|\\bm{\\theta}^{f})\\otimes\\mathbf{\\Omega}^{f}). Beginning with the lowest fidelity level f=1ğ‘“1f=1, we assume the prior (3) and use inputs ğ’³1superscriptğ’³1\\mathcal{X}^{1} together with outputs ğ˜1superscriptğ˜1{\\bf Y}^{1} to learn {Ï„1,ğœ½1}superscriptğœ1superscriptğœ½1\\{\\tau^{1},\\bm{\\theta}^{1}\\} and find the predictive posterior ğ«l1â€‹(ğƒ)âˆ¼ğ’©â€‹(ğ«l1â€‹(ğƒ)âˆ£ğr1â€‹(ğƒ),ğ•r1â€‹(ğƒ))similar-tosubscriptsuperscriptğ«1ğ‘™ğƒğ’©conditionalsubscriptsuperscriptğ«1ğ‘™ğƒsubscriptsuperscriptğ1ğ‘Ÿğƒsubscriptsuperscriptğ•1ğ‘Ÿğƒ{\\bf r}^{1}_{l}(\\bm{\\xi})\\sim\\mathcal{N}\\left({\\bf r}^{1}_{l}(\\bm{\\xi})\\mid\\bm{\\mu}^{1}_{r}(\\bm{\\xi}),{\\bf V}^{1}_{r}(\\bm{\\xi})\\right). At the next step we use inputs ğ’³2superscriptğ’³2\\mathcal{X}^{2} and outputs ğ‘2:=ğ˜2âˆ’ğ˜ğ21assignsuperscriptğ‘2superscriptğ˜2subscriptsuperscriptğ˜1subscriptğ2{\\bf R}^{2}:={\\bf Y}^{2}-{\\bf Y}^{1}_{{\\bf e}_{2}} in the same procedure to learn {Ï„2,ğœ½2}superscriptğœ2superscriptğœ½2\\{\\tau^{2},\\bm{\\theta}^{2}\\}, and obtain the posterior for ğ«l2â€‹(ğƒ)âˆ¼ğ’©â€‹(ğ«l2â€‹(ğƒ)âˆ£ğr2â€‹(ğƒ),ğ•r2â€‹(ğƒ))similar-tosuperscriptsubscriptğ«ğ‘™2ğƒğ’©conditionalsubscriptsuperscriptğ«2ğ‘™ğƒsubscriptsuperscriptğ2ğ‘Ÿğƒsubscriptsuperscriptğ•2ğ‘Ÿğƒ{\\bf r}_{l}^{2}(\\bm{\\xi})\\sim\\mathcal{N}\\left({\\bf r}^{2}_{l}(\\bm{\\xi})\\mid\\bm{\\mu}^{2}_{r}(\\bm{\\xi}),{\\bf V}^{2}_{r}(\\bm{\\xi})\\right). Here we define a matrix of residuals ğ‘2superscriptğ‘2{\\bf R}^{2} using the notation introduced in section 2. This procedure is repeated up to fidelity level f=Fğ‘“ğ¹f=F, i.e., independently learning the hyperparameters associated with each ğ«fâ€‹(ğƒ)superscriptğ«ğ‘“ğƒ{\\bf r}^{f}(\\bm{\\xi}), given observations {ğ‘f=ğ˜fâˆ’ğ˜ğffâˆ’1}f=2Fsuperscriptsubscriptsuperscriptğ‘ğ‘“superscriptğ˜ğ‘“subscriptsuperscriptğ˜ğ‘“1subscriptğğ‘“ğ‘“2ğ¹\\{{\\bf R}^{f}={\\bf Y}^{f}-{\\bf Y}^{f-1}_{{\\bf e}_{f}}\\}_{f=2}^{F}. The predictive posteriors over ğ«lfâ€‹(ğƒ)superscriptsubscriptğ«ğ‘™ğ‘“ğƒ{\\bf r}_{l}^{f}(\\bm{\\xi}) are derived using standard Gaussian conditioning rules , and the posterior for the high-fidelity latent function ğ²lFâ€‹(ğƒ)superscriptsubscriptğ²ğ‘™ğ¹ğƒ{\\bf y}_{l}^{F}(\\bm{\\xi}) can be written compactly as the following sum of GPs where vecâ€‹(â‹…)vecâ‹…{\\rm vec}{(\\cdot)} denotes vectorization and ğ¤fâ€‹(ğƒ)=(kfâ€‹(ğƒ,ğƒ1|ğœ½f),â€¦,kfâ€‹(ğƒ,ğƒNf|ğœ½f))Tsuperscriptğ¤ğ‘“ğƒsuperscriptsuperscriptğ‘˜ğ‘“ğƒconditionalsubscriptğƒ1superscriptğœ½ğ‘“â€¦superscriptğ‘˜ğ‘“ğƒconditionalsubscriptğƒsubscriptğ‘ğ‘“superscriptğœ½ğ‘“ğ‘‡{\\bf k}^{f}(\\bm{\\xi})=(k^{f}(\\bm{\\xi},\\bm{\\xi}_{1}|\\bm{\\theta}^{f}),\\ldots,k^{f}(\\bm{\\xi},\\bm{\\xi}_{N_{f}}|\\bm{\\theta}^{f}))^{T} is the vector of covariances between the latent function values ğ«lfâ€‹(â‹…)superscriptsubscriptğ«ğ‘™ğ‘“â‹…{\\bf r}_{l}^{f}(\\cdot) at ğƒğƒ\\bm{\\xi} and points in ğ’³fsuperscriptğ’³ğ‘“\\mathcal{X}^{f}. We note that since the posteriors over each ğ«fâ€‹(ğƒ)superscriptğ«ğ‘“ğƒ{\\bf r}^{f}(\\bm{\\xi}) are learned independently, the training procedure is parallelizable if implemented without active learning (discussed below). The formulation (7) considers a general case in which noise terms Ï„fâ€‹ğˆsuperscriptğœğ‘“ğˆ\\tau^{f}{\\bf I} are included.\nA computational model (of any fidelity) can, however, be treated as deterministic function without random noise . Thus, we can consider the residual information to be deterministic and model it using GPs without the noise terms, in which case ğ²lFâ€‹(ğƒ)=ğ²Fâ€‹(ğƒ)superscriptsubscriptğ²ğ‘™ğ¹ğƒsuperscriptğ²ğ¹ğƒ{\\bf y}_{l}^{F}(\\bm{\\xi})={\\bf y}^{F}(\\bm{\\xi}) and the predictive posterior reduces to Note that the coregionalization matrices ğ›€fsuperscriptğ›€ğ‘“\\mathbf{\\Omega}^{f} cancel out for the expectation predictions (of any fidelity). This is consistent with autokrigeability , which is utilized by Xing etÂ al.  to deal with high-dimensional, single-fidelity mechanical design simulations. Noiseless data is the usual assumption for simulations (the so-called ground truth approximation), but one could attempt to incorporate systematic errors arising from the model formulation (model inadequacy), from the numerical approximation (numerical errors), or from parameter uncertainty. In the same spirit as Xing etÂ al. , we now simplify the model by setting ğ›€f=ğˆsuperscriptğ›€ğ‘“ğˆ\\mathbf{\\Omega}^{f}={\\bf I} for f=1,â€¦,Fğ‘“1â€¦ğ¹f=1,\\dots,F. This assumes that the components of ğ²Fâ€‹(ğƒ)superscriptğ²ğ¹ğƒ{\\bf y}^{F}(\\bm{\\xi}) are mutually independent given {ğœ½f}superscriptğœ½ğ‘“\\{\\bm{\\theta}^{f}\\}. Note that the variance of the prediction then ignores the spatial correlations, which will affect the uncertainty estimation, but the mean prediction is unaffected.\nWe will discuss this issue further in Section 3.2, in which we see that the active learning process is not affected. Retaining the noise terms and coregionalisation matrices, i.e., retaining Eq. (7), requires an additional set of hyperparameters, namely the Ï„fsuperscriptğœğ‘“\\tau^{f} and the entries of each ğ›€fsuperscriptğ›€ğ‘“\\mathbf{\\Omega}^{f}. We may even use a richer covariance structure, such as the linear model of coregionalisation [ref]25 with linear combinations of separable covariances. In low-dimensional output spaces, this could be of benefit in terms of accuracy and variance capture. In very high-dimensional spaces, on the other hand, these approaches lead to problems in terms of maximizing the likelihood (or evidence lower bound) due to the higher number of inputs in the optimization problem [ref]25 (see section 3.3). If modelling the noise is deemed important for a particular problem (perhaps because the solutions are known to be corrupted), a compromise could be achieved by retaining the {Ï„f}superscriptğœğ‘“\\{\\tau^{f}\\} terms and setting ğ›€f=ğˆsuperscriptğ›€ğ‘“ğˆ\\mathbf{\\Omega}^{f}={\\bf I}. We discuss this formulation later. We point out, however, that in the examples in Section 4, the noise terms are not included. For most data sets, Nfsubscriptğ‘ğ‘“N_{f} decreases in size with fğ‘“f, and is typically small for large values of fğ‘“f. The uncertainty in the predictions for ğ«lfsuperscriptsubscriptğ«ğ‘™ğ‘“{\\bf r}_{l}^{f} (or ğ«fsuperscriptğ«ğ‘“{\\bf r}^{f}) therefore increases with fğ‘“f. We have to keep in mind, however, that that uncertainty is also bounded by the scales of the residuals between different fidelities. For high values of fğ‘“f, the simulations for adjacent fidelities would tend to be similar, and therefore the residuals would tend to be small. Thus, the added uncertainties for large fğ‘“f will be small. For small fğ‘“f, the residuals between adjacent fidelities are expected to be larger but the uncertainties would be smaller by virtue of the greater number of samples. The additive structure can, therefore, potentially ensure that the final uncertainty reflects the true model uncertainty. We discuss this further in the first example in section 4.2.\n\nReference markers found: [25]\n\nContext windows for each [ref]:\n[ref]25: namely the Ï„fsuperscriptğœğ‘“\\tau^{f} and the entries of each ğ›€fsuperscriptğ›€ğ‘“\\mathbf{\\Omega}^{f}. We may even use a richer covariance structure, such as the linear model of coregionalisation [ref]25 with linear combinations of separable covariances. In low-dimensional output spaces, this could be of benefit in terms of accuracy and variance capture. In very high-dimensional spaces, on\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 25,\n            \"titles\": [\n                \"A General Framework for Multi-fidelity Bayesian Optimization with Gaussian Processes\",\n                \"Survey of multifidelity methods in uncertainty propagation, inference, and optimization\",\n                \"Predicting the output from a complex computer code when fast approximations are available\",\n                \"Special section on multidisciplinary design optimization: Metamodeling in multidisciplinary design optimization: How far have we really come?\",\n                \"High-performance integrated control of water quality and quantity in urban water reservoirs\",\n                \"The Design and Analysis of Computer Experiments\",\n                \"Comparison of surrogate-based uncertainty quantification methods for computationally expensive simulators\",\n                \"Bayesian emulation of complex multi-output and dynamic computer models\",\n                \"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations\",\n                \"Construction of response surface approximations for design optimization\",\n                \"Reduced-order modeling of time-dependent PDEs with multiple parameters in the boundary data\",\n                \"Review of multi-fidelity models\",\n                \"Aerodynamic shape optimization by variable-fidelity computational fluid dynamics models: A review of recent progress\",\n                \"Multi-fidelity Gaussian process regression for computer experiments\",\n                \"Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling\",\n                \"Multi-fidelity Gaussian process regression for prediction of random fields\",\n                \"Deep Gaussian processes\",\n                \"A stochastic collocation algorithm with multifidelity models\",\n                \"Greedy nonlinear autoregression for multifidelity computer models at different scales\",\n                \"Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control\",\n                \"Deep Gaussian processes for multi-fidelity modeling\",\n                \"Gaussian Processes for Machine Learning\",\n                \"Bayesian calibration of computer models\",\n                \"Shared-GP: Learning interpretable shared hidden structure across data spaces for design space analysis and exploration\",\n                \"Kernels for vector-valued functions: A review\",\n                \"A new multi-task learning framework for fuel cell model outputs in high-dimensional spaces\",\n                \"Doubly stochastic variational inference for deep gaussian processes\",\n                \"Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences\",\n                \"Improved error bounds for scattered data interpolation by radial basis functions\",\n                \"Improving the sample complexity using global data\",\n                \"Information-theoretic regret bounds for Gaussian process optimization in the bandit setting\",\n                \"On Kernelized Multi-armed Bandits\",\n                \"Exponential regret bounds for Gaussian process bandits with deterministic observations\",\n                \"On prediction properties of kriging: Uniform error bounds and robustness\",\n                \"Uniform Error and Posterior Variance Bounds for Gaussian Process Regression with Application to Safe Control\",\n                \"Random fields and geometry\",\n                \"Convergence rate of the data-independent p-greedy algorithm in kernel-based approximation\",\n                \"Emulation of physical processes with Emukit\",\n                \"Computational materials science: An introduction\",\n                \"Generated Reference Title 39 for [ref]25\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]25 based on the context and research area of Residual Gaussian Process: A Tractable Nonparametric Bayesian Emulator for Multi-fidelity Simulations. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}