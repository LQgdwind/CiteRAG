{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Automated Speaker Independent Visual Speech Recognition: A Comprehensive Survey\n\nPaper Abstract: \n\nPaper Section Text:\nPreprocessing and feature extraction are essential steps in the process of speaker independent VSR. Preprocessing aims to enhance the quality of the data by eliminating unwanted distortions or enhancing specific visual properties necessary for subsequent processing and analysis tasks. Figure 6 depicts the face localization and preprocessing process used for extracting the lip region of interest. This section outlines the various techniques used for dataset preprocessing and lip region of interest extraction. The concept of fuzzy clustering, which allows a data point to belong to more than one cluster, has become increasingly important in many DL classification tasks   . In the context of VSR, a novel technique for lip contour extraction was proposed by Srinivasa Rao Chalamala et al. , which employs fuzzy clustering with active contour modeling (ACM)  and elliptic shape information. The proposed approach involves the extraction of lip structure using the fuzzy membership distribution, followed by the application of an ellipse function to refine the extracted lip structure. The authors defined their base model as an ellipse enclosing the lip area, which was then used to develop active contours to precisely match the lip’s contour. By combining image and model-based lip contour extractions, the proposed technique aims to increase the accuracy of lip contour extraction for VSR tasks. Overall, this research presents a promising approach to improve lip contour extraction accuracy for VSR, utilizing the concepts of fuzzy clustering and active contour modeling. The use of an ellipse function to refine the lip structure is a novel addition to existing techniques, and the combination of image and model-based extractions is expected to provide more robust and accurate results. However, it is noteworthy that the authors do not explicitly address the effectiveness of their proposed approach across different populations. Specifically, they do not provide explicit insights into whether the method performs consistently well across individuals of different ages, ethnicities, or other demographic factors. The researchers contend that the ACM, when applied solely by itself, fails to yield accurate lip contours. They specifically highlight scenarios in which the pixel values in the lip region and its surrounding areas lack distinct separation, making it challenging for the ACM to accurately delineate the lip boundaries. This aspect leaves room for further investigation and validation of the approach’s generalizability and applicability to diverse populations. Chengjia Yang et al.  proposed an improved version of the region growing algorithm  which utilizes the RGB colorspace for lip contour extraction. The proposed method estimates the association between the color form and adjacent pixels, thereby automatically selecting seed pixels. Subsequently, the watershed algorithm is employed after a series of image processing operations to extract the lip contour region. The experimental analysis conducted on the proposed approach demonstrates its straightforward nature and effectiveness in extracting the lip contour region, achieving an accuracy rate of 80%. Notably, the approach leverages the full RGB color space, eliminating the necessity for color space conversion, which in turn saves both memory space and computing time. Given the absence of comprehensive details regarding the customized dataset utilized in the study, it becomes challenging to ascertain the generalizability of the methodology across different ethnicities. This lack of specificity pertaining to the dataset poses limitations on the broader applicability and effectiveness of the proposed approach across diverse ethnic populations. Sukesh Kumar Das et al.  proposed an automated mechanism for lip contour detection, utilizing segmentation based on pixel characteristics and piecewise polynomial interpolation. The initial RGB input color frame undergoes binary image thresholding. The region of interest (ROI) is identified by constructing a mathematical model that divides the data into two distinct clusters, namely, lip pixels and non-lip pixels, with discriminatory color information. The experimental analysis indicates that the performance of this proposed methodology is slightly better for females (accuracy = 99.46%) than for males (accuracy = 98.84%) due to the greater color discriminant intensity weights between female subjects’ lips and non-lip areas. Although it is important to acknowledge the limitations of solely relying on the performance on a single dataset, the successful results obtained on the GRID Database provide encouraging evidence of the methodology’s efficacy. As the GRID Database consists of recordings from individuals of various ethnic backgrounds, the methodology’s high accuracy on this dataset implies its potential for generalization across different ethnicities. Active contour is a popular segmentation technique in image processing that employs energy forces and limitations to extract essential pixels from an image for further analysis and processing. The primary purpose of active contours is to define smooth shapes in images and identify asymmetrical shapes. Xin Liu et al.  proposed a localized active contour model with automatic parameter assignment for lip segmentation. The proposed technique involves identifying the lips’ initial edge landmarks to determine the lips’ minimum-bounding ellipse. At each iteration, local energies are computed at each point along the curve, resulting in the deformation of the evolving curve. The iterations continue until the minimum bounding ellipse is the best fit. With an impressive accuracy rate of 96.4%, the authors confidently assert that their methodology successfully extracted lip contours, even in cases involving deformable or irregular lips. The proposed approach exhibits a higher tolerance towards challenges such as uneven illumination, rotation, deformation, and the presence of teeth and tongue, which can significantly impact the accuracy of lip contour extraction. Moreover, the authors conducted a thorough examination of the unsatisfactory results, which accounted for only 3.6% of the cases, and identified that these instances were primarily attributed to either poor contrast between the lip and surrounding skin regions or the presence of noticeable beard effects around the lips. In contrast to typical ACMs with global information, Chin et al.  proposed a region-based ACM with local energy terms, assisted by the watershed algorithm. The framework includes localized energy estimation based on watershed segmentation, delivering a more satisfactory contour extraction mechanism than global energies computation. Additionally, a modified H mechanism is presented to efficiently identify lip feature landmarks, which counteracts the delicate nature of ACM with the initial contour position. The proposed framework is experimented with the CUAVE and XM2VTS benchmark video datasets, and results show a Percentage of Overlap (POL) of 90% and 86.2%, respectively.\nThe authors assert that their proposed system surpasses existing methodologies when confronted with scenarios where the color differences between the lips and surrounding skin regions are indistinct. The localized analysis employed in their approach, which focuses on local details rather than solely seeking global energies, proves to be more effective in these challenging situations. Additionally, the authors claim that their algorithm demonstrates superior lip contour detection for subjects with darker skin tones and individuals with facial hair, specifically mentioning black subjects. Since the experimentation encompassed two benchmark datasets, it can be reasonably concluded that the proposed algorithm exhibits proper generalization across different ethnicities. Alan Wee-Chung Liew et al.  proposed a spatial fuzzy clustering algorithm  for binary-class clustering and segmentation tasks. The algorithm leverages feature space data and the spatial associations between adjacent pixels to achieve successful segmentation. This algorithm includes appropriate preprocessing and postprocessing phases to enhance the image quality further. According to the results obtained, the proposed algorithm demonstrated successful segmentation, achieving a POL of 95% or a Segmentation Error (SE) of 5%. However, it is important to note that a portion of the error can be attributed to the mismatch between the parametric lip model employed and the actual lip characteristics. Based on the author’s experience, this type of error typically accounts for approximately 2% to 5% of both POL and SE, with the specific percentage varying depending on the actual shape of the lips. It should be noted that the algorithm does not incorporate the consideration of facial hair, such as mustaches and beards. Shu-Hung Leung et al.  introduced a spatial-color-based fuzzy clustering framework for lip region segmentation. This framework called fuzzy c-means with shape function (FCMS), utilizes color information and spatial distance to extract the lip region. The proposed framework presents a new dissimilarity measure incorporating the shape function in its embeddings, which improves the segmentation performance. The framework’s implementation involves color transformation, image preprocessing, lip extraction with FCMS, and postprocessing. However, the computational time complexity of the proposed FCMS is nearly 1.78 times that of the classical fuzzy c-means (FCM) algorithm. In conclusion, the research papers by Alan Wee-Chung Liew et al.  and Shu-Hung Leung et al.  introduce novel approaches for image segmentation using spatial fuzzy clustering algorithms. These techniques are valuable in extracting essential features from an image by leveraging spatial and color information. The proposed frameworks demonstrate improved segmentation performance, but the computational complexity should be considered when applying them in real-world applications. Yao Wen Juan et al.  proposed a real-time lip localization framework comprising two phases: lip region location and lip tracking. Firstly, the framework detects the face and eyes using AdaBoost   and Haar Features   and, based on their association with the mouth, locates the lips. Subsequently, the lips are tracked using the a𝑎\\mathit{a} component of the Lab color space. In the study, it is reported that the proposed lip tracking method achieved accurate tracking for 98% of the lips. The authors substantiate this by demonstrating that the ac​o​m​p​o​n​e​n​tsubscript𝑎𝑐𝑜𝑚𝑝𝑜𝑛𝑒𝑛𝑡a_{component} of the lab color space exhibits superior separation ability compared to other color space components. Additionally, the authors highlight the successful improvement of their lip location method in enhancing lip tracking performance across various lighting conditions, diverse lip shapes, and different head poses. In a similar vein, a color-based geometrical algorithm for lip region extraction was proposed by Shemshaki et al. . This algorithm defines geometrical rules for skin and lip distributions in the chromatic and YCbCr color spaces, respectively, to extract pixels containing skin and lip separately in an image. After segmenting the lip region, the proposed algorithm further improves its accuracy by performing perpendicular and horizontal accumulation curves, achieving a detection rate of 93.33%. The results obtained from the study provide evidence that the proposed method is capable of accurately detecting the lip region in images with complex backgrounds. The authors also claim that the method demonstrates efficiency in terms of both implementation and execution time, making it a practical solution. Additionally, the proposed method exhibits high precision in detecting lips in images with rotated faces. It should be noted that the study does not provide specific information regarding the performance of the methodology on different ethnicities. The authors do not explicitly address the methodology’s effectiveness or generalizability across diverse ethnic populations. Xinjun et al.  proposed an improved jumping-snake model for lip segmentation that considers the geometric parameters of the lips. This model includes a division-detection procedure, which involves the estimation of the lip region and the detection of lip landmarks. The method starts by utilizing the frontal face template matching technique to segment the lips, which is followed by the application of the improved jumping-snake technique to identify the lip landmarks. The validation of the methodology on a customized dataset consisting of Chinese speakers, with an average accuracy of 90%, is indeed valuable information. However, it is important to recognize that the absence of other ethnic speakers in the dataset limits the ability to directly assess the performance of the methodology on different ethnicities. Another approach to lip segmentation is the use of dynamic deformable templates, which was proposed by Zhiyong Wu et al. . This method utilizes different templates based on varying lighting conditions, head poses, and lip shapes. For variable illumination conditions, a nostril-searching technique is employed for lip segmentation. For variable head poses, the lips are tracked on facial images with the inclusion of a dynamic mechanism for conventional systems. Finally, for variable lip shapes, the lips are tracked based on the contours dynamically. The proposed method was evaluated on the CU-TTAVS dataset and achieved promising results. By incorporating ”dynamicity” into the deformable templates, the proposed method addresses the challenges posed by changes in head pose, face illumination, and lip shapes. These variations can significantly impact the accuracy and reliability of lip-tracking algorithms. The dynamic deformable templates, being adaptable and flexible, effectively account for these variations, resulting in improved tracking performance. These approaches demonstrate the advancements in lip segmentation techniques and highlight the importance of considering various factors such as geometric parameters, lighting conditions, head poses, and lip shapes in achieving accurate lip segmentation. Such advances in lip segmentation techniques can have practical applications in various fields, such as speech recognition, facial expression analysis, and biometric identification. Improving the accuracy of lip tracking and segmentation remains a challenging task, especially for extremely low-contrast images. To tackle this problem, Darvish Zadeh et al.  proposed a novel approach that utilizes an improved particle filter lip tracker based on fuzzy logic. The method is suitable for non-linear modeling and is appropriate for dealing with non-Gaussian noise. The proposed technique involves sampling lips using a modified particle filter, and the evaluation of features is accomplished based on fuzzy rules. The results of the proposed particle filter lip tracker indicate that the detection rate is approximately 78% within a tolerance of 5% of distances, which is a promising outcome. It is important to note that the methodology under consideration was validated using a customized dataset. While this validation provides valuable insights into the performance of the method on the specific dataset, it does not provide direct information about the methodology’s effectiveness on different ethnicities. In addition, Behrooz et al. [ref]62 proposed an improved lip-map algorithm for lip region segmentation. In this approach, the lower half part of the image is considered as the input, followed by an improved lip map algorithm, which is further transformed into a wavelet form. The algorithm emphasizes the lip region during these phases, which helps to determine the lip regions and locations accurately. Finally, the lip pixels are extracted from the skin pixels with Top Hat transformation. A significant advantage of this work lies in its evaluation on multiple benchmark datasets, which provides evidence of its ability to generalize across different ethnicities. By evaluating the methodology on various datasets, researchers were able to assess its performance in diverse scenarios and populations, thereby enhancing confidence in its generalizability. Furthermore, the reported high accuracies of 99.1%, 98.0%, 98.5%, and 97.8% on the CVL, IMM, GTAV, and CID databases, respectively, highlight the remarkable efficiency of the methodology. In our survey, we recognize the importance of analyzing the feature extraction techniques employed on widely-used datasets, such as the LRW dataset. To address this, we included an overview of the feature extraction works that have been performed on this dataset. We acknowledge the significance of this dataset in the field of VSR and aim to provide a comprehensive analysis of the techniques employed for feature extraction. Ma et al.  utilized the RetinaFace tracker  to detect the faces present in the video frames. Following this, the authors utilized the Face Alignment Network (FAN)  for the detection of facial landmarks, which helps to accurately locate the lip area in each frame. They also normalized the frames by removing any size and rotation differences by registering the faces to the mean face in the training set. In order to focus on the mouth region of the face, they cropped a bounding box of size 96 × 96 around the lip area. Furthermore, they applied normalization to each frame by subtracting the mean and dividing by the standard deviation of the training set. This helped them to standardize the data and ensure that the model could effectively learn from the input data without being biased towards any particular distribution. Similar methods of ROI extraction were stated in    Facial feature extraction plays a vital role in the success of speaker independent VSR systems, and accurate localization and extraction are critical components of facial recognition pipelines. The section highlights various methods used for facial feature extraction and face localization, with Table II summarizing the methodologies of different techniques. The datasets used in these works vary from VidTimit, GRID, CVL, GTAV, XM2VTS, AR face, CR face, dual-mode video database, Bao Image database, CU-TTAVS, CID, IMM, to LRW. Most of these datasets contain a large number of images or videos, with different angles, poses, and expressions of the subjects. Fuzzy clustering has emerged as a popular choice for lip segmentation due to its ability to accurately classify pixels as part of the lip or non-lip area, even when the lip area is partially obscured or blended with the surrounding skin. Furthermore, fuzzy clustering can handle variations in lighting and noise, making it a practical choice for lip segmentation in real-world environments. Regarding lip contour detection, the majority of works used active contour models, while some utilized the watershed algorithm. Active contour models excel at handling variations in lip texture, shape, and illumination, making them well-suited for lip contour detection in challenging conditions. Moreover, active contour models can adapt to lip shape and motion, making them ideal for tracking lip movements over time. These models can also detect subtle lip movements that are crucial for accurate VSR. It is also essential to understand the computational complexity of It is worth noting that accurate facial feature extraction requires a correct measure to represent the extracted features. This measure can be in the form of feature vectors that represent the unique characteristics of each face. Therefore, it is essential to use appropriate methods for feature extraction to ensure that the feature vectors accurately represent the individual’s facial features. In the subsequent section, we will focus on different techniques used for transforming the feature vectors obtained from feature extraction into suitable forms for various VSR architectures.\n\nReference markers found: [62]\n\nContext windows for each [ref]:\n[ref]62: into the performance of the method on the specific dataset, it does not provide direct information about the methodology’s effectiveness on different ethnicities. In addition, Behrooz et al. [ref]62 proposed an improved lip-map algorithm for lip region segmentation. In this approach, the lower half part of the image is considered as the input, followed by an improved lip map algorithm,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 62,\n            \"titles\": [\n                \"Training Strategies For Improved Lip-reading\",\n                \"A review on speech recognition technique\",\n                \"Speech recognition by machine: A review\",\n                \"Discrete cosine transform\",\n                \"Principal component analysis\",\n                \"Discrete wavelet transforms: Theory and implementation\",\n                \"Robust biometric person identification using automatic classifier fusion of speech, mouth, and face experts\",\n                \"Lip as biometric and beyond: a survey\",\n                \"The lip as a biometric\",\n                \"Deep learning based assistive technology on audio visual speech recognition for hearing impaired\",\n                \"Multimedia content analysis-using both audio and visual clues\",\n                \"“Some normative data on lip-reading skills (l)\",\n                \"“Viseme definitions comparison for visual-only speech recognition\",\n                \"Decoding visemes: improving machine lip-reading\",\n                \"“The syllabic phonemes of english\",\n                \"“Frequency of occurrence of phonemes in conversational english\",\n                \"“Facial action recognition for facial expression analysis from static face images\",\n                \"“On the relationship between face movements\",\n                \"“Is lip region-of-interest sufficient for lipreading?” in Proceedings of the 2022 International Conference on Multimodal Interaction\",\n                \"D. Bhattacharjee\",\n                \"“Roi processing for visual features extraction in lip-reading\",\n                \"“Feature extraction and normalization algorithms for high-density oligonucleotide gene expression array data\",\n                \"“A multi-scale feature extraction-based normalized attention neural network for image denoising\",\n                \"“A novel automatic method on pseudo-invariant features extraction for enhancing the relative radiometric normalization of high-resolution images\",\n                \"“Channel separability in the audio-visual integration of speech: A bayesian approach\",\n                \"“Combining evidence in personal identity verification systems\",\n                \"“Extraction of visual features for lipreading\",\n                \"“The challenge of multispeaker lip-reading\",\n                \"“Lipreading with local spatiotemporal descriptors\",\n                \"Lip Reading Sentences in the Wild\",\n                \"LRS3-TED: a large-scale dataset for visual speech recognition\",\n                \"“Temporal multimodal learning in audiovisual speech recognition\",\n                \"“A new visual speech recognition approach for rgb-d cameras\",\n                \"“Lip reading in the wild\",\n                \"“Lipreading model based on whole-part collaborative learning\",\n                \"A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading\",\n                \"A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning\",\n                \"“Cn-cvs: A mandarin audio-visual dataset for large vocabulary continuous visual to speech synthesis.”\",\n                \"“RUSAVIC corpus: Russian audio-visual speech in cars\",\n                \"OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]62 based on the context and research area of Automated Speaker Independent Visual Speech Recognition: A Comprehensive Survey. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}