{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Image Reconstruction of Static and Dynamic Scenes through Anisoplanatic Turbulence\n\nPaper Abstract: \n\nPaper Section Text:\nThe objective of the blind deconvolution step is to remove the residual blur in the lucky frame. This blur comes from three sources: (i) The diffraction limit of the optical system, which is usually not severe. (ii) Weighted temporal averaging in the lucky region fusion step. (iii) Limited number of available frames, which does not allow us to pick the sharpest lucky frame. Proposed approach. Following most of the literature on blind deconvolution, our proposed method starts with a pair of alternating minimization steps. The first step update the latent image, and the second step updates the unknown point spread function (PSF): In this pair of equations, 𝒚𝒚\\boldsymbol{y} is the entire lucky image which strictly speaking should be denoted by 𝒚^lucky​(t)subscriptbold-^𝒚lucky𝑡\\boldsymbol{\\widehat{y}}_{\\text{lucky}}(t). We dropped the time index because blind deconvolution is performed per frame. The variable 𝒉𝒉\\boldsymbol{h} is the unknown PSF, and 𝒛𝒛\\boldsymbol{z} is the latent clean image. The functions g​(⋅)𝑔⋅g(\\cdot) and r​(⋅)𝑟⋅r(\\cdot) are regularizations for 𝒛𝒛\\boldsymbol{z} and 𝒉𝒉\\boldsymbol{h}, respectively. In this paper, we adopt the Plug-and-Play prior  for g​(⋅)𝑔⋅g(\\cdot) with BM3D as the denoiser. The regularization r​(⋅)𝑟⋅r(\\cdot) will be discussed next. Additional details of the Plug-and-Play prior can be found in  [ref]32 . Modeling hℎ\\boldsymbol{h}. A major difference between a turbulence deconvolution and a standard deconvolution is that we have a well-defined model for the turbulence PSF as we discussed in the Background section. Specific to the context of turbulence, we highlight a few key points: The turbulence PSF is defined through the random phase distortion function ϕ​(𝒇)italic-ϕ𝒇\\phi(\\boldsymbol{f}) in the frequency domain. See [29, Eqs 6-9]. The random phase distortion ϕ​(𝒇)italic-ϕ𝒇\\phi(\\boldsymbol{f}) can be represented in the Zernike space via [29, Eq. 15]: with Zjsubscript𝑍𝑗Z_{j} being the j𝑗j-th Zernike Basis, and ajsubscript𝑎𝑗a_{j} is the corresponding Zernike coefficient. The Zernike coefficients have physical interpretations. E.g., a2subscript𝑎2a_{2} and a3subscript𝑎3a_{3} are the tilts, and a4subscript𝑎4a_{4} is the defocus, etc. The statistics of ajsubscript𝑎𝑗a_{j}’s are defined through intermodal and spatial correlations. See [29, Eq 20, Eq 33]. In order to encapsulate the physics of the PSFs (which are defined in the phase space), we propose the following linear expansion scheme by translating the distortions in the phase domain to the principal components in the spatial domain. Consider a simulator that generates a vector of random Zernike coefficients 𝒂={aj}j=1N𝒂superscriptsubscriptsubscript𝑎𝑗𝑗1𝑁\\boldsymbol{a}=\\{a_{j}\\}_{j=1}^{N}. Suppose the simulator has generated a set of M𝑀M vectors, {𝒂1,…,𝒂M}subscript𝒂1…subscript𝒂𝑀\\{\\boldsymbol{a}_{1},\\ldots,\\boldsymbol{a}_{M}\\}. Since every 𝒂isubscript𝒂𝑖\\boldsymbol{a}_{i} gives to a random phase, and every random phase corresponds to a random PSF, by generating M𝑀M Zernike coefficient vectors we will obtain a collection of M𝑀M random PSFs {𝒉1,…,𝒉M}subscript𝒉1…subscript𝒉𝑀\\{\\boldsymbol{h}_{1},\\ldots,\\boldsymbol{h}_{M}\\}. By decomposing the random PSFs using the principal component analysis (PCA), we can extract a set of basis representations {𝒖j}j=1psuperscriptsubscriptsubscript𝒖𝑗𝑗1𝑝\\{\\boldsymbol{u}_{j}\\}_{j=1}^{p} via Thus for every 𝒉𝒉\\boldsymbol{h}, there exist {wj}j=1psuperscriptsubscriptsubscript𝑤𝑗𝑗1𝑝\\{w_{j}\\}_{j=1}^{p} such that where wjsubscript𝑤𝑗w_{j} is the basis expansion coefficient. In other words, we converted the linear expansion of the phase numerically to a linear expansion of the PSF. This is a data-driven procedure, where the data comes from the physical model of the turbulence. Removing the tilts. The above linear expansion model is valid when 𝒚𝒚\\boldsymbol{y} is the raw turbulent distorted image. However, since 𝒚𝒚\\boldsymbol{y} is the lucky frame, additional modeling of the other parts of the pipeline is needed. Referring back to the random phase concept in (15), we note that since the optical flow has compensated for the pixel movements, the tilts of the phase can be approximately dropped.\nMore precisely, we consider a random phase ϕ​(𝒇)italic-ϕ𝒇\\phi(\\boldsymbol{f}) generated by a set of Zernike coefficients {a1,…,aN}subscript𝑎1…subscript𝑎𝑁\\{a_{1},\\ldots,a_{N}\\}. Using [29, Eq. 13], we define the tilt-compensated phase as for some linear terms 𝜶𝜶\\boldsymbol{\\alpha}. The new PSFs defined through the tilt-compensated phase φ​(𝒇)𝜑𝒇\\varphi(\\boldsymbol{f}) are center-aligned, which can be interpreted as outputs of the optical flow. When synthesizing the simulated training data, the tilts can be removed by setting a2=a3=0subscript𝑎2subscript𝑎30a_{2}=a_{3}=0. After removing the tilts, the lucky fusion step is used to pick select patches according to two criteria we described in the lucky fusion section. To model this during the training data synthesis, we note that the “wildness” of the PSF is determined by the magnitude of the Zernike coefficients as shown in Figure 7. Therefore, to obtain good patches, we perform an importance sampling by rejecting large Zernike coefficients. Specifically, we define 𝒂4:N={a4,…,aN}subscript𝒂:4𝑁subscript𝑎4…subscript𝑎𝑁\\boldsymbol{a}_{4:N}=\\{a_{4},\\ldots,a_{N}\\} as the vector of Zernike coefficients from a4subscript𝑎4a_{4} (because a2=a3=0subscript𝑎2subscript𝑎30a_{2}=a_{3}=0, and a1subscript𝑎1a_{1} is a constant which does not affect the shape of the PSF.) We check whether ‖𝒂4:N‖2≤κsuperscriptnormsubscript𝒂:4𝑁2𝜅\\|\\boldsymbol{a}_{4:N}\\|^{2}\\leq\\kappa for some threshold κ𝜅\\kappa. If the magnitude ‖𝒂4:N‖2superscriptnormsubscript𝒂:4𝑁2\\|\\boldsymbol{a}_{4:N}\\|^{2} is above the threshold, we say that the PSF should not be used to form the PCA basis because lucky fusion will likely skip this PSF. The proposed modeling of the optical flow and the lucky frame is justified by the optics and turbulence literature. In particular, tilt-compensation is a standard practice in adaptive optics where the tilt vector 𝜶𝜶\\boldsymbol{\\alpha} is measured by phase offsets. When averaged over many random realizations, the tilt-compensated PSFs will give the short-exposure PSF. The small coefficient approach is also justified. It was first analyzed by Fried in , where the threshold was 1 rad2 for the Karhunen–Loève expansion of the phase. In our case, the principle remains but the threshold is set empirically. Updating hℎ\\boldsymbol{h}. The update of the PSF in (14) now becomes The first step (18) defines the weight of the latent PSF. The prior for the weight is denoted as r​(𝒘)𝑟𝒘r(\\boldsymbol{w}) for some regularization function r​(⋅)𝑟⋅r(\\cdot). The second step (19) updates the PSF using the linear expansion concept. Depending on the prior r​(⋅)𝑟⋅r(\\cdot), e.g., ℓ1subscriptℓ1\\ell_{1} prior, (18) can be solved using standard optimization packages, e.g., CVX  or the alternating direction methods of multiplier  . Defining r​(w)𝑟𝑤r(\\boldsymbol{w}). The prior distribution r​(𝒘)𝑟𝒘r(\\boldsymbol{w}) can be determined empirically. Consider the set of generated PSFs {𝒉1,…,𝒉M}subscript𝒉1…subscript𝒉𝑀\\{\\boldsymbol{h}_{1},\\ldots,\\boldsymbol{h}_{M}\\}. For each PSF 𝒉isubscript𝒉𝑖\\boldsymbol{h}_{i}, we seek the sparest representation using the bases {𝒖j}j=1psuperscriptsubscriptsubscript𝒖𝑗𝑗1𝑝\\{\\boldsymbol{u}_{j}\\}_{j=1}^{p}. We are interested in such representation because during the blind deconvolution step, the current estimate of the PSF is never perfect. Sparse representation entails the most influential bases while rejecting potential residue errors. The regularization function is defined as where σisubscript𝜎𝑖\\sigma_{i} is the standard deviation of wisubscript𝑤𝑖w_{i} learned from data. This is equivalent to assuming that the prior distribution of 𝒘𝒘\\boldsymbol{w} follows a field-of-expert model: such that r​(𝒘)=−log⁡p​(𝒘)𝑟𝒘𝑝𝒘r(\\boldsymbol{w})=-\\log p(\\boldsymbol{w}). Note that the regularization in Equation (20) is a weight ℓ1subscriptℓ1\\ell_{1} norm. The ℓ1subscriptℓ1\\ell_{1} norm is used here as a compromise between model fidelity and complexity. Norms other than ℓ1subscriptℓ1\\ell_{1} can potentially lead to better fitting (as in the mean squared fitting error), but the computational complexity of solving the corresponding optimization will become higher. The choice of the ℓ1subscriptℓ1\\ell_{1} norm is further justified by the shape of the empirical distributions shown in Figure 8. We observe that the distributions indeed follow an exponentially decaying function, and has a tail heavier than a Gaussian. To find the sparse representation, we solve the following problem offline (during the training phase): where τ𝜏\\tau is a pre-define threshold. Note that Equation (22) is a standard ℓ0subscriptℓ0\\ell_{0} basis pursuit denoise problem, which can be solved using tools such as orthogonal matching pursuit . After solving the problem, we will have a collection of the expansion coefficients {𝒘(i)}i=1Msuperscriptsubscriptsuperscript𝒘𝑖𝑖1𝑀\\{\\boldsymbol{w}^{(i)}\\}_{i=1}^{M}. Plotting the empirical density of the collection {wj(1),…,wj(M)}superscriptsubscript𝑤𝑗1…superscriptsubscript𝑤𝑗𝑀\\{w_{j}^{(1)},\\ldots,w_{j}^{(M)}\\} will give us plots shown in (8), for each of the j𝑗j-th basis. The empirical distributions will then define the regularization function r​(𝒘)𝑟𝒘r(\\boldsymbol{w}).\n\nReference markers found: [32]\n\nContext windows for each [ref]:\n[ref]32: the Plug-and-Play prior  for g​(⋅)𝑔⋅g(\\cdot) with BM3D as the denoiser. The regularization r​(⋅)𝑟⋅r(\\cdot) will be discussed next. Additional details of the Plug-and-Play prior can be found in  [ref]32 . Modeling hℎ\\boldsymbol{h}. A major difference between a turbulence deconvolution and a standard deconvolution is that we have a well-defined model for the turbulence PSF as we discussed in\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 32,\n            \"titles\": [\n                \"Performance Analysis of Plug-and-Play ADMM: A Graph Signal Processing Perspective\",\n                \"Wave propagation in a turbulent medium\",\n                \"Imaging Through Turbulence\",\n                \"Anisoplanatism in adaptive optics\",\n                \"Removing atmospheric turbulence via space-invariant deconvolution\",\n                \"High-quality motion deblurring from a single image\",\n                \"Principles of Adaptive Optics\",\n                \"Attainment of Diffraction Limited Resolution in Large Telescopes by Fourier Analysing Speckle Patterns in Star Images\",\n                \"Recovery of Images from Atmospherically Degraded Short-Exposure Photographs\",\n                \"Speckle masking in astronomy: triple correlation theory and applications\",\n                \"Speckle imaging over horizontal paths\",\n                \"Statistical Optics\",\n                \"The Local Structure of Turbulence in Incompressible Viscous Fluid for Very Large Reynolds’ Numbers\",\n                \"Optical resolution through a randomly inhomogeneous medium for very long and very short exposures\",\n                \"Beyond pixels: Exploring new representations and applications for motion analysis.\",\n                \"Automated video enhancement from a stream of atmospherically-distorted images: the lucky-region fusion approach\",\n                \"Video stabilization of atmospheric turbulence distortion\",\n                \"Atmospheric turbulence mitigation using complex wavelet-based fusion\",\n                \"Wavelet Burst Accumulation for turbulence mitigation.\",\n                \"Block matching and wiener filtering approach to optical turbulence mitigation and its application to simulated and real imagery with quantitative error analysis\",\n                \"Removing turbulence effect via hybrid total variation and deformation-guided kernel regression\",\n                \"Restoration of Atmospheric Turbulence-distorted Images via RPCA and Quasiconformal Maps\",\n                \"Geometric correction of atmospheric turbulence-degraded video containing moving objects\",\n                \"Simultaneous video stabilization and moving object detection in turbulence\",\n                \"Atmospheric turbulence mitigation for sequences with moving objects using recursive image fusion\",\n                \"Probability of getting a lucky short-exposure image through turbulence∗∗\\\\ast\",\n                \"Atmospheric turbulence mitigation based on turbulence extraction\",\n                \"Deep learning for software-based turbulence mitigation in long-range imaging\",\n                \"Simulating anisoplanatic turbulence by sampling correlated Zernike coefficients\",\n                \"Plug-and-play ADMM for image restoration: Fixed-point convergence and applications\",\n                \"Plug-and-play methods for magnetic resonance imaging\",\n                \"Plug-and-play unplugged: optimization free reconstruction ufing consensus equilibrium\",\n                \"CVX: Matlab software for disciplined convex programming, version 2.1\",\n                \"Distributed optimization and statistical learning via the alternating direction method of multipliers\",\n                \"Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition\",\n                \"Efficient filter flow for space-variant multiframe blind deconvolution\",\n                \"Simulation of atmospheric turbulence effects and mitigation algorithms on stand-off automatic facial recognition\",\n                \"The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices\",\n                \"A Neural Approach to Blind Motion Deblurring\",\n                \"Motion blur kernel estimation via deep learning\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]32 based on the context and research area of Image Reconstruction of Static and Dynamic Scenes through Anisoplanatic Turbulence. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}