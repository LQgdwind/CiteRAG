{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: 1 Introduction\n\nPaper Abstract: \n\nPaper Section Text:\nDenoising diffusion model   defines a forward process that gradually adds noise to the data x0∼q​(x0)similar-tosubscript𝑥0𝑞subscript𝑥0x_{0}\\sim q(x_{0}). The process has T>0𝑇0T>0 steps and the noise is Gaussian. where q​(xt|xt−1)=𝒩​(xt;1−βt​xt−1,βt​I)𝑞conditionalsubscript𝑥𝑡subscript𝑥𝑡1𝒩subscript𝑥𝑡1subscript𝛽𝑡subscript𝑥𝑡1subscript𝛽𝑡𝐼q(x_{t}|x_{t-1})=\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}}x_{t-1},\\beta_{t}I) with the pre-defined variance schedule βtsubscript𝛽𝑡\\beta_{t}. Training denoising diffusion model is to estimate some parameters ϕitalic-ϕ\\phi of a reverse denoising process defined by: where pϕ​(xt−1|xt)=𝒩​(xt−1;μϕ​(xt,t),σt2​I)subscript𝑝italic-ϕconditionalsubscript𝑥𝑡1subscript𝑥𝑡𝒩subscript𝑥𝑡1subscript𝜇italic-ϕsubscript𝑥𝑡𝑡superscriptsubscript𝜎𝑡2𝐼p_{\\phi}(x_{t-1}|x_{t})=\\mathcal{N}(x_{t-1};\\mu_{\\phi}(x_{t},t),\\sigma_{t}^{2}I). The conventional training uses the maximum likelihood approach by maximizing\nthe evidence lower bound ℒ≤pϕ​(x0)=∫pϕ​(x0:T)​𝑑x1:Tℒsubscript𝑝italic-ϕsubscript𝑥0subscript𝑝italic-ϕsubscript𝑥:0𝑇differential-dsubscript𝑥:1𝑇\\mathcal{L}\\leq p_{\\phi}(x_{0})=\\int p_{\\phi}(x_{0:T})dx_{1:T}, which can be rewritten as: where K​L𝐾𝐿KL denotes the Kullback-Leibler divergence, and C𝐶C is a known constant. Implicit denoising model. To reduce the number of steps T𝑇T for faster generation, denoising diffusion GANs [ref]48 proposes to use the implicit denoising model pϕ​(xt−1|xt)=∫p​(ϵ)​q​(xt−1|xt,x0=Gϕ​(xt,ϵ,t))subscript𝑝italic-ϕconditionalsubscript𝑥𝑡1subscript𝑥𝑡𝑝italic-ϵ𝑞conditionalsubscript𝑥𝑡1subscript𝑥𝑡subscript𝑥0subscript𝐺italic-ϕsubscript𝑥𝑡italic-ϵ𝑡p_{\\phi}(x_{t-1}|x_{t})=\\int p(\\epsilon)q(x_{t-1}|x_{t},x_{0}=G_{\\phi}(x_{t},\\epsilon,t)) where ϵ∼𝒩​(0,I)similar-toitalic-ϵ𝒩0𝐼\\epsilon\\sim\\mathcal{N}(0,I). After that, [ref]48 use adversarial training to estimate the parameters i.e, where Da​d​vsubscript𝐷𝑎𝑑𝑣D_{adv} is the GAN objective or the Jensen Shannon divergence [ref]17. Augmented Generalized Mini-batch Energy distances. In this paper, we replace Da​d​vsubscript𝐷𝑎𝑑𝑣D_{adv} by the generalized mini-batch Energy distance . In particular, for two measures μ𝜇\\mu and ν𝜈\\nu, and mini-batch size m≥1𝑚1m\\geq 1, we have the generalized Mini-batch Energy distance with SW kernel: where X,X′​∼i.i.d​μ⊗m𝑋superscript𝑋′formulae-sequence𝑖𝑖𝑑similar-tosuperscript𝜇tensor-productabsent𝑚X,X^{\\prime}\\overset{i.i.d}{\\sim}\\mu^{\\otimes m} and X,X′​∼i.i.d​ν⊗m𝑋superscript𝑋′formulae-sequence𝑖𝑖𝑑similar-tosuperscript𝜈tensor-productabsent𝑚X,X^{\\prime}\\overset{i.i.d}{\\sim}\\nu^{\\otimes m}, PX=1m​∑i=1mδxisubscript𝑃𝑋1𝑚superscriptsubscript𝑖1𝑚subscript𝛿subscript𝑥𝑖P_{X}=\\frac{1}{m}\\sum_{i=1}^{m}\\delta_{x_{i}} with X=(x1,…,xm)𝑋subscript𝑥1…subscript𝑥𝑚X=(x_{1},\\ldots,x_{m}), and 𝒟𝒟\\mathcal{D} is a SW variant. In the denoising diffusion case, μ𝜇\\mu is q​(xt−1|xt)𝑞conditionalsubscript𝑥𝑡1subscript𝑥𝑡q(x_{t-1}|x_{t}) and ν𝜈\\nu is pϕ(xt−1|xt))p_{\\phi}(x_{t-1}|x_{t})) for t=1,…,T𝑡1…𝑇t=1,\\ldots,T. Although the generalized Mini-batch Energy distance can guarantee statistical convergence, it is known to be not good in practical training due to the weak training signal (lack of non-linearity which is essential for images) . To address the issue, we propose the augmented generalized Mini-batch Energy distance:\nA​G​M​E𝒟2​(μ,ν;g)=G​M​E𝒟2​(μ¯,ν¯),𝐴𝐺𝑀superscriptsubscript𝐸𝒟2𝜇𝜈𝑔𝐺𝑀superscriptsubscript𝐸𝒟2¯𝜇¯𝜈AGME_{\\mathcal{D}}^{2}(\\mu,\\nu;g)=GME_{\\mathcal{D}}^{2}(\\bar{\\mu},\\bar{\\nu}),\nwhere μ¯=f​♯​μ¯𝜇𝑓♯𝜇\\bar{\\mu}=f\\sharp\\mu and ν¯=f​♯​ν¯𝜈𝑓♯𝜈\\bar{\\nu}=f\\sharp\\nu with f​(x)=(x,g​(x))𝑓𝑥𝑥𝑔𝑥f(x)=(x,g(x)) for g:ℝd→ℝ:𝑔→superscriptℝ𝑑ℝg:\\mathbb{R}^{d}\\to\\mathbb{R} is a non-linear function. For any g𝑔g, A​G​M​E𝒟2​(μ,ν;g)𝐴𝐺𝑀superscriptsubscript𝐸𝒟2𝜇𝜈𝑔AGME_{\\mathcal{D}}^{2}(\\mu,\\nu;g) is still a valid distance since f𝑓f is injective. In the diffusion model setting, we train gγsubscript𝑔𝛾g_{\\gamma} (parameterized by γ𝛾\\gamma) as a time-conditional discriminator as in [48, Equation (5)]. We refer the reader to Algorithm 3 in Appendix C for a detailed algorithm, and to Appendix D for more detailed experimental settings. Results. We follow the setting in [ref]48 for diffusion models on CIFAR10 [ref]23 with N=1800𝑁1800N=1800 epochs. We set L=104𝐿superscript104L=10^{4} for SW, DSW, EBSW, RPSW, and IWRPSW. We set T∈{2,5,10}𝑇2510T\\in\\{2,5,10\\} for DSW and T∈{100,1000}𝑇1001000T\\in\\{100,1000\\} for Max-SW. It is worthing noting that the training time of Max-SW and DSW is more than two times the time for SW, EBSW, RPSW, and IWRPSW, hence, we cannot increase the value of T𝑇T further. We adjust concentration parameter κ𝜅\\kappa of the PS distribution for each epoch t𝑡t as κ​(t)=(κ0−1)​(N−t−1N−1)10+1𝜅𝑡subscript𝜅01superscript𝑁𝑡1𝑁1101\\kappa(t)=(\\kappa_{0}-1)\\left(\\frac{N-t-1}{N-1}\\right)^{10}+1 with N=1800𝑁1800N=1800 and κ0∈{100,50}subscript𝜅010050\\kappa_{0}\\in\\{100,50\\}. We report the FID scores , the number of function evaluations, and the generation time in Table 1. Using an implicit denoising model with only 444 time steps, our SW diffusion models have a much faster generation time compared to existing diffusion models such as DDPM, Score SDE, DDIM, Probability Flow, and LSGM. Compared to DDGAN with the same 444 time steps, our SW diffusion models lead to a lower FID score and the same generation time. Among SW variants, the proposed RPSW and IWRPSW achieve the lowest scores of 2.82 and 2.70 in turn. We show some random generated images from DDGAN, RPSW-DD, and IWRPSW-DD in Figure 2 as qualitative results.\nOverall, AGME distance with RPSW and IWRPSW kernels could be a potential effective loss for improving training implicit diffusion models.\n\nReference markers found: [48]\n\nContext windows for each [ref]:\n[ref]48: as: where K​L𝐾𝐿KL denotes the Kullback-Leibler divergence, and C𝐶C is a known constant. Implicit denoising model. To reduce the number of steps T𝑇T for faster generation, denoising diffusion GANs [ref]48 proposes to use the implicit denoising model\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 48,\n            \"titles\": [\n                \"Sliced Wasserstein Variational Inference\",\n                \"Sliced Optimal Partial Transport\",\n                \"Partial Transport for Point-Cloud Registration\",\n                \"Spherical sliced-Wasserstein\",\n                \"Hyperbolic sliced-wasserstein via geodesic and horospherical projections\",\n                \"Efficient gradient flows in sliced-Wasserstein space\",\n                \"Sliced-Wasserstein on Symmetric Positive Definite Matrices for M/EEG Signals\",\n                \"Spot: sliced partial optimal transport\",\n                \"Sliced and Radon Wasserstein barycenters of measures\",\n                \"Unidimensional and evolution methods for optimal transportation\",\n                \"Partial Adversarial Domain Adaptation\",\n                \"The Power Spherical distribution\",\n                \"Max-Sliced Wasserstein Distance and its use for GANs\",\n                \"Generative modeling using the sliced Wasserstein distance\",\n                \"A probabilistic theory of pattern recognition, volume 31\",\n                \"Interpolating between optimal transport and MMD using Sinkhorn divergences\",\n                \"Pot: Python optimal transport\",\n                \"Generative adversarial nets\",\n                \"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium\",\n                \"Denoising Diffusion Probabilistic Models\",\n                \"Maximum likelihood estimators for the matrix von Mises-Fisher and bingham distributions\",\n                \"Auto-encoding variational bayes\",\n                \"Sliced Wasserstein Distance for Learning Gaussian Mixture Models\",\n                \"Learning multiple layers of features from tiny images\",\n                \"Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation\",\n                \"A convexity principle for interacting gases\",\n                \"Approximate Bayesian computation with the sliced-Wasserstein distance\",\n                \"Statistical and Topological Properties of Sliced Probability Divergences\",\n                \"Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance\",\n                \"Quasi-monte carlo for 3d sliced wasserstein\",\n                \"Energy-Based Sliced Wasserstein Distance\",\n                \"Distributional sliced-Wasserstein and applications to generative modeling\",\n                \"Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction\",\n                \"Markovian Sliced Wasserstein Distances: Beyond Independent Projections\",\n                \"Statistical, Robustness, and Computational Guarantees for Sliced Wasserstein Distances\",\n                \"Sliced optimal transport sampling\",\n                \"Computational optimal transport\",\n                \"Improving GANs using optimal transport\",\n                \"Optimal transport for applied mathematicians\",\n                \"Lidar Upsampling with Sliced Wasserstein Distance\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]48 based on the context and research area of 1 Introduction. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}