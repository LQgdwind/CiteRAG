{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: 1 Introduction\n\nPaper Abstract: \n\nPaper Section Text:\nDenoising diffusion modelÂ   defines a forward process that gradually adds noise to the data x0âˆ¼qâ€‹(x0)similar-tosubscriptğ‘¥0ğ‘subscriptğ‘¥0x_{0}\\sim q(x_{0}). The process has T>0ğ‘‡0T>0 steps and the noise is Gaussian. where qâ€‹(xt|xtâˆ’1)=ğ’©â€‹(xt;1âˆ’Î²tâ€‹xtâˆ’1,Î²tâ€‹I)ğ‘conditionalsubscriptğ‘¥ğ‘¡subscriptğ‘¥ğ‘¡1ğ’©subscriptğ‘¥ğ‘¡1subscriptğ›½ğ‘¡subscriptğ‘¥ğ‘¡1subscriptğ›½ğ‘¡ğ¼q(x_{t}|x_{t-1})=\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}}x_{t-1},\\beta_{t}I) with the pre-defined variance schedule Î²tsubscriptğ›½ğ‘¡\\beta_{t}. Training denoising diffusion model is to estimate some parameters Ï•italic-Ï•\\phi of a reverse denoising process defined by: where pÏ•â€‹(xtâˆ’1|xt)=ğ’©â€‹(xtâˆ’1;Î¼Ï•â€‹(xt,t),Ïƒt2â€‹I)subscriptğ‘italic-Ï•conditionalsubscriptğ‘¥ğ‘¡1subscriptğ‘¥ğ‘¡ğ’©subscriptğ‘¥ğ‘¡1subscriptğœ‡italic-Ï•subscriptğ‘¥ğ‘¡ğ‘¡superscriptsubscriptğœğ‘¡2ğ¼p_{\\phi}(x_{t-1}|x_{t})=\\mathcal{N}(x_{t-1};\\mu_{\\phi}(x_{t},t),\\sigma_{t}^{2}I). The conventional training uses the maximum likelihood approach by maximizing\nthe evidence lower bound â„’â‰¤pÏ•â€‹(x0)=âˆ«pÏ•â€‹(x0:T)â€‹ğ‘‘x1:Tâ„’subscriptğ‘italic-Ï•subscriptğ‘¥0subscriptğ‘italic-Ï•subscriptğ‘¥:0ğ‘‡differential-dsubscriptğ‘¥:1ğ‘‡\\mathcal{L}\\leq p_{\\phi}(x_{0})=\\int p_{\\phi}(x_{0:T})dx_{1:T}, which can be rewritten as: where Kâ€‹Lğ¾ğ¿KL denotes the Kullback-Leibler divergence, and Cğ¶C is a known constant. Implicit denoising model. To reduce the number of steps Tğ‘‡T for faster generation, denoising diffusion GANsÂ [ref]48 proposes to use the implicit denoising model pÏ•â€‹(xtâˆ’1|xt)=âˆ«pâ€‹(Ïµ)â€‹qâ€‹(xtâˆ’1|xt,x0=GÏ•â€‹(xt,Ïµ,t))subscriptğ‘italic-Ï•conditionalsubscriptğ‘¥ğ‘¡1subscriptğ‘¥ğ‘¡ğ‘italic-Ïµğ‘conditionalsubscriptğ‘¥ğ‘¡1subscriptğ‘¥ğ‘¡subscriptğ‘¥0subscriptğºitalic-Ï•subscriptğ‘¥ğ‘¡italic-Ïµğ‘¡p_{\\phi}(x_{t-1}|x_{t})=\\int p(\\epsilon)q(x_{t-1}|x_{t},x_{0}=G_{\\phi}(x_{t},\\epsilon,t)) where Ïµâˆ¼ğ’©â€‹(0,I)similar-toitalic-Ïµğ’©0ğ¼\\epsilon\\sim\\mathcal{N}(0,I). After that,Â [ref]48 use adversarial training to estimate the parameters i.e, where Daâ€‹dâ€‹vsubscriptğ·ğ‘ğ‘‘ğ‘£D_{adv} is the GAN objective or the Jensen Shannon divergenceÂ [ref]17. Augmented Generalized Mini-batch Energy distances. In this paper, we replace Daâ€‹dâ€‹vsubscriptğ·ğ‘ğ‘‘ğ‘£D_{adv} by the generalized mini-batch Energy distanceÂ . In particular, for two measures Î¼ğœ‡\\mu and Î½ğœˆ\\nu, and mini-batch size mâ‰¥1ğ‘š1m\\geq 1, we have the generalized Mini-batch Energy distance with SW kernel: where X,Xâ€²â€‹âˆ¼i.i.dâ€‹Î¼âŠ—mğ‘‹superscriptğ‘‹â€²formulae-sequenceğ‘–ğ‘–ğ‘‘similar-tosuperscriptğœ‡tensor-productabsentğ‘šX,X^{\\prime}\\overset{i.i.d}{\\sim}\\mu^{\\otimes m} and X,Xâ€²â€‹âˆ¼i.i.dâ€‹Î½âŠ—mğ‘‹superscriptğ‘‹â€²formulae-sequenceğ‘–ğ‘–ğ‘‘similar-tosuperscriptğœˆtensor-productabsentğ‘šX,X^{\\prime}\\overset{i.i.d}{\\sim}\\nu^{\\otimes m}, PX=1mâ€‹âˆ‘i=1mÎ´xisubscriptğ‘ƒğ‘‹1ğ‘šsuperscriptsubscriptğ‘–1ğ‘šsubscriptğ›¿subscriptğ‘¥ğ‘–P_{X}=\\frac{1}{m}\\sum_{i=1}^{m}\\delta_{x_{i}} with X=(x1,â€¦,xm)ğ‘‹subscriptğ‘¥1â€¦subscriptğ‘¥ğ‘šX=(x_{1},\\ldots,x_{m}), and ğ’Ÿğ’Ÿ\\mathcal{D} is a SW variant. In the denoising diffusion case, Î¼ğœ‡\\mu is qâ€‹(xtâˆ’1|xt)ğ‘conditionalsubscriptğ‘¥ğ‘¡1subscriptğ‘¥ğ‘¡q(x_{t-1}|x_{t}) and Î½ğœˆ\\nu is pÏ•(xtâˆ’1|xt))p_{\\phi}(x_{t-1}|x_{t})) for t=1,â€¦,Tğ‘¡1â€¦ğ‘‡t=1,\\ldots,T. Although the generalized Mini-batch Energy distance can guarantee statistical convergence, it is known to be not good in practical training due to the weak training signal (lack of non-linearity which is essential for images)Â . To address the issue, we propose the augmented generalized Mini-batch Energy distance:\nAâ€‹Gâ€‹Mâ€‹Eğ’Ÿ2â€‹(Î¼,Î½;g)=Gâ€‹Mâ€‹Eğ’Ÿ2â€‹(Î¼Â¯,Î½Â¯),ğ´ğºğ‘€superscriptsubscriptğ¸ğ’Ÿ2ğœ‡ğœˆğ‘”ğºğ‘€superscriptsubscriptğ¸ğ’Ÿ2Â¯ğœ‡Â¯ğœˆAGME_{\\mathcal{D}}^{2}(\\mu,\\nu;g)=GME_{\\mathcal{D}}^{2}(\\bar{\\mu},\\bar{\\nu}),\nwhere Î¼Â¯=fâ€‹â™¯â€‹Î¼Â¯ğœ‡ğ‘“â™¯ğœ‡\\bar{\\mu}=f\\sharp\\mu and Î½Â¯=fâ€‹â™¯â€‹Î½Â¯ğœˆğ‘“â™¯ğœˆ\\bar{\\nu}=f\\sharp\\nu with fâ€‹(x)=(x,gâ€‹(x))ğ‘“ğ‘¥ğ‘¥ğ‘”ğ‘¥f(x)=(x,g(x)) for g:â„dâ†’â„:ğ‘”â†’superscriptâ„ğ‘‘â„g:\\mathbb{R}^{d}\\to\\mathbb{R} is a non-linear function. For any gğ‘”g, Aâ€‹Gâ€‹Mâ€‹Eğ’Ÿ2â€‹(Î¼,Î½;g)ğ´ğºğ‘€superscriptsubscriptğ¸ğ’Ÿ2ğœ‡ğœˆğ‘”AGME_{\\mathcal{D}}^{2}(\\mu,\\nu;g) is still a valid distance since fğ‘“f is injective. In the diffusion model setting, we train gÎ³subscriptğ‘”ğ›¾g_{\\gamma} (parameterized by Î³ğ›¾\\gamma) as a time-conditional discriminator as inÂ [48, Equation (5)]. We refer the reader to AlgorithmÂ 3 in AppendixÂ C for a detailed algorithm, and to AppendixÂ D for more detailed experimental settings. Results. We follow the setting inÂ [ref]48 for diffusion models on CIFAR10Â [ref]23 with N=1800ğ‘1800N=1800 epochs. We set L=104ğ¿superscript104L=10^{4} for SW, DSW, EBSW, RPSW, and IWRPSW. We set Tâˆˆ{2,5,10}ğ‘‡2510T\\in\\{2,5,10\\} for DSW and Tâˆˆ{100,1000}ğ‘‡1001000T\\in\\{100,1000\\} for Max-SW. It is worthing noting that the training time of Max-SW and DSW is more than two times the time for SW, EBSW, RPSW, and IWRPSW, hence, we cannot increase the value of Tğ‘‡T further. We adjust concentration parameter Îºğœ…\\kappa of the PS distribution for each epoch tğ‘¡t as Îºâ€‹(t)=(Îº0âˆ’1)â€‹(Nâˆ’tâˆ’1Nâˆ’1)10+1ğœ…ğ‘¡subscriptğœ…01superscriptğ‘ğ‘¡1ğ‘1101\\kappa(t)=(\\kappa_{0}-1)\\left(\\frac{N-t-1}{N-1}\\right)^{10}+1 with N=1800ğ‘1800N=1800 and Îº0âˆˆ{100,50}subscriptğœ…010050\\kappa_{0}\\in\\{100,50\\}. We report the FID scoresÂ , the number of function evaluations, and the generation time in TableÂ 1. Using an implicit denoising model with only 444 time steps, our SW diffusion models have a much faster generation time compared to existing diffusion models such as DDPM, Score SDE, DDIM, Probability Flow, and LSGM. Compared to DDGAN with the same 444 time steps, our SW diffusion models lead to a lower FID score and the same generation time. Among SW variants, the proposed RPSW and IWRPSW achieve the lowest scores of 2.82 and 2.70 in turn. We show some random generated images from DDGAN, RPSW-DD, and IWRPSW-DD in FigureÂ 2 as qualitative results.\nOverall, AGME distance with RPSW and IWRPSW kernels could be a potential effective loss for improving training implicit diffusion models.\n\nReference markers found: [48]\n\nContext windows for each [ref]:\n[ref]48: as: where Kâ€‹Lğ¾ğ¿KL denotes the Kullback-Leibler divergence, and Cğ¶C is a known constant. Implicit denoising model. To reduce the number of steps Tğ‘‡T for faster generation, denoising diffusion GANsÂ [ref]48 proposes to use the implicit denoising model\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 48,\n            \"titles\": [\n                \"Sliced Wasserstein Variational Inference\",\n                \"Sliced Optimal Partial Transport\",\n                \"Partial Transport for Point-Cloud Registration\",\n                \"Spherical sliced-Wasserstein\",\n                \"Hyperbolic sliced-wasserstein via geodesic and horospherical projections\",\n                \"Efficient gradient flows in sliced-Wasserstein space\",\n                \"Sliced-Wasserstein on Symmetric Positive Definite Matrices for M/EEG Signals\",\n                \"Spot: sliced partial optimal transport\",\n                \"Sliced and Radon Wasserstein barycenters of measures\",\n                \"Unidimensional and evolution methods for optimal transportation\",\n                \"Partial Adversarial Domain Adaptation\",\n                \"The Power Spherical distribution\",\n                \"Max-Sliced Wasserstein Distance and its use for GANs\",\n                \"Generative modeling using the sliced Wasserstein distance\",\n                \"A probabilistic theory of pattern recognition, volume 31\",\n                \"Interpolating between optimal transport and MMD using Sinkhorn divergences\",\n                \"Pot: Python optimal transport\",\n                \"Generative adversarial nets\",\n                \"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium\",\n                \"Denoising Diffusion Probabilistic Models\",\n                \"Maximum likelihood estimators for the matrix von Mises-Fisher and bingham distributions\",\n                \"Auto-encoding variational bayes\",\n                \"Sliced Wasserstein Distance for Learning Gaussian Mixture Models\",\n                \"Learning multiple layers of features from tiny images\",\n                \"Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation\",\n                \"A convexity principle for interacting gases\",\n                \"Approximate Bayesian computation with the sliced-Wasserstein distance\",\n                \"Statistical and Topological Properties of Sliced Probability Divergences\",\n                \"Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance\",\n                \"Quasi-monte carlo for 3d sliced wasserstein\",\n                \"Energy-Based Sliced Wasserstein Distance\",\n                \"Distributional sliced-Wasserstein and applications to generative modeling\",\n                \"Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction\",\n                \"Markovian Sliced Wasserstein Distances: Beyond Independent Projections\",\n                \"Statistical, Robustness, and Computational Guarantees for Sliced Wasserstein Distances\",\n                \"Sliced optimal transport sampling\",\n                \"Computational optimal transport\",\n                \"Improving GANs using optimal transport\",\n                \"Optimal transport for applied mathematicians\",\n                \"Lidar Upsampling with Sliced Wasserstein Distance\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]48 based on the context and research area of 1 Introduction. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}