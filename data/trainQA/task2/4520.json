{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Towards Semantic Communications: Deep Learning-Based Image Semantic Coding\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, the received bit streams are decoded into semantic concepts and then combined as the reconstructed image. Some learning-based codecs are based on convolutional neural networks (CNNs) [ref]4   , while others on recurrent neural networks (RNNs)  .\nRecent\nlearning-based codecs   are competitive with or\neven superior to the classical codecs. The work  proposed to use discretized Gaussian Mixture Likelihoods to\nparameterize the distributions of latent codes, which can achieve a more accurate and flexible entropy model.\nThe work  propose a versatile deep image compression network based on Spatial Feature Transform, which enables task-aware image compression for various tasks, e.g., classification, with variable rates. However, all of the aforementioned works optimizes the rate-distortion (RD) performance and cannot meet the requirements of ISC. Different from conventional methods that reduce pixel errors, the proposed semantic decoder should be tolerant of pixel errors to some extent. GANs  has shown impressive ability in synthesizing high-resolution images. Instead of forcing the generated images to be exactly the same as the ground truth image, GANs attempt to approximate the intractable distributions of the real image dataset. Therefore, the generated image maintains the underlying meaning of the ground truth image while the two images may differ in pixel values. Therefore, GANs fits the spirit of the ISC and are adopted as the semantic decoder in the proposed method. Adversarial training of the GANs has shown advantage in reducing compression artifacts in\ndeep compression systems   [ref]4 [ref]3. Inspired by existing methods, we adopt conditional GANs in the semantic decoder that translates the semantic concepts into the reconstructed image. The reconstructed image should be natural and similar to the input image.\nThe naturalness can be measured by the adversarial loss that indicates the probability distributions divergence of real and reconstructed images, while the visual similarity could be measured by the aforementioned perceptual loss that indicates the distance in feature space . The detailed architecture of the generative semantic decoder is illustrated in Fig. 4, which consists of a local generator 𝒢lsubscript𝒢𝑙\\mathcal{G}_{l}, a global generator 𝒢gsubscript𝒢𝑔\\mathcal{G}_{g}, and an attention module. Local Generator 𝒢lsubscript𝒢𝑙\\mathcal{G}_{l}: The quantity and spatial occupation of different categories are imbalanced in the training dataset. It is extremely difficult to generate small object classes and texture details since the dataset are dominated by frequently occurred or large object classes. To avoid the interference from other classes, we propose a novel local image generator 𝒢lsuperscript𝒢𝑙\\mathcal{G}^{l} to separately reconstruct the class-specific objects 𝒚(m)∈ℝH×Wsuperscript𝒚𝑚superscriptℝ𝐻𝑊\\boldsymbol{y}^{(m)}\\in\\mathbb{R}^{H\\times W} from the quantized class-specific feature maps 𝒇^(m)superscriptbold-^𝒇𝑚\\boldsymbol{\\hat{f}}^{(m)} as The locally reconstructed image 𝒙^l∈ℝH×Wsubscriptbold-^𝒙𝑙superscriptℝ𝐻𝑊\\boldsymbol{\\hat{x}}_{l}\\in\\mathbb{R}^{H\\times W} is obtained by element-wise addition of all the individual 𝒚(m)superscript𝒚𝑚\\boldsymbol{y}^{(m)}’s as As shown in Fig. 4, we feed M𝑀M feature maps 𝒇^(m)superscriptbold-^𝒇𝑚\\boldsymbol{\\hat{f}}^{(m)} into a convolution layer and five consecutive residual blocks, and the outputs are then up-scaled by two consecutive deconvolutional layers. Then, the up-scaled feature maps of M𝑀M semantic concepts are separately fed into M𝑀M corresponding deconvolutional layers to produce 𝒚(m)superscript𝒚𝑚\\boldsymbol{y}^{(m)}. Each deconvolutional layer has independent network parameters and is able to effectively preserve the class-specific features with rich details. Global Generator 𝒢gsubscript𝒢𝑔\\mathcal{G}_{g}:\nWe attempt to capture the global structure information as well as the spatial layout by global generation. Inspired from GauGAN , the well designed 𝒢gsubscript𝒢𝑔\\mathcal{G}_{g} adopts spatially adaptive normalization (SPADE) to fuse the spatial layout information from 𝒔𝒔\\boldsymbol{s} into the feature maps 𝒇^bold-^𝒇\\boldsymbol{\\hat{f}}. The SPADE modulates the layer activation in accordance with 𝒔𝒔\\boldsymbol{s} to guide the reconstruction of different categories. The globally reconstructed image 𝒙^gsubscriptbold-^𝒙𝑔\\boldsymbol{\\hat{x}}_{g} can be obtained as Particularly, we feed 𝒇^bold-^𝒇\\boldsymbol{\\hat{f}} into three residual blocks with SPADE (SPADE-R), the output of which is upsampled three times to yield the global reconstruction 𝒙^gsubscriptbold-^𝒙𝑔\\boldsymbol{\\hat{x}}_{g}. Attention Module: In order to better combine the outputs of 𝒢lsubscript𝒢𝑙\\mathcal{G}_{l} and 𝒢gsubscript𝒢𝑔\\mathcal{G}_{g}, we further propose an attention module to learn the local weight matrix 𝑾l∈ℝH×Wsubscript𝑾𝑙superscriptℝ𝐻𝑊\\boldsymbol{W}_{l}\\in\\mathbb{R}^{H\\times W} and global weight matrix 𝑾g∈ℝH×Wsubscript𝑾𝑔superscriptℝ𝐻𝑊\\boldsymbol{W}_{g}\\in\\mathbb{R}^{H\\times W}. The feature map 𝒇^bold-^𝒇\\boldsymbol{\\hat{f}} is fed into the attention module that includes three consecutive deconvolutional layers and a convolutional layer. The output of the convolutional layer has two channels that correspond to the two weight matrices and is further normalized by a softmax layer in channel-wise. Therefore, we obtain 𝑾gsubscript𝑾𝑔\\boldsymbol{W}_{g} and 𝑾lsubscript𝑾𝑙\\boldsymbol{W}_{l} whose values are in the range of (0,1) and meet the condition 𝑾l​(i,j)+𝑾g​(i,j)=1subscript𝑾𝑙𝑖𝑗subscript𝑾𝑔𝑖𝑗1\\boldsymbol{W}_{l}(i,j)+\\boldsymbol{W}_{g}(i,j)=1 in each location (i,j)𝑖𝑗(i,j). The final output image 𝒙^bold-^𝒙\\boldsymbol{\\hat{x}} can be written as element-wise summation of weighted local reconstruction 𝒙^lsubscriptbold-^𝒙𝑙\\boldsymbol{\\hat{x}}_{l} and global reconstruction 𝒙^gsubscriptbold-^𝒙𝑔\\boldsymbol{\\hat{x}}_{g}: We adopt a multi-scale patch-discriminator 𝒟𝒟\\mathcal{D} to identify the real image 𝒙𝒙\\boldsymbol{x} and the final output image 𝒙^bold-^𝒙\\boldsymbol{\\hat{x}}. The generator 𝒢l,𝒢gsubscript𝒢𝑙subscript𝒢𝑔\\mathcal{G}_{l},\\mathcal{G}_{g} are trained alternatively with the discriminator 𝒟𝒟\\mathcal{D}. Denote 𝒟​(𝒙,𝒔)𝒟𝒙𝒔\\mathcal{D}(\\boldsymbol{x},\\boldsymbol{s}) or 𝒟​(𝒙^,𝒔)𝒟bold-^𝒙𝒔\\mathcal{D}(\\boldsymbol{\\hat{x}},\\boldsymbol{s}) as the predicted probability that the image pair (𝒙,𝒔)𝒙𝒔(\\boldsymbol{x},\\boldsymbol{s}) or (𝒙^,𝒔)bold-^𝒙𝒔(\\boldsymbol{\\hat{x}},\\boldsymbol{s}) are from real samples. The discriminator 𝒟𝒟\\mathcal{D} learns to distinguish real image pairs from fake ones by maximizing 𝒟​(𝒙,𝒔)𝒟𝒙𝒔\\mathcal{D}(\\boldsymbol{x},\\boldsymbol{s}) and minimizing 𝒟​(𝒙^,𝒔)𝒟bold-^𝒙𝒔\\mathcal{D}(\\boldsymbol{\\hat{x}},\\boldsymbol{s}). The generators 𝒢lsubscript𝒢𝑙\\mathcal{G}_{l} and 𝒢gsubscript𝒢𝑔\\mathcal{G}_{g} learn to fool 𝒟𝒟\\mathcal{D} by minimizing (−𝒟​(𝒙^,𝒔))𝒟bold-^𝒙𝒔(-\\mathcal{D}(\\boldsymbol{\\hat{x}},\\boldsymbol{s})). We adopt the hinge loss for GANs, whose objective function is Note that the hinge loss L𝒟subscript𝐿𝒟L_{\\mathcal{D}} is used for “maximum-margin” classification, which penalizes the positive samples of 𝒟​(𝒙,𝒔)<1𝒟𝒙𝒔1\\mathcal{D}(\\boldsymbol{x},\\boldsymbol{s})<1 and the negative samples of 𝒟​(𝒙^,𝒔)>−1𝒟bold-^𝒙𝒔1\\mathcal{D}(\\boldsymbol{\\hat{x}},\\boldsymbol{s})>-1. Only the samples that are not classified properly will impact the gradient update of 𝒢lsubscript𝒢𝑙\\mathcal{G}_{l}, 𝒢gsubscript𝒢𝑔\\mathcal{G}_{g} and 𝒟𝒟\\mathcal{D}. The formulation (32) forces the probability distribution of 𝒙^bold-^𝒙\\boldsymbol{\\hat{x}} to approximate that of the training image set. If the feature extraction network ℰℰ\\mathcal{E} cannot afford to store the exact detail in 𝒙𝒙\\boldsymbol{x}, then 𝒢lsuperscript𝒢𝑙\\mathcal{G}^{l} and 𝒢gsuperscript𝒢𝑔\\mathcal{G}^{g} are able to synthesize the detail to satisfy natural image distribution instead of showing blocky and blurry effects. Equilibrium will be achieved when 𝒟𝒟\\mathcal{D} classifies the reconstructed image 𝒙^bold-^𝒙\\boldsymbol{\\hat{x}} as real. The final reconstruction 𝒙^bold-^𝒙\\boldsymbol{\\hat{x}} should not only be consistent with the natural image distribution, which coincides with the target of traditional GANs, but also meet the requirements of semantic communications and recover the semantic concepts of a specific input image 𝒙𝒙\\boldsymbol{x}. We adopt the perceptual loss (11) to guide the semantic reconstruction process and ensure 𝒙^bold-^𝒙\\boldsymbol{\\hat{x}} approximates the feature space of 𝒙𝒙\\boldsymbol{x}. The proposed semantic encoder and semantic decoder are trained jointly, and the objective function can be formulated as the weighted combination of adversarial loss L𝒟subscript𝐿𝒟L_{\\mathcal{D}}, perceptual loss LPsubscript𝐿𝑃L_{P}, and feature classification loss LCsubscript𝐿𝐶L_{C} as where λ1subscript𝜆1\\lambda_{1} and λ2subscript𝜆2\\lambda_{2} are the weighting parameters. Due to the adequately preserved semantic information, the proposed generative semantic decoder produces more incredible reconstructions compared with the leading image generation models  [ref]2 [ref]3. Note that the work [ref]2 performs poor in preserving appearance features, and the reconstructed image deviates apparently from the source image. The work  can merely generate a general image from the distribution of the training dataset without the ability to control the appearance of certain objects.\n\nReference markers found: [4]\n\nContext windows for each [ref]:\n[ref]4: In this section, the received bit streams are decoded into semantic concepts and then combined as the reconstructed image. Some learning-based codecs are based on convolutional neural networks (CNNs) [ref]4   , while others on recurrent neural networks (RNNs)  .\nRecent\nlearning-based codecs   are competitive with or\neven superior to the classical codecs. The work  proposed to use discretized\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 4,\n            \"titles\": [\n                \"Demystifying MMD GANs\",\n                \"인공지능 기반 공격 그래프 생성\",\n                \"Generative Adversarial Networks for Extreme Learned Image Compression\",\n                \"DSSLIC: Deep Semantic Segmentation-based Layered Image Compression\",\n                \"End-to-end Optimized Image Compression\",\n                \"Calculation of average PSNR differences between RD-curves\",\n                \"The Perception-Distortion Tradeoff\",\n                \"Deep Joint Source-Channel Coding for Wireless Image Transmission\",\n                \"Learning based Facial Image Compression with Semantic Fidelity Metric\",\n                \"Learned image compression with discretized gaussian mixture likelihoods and attention modules\",\n                \"Cisco visual networking index: Global mobile data traffic forecast update 2017-2022\",\n                \"The Cityscapes Dataset for Semantic Urban Scene Understanding\",\n                \"Understanding how image quality affects deep neural networks\",\n                \"Deep Learning for Joint Source-Channel Coding of Text\",\n                \"Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks\",\n                \"Generative adversarial nets\",\n                \"Strengths and weaknesses of deep learning models for face recognition against image degradations\",\n                \"Mask R-CNN\",\n                \"Beyond coding: Detection-driven image compression with semantically structured bit-stream\",\n                \"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium\",\n                \"Coarse-to-fine hyper-prior modeling for learned image compression\",\n                \"Perceptual Losses for Real-Time Style Transfer and Super-Resolution\",\n                \"Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks\",\n                \"Adam: A method for stochastic optimization\",\n                \"Bandwidth-Agile Image Transmission with Deep Joint Source-Channel Coding\",\n                \"Context-adaptive entropy model for end-to-end optimized image compression\",\n                \"Region-of-interest and channel attention-based joint optimization of image compression and computer vision\",\n                \"Task-driven Semantic Coding via Reinforcement Learning\",\n                \"On The Classification-Distortion-Perception Tradeoff\",\n                \"Reinforcement Learning-powered Semantic Communication via Semantic Similarity\",\n                \"Conditional Probability Models for Deep Image Compression\",\n                \"High-Fidelity Generative Image Compression\",\n                \"Joint Autoregressive and Hierarchical Priors for Learned Image Compression\",\n                \"Semantic Image Synthesis with Spatially-Adaptive Normalization\",\n                \"Semantic communications: Principles and challenges\",\n                \"JPEG2000: Image compression fundamentals, standards and practice\",\n                \"Real-Time Adaptive Image Compression\",\n                \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\",\n                \"Generative compression\",\n                \"The mathematical theory of communication\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]4 based on the context and research area of Towards Semantic Communications: Deep Learning-Based Image Semantic Coding. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}