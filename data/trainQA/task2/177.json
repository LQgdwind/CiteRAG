{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Natural Language Processing Advancements By Deep Learning: A Survey\n\nPaper Abstract: \n\nPaper Section Text:\nIt was after the success of the neural network in image classification tasks that researchers started to use neural networks in machine translation (NMT).\nAround 2013, research groups started to achieve breakthrough results in NMT.\nUnlike traditional statistical machine translation, NMT is based on an end-to-end neural networkÂ . This implies that there is no need for extensive preprocessing and word alignments. Instead, the focus shifted toward network structure. Fig.Â 11 shows an example of an end-to-end recurrent neural network for machine translation.\nA sequence of input tokens is fed into the network.\nOnce it reaches an end-of-sentence (EOS) token, it starts generating the output sequence.\nThe output sequence is generated in the same recurrent manner as the input sequence until it reaches an end-of-sentence token.\nOne major advantage of this approach is that there is no need to specify the length of the sequence; the network takes it into account automatically.\nIn other words, the end-of-sentence token determines the length of the sequence.\nNetworks implicitly learn that longer input sentences usually lead to longer output sentences with varying length, and that ordering can change.\nFor instance, the second example in Fig.Â 9 shows that adjectives generally come before nouns in English but after nouns in Spanish.\nThere is no need to explicitly specify this since the network can capture such properties.\nMoreover, the amount of memory that is used by NMT is just a fraction of the memory that is used in traditional statistical machine translationÂ .  was one of the early works that incorporated recurrent neural networks for machine translation. They were able to achieve a perplexity (a measure where lower values indicate better models) that was 43% less than the state-of-the-art alignment based translation models.\nTheir recurrent continuous translation model (RCTM) is able to capture word ordering, syntax, and meaning of the source sentence explicitly.\nIt maps a source sentence into a probability distribution over sentences in the target language. RCTM estimates the probability Pâ€‹(f|e)ğ‘ƒconditionalğ‘“ğ‘’P(f|e) of translating a sentence e=e1+â€¦+ekğ‘’subscriptğ‘’1â€¦subscriptğ‘’ğ‘˜e=e_{1}+...+e_{k} in the source language to target language sentence f=f1+â€¦+fmğ‘“subscriptğ‘“1â€¦subscriptğ‘“ğ‘šf=f_{1}+...+f_{m}.\nRCTM estimates Pâ€‹(f|e)ğ‘ƒconditionalğ‘“ğ‘’P(f|e) by considering source sentence eğ‘’e as well as the preceding words in the target language f1:iâˆ’1subscriptğ‘“:1ğ‘–1f_{1:i-1}: The representation generated by RCTM acts on n-grams in the lower layers, and acts more on the whole sentence as one moves to the upper layers.\nThis hierarchical representation is performed by applying different layers of convolution.\nFirst a continuous representation of each word is generated; i.e., if the sentence is e=e1â€‹â€¦â€‹ekğ‘’subscriptğ‘’1â€¦subscriptğ‘’ğ‘˜e=e_{1}...e_{k}, the representation of the word eisubscriptğ‘’ğ‘–e_{i} will be vâ€‹(ei)âˆˆâ„qÃ—1ğ‘£subscriptğ‘’ğ‘–superscriptâ„ğ‘1v(e_{i})\\in\\mathbb{R}^{q\\times 1}.\nThis will result in sentence matrix Eeâˆˆâ„qÃ—ksuperscriptEğ‘’superscriptâ„ğ‘ğ‘˜\\textbf{E}^{e}\\in\\mathbb{R}^{q\\times{k}} in which E:,ie=vâ€‹(ei)superscriptsubscriptE:ğ‘–ğ‘’ğ‘£subscriptğ‘’ğ‘–\\textbf{E}_{:,i}^{e}=v(e_{i}).\nThis matrix representation of the sentence will be fed into a series of convolution layers in order to generate the final representation e for the recurrent neural network. The approach is illustrated in Fig.Â 10.\nEquations for the pipeline are as follows. In order to take into account the sentence length, the authors introduced RCTM II which estimates the length of the target sentence.\nRCTM II was able to achieve better perplexity on WMT datasets (see top portion of TableÂ I) than other existing machine translation systems. In another line of work, Â [ref]170 presented an end-to-end sequence learning approach without heavy assumptions on the structure of the sequence.\nTheir approach consists of two LSTMs, one for mapping the input to a vector of fixed dimension and another LSTM for decoding the output sequence from the vector.\nTheir model was able to handle long sentences as well as sentence representations that are sensitive to word order.\nAs shown in Fig.Â 11, the model reads â€ABCâ€ as an input sequence and produces â€WXYZâ€ as output sequence.\nThe <Eâ€‹Oâ€‹S>expectationğ¸ğ‘‚ğ‘†<EOS> token indicates the end of prediction.\nThe network was trained by maximizing the log probability of the translation (Î·ğœ‚\\eta) given the input sequence (Î¶ğœ\\zeta).\nIn other words, the objective function is: ğ’Ÿğ’Ÿ\\mathcal{D} is the training set and |ğ’Ÿ|ğ’Ÿ|\\mathcal{D}| is its size.\nOne of the novelties of their approach was reversing word order of the source sentence.\nThis helps the LSTM to learn long term dependencies. Having a fixed-length vector in the decoder phase is one of the bottlenecks of the encoder-decoder approach. Â  argues that a network will have a hard time compressing all the information from the input sentence into a fixed-size vector.\nThey address this by allowing the network to search segments of the source sentence that are useful for predicting the translation.\nInstead of representing the input sentence as a fixed-size vector, in  the input sentence is encoded to a sequence of vectors and a subset of them is chosen by using a method called attention mechanism as shown in Fig.Â 12. In their approach Pâ€‹(yi|y1,â€¦,yiâˆ’1,X)=gâ€‹(yiâˆ’1,si,ci)ğ‘ƒconditionalsubscriptğ‘¦ğ‘–subscriptğ‘¦1â€¦subscriptğ‘¦ğ‘–1ğ‘‹ğ‘”subscriptğ‘¦ğ‘–1subscriptğ‘ ğ‘–subscriptğ‘ğ‘–P(y_{i}|y_{1},...,y_{i-1},X)=g(y_{i-1},s_{i},c_{i}), in which si=fâ€‹(siâˆ’1,yiâˆ’1,ci)subscriptğ‘ ğ‘–ğ‘“subscriptğ‘ ğ‘–1subscriptğ‘¦ğ‘–1subscriptğ‘ğ‘–s_{i}=f(s_{i-1},y_{i-1},c_{i}).\nWhile previously cğ‘c was the same for all time steps, here cğ‘c takes a different value, cisubscriptğ‘ğ‘–c_{i}, at each time step.\nThis accounts for the attention mechasim (context vector) around that specific time step.\ncisubscriptğ‘ğ‘–c_{i} is computed according to the following: ci=âˆ‘j=1TxÎ±iâ€‹jâ€‹hj,Î±iâ€‹j=eâ€‹xâ€‹pâ€‹(eiâ€‹j)âˆ‘k=1Txeâ€‹xâ€‹pâ€‹(eiâ€‹k),eiâ€‹j=aâ€‹(siâˆ’1,hj)formulae-sequencesubscriptğ‘ğ‘–superscriptsubscriptğ‘—1subscriptğ‘‡ğ‘¥subscriptğ›¼ğ‘–ğ‘—subscriptâ„ğ‘—formulae-sequencesubscriptğ›¼ğ‘–ğ‘—ğ‘’ğ‘¥ğ‘subscriptğ‘’ğ‘–ğ‘—superscriptsubscriptğ‘˜1subscriptğ‘‡ğ‘¥ğ‘’ğ‘¥ğ‘subscriptğ‘’ğ‘–ğ‘˜subscriptğ‘’ğ‘–ğ‘—ğ‘subscriptğ‘ ğ‘–1subscriptâ„ğ‘—c_{i}=\\sum_{j=1}^{T_{x}}\\alpha_{ij}h_{j},\\ \\alpha_{ij}=\\frac{exp(e_{ij})}{{\\sum_{k=1}^{T_{x}}exp(e_{ik})}},\\ e_{ij}=a(s_{i-1},h_{j}). Here ağ‘a is the alignment model that is represented by a feed forward neural network.\nAlso hj=[hjTâ†’,hjTâ†]subscriptâ„ğ‘—â†’superscriptsubscriptâ„ğ‘—ğ‘‡â†superscriptsubscriptâ„ğ‘—ğ‘‡h_{j}=[\\overset{\\rightarrow}{h_{j}^{T}},\\overset{\\leftarrow}{h_{j}^{T}}], which is a way to include information both about preceding and following words in hjsubscriptâ„ğ‘—h_{j}.\nThe model was able to outperform the simple encoder-decoder approach regardless of input sentence length. Improved machine translation models continue to emerge, driven in part by the growth in peopleâ€™s interest and need to understand other languages\nMost of them are variants of the end-to-end decoder-encoder approach. For example, Â  tries to deal with the problem of rare words.\nTheir LSTM network consists of encoder and decoder layers using residual layers along with the attention mechanism.\nTheir system was able to decrease training time, speed up inference, and handle translation of rare words.\nComparisons between some of the state-of-the-art neural machine translation models are summarized in TableÂ VII. More recently, [ref]177 provides an interesting single-model implementation of massively multilingual NMT. In , authors use BERT to extract contextual embeddings and combine BERT with an attention-based NMT model and provide state-of-the-art results on various benchmark datasets. [ref]179 proposes mBART which is a seq-to-seq denoising autoencoder and reports that using a pretrained, locked (i.e. no modifications) mBART improves performance in terms of the BLEU point.  proposes an interesting adversarial framework for robustifying NMT against noisy inputs and reports performance gains over the Transformer model.  is also an insightful recent work where the authors sample context words from the predicted sequence as well as the ground truth to try to reconcile the training and inference processes. Finally,  is a successful recent effort to prevent the forgetting that often accompanies in translating pre-trained language models to other NMT task.  achieves that aim primarily by using a dynamically gated model and asymptotic distillation.\n\nReference markers found: [170]\n\nContext windows for each [ref]:\n[ref]170: length of the target sentence.\nRCTM II was able to achieve better perplexity on WMT datasets (see top portion of TableÂ I) than other existing machine translation systems. In another line of work, Â [ref]170 presented an end-to-end sequence learning approach without heavy assumptions on the structure of the sequence.\nTheir approach consists of two LSTMs, one for mapping the input to a vector of\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 170,\n            \"titles\": [\n                \"Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\",\n                \"Foundations of statistical natural language processing\",\n                \"Character-level convolutional networks for text classification\",\n                \"Learning phrase representations using RNN encoder-decoder for statistical machine translation\",\n                \"Deep learning in clinical natural language processing: a methodical review\",\n                \"A unified architecture for natural language processing: Deep neural networks with multitask learning\",\n                \"Large-scale video classification with convolutional neural networks\",\n                \"Learning and transferring mid-level image representations using convolutional neural networks\",\n                \"Learning from simulated and unsupervised images through adversarial training\",\n                \"Deep Learning for Computer Vision: A Brief Review\",\n                \"Deep learning vs. traditional computer vision\",\n                \"Towards end-to-end speech recognition with recurrent neural networks\",\n                \"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\",\n                \"Deep learning for NLP and speech recognition\",\n                \"Learning character-level representations for part-of-speech tagging\",\n                \"Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss\",\n                \"Part-of-speech tagging from 97% to 100%: is it time for some linguistics?\",\n                \"Deep learning techniques for part of speech tagging by natural language processing\",\n                \"Neural Architectures for Named Entity Recognition\",\n                \"Named entity recognition with bidirectional LSTM-CNNs\",\n                \"A Survey on Recent Advances in Named Entity Recognition from Deep Learning models\",\n                \"A Survey on Deep Learning for Named Entity Recognition\",\n                \"End-to-end learning of semantic role labeling using recurrent neural networks\",\n                \"A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling\",\n                \"Deep semantic role labeling: What works and whatâ€™s next\",\n                \"Syntax-aware Multilingual Semantic Role Labeling\",\n                \"Recent Trends in Deep Learning Based Natural Language Processing\",\n                \"Natural language processing (NLP) in management research: A literature review\",\n                \"What exactly is artificial intelligence, anyway?.\",\n                \"Critical analysis of big data challenges and analytical methods\",\n                \"A Critical Review of Recurrent Neural Networks for Sequence Learning\",\n                \"Convolutional Neural Networks for Sentence Classification\",\n                \"Parsing natural scenes and natural language with recursive neural networks\",\n                \"Imagenet classification with deep convolutional neural networks\",\n                \"Deep convolutional neural networks for sentiment analysis of short texts\",\n                \"Effective use of word order for text categorization with convolutional neural networks\",\n                \"Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding\",\n                \"Relation classification via convolutional deep neural network\",\n                \"Relation extraction: Perspective from convolutional neural networks\",\n                \"Recurrent neural network based language model\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]170 based on the context and research area of Natural Language Processing Advancements By Deep Learning: A Survey. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}