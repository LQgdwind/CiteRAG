{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Massively Parallel Implementation of Multilevel Monte Carlo for Finite Element Models\n\nPaper Abstract: \n\nPaper Section Text:\nUQ requires the solution of stochastic PDEs with random data. Some methods for solving stochastic PDEs, e.g. stochastic Galerkin  , are based on a standard approximation in space like Finite Elements or finite volumes, and different types of polynomial expansions in the stochastic direction . Although generally powerful, these techniques typically suffer two significant drawbacks. The first one is being intrusive, i.e. a code that can be used to solve a deterministic problem must be modified to solve the stochastic analogue. The second one is the poor scaling in the dimension of the stochastic space, i.e. suffering from the so-called ‚Äúcurse of dimensionality‚Äù [ref]2. In contrast, sampling methods, i.e.  Monte Carlo (MC) and its variants, have a convergence rate which is independent of the stochastic dimension and are non-intrusive. The only necessary assumption for such methods to converge to the exact statistics of the solution when the number of input samples tends to infinity is the existence of finite second moments, i.e. finite variance. This assumption is easily satisfied for many physical systems, making the method generally applicable to many problems of practical interest. They require the repeated evaluation of a deterministic model with different randomly generated inputs and can be implemented in a way that does not require intrusive modifications of the deterministic code. However, the number of samples required to achieve statistical convergence combined with the complexity of the computational model required to have enough spatial and/or temporal accuracy can make the MC method rather expensive. Indeed, the reduction of overall computational cost to achieve a desired error tolerance is the principal motivation for the development of its variants, like the MLMC method considered herein. Stochastic collocation methods [ref]2 are also based on polynomial expansions but using orthogonal basis and appropriate quadratures. The final computation requires independent evaluations of the deterministic model, in the same way as sampling methods, thus sharing the non-intrusiveness property. Appropriate quadratures are required to mitigate the curse of dimensionality, which is a field of intense research  . Similarly, importance sampling methods   , and the deterministic design of experiments [ref]20 are also known to reduce the necessary number of model evaluations, particularly in the contexts of function approximation . The algorithms discussed herein can be extended for parallelization of these ‚Äúsmart sampling‚Äù methods as well, but the required modifications are beyond the scope of this article. Focus on this important complexity reduction makes MLMC      , and its adaptive variants  5  6 arguably the most practical extensions of MC. These methods rely on different levels of computational effort for the deterministic model, e.g. a hierarchy of spatial or temporal meshes. These methods seek to combine samples on each level to benefit from the high accuracy of the expensive, higher levels, and the low computational cost of the less accurate, lower levels. A key aspect of the algorithm is the evaluation of how many samples should be used at each level. Sampling methods for UQ, including MLMC, belong to a general class of methods for outer-loop applications [44, Section 1.2], defined as computational applications in which there is an outer loop around a model that is called at each iteration to evaluate a function. This includes optimization and statistical inference apart from UQ. For example, in the case of optimization under uncertainty, a UQ problem is solved as an inner loop within each step of the outer loop of the optimization algorithm 9. A big effort was performed during the 90s for the development of the Dakota library   aimed to deal with this class of problems. Parallelization opportunities where classified into two areas, algorithmic and function evaluation111The classification in   actually included four categories but two are enough for the current discussion about MLMC. and it was already observed that the former requires very little or no inter-processor communication in contrast to the latter. Therefore, the first question posed was how to select the amount of parallelism used on each area, i.e. one has to decide between the assignment of processors to the parallelization of the model or the parallelization of the outer loop (with the concurrent execution of several models). The analysis in   shows that the most efficient choice is to use the minimum number of processors that permit to run the model, for the simple reason that some loss of efficiency occurs in strong scaling. The parallelization of MLMC has been considered in recent years       1 as multiple levels of spatial/temporal discretization introduce additional scheduling challenges to effectively utilize resources of a parallel environment with minimal idling. One can conceptually exploit three levels of parallelism in MLMC.\nThe first is the parallelization of the (deterministic) PDE solver itself, which is often dependent on the level, meaning that different levels will use different computational resources. The second is the parallelization of the different MC samples on each level of MLMC, which is the easier one given the independence between them and the large number required, especially at lower discretization levels. The third one is the parallelization between levels, which we found important for the optimality of the algorithm, as discussed below. The two main ingredients of the implementation are i) a processor partitioning scheme and ii) a scheduling strategy and the goal is to maximize efficiency. Scheduling strategies for MLMC sampling can be classified  in three different ways. First, they are classified according to the number of parallelization layers exploited simultaneously, inside samples, between samples and between levels. Second, they are classified as homogeneous, when all the processors are working to sample on the same discretization level with the same number of processors per sample, and heterogeneous otherwise. It is important to note that, according to this definition, heterogeneous scheduling includes many different possibilities, e.g. using different number of processors for running samples at the same level (which is considered in ) or running samples at different levels simultaneously (which is not considered in ). A final classification is between static scheduling, when the assignment of work is done before starting to sample, and dynamic scheduling, where assignment is made ‚Äúon the fly‚Äù. It is also important to note that the term ‚Äúdynamic‚Äù is used in  to refer to scheduling strategies where the work is assigned depending on a cost estimation made at the beginning of the calculation but before starting to sample. Several scheduling algorithms are studied in , mostly homogeneous, but also a couple of heterogeneous variants (with and without strong scaling), both static and dynamic, although three layer parallelism was not investigated. A scheduling with three layer parallelism was proposed in    with two variants, a static one in which the distribution is made based on work estimates given by the size of the discretization at each level and one in which the distribution of work depends also on the PDE coefficients (which are sample dependent) and is performed at execution time but before starting to sample. In these references, ‚Äúadaptive load balancing‚Äù is used to refer to this second algorithm and it is explicitly stated that it is not ‚Äúdynamic‚Äù see [49, page 111], although it is categorized in this way in . The processor partitioning scheme in    divides available units into levels, each of them composed by samplers that contain several cores each, that is, the computational power is distributed into many levels and samples at a given time and assignment is made depending on the load balancing strategy just mentioned. For this scheme a 2-approximation is proved in   assuming strong scaling of the PDE solver, that is, the execution time is at most twice the optimal. The observed efficiency is improved by the adaptive load balancing algorithm although actual computational times are slower than in the static case [48, Figs. 3 and 4], which shows a very good strong and weak scaling up to 40k cores. A similar strategy was followed in , where an object oriented implementation of several sampling methods, including MC and  Quasi Monte Carlo (QMC), either single or multilevel is presented. Another dynamic scheduling strategy for MLMC was recently proposed in . It exploits general purpose scheduling algorithms in  which permit to deal with complex dependencies between tasks and this is exploited to extract parallelism between samples and levels. Although in principle it can be also used with parallel sampling, the mapping of tasks to processors and its effects on load balancing is not discussed. Moreover, parallelization at the sample level is not shown in the numerical examples. In this article we propose the first dynamic scheduling strategy with three layer parallelism that works on top of a multiple partition of the set of available processors. Given the number of levels of the MLMC algorithm and assuming a weak scaling of the PDE solver we define the number of processors per sample in terms of the number of processors used in the coarsest one, as in . We then consider multiple partitions of the set of available processors by these numbers of processors, thus obtaining LùêøL different partitions, where LùêøL is the maximum number of levels of the MLMC method. Together with this multiple partition we present a dynamic scheduling algorithm that assigns tasks to available resources prioritizing finer levels but allowing the concurrent execution of samples at different levels. This distinctive feature of our implementation admits a proof that the algorithm is a 2-approximation, with a very small idling actually occurring only at the end of the computation or at the end of an adaptive MLMC iteration. Besides, excellent scalability is obtained, both strong and weak. With some additional modifications discussed below, this scalability is observed even for the difficult case of models having low per-sample evaluation times. Therefore, the novelty of this approach includes a dynamic scheduling algorithm that exploits three layer parallelism: for each individual sample, across samples, and across levels; the description of an MPI implementation based on a master-slave strategy with a multiple partition of slave processors; a simple yet robust, dynamic batch sampling mechanism to reduce the communication between the master and the slaves; a parallelization of the master for eliminating a bottleneck at extreme scales; the numerical demonstration of the scalability of the implementation, also when stressed by short sampling times. The article is structured as follows. Section¬†2 describes the MLMC method and an adaptive variant implemented herein and includes the description of the PDE we consider as an example. Section¬†3 describes the scheduling in an abstract way and its 2-approximation property. Section¬†4 describes the actual implementation, including the parallel partition strategy. Section¬†5 presents several examples to demonstrate the weak and strong scalability of the implementation efficiency.\n\nReference markers found: [2]\n\nContext windows for each [ref]:\n[ref]2: problem must be modified to solve the stochastic analogue. The second one is the poor scaling in the dimension of the stochastic space, i.e. suffering from the so-called ‚Äúcurse of dimensionality‚Äù [ref]2. In contrast, sampling methods, i.e.  Monte Carlo (MC) and its variants, have a convergence rate which is independent of the stochastic dimension and are non-intrusive. The only necessary\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 2,\n            \"titles\": [\n                \"Embedded multilevel Monte Carlo for uncertainty quantification in random domains\",\n                \"Breaking the coherence barrier: A new theory for compressed sensing\",\n                \"A stochastic collocation method for elliptic partial differential equations with random input data\",\n                \"A tutorial-driven introduction to the parallel finite element library FEMPAR v1.0.0\",\n                \"Implementation and Scalability Analysis of Balancing Domain Decomposition Methods\",\n                \"Multilevel balancing domain decomposition at extreme scales\",\n                \"FEMPAR: An Object-Oriented Parallel Finite Element Framework\",\n                \"The aggregated unfitted finite element method for elliptic problems\",\n                \"PETSc Web page\",\n                \"Multi-level Monte Carlo Finite Element method for elliptic PDEs with stochastic coefficients\",\n                \"The parallel finite element system M++ with integrated multilevel preconditioning and multilevel Monte Carlo methods\",\n                \"Handbook on Scheduling\",\n                \"Efficient distribution estimation and uncertainty quantification for elliptic problems on domains with stochastic boundaries\",\n                \"Sparse quadrature for high-dimensional integration with Gaussian measure\",\n                \"Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients\",\n                \"A continuation multilevel Monte Carlo algorithm\",\n                \"Numerical solution of the Poisson equation on domains with a thin layer of random thickness\",\n                \"Numerical solution of the homogeneous Neumann boundary value problem on domains with a thin layer of random thickness\",\n                \"Topology optimization under uncertainty using a stochastic gradient-based approach\",\n                \"Sparse polynomial chaos expansions via compressed sensing and D-optimal design\",\n                \"Scheduling massively parallel multigrid for multilevel Monte Carlo Methods\",\n                \"Design and implementation of multilevel parallel optimization on the Intel teraflops\",\n                \"Multilevel parallelism for optimization on MP computers - Theory and experiment\",\n                \"A multilevel Monte Carlo method for computing failure probabilities\",\n                \"The design and implementation of hypre, a library of parallel high performance preconditioners\",\n                \"A generic C++ library for multilevel quasi-Monte Carlo\",\n                \"Stochastic Finite Elements: A Spectral Approach\",\n                \"Multilevel Monte Carlo Path Simulation\",\n                \"Stochastic finite differences and multilevel Monte Carlo for a class of SPDEs in finance\",\n                \"Multilevel quasi-Monte Carlo path simulation\",\n                \"Bounds for Certain Multiprocessing Anomalies\",\n                \"Bounds on Multiprocessing Timing Anomalies\",\n                \"Compressive sampling of polynomial chaos expansions: Convergence analysis and sampling strategies\",\n                \"Analysis of the domain mapping method for elliptic diffusion problems on random domains\",\n                \"First order second moment analysis for stochastic interface problems based on low-rank approximation\",\n                \"Sparse second moment analysis for elliptic problems in stochastic domains\",\n                \"Boomeramg: A parallel algebraic multigrid solver and preconditioner\",\n                \"Statistical Romberg extrapolation: A new variance reduction method and applications to option pricing\",\n                \"Spectral Methods for Uncertainty Quantification: With Applications to Computational Fluid Dynamics\",\n                \"Sparse polynomial chaos expansions: Literature survey and benchmark\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]2 based on the context and research area of A Massively Parallel Implementation of Multilevel Monte Carlo for Finite Element Models. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}