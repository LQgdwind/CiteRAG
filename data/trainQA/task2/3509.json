{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Towards Effective and Robust Neural Trojan Defenses via Input Filtering\n\nPaper Abstract: \n\nPaper Section Text:\nFollowing previous works 1  [ref]38,\nwe evaluate our proposed defenses on four image datasets namely MNIST,\nCIFAR10 , GTSRB ,\nand CelebA . For CelebA, we follow Salem et al.\n[ref]38 and select the top 3 most balanced binary\nattributes (out of 40) to form an 8-class classification problem.\nThe chosen attributes are “Heavy Makeup”, “Mouth\nSlightly Open”, and “Smiling”. Like other works  7,\nwe assume that we have access to the test set of these datasets. We\nuse 70% data of the test set for training our defense methods (𝒟valsubscript𝒟val\\mathcal{D}_{\\text{val}}\nin Sections 3, 4)\nand 30% for testing (denoted as 𝒟testsubscript𝒟test\\mathcal{D}_{\\text{test}}). For more\ndetails about the datasets, please refer to Appdx. 0.B.1.\nSometimes, we do not test our methods on all images in 𝒟testsubscript𝒟test\\mathcal{D}_{\\text{test}}\nbut on those not belonging to the target class. This set is\ndenoted as 𝒟test′subscriptsuperscript𝒟′test\\mathcal{D}^{\\prime}_{\\text{test}}. We also provide results with less\ntraining data in Appdx. 5.4.2. We use 5 different benchmark Trojan attacks for our defenses, which\nare BadNet+, noise-BI+, image-BI+, InpAwAtk ,\nand WaNet . InpAwAtk and WaNet are recent SOTA\nattacks that were shown to break many strong defenses completely.\nBadNet+ and noise/image-BI+ are variants of BadNet 1\nand Blended Injection (BI)  that use multiple\ntriggers instead of one. They are described in detail in Appdx. 0.C.1.\nThe training settings for the 5 attacks are given in Appdx. 0.B.2. We also consider 2 attack modes namely single-target and all-target\n . In the first mode, only\none class t𝑡t is chosen as target. Every Trojan image x~~𝑥\\tilde{x}\nis classified as t𝑡t regardless of the ground-truth label of its\nclean counterpart x𝑥x. Without loss of generality, t𝑡t is set to\n0. In the second mode, x~~𝑥\\tilde{x} is classified as (k+1)modKmodulo𝑘1𝐾(k+1)\\mod K\nif x𝑥x belongs to the class k𝑘k. If not clearly stated, attacks\nare assumed to be single-target. We report the test clean and Trojan accuracies of the benchmark attacks\n(in single-target mode) in Table 5.1.\nIt is clear that all attacks achieve very high Trojan accuracies with\nlittle or no decrease in clean accuracy compared to the benign model’s,\nhence, are qualified for our experimental purpose. For results of\nthe attacks on 𝒟testsubscript𝒟test\\mathcal{D}_{\\text{test}}, please refer to Appdx. 0.C. We consider 5 well-known baseline defenses namely Neural Cleanse (NC)\n7, STRIP , Network\nPruning (NP) , Neural Attention Distillation (NAD)\n, and Februus . Neural Cleanse (NC) assumes that attacks (i) choose only one target\nclass t𝑡t and (ii) use at least (not exactly) one input-agnostic\ntrigger associated with t𝑡t. We refer to (i) as the “single\ntarget class” assumption and (ii) as the “input-agnostic\ntrigger” assumption. Based on these assumptions, NC finds a trigger\nψk=(mk,pk)subscript𝜓𝑘subscript𝑚𝑘subscript𝑝𝑘\\psi_{k}=(m_{k},p_{k}) for every class k∈𝒴𝑘𝒴k\\in\\mathcal{Y} via reverse-engineering\n(Eq. 2), and uses the L1 norms of the\nsynthetic trigger masks {m1,…,mK}subscript𝑚1…subscript𝑚𝐾\\{m_{1},...,m_{K}\\} to detect the target\nclass. The intuition is that if t𝑡t is the target class, ‖mt‖1subscriptnormsubscript𝑚𝑡1{\\|m_{t}\\|}_{1}\nwill be much smaller than the rest. A z𝑧z-value of each mask norm\nis calculated via Median Absolute Deviation and the z𝑧z-value of\nthe smallest mask norm (referred to as the anomaly index) is\ncompared against a threshold ζ𝜁\\zeta (2.0 by default). If the anomaly\nindex is smaller than ζ𝜁\\zeta, 𝙲𝙲\\mathtt{C} is marked as clean.\nOtherwise, 𝙲𝙲\\mathtt{C} is marked Trojan-infected with the target\nclass corresponding to the smallest mask norm. In this case, the Trojans\nin 𝙲𝙲\\mathtt{C} can be mitigated via pruning or via checking the\ncleanliness of input images. Both mitigation methods make use of ψtsubscript𝜓𝑡\\psi_{t}\nand are analyzed in Appdx. 0.D. STRIP assumes triggers are input-agnostic and argues that if an input\nimage x𝑥x contains triggers then these triggers still have effect\nif x𝑥x is superimposed (blended) with other images. Therefore, STRIP\nsuperimposes x𝑥x with Nssubscript𝑁sN_{\\text{s}} random clean images from 𝒟valsubscript𝒟val\\mathcal{D}_{\\text{val}}\nand computes the average entropy ℋ​(x)ℋ𝑥\\mathcal{H}(x) of Nssubscript𝑁sN_{\\text{s}}\npredicted class probabilities corresponding to Nssubscript𝑁sN_{\\text{s}} superimposed\nversions of x𝑥x. If ℋ​(x)ℋ𝑥\\mathcal{H}(x) is smaller than a predefined\nthreshold, x𝑥x is considered as trigger-embedded, otherwise, clean.\nThe threshold is set according to the false positive rate (FPR) over\nthe average entropies of all images in 𝒟valsubscript𝒟val\\mathcal{D}_{\\text{val}},\nusually at FPR equal to 1/5/10%. We evaluate the performance of STRIP\nagainst an attack using Mssubscript𝑀sM_{\\text{s}} random clean images from 𝒟testsubscript𝒟test\\mathcal{D}_{\\text{test}}\nand Mssubscript𝑀sM_{\\text{s}} corresponding Trojan images generated by that\nattack. Following , we set Ns=100subscript𝑁s100N_{\\text{s}}=100\nand Ms=2000subscript𝑀s2000M_{\\text{s}}=2000. Network Pruning (NP) hypothesizes that idle neurons are more likely\nto store Trojan-related information. Thus, it ranks neurons in the\nsecond top layer of 𝙲𝙲\\mathtt{C} according to their average activation\nover all data in 𝒟valsubscript𝒟val\\mathcal{D}_{\\text{val}} and gradually prunes them until\na certain decrease in clean accuracy is reached, usually at 1/5/10%\ndecrease in clean accuracy. Neural Attention Distillation (NAD)  is a distillation-based\nTrojan defense. It first fine-tunes the pretrained classifier 𝙲𝙲\\mathtt{C}\non clean images in 𝒟valsubscript𝒟val\\mathcal{D}_{\\text{val}} to obtain a fine-tune classifier\n𝚃𝚃\\mathtt{T}. Then, it treats 𝚃𝚃\\mathtt{T} and 𝙲𝙲\\mathtt{C} as\nthe teacher and student respectively, and performs attention-based\nfeature distillation 2 between 𝚃𝚃\\mathtt{T}\nand 𝙲𝙲\\mathtt{C} on 𝒟valsubscript𝒟val\\mathcal{D}_{\\text{val}} again. Since 𝚃𝚃\\mathtt{T}\nis 𝙲𝙲\\mathtt{C} fine-tuned on clean data, 𝚃𝚃\\mathtt{T} is expected\nto have most of the Trojan in 𝙲𝙲\\mathtt{C} removed. Via distillation,\nsuch Trojan-free knowledge is transferred from 𝚃𝚃\\mathtt{T} to 𝙲𝙲\\mathtt{C}\nwhile performance of 𝙲𝙲\\mathtt{C} on clean data is still preserved. Among the baselines, Februus is the most related to our filtering\ndefenses since it mitigates Trojan attacks via input purification.\nIt uses GradCAM 0 to detect regions in an\ninput image x𝑥x that may contain triggers. Then, it removes all pixels\nin the suspected regions and generates new ones via inpainiting. The\ninpainted image is expected to contain no trigger and is fed to 𝙲𝙲\\mathtt{C}\ninstead of x𝑥x. Please refer to Appdx. 0.B.3. We evaluate VIF/AIF using 3 metrics namely decrease in clean\naccuracy (↓↓\\downarrowC), Trojan accuracy (T), and decrease\nin recovery accuracy (↓↓\\downarrowR). The first is the difference\nbetween the classification accuracies of clean images before and after\nfiltering. The second is the attack success rate of Trojan images\nafter filtering. The last is the difference between the classification\naccuracy of clean images before filtering and that of the corresponding\nTrojan images after filtering. Smaller values of the metrics indicate\nbetter results. ↓↓\\downarrowC and ↓↓\\downarrowR are computed on\n𝒟testsubscript𝒟test\\mathcal{D}_{\\text{test}}. T is computed on 𝒟test′subscriptsuperscript𝒟′test\\mathcal{D}^{\\prime}_{\\text{test}} under\nsingle-target attacks and 𝒟testsubscript𝒟test\\mathcal{D}_{\\text{test}} under all-target\nattacks. This ensures that T can be 00 in the best case. Otherwise,\nT will be around 1/K1𝐾1/K where K𝐾K is the total number of classes.\n↓↓\\downarrowC and ↓↓\\downarrowR are upper-bounded by 1 and can be\nnegative. We evaluate VIFtC/AIFtC using FPR and FNR. FPR/FNR is defined as the\nproportion of clean/Trojan images having different/similar class predictions\nwhen the filter 𝙵𝙵\\mathtt{F} is applied and not applied. FPR is computed\non 𝒟testsubscript𝒟test\\mathcal{D}_{\\text{test}}. FNR is computed on 𝒟test′subscriptsuperscript𝒟′test\\mathcal{D}^{\\prime}_{\\text{test}}\nunder single-target attacks and 𝒟testsubscript𝒟test\\mathcal{D}_{\\text{test}} under all-target\nattacks. Both metrics are in   and smaller values of them\nare better. Interestingly, FPR and FNR are strongly correlated to\n↓↓\\downarrowC and T, respectively. FPR/FNR is exactly equal to ↓↓\\downarrowC/T\nif 𝙲𝙲\\mathtt{C} achieves perfect clean/Trojan accuracy.\n\nReference markers found: [38]\n\nContext windows for each [ref]:\n[ref]38: Following previous works 1  [ref]38,\nwe evaluate our proposed defenses on four image datasets namely MNIST,\nCIFAR10 , GTSRB ,\nand CelebA . For CelebA, we follow Salem et al.\n[ref]38 and select the top 3 most balanced\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"FaceNet: A Unified Embedding for Face Recognition and Clustering\",\n                \"Bayesian Bits: Unifying Quantization and Pruning\",\n                \"Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering\",\n                \"DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks\",\n                \"Targeted backdoor attacks on deep learning systems using data poisoning\",\n                \"Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems\",\n                \"Black-box Detection of Backdoor Attacks with Limited Information and Data\",\n                \"Adversarial vulnerability for any classifier\",\n                \"Strip: A Defence Against Trojan Attacks on Deep Neural Networks\",\n                \"Generative adversarial nets\",\n                \"Explaining and Harnessing Adversarial Examples\",\n                \"Badnets: Identifying vulnerabilities in the machine learning model supply chain\",\n                \"TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems\",\n                \"Scalable Backdoor Detection in Neural Networks\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Identity Mappings in Deep Residual Networks\",\n                \"Backdoor attacks against learning systems\",\n                \"Adam: A Method for Stochastic Optimization\",\n                \"Auto-encoding variational bayes\",\n                \"Learning Multiple Layers of Features from Tiny Images\",\n                \"Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks\",\n                \"Backdoor attack with sample-specific triggers\",\n                \"Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks\",\n                \"ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation\",\n                \"Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks\",\n                \"Neural Trojans\",\n                \"Deep learning face attributes in the wild\",\n                \"Bayesian Compression for Deep Learning\",\n                \"Towards Deep Learning Models Resistant to Adversarial Attacks\",\n                \"Adversarial training and dilated convolutions for brain MRI segmentation\",\n                \"Variational Dropout Sparsifies Deep Neural Networks\",\n                \"Poisoning Attacks with Generative Adversarial Nets\",\n                \"Input-aware dynamic backdoor attack\",\n                \"Wanet–imperceptible warping-based backdoor attack\",\n                \"Deep face recognition\",\n                \"Defending Neural Backdoors via Generative Distribution Modeling\",\n                \"You Only Look Once: Unified, Real-Time Object Detection\",\n                \"Hidden trigger backdoor attacks\",\n                \"Dynamic Backdoor Attacks Against Machine Learning Models\",\n                \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]38 based on the context and research area of Towards Effective and Robust Neural Trojan Defenses via Input Filtering. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}