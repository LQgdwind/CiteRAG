{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: 1 Introduction\n\nPaper Abstract: \n\nPaper Section Text:\nUtilizing the closed-form solution of optimal transport in one dimensionÂ , the sliced WassersteinÂ  (SW) distance is computationally and statistically scalable. In particular, SW has the time complexity of ğ’ªâ€‹(nâ€‹logâ¡n)ğ’ªğ‘›ğ‘›\\mathcal{O}(n\\log n) and the space complexity of ğ’ªâ€‹(n)ğ’ªğ‘›\\mathcal{O}(n) when dealing with two probability measures that have at most nğ‘›n supports. Moreover, the SW does not suffer from the curse of dimensionality since it has sample complexity of ğ’ªâ€‹(nâˆ’1/2)ğ’ªsuperscriptğ‘›12\\mathcal{O}(n^{-1/2})Â [ref]27 . As evidence for the effectiveness of the SW distance, many applications have leveraged SW as the key to improving performance e.g., generative modelsÂ , domain adaptationÂ , point-cloud upsamplingÂ , clusteringÂ , gradient flowsÂ [ref]5, and so on. A key component of the SW is the slicing distribution, i.e., the distribution of the projecting direction. The slicing distribution controls the contribution of each projecting direction to the final value of the SW. A statistically desired slicing distribution should satisfy two properties. The first property is to assign a high probability to â€œinformative\" projecting directions. In some applications that involve weak convergence of measures, such as generative modelingÂ  and gradient flowÂ [ref]5, an informative direction can be interpreted as a discriminative direction that highlights the difference between two measures after projection. The second property is to have the maximal projecting direction in the support set, i.e., the projecting direction that can lead to the maximum projected distance. This property ensures the identity of indiscernible of the expected value. To ensure the second property, a more relaxed criterion can be considered, which involves having a continuous density on the unit-hypersphere. Beyond statistical properties, two computational properties are desired for the slicing distribution. Firstly, the slicing distribution should be stable to obtain, meaning that parameters of the slicing distribution can be computationally identifiable. Secondly, it should be fast and efficient to sample projecting directions from the slicing distribution. This property is essential, as Monte Carlo samples are used to approximate the intractable expectation with respect to the slicing distribution. The conventional SWÂ  utilizes the uniform distribution over the unit-hypersphere as the slicing distribution, which is a fixed, continuous, and easy-to-sample distribution. However, the uniform slicing distribution is not discriminative. Distributional sliced WassersteinÂ [ref]31 (DSW) performs optimization to find the best slicing distribution belonging to a chosen family of distributions over the unit-hypersphere, aiming to maximize the expected projected distance. When the family is a collection of Dirac measures, DSW reverts to max sliced WassersteinÂ  (Max-SW). DSW and Max-SW are discriminative, but they are not stable, computationally expensive, and require a differentiable ground metric. This is because they involve iterative gradient-based optimization, which may not guarantee a global optimum. Energy-based sliced WassersteinÂ  (EBSW) utilizes the energy-based slicing distribution, which has a density proportional to an increasing function of the projected distance. The energy-based slicing distribution is continuous, optimization-free, and discriminative. However, it requires more sophisticated sampling techniques to obtain projecting directions, such as importance sampling and Markov chain Monte Carlo (MCMC), due to the unnormalized density. Moreover, these sampling techniques lead to biased weighted estimates. To design an effective slicing distribution, the key is to relate the continuous slicing distribution with the two input measures. This connection can be achieved through optimization, as demonstrated in DSWÂ [ref]31, or by directly defining the slicing distribution based on the two input measures, as seen in EBSWÂ . Designing an efficient slicing distribution poses a more challenging task. Interactive procedures, such as optimization (as in DSW) and sequential sampling (as in EBSW with MCMC), should be avoided. The ideal scenario is to have a slicing distribution that allows for i.i.d. sampling, making it parallelizable. Using a proposal distribution, importance sampling in EBSW could yield a fast estimate. However, importance sampling requires a careful choice of the proposal distribution for accurate estimation, especially in high dimensions. Additionally, importance sampling leads to biased estimates in the case of unnormalized densities. To address the challenge, we propose a novel, effective, and efficient slicing distribution obtained based on a new notation of projecting direction, named random-path projecting direction (RPD). The idea is to first create a â€˜pathâ€™ between two input measures, then project the two measures along that path and conduct the one-dimensional Wasserstein comparison. Here, a path is defined as the normalized difference between two random variables that are distributed with two input measures, respectively, with an additional random perturbation for continuity guarantee. Since the path is defined as the random difference, it captures the directions in which the two measures differ. Therefore, using the distribution of the random-paths can lead to a discriminative slicing distribution. In addition, sampling a random path is efficient since it only requires the mild assumption that two input measures are easy to sample from, a condition usually held in machine learning applications. Contribution. In summary, our contributions are three-fold: 1. We propose a new type of projecting direction, named random-path projecting direction (RPD), which involves a random perturbation of the normalized difference between two random variables distributed under two input measures. From the RPD, we derive the random-path slicing distribution (RPSD), which is guaranteed to be continuous. Despite having intractable density, the RPSD is fast and simple to sample random projecting directions. 2. From the RPSD, we introduce two novel variants of sliced Wasserstein. The first variant is called random-path projection sliced Wasserstein (RPSW), which replaces the uniform distribution in SW with the RPSD. The second variant is importance-weighted random-path projection sliced Wasserstein (IWRPSW). It is defined as an expectation of a weighted average of multiple randomly projected distances with RPDs. We then establish theoretical properties of RPSW and IWRPSW, encompassing topological, statistical, and computational aspects. In greater detail, we delve into the metricity of RPSW and IWRPSW, their connections to other SW variants, their sample complexity, and their computational complexities using Monte Carlo methods. 3. We conduct a comparative analysis of the proposed RPSW and IWRPSW against existing SW variants, including SW, Max-SW, DSW, and EBSW. Our first application involves gradient flow, where distances are utilized to guide a source distribution towards a target distribution. Furthermore, we introduce a new framework for training denoising diffusion models using SW metrics through the augmented generalized mini-batch energy (AGME) distance. In this framework, SW variants serve as the kernel to assess the difference between two random sets. Here, AGME functions as the loss function, aiming to minimize the difference between forward transition distributions and their corresponding reverse transition distributions in denoising diffusion models. In addition to serving as a benchmark for comparing SW variants, our proposed training approach can improve generative quality and sampling speed. Organization. We begin by reviewing some preliminaries in SectionÂ 2. Following that, in SectionÂ 3, we define the random-path projection direction, the random-path slicing distribution, RPSW, and IWRPSW. We then derive their theoretical and computational properties. SectionÂ 4 presents experiments comparing RPSW and IWRPSW with other SW variants. In SectionÂ 5, we provide concluding remarks. Finally, we defer the proofs of key results and additional materials to the Appendices. Notations. For any dâ‰¥2ğ‘‘2d\\geq 2, we denote ğ•Šdâˆ’1:={Î¸âˆˆâ„dâˆ£â€–Î¸â€–22=1}assignsuperscriptğ•Šğ‘‘1conditional-setğœƒsuperscriptâ„ğ‘‘superscriptsubscriptnormğœƒ221\\mathbb{S}^{d-1}:=\\{\\theta\\in\\mathbb{R}^{d}\\mid||\\theta||_{2}^{2}=1\\} and ğ’°â€‹(ğ•Šdâˆ’1)ğ’°superscriptğ•Šğ‘‘1\\mathcal{U}(\\mathbb{S}^{d-1}) as the unit hyper-sphere and its corresponding uniform distribution . We denote ğ’«â€‹(ğ’³)ğ’«ğ’³\\mathcal{P}(\\mathcal{X}) as the set of all probability measures on the set ğ’³ğ’³\\mathcal{X}. For pâ‰¥1ğ‘1p\\geq 1, ğ’«pâ€‹(ğ’³)subscriptğ’«ğ‘ğ’³\\mathcal{P}_{p}(\\mathcal{X}) is the set of all probability measures on the set ğ’³ğ’³\\mathcal{X} that have finite pğ‘p-moments. For any two sequences ansubscriptğ‘ğ‘›a_{n} and bnsubscriptğ‘ğ‘›b_{n}, the notation an=ğ’ªâ€‹(bn)subscriptğ‘ğ‘›ğ’ªsubscriptğ‘ğ‘›a_{n}=\\mathcal{O}(b_{n}) means that anâ‰¤Câ€‹bnsubscriptğ‘ğ‘›ğ¶subscriptğ‘ğ‘›a_{n}\\leq Cb_{n} for all nâ‰¥1ğ‘›1n\\geq 1, where Cğ¶C is some universal constant.\n\nReference markers found: [27]\n\nContext windows for each [ref]:\n[ref]27: probability measures that have at most nğ‘›n supports. Moreover, the SW does not suffer from the curse of dimensionality since it has sample complexity of ğ’ªâ€‹(nâˆ’1/2)ğ’ªsuperscriptğ‘›12\\mathcal{O}(n^{-1/2})Â [ref]27 . As evidence for the effectiveness of the SW distance, many applications have leveraged SW as the key to improving performance e.g., generative modelsÂ , domain adaptationÂ , point-cloud\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 27,\n            \"titles\": [\n                \"Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance\",\n                \"Sliced Optimal Partial Transport\",\n                \"Partial Transport for Point-Cloud Registration\",\n                \"Spherical sliced-Wasserstein\",\n                \"Hyperbolic sliced-wasserstein via geodesic and horospherical projections\",\n                \"Efficient gradient flows in sliced-Wasserstein space\",\n                \"Sliced-Wasserstein on Symmetric Positive Definite Matrices for M/EEG Signals\",\n                \"Spot: sliced partial optimal transport\",\n                \"Sliced and Radon Wasserstein barycenters of measures\",\n                \"Unidimensional and evolution methods for optimal transportation\",\n                \"Partial Adversarial Domain Adaptation\",\n                \"The Power Spherical distribution\",\n                \"Max-Sliced Wasserstein Distance and its use for GANs\",\n                \"Generative modeling using the sliced Wasserstein distance\",\n                \"A probabilistic theory of pattern recognition, volume 31\",\n                \"Interpolating between optimal transport and MMD using Sinkhorn divergences\",\n                \"Pot: Python optimal transport\",\n                \"Generative adversarial nets\",\n                \"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium\",\n                \"Denoising Diffusion Probabilistic Models\",\n                \"Maximum likelihood estimators for the matrix von Mises-Fisher and bingham distributions\",\n                \"Auto-encoding variational bayes\",\n                \"Sliced Wasserstein Distance for Learning Gaussian Mixture Models\",\n                \"Learning multiple layers of features from tiny images\",\n                \"Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation\",\n                \"A convexity principle for interacting gases\",\n                \"Approximate Bayesian computation with the sliced-Wasserstein distance\",\n                \"Statistical and Topological Properties of Sliced Probability Divergences\",\n                \"Quasi-monte carlo for 3d sliced wasserstein\",\n                \"Energy-Based Sliced Wasserstein Distance\",\n                \"Distributional sliced-Wasserstein and applications to generative modeling\",\n                \"Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction\",\n                \"Markovian Sliced Wasserstein Distances: Beyond Independent Projections\",\n                \"Statistical, Robustness, and Computational Guarantees for Sliced Wasserstein Distances\",\n                \"Sliced optimal transport sampling\",\n                \"Computational optimal transport\",\n                \"Improving GANs using optimal transport\",\n                \"Optimal transport for applied mathematicians\",\n                \"Lidar Upsampling with Sliced Wasserstein Distance\",\n                \"Unbalanced Optimal Transport meets Sliced-Wasserstein\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]27 based on the context and research area of 1 Introduction. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}