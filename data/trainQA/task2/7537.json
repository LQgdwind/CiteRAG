{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Brain-inspired and Self-based Artificial Intelligence\n\nPaper Abstract: \n\nPaper Section Text:\nIn BriSe AI, a variety of fundamental learning strategies are employed to support the realization different levels of Self. These learning strategies draw inspiration from the biological brain’s structure, mechanisms, and information processing. Incorporating multi-scale biological plasticities, BriSe AI integrates ten categories of SNN-based learning strategies, ranging from basic unsupervised learning, supervised learning, reinforcement learning, and associative learning, to few-shot learning, multi-modal concept learning, continual learning, transfer learning, and evolutionary learning. Unsupervised Learning.\nBriSe AI-integrated unsupervised learning is based on Spike-Timing-Dependent Plasticity (STDP), and introduces adaptive synaptic filters for regulating regulate input currents, adaptive lateral inhibitory connections to ensure diverse feature extraction, and adaptive threshold balancing for maintaining the input-output balance across neural layers [ref]19. Together, these adaptation mechanisms improve the network’s information processing, learning efficiency, and feature representation capabilities. Supervised Learning.\nTraditional SNN training algorithms mainly focus on the guiding role of label information in predictions, often neglecting the variability in the network’s output distribution across different time steps. This variability can lead to conflicts in the direction of optimization during the training process, thereby hindering effective optimization of the network. Similarly, during the testing process, differences in distribution between time steps can affect the network’s performance due to inaccurate output results. Our supervised learning algorithm  enhances the consistency across different time steps, thereby improving the stability of SNN training and significantly enhancing performance under low-latency conditions. This algorithm combines traditional cross-entropy loss with a novel enhanced temporal consistency loss, the latter reducing differences between time steps through Kullback-Leibler divergence. Reinforcement Learning.\nWe adopt a layered membrane potential regularization method, and construct an efficient brain-inspired spiking neural network model for deep reinforcement learning with efficient spike information transmission . Inspired by the brain’s state value distribution decision-making mechanism, we employ neuronal population coding to enhance the representational capability of spike sequence information, creating Gaussian neuron populations for the dimensional expansion and separable representation of binary spike sequences corresponding to continuous quantile values. Drawing inspiration from the structure of brain pyramidal neurons, we use multi-compartment neural networks to address the fusion of decision-making information from diverse sources  . The population neuron coding approach and multi-compartment neuron information integration structure are more effective at handling multi-source decision information than the commonly used point neuron models and membrane potential computation methods in spiking neural networks. Associative Learning. Motor-visual associative learning is the process of establishing connections between self-generated movements and visual feedback during the learning of motion. The somatosensory pathway and the dorsal visual pathway are the main pathways involved in this process. If signals from these pathways occur simultaneously within the same time window, new connections may form or existing connections may be strengthened to represent their association. Appearance learning is the process of learning the appearance of body parts when the expected visual feedback is consistent with the movement. The proprioceptive pathway and the ventral visual pathway are the main pathways involved in this process. Through motor-visual associative learning, robots can associate expected visual feedback with their self-generated movements. When these self-generated movements are executed again, real visual feedback is received. If this real visual feedback is consistent with the expected feedback, relevant brain areas are activated, connections are established through synaptic plasticity mechanisms, and the robot learns the appearance of body parts. Few-shot Learning. We develop a neuromorphic dataset that reflects the complexities of human visual perception and cognition, N-Omniglot . The dataset comprises an extensive array of handwritten characters, each category represented by a minimal number of examples. In addition, we have developed a variety of few-shot learning methods, including improved nearest neighbor, convolutional network, SiameseNet, and meta-learning algorithm, demonstrate the versatility and efficacy of N-Omniglot. These methods are carefully tailored to exploit the unique characteristics of the N-Omniglot dataset, exemplify BriSe AI’s multifaceted approach to tackling complex AI challenges. Multi-modal Concept Learning. We develop a computational model for concept learning that accurately emulates the architectural paradigm of human cognition[ref]47.\nThe tripartite structure of the computational model - consisting of a multisensory information processing module, a text-derived information processing module, and a self-based semantic control module - mirrors the three networks of human cognition: a multimodal experiential system, a language-based semantic system, and a semantic control system.\nThe main idea of our human-like concept learning is that despite the myriad of external inputs encountered by the human brain during the learning of new concepts, these inputs are all processed as unified signal transmissions within the brain.\nWe convert the two types of concept representations into discrete neural stimuli, transforming the original representation into a spike train, thereby standardizing the data representation and simulating the initial process of concept acquisition in the human brain.\nWhat’s more, we innovatively leverage the properties of spiking neural networks[ref]49.\nThe information in the temporal domain is harmonized by controlling the duration T of the performance of neurons simulated by external information, and the information from the two representations is integrated via sliding coordination.\nThe final human-like concept representation is derived once the concept representation has been coordinated both spatially and temporally. Continual Learning. The human brain possesses the ability to learn multiple cognitive tasks sequentially and its structural developmental mechanisms allow the nervous system to dynamically expand and contract to flexibly invoke task-specific neural circuits. Motivated by this, a Dynamic Structure Development of Spiking Neural Networks (DSD-SNN) is proposed for efficient and adaptive continual learning . When a new task is coming, the DSD-SNN model first randomly grows a part of neurons forming a new pathway to learn new knowledge and receive old knowledge that can be reused. During training for new tasks, the DSD-SNN gradually prune the redundant neurons that are continuously inactive for several epochs. Dynamic neuronal expansion and pruning increase memory capacity and reduce energy consumption, and the overlapping shared structure helps to quickly leverage all acquired knowledge to new tasks. In addition to a continual learning approach based on dynamic development, we also implement a continual learning model based on local synapse importance evaluation and sleep-induced memory consolidation. This model has two phases: waking and sleep. In the waking stage, the model realizes the important weights of different tasks through the local computation rule, the Hebbian algorithm. The important weights are constrained to not change largely so as to restrain the learning process of new tasks successively, reducing the interference of old knowledge. During sleep, the homeostasis realized by a global adjustment of the network activities enhances the working memory and neural replay by the reorganized connection between the working and long-term memories, to further consolidate the long-term memory. Transfer Learning. Transfer learning enables spiking neural networks to adapt knowledge from static image data and then effectively transfer and apply this knowledge to event-driven data processing. This self-adaptive ability enhances the network’s ability to generalize when processing event data and improves the network’s flexibility and adaptability across multiple data types. In detail, we propose a knowledge transfer method that combines domain alignment loss and spatio-temporal regularization. This method learns domain-invariant spatial features by reducing the marginal distribution distance between static images and event data. Meanwhile, spatio-temporal regularisation captures temporal attributes in the data by providing dynamically learnable coefficients for the domain alignment loss, which ensures a specific weight assignment to the data features at each time step.\nOur method is represented as a loss function that constrains and guides the network to learn. The knowledge transfer loss can be expressed as: where ηtsubscript𝜂𝑡\\eta_{t} denotes the learnable coefficient at time step t𝑡t and σ𝜎\\sigma represents the sigmoid function.\nC​K​A′𝐶𝐾superscript𝐴′\\mathop{CKA^{\\prime}} represents the computation of the kernel function of the vectors followed by the computation of central kernel alignment (CKA) . g​(𝐱si,t)𝑔superscriptsubscript𝐱𝑠𝑖𝑡g\\left(\\mathbf{x}_{s}^{i},t\\right) and g​(𝐱tj,t)𝑔superscriptsubscript𝐱𝑡𝑗𝑡g\\left(\\mathbf{x}_{t}^{j},t\\right) denote the features of the data in the source and target domains respectively after the encoder g𝑔g. ℓc​l​s−esubscriptℓ𝑐𝑙𝑠𝑒\\ell_{cls-e}\nis classification loss of event data. Evolutionary Learning.\nFor evolutionary learning, the NeuEvo  framework introduces an integrated approach for evolving the neural circuitry of SNNs, inspired by the dynamic and adaptive nature of biological neural systems. This integrated approach is not solely focused on evolutionary computation, but rather encompasses a comprehensive strategy that blends local synaptic dynamics with global network optimization. The framework leverages a synergistic process for synaptic weight adaptation. This process combines local synaptic activity, governed by the STDP rule, with global error feedback for network-wide optimization. The synaptic weight (w𝑤w) at any given neuron is updated based on the following formulation: Here, wtsubscript𝑤𝑡w_{t} represents the synaptic weight at time t𝑡t, η𝜂\\eta and β𝛽\\beta are the learning rates for local and global updates, respectively, and Δ​wl​o​c​a​lΔsubscript𝑤𝑙𝑜𝑐𝑎𝑙\\Delta w_{local} and Δ​wg​l​o​b​a​lΔsubscript𝑤𝑔𝑙𝑜𝑏𝑎𝑙\\Delta w_{global} are the weight changes due to local and global learning processes. The structural configuration of neural circuits in the NeuEvo framework is dynamically adjusted based on the interplay of local and global learning signals. Initially, a diverse and dense connection pattern is established, simulating the potential synaptic pathways in the network. These connections are then selectively strengthened or weakened based on their contribution to the network’s performance, as well as their efficiency and reliability in information processing. The adjustment of these connections is driven by a balance between reinforcing beneficial pathways and pruning less effective ones, thereby optimizing the network’s architecture for specific tasks. In addition, the ELSM  and MSE-NAS  specially design adaptive structure evolution for recurrent and deep spiking neural networks, respectively. Through exploring the small-world characteristics that exist in the human brain, ELSM guides the evolution of network topology from both static and dynamic perspectives. MSE-NAS evolves microscopic neuron computations, mesoscopic microcircuit combinations, and macroscopic global brain area connectivity, supplemented by customised brain-inspired evaluation function, encoding scheme, and genetic operations. Adaptive LSM  deeply draws inspiration from brain evolution mechanisms, integrating structural evolution with multi-scale biological plasticity learning rules.\n\nReference markers found: [19]\n\nContext windows for each [ref]:\n[ref]19: regulate input currents, adaptive lateral inhibitory connections to ensure diverse feature extraction, and adaptive threshold balancing for maintaining the input-output balance across neural layers [ref]19. Together, these adaptation mechanisms improve the network’s information processing, learning efficiency, and feature representation capabilities. Supervised Learning.\nTraditional SNN\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 19,\n            \"titles\": [\n                \"Improving Stability and Performance of Spiking Neural Networks through Enhancing Temporal Consistency\",\n                \"Computing machinery and intelligence\",\n                \"Minds, brains and science\",\n                \"Discours de la méthode\",\n                \"Robot Self (1–9)\",\n                \"Unified theories of cognition\",\n                \"Language, memory, and thought\",\n                \"Soar: An architecture for general intelligence\",\n                \"The Soar cognitive architecture\",\n                \"The leabra model of neural interactions and learning in the neocortex\",\n                \"Sal: An explicitly pluralistic cognitive architecture\",\n                \"A tutorial on clarion 5.0\",\n                \"Non-axiomatic reasoning system: Exploring the essence of intelligence\",\n                \"A large-scale model of the functioning brain\",\n                \"A cognitive theory of consciousness\",\n                \"Consciousness is computational: The lida model of global workspace theory\",\n                \"Networks of spiking neurons: the third generation of neural network models\",\n                \"BrainCog: A Spiking Neural Network based Brain-inspired Cognitive Intelligence Engine for Brain-inspired AI and Brain Simulation\",\n                \"Five kinds of self-knowledge\",\n                \"An Unsupervised STDP-based Spiking Neural Network Inspired By Biologically Plausible Learning Rules and Connections\",\n                \"N-omniglot, a large-scale neuromorphic dataset for spatio-temporal sparse few-shot learning\",\n                \"Enhancing efficient continual learning with dynamic structure development of spiking neural networks\",\n                \"Improving the performance of spiking neural networks on event-based datasets with knowledge transfer\",\n                \"Brain-inspired neural circuit evolution for spiking neural networks\",\n                \"Rubber hands ‘feel’touch that eyes see\",\n                \"Brain-inspired bodily self-perception model for robot rubber hand illusion\",\n                \"Toward robot self-consciousness (ii): brain-inspired robot bodily self model for self-recognition\",\n                \"Distinct regions of the medial prefrontal cortex are associated with self-referential processing and perspective taking\",\n                \"Hierarchical rhythmic propagation of corticothalamic interactions for consciousness: A computational study\",\n                \"Consciousness and anesthesia\",\n                \"Thalamocortical mechanisms for the anteriorization of alpha rhythms during propofol-induced unconsciousness\",\n                \"Conditioned Reflexes: An Investigation of the Physiological Activity of the Cerebral Cortex\",\n                \"Brain-inspired classical conditioning model\",\n                \"A brain-inspired decision-making spiking neural network and its application in unmanned aerial vehicle\",\n                \"Solving the Spike Feature Information Vanishing Problem in Spiking Deep Q Network with Potential Based Normalization\",\n                \"A brain-inspired decision making model based on top-down biasing of prefrontal cortex to basal ganglia and its application in autonomous uav explorations\",\n                \"Nature-inspired self-organizing collision avoidance for drone swarm based on reward-modulated spiking neural network\",\n                \"Does the chimpanzee have a theory of mind?\",\n                \"Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children’s understanding of deception\",\n                \"A brain-inspired model of theory of mind\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]19 based on the context and research area of Brain-inspired and Self-based Artificial Intelligence. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}