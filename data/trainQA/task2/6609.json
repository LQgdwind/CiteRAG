{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching\n\nPaper Abstract: \n\nPaper Section Text:\nThe implementation for the pre-training stage is the same as our single-scale setting in the main manuscript.\nIn this section, we focus on the setting for the meta-training stage.\nTo achieve the best results for those few-shot learning methods  [ref]32  , we use different settings.\nThere are also slight differences in the settings of the same method on different datasets. ProtoNet [ref]32.\nWe utilize the AGCN (a single-scale model, detailed in Figure 5 of the Appendix) as the backbone for ProtoNet.\nDuring the meta-learning stage, features for ProtoNet learning are extracted using the pre-trained models, specifically from the global pooled feature of the 9th AGCN block, sized at 1√ó1√óC(9)11superscriptùê∂91\\times 1\\times C^{(9)}.\nRegarding training specifics, for both NTU RGB+D and NTU RGB+D 120 datasets, we set the learning rate to 0.001.\nFor the PKU-MMD dataset, the initial learning rate is set at 0.0005.\nAcross all three datasets, we reduce the learning rate by half every 10 epochs.\nFor every 5-way 1-shot task, there‚Äôs a single prototype for each class, meaning each meta task encompasses 5 prototypes. FEAT .\nWe utilize the AGCN (detailed in Figure 5 of the Appendix) as the backbone for FEAT. During the meta-learning stage, features for FEAT learning are extracted using the pre-trained models, specifically from the global pooled feature of the 9th AGCN block, sized at 1√ó1√óC(9)11superscriptùê∂91\\times 1\\times C^{(9)}.\nDiving into the training specifics, across the NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets, our initial learning rate is set at 0.0005, which we reduce by half every 10 epochs.\nAdditionally, there is a weight value to balance the contrastive term in the learning objective.\nHere we set the balance value to 0.1 for NTU RGB+D and NTU RGB+D 120 datasets and 0.01 for the PKU-MMD dataset. Subspace .\nSimilar to ProtoNet and FEAT, we also leverage the single-scale model as the backbone for Subspace and the feature for Subspace model learning is also in the size of 1√ó1√óC(9)11superscriptùê∂91\\times 1\\times C^{(9)}.\nFor NTU RGB+D and NTU RGB+D 120 datasets, we set the learning rate to 0.005.\nFor the PKU-MMD dataset, the initial learning rate is set at 0.0005.\nWe cut the learning rate to half every 5 epochs for all three datasets. Dynamic Filter .\nConsistent with ProtoNet, FEAT, and Subspace, we employ the single-scale model as the backbone for Dynamic Filter . Notably, while Dynamic Filter  operates based on feature maps, the input feature for Dynamic Filter learning is in size of H(9)√óW(9)√óC(9)superscriptùêª9superscriptùëä9superscriptùê∂9H^{(9)}\\times W^{(9)}\\times C^{(9)}, which represents the feature prior to global pooling.\nFor NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets, we set the learning rate to 0.05 and cut the rate by half every 10 epochs.\nThere‚Äôs a weighting factor to balance the few-shot classification and global classification objectives. This balance value is set at 0.2 for NTU RGB+D and NTU RGB+D 120 datasets, and 0.1 for the PKU-MMD dataset. Experiments for all these methods¬†  [ref]32   are optimized using the SGD optimizer with Nesterov momentum (0.9), and the training last for 100 epochs.\n\nReference markers found: [32]\n\nContext windows for each [ref]:\n[ref]32: is the same as our single-scale setting in the main manuscript.\nIn this section, we focus on the setting for the meta-training stage.\nTo achieve the best results for those few-shot learning methods  [ref]32  , we use different settings.\nThere are also slight differences in the settings of the same method on different datasets. ProtoNet [ref]32.\nWe utilize the AGCN (a single-scale model, detailed\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 32,\n            \"titles\": [\n                \"Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions\",\n                \"NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding\",\n                \"Skeleton-DML: Deep Metric Learning for Skeleton-Based One-Shot Action Recognition\",\n                \"SL-DML: Signal Level Deep Metric Learning for Multimodal One-Shot Action Recognition\",\n                \"One-shot action recognition in challenging therapy scenarios\",\n                \"Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks\",\n                \"Collaborative learning of gesture recognition and 3d hand pose estimation with multi-order feature analysis\",\n                \"Self-Supervised 3D Action Representation Learning with Skeleton Cloud Colorization\",\n                \"Multi-Scale Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition\",\n                \"Disentangling and unifying graph convolutions for skeleton-based action recognition\",\n                \"Sinkhorn distances: Lightspeed computation of optimal transport\",\n                \"Optimal transport: old and new\",\n                \"The earth mover‚Äôs distance as a metric for image retrieval\",\n                \"Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations\",\n                \"Human action recognition by representing 3d skeletons as points in a lie group\",\n                \"Learning actionlet ensemble for 3d human action recognition\",\n                \"Hierarchical recurrent neural network for skeleton based action recognition\",\n                \"Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates\",\n                \"Spatio-temporal lstm with trust gates for 3d human action recognition\",\n                \"Skeleton based action recognition with convolutional neural network\",\n                \"A New Representation of Skeleton Sequences for 3D Action Recognition\",\n                \"Skeleton-based Action Recognition with Convolutional Neural Networks\",\n                \"Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation\",\n                \"Skeleton-based action recognition with shift graph convolutional network\",\n                \"Two-stream adaptive graph convolutional networks for skeleton-based action recognition\",\n                \"Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition\",\n                \"3mformer: Multi-order multi-mode transformer for skeletal action recognition\",\n                \"Selective hypergraph convolutional networks for skeleton-based action recognition\",\n                \"Hypergraph neural network for skeleton-based action recognition\",\n                \"Star-transformer: a spatio-temporal cross attention transformer for human action recognition\",\n                \"Is Space-Time Attention All You Need for Video Understanding?\",\n                \"Neural koopman pooling: Control-inspired temporal dynamics encoding for skeleton-based action recognition\",\n                \"Prototypical Networks for Few-shot Learning\",\n                \"Adaptive subspaces for few-shot learning\",\n                \"Learning dynamic alignment via meta-filter for few-shot learning\",\n                \"Learning spatial-preserved skeleton representations for few-shot action recognition\",\n                \"Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action Recognition\",\n                \"Uncertainty-DTW for Time Series and Sequences\",\n                \"Deepemd: Few-shot image classification with differentiable earth mover‚Äôs distance and structured classifiers\",\n                \"TARN: Temporal Attentive Relation Network for Few-Shot and Zero-Shot Action Recognition\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]32 based on the context and research area of One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}