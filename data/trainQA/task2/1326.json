{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Comprehensive Study of Deep Video Action Recognition\n\nPaper Abstract: \n\nPaper Section Text:\nDeep learning methods usually improve in accuracy when the volume of the training data grows. In the case of video action recognition, this means we need large-scale annotated datasets to learn effective models. For the task of video action recognition, datasets are often built by the following process: (1) Define an action list, by combining labels from previous action recognition datasets and adding new categories depending on the use case. (2) Obtain videos from various sources, such as YouTube and movies, by matching the video title/subtitle to the action list. (3) Provide temporal annotations manually to indicate the start and end position of the action, and (4) finally clean up the dataset by de-duplication and filtering out noisy classes/samples.\nBelow we review the most popular large-scale video action recognition datasets in Table 1 and Figure 2. HMDB51  was introduced in 2011. It was collected mainly from movies, and a small proportion from public databases such as the Prelinger archive, YouTube and Google videos. The dataset contains 6,84968496,849 clips divided into 51 action categories, each containing a minimum of 101 clips. The dataset has three official splits. Most previous papers either report the top-1 classification accuracy on split 1 or the average accuracy over three splits.\nUCF101 90 was introduced in 2012 and is an extension of the previous UCF50 dataset. It contains 13,3201332013,320 videos from YouTube spreading over 101 categories of human actions. The dataset has three official splits similar to HMDB51, and is also evaluated in the same manner.\nSports1M  was introduced in 2014 as the first large-scale video action dataset which consisted of more than 1 million YouTube videos annotated with 487 sports classes. The categories are fine-grained which leads to low inter-class variations. It has an official 10-fold cross-validation split for evaluation. \nActivityNet  was originally introduced in 2015 and the ActivityNet family has several versions since its initial launch. The most recent ActivityNet 200 (V1.3) contains 200 human daily living actions. It has 10,0241002410,024 training, 4,92649264,926 validation, and 5,04450445,044 testing videos. On average there are 137 untrimmed videos per class and 1.41 activity instances per video.\nYouTube8M  was introduced in 2016 and is by far the largest-scale video dataset that contains 8 million YouTube videos (500K hours of video in total) and annotated with 3,86238623,862 action classes. Each video is annotated with one or multiple labels by a YouTube video annotation system.\nThis dataset is split into training, validation and test in the ratio 70:20:10. The validation set of this dataset is also extended with human-verified segment annotations to provide temporal localization information.\nCharades 86 was introduced in 2016 as a dataset for real-life concurrent action understanding. It contains 9,84898489,848 videos with an average length of 30 seconds. This dataset includes 157 multi-label daily indoor activities, performed by 267 different people. It has an official train-validation split that has 7,98579857,985 videos for training and the remaining 1,86318631,863 for validation.\nKinetics Family is now the most widely adopted benchmark. Kinetics400 00 was introduced in 2017 and it consists of approximately 240k training and 20k validation videos trimmed to 10 seconds from 400 human action categories. The Kinetics family continues to expand, with Kinetics-600 2 released in 2018 with 480K videos and Kinetics7003 in 2019 with 650K videos. \n20BN-Something-Something [ref]69 V1 was introduced in 2017 and V2 was introduced in 2018. This family is another popular benchmark that consists of 174 action classes that describe humans performing basic actions with everyday objects. There are 108,499108499108,499 videos in V1 and 220,847220847220,847 videos in V2. Note that the Something-Something dataset requires strong temporal modeling because most activities cannot be inferred based on spatial features alone (e.g. opening something, covering something with something). \nAVA  was introduced in 2017 as the first large-scale spatio-temporal action detection dataset. It contains 430 15-minute video clips with 80 atomic actions labels (only 60 labels were used for evaluation). The annotations were provided at each key-frame which lead to 214,622214622214,622 training, 57,4725747257,472 validation and 120,322120322120,322 testing samples. The AVA dataset was recently expanded to AVA-Kinetics with 352,091352091352,091 training, 89,8828988289,882 validation and 182,457182457182,457 testing samples 17.\nMoments in Time 42 was introduced in 2018 and it is a large-scale dataset designed for event understanding. It contains one million 3 second video clips, annotated with a dictionary of 339 classes. Different from other datasets designed for human action understanding, Moments in Time dataset involves people, animals, objects and natural phenomena. The dataset was extended to Multi-Moments in Time (M-MiT) 43 in 2019 by increasing the number of videos to 1.02 million, pruning vague classes, and increasing the number of labels per video. \nHACS  was introduced in 2019 as a new large-scale dataset for recognition and localization of human actions collected from Web videos.\nIt consists of two kinds of manual annotations. HACS Clips contains 1.55M 2-second clip annotations on 504K videos, and HACS Segments has 140K complete action segments (from action start to end) on 50K videos. The videos are annotated with the same 200 human action classes used in ActivityNet (V1.3) .\nHVU  dataset was released in 2020 for multi-label multi-task video understanding. This dataset has 572K videos and 3,14231423,142 labels. The official split has 481K, 31K and 65K videos for train, validation, and test respectively. This dataset has six task categories: scene, object, action, event, attribute, and concept. On average, there are about 2,11221122,112 samples for each label. The duration of the videos varies with a maximum\nlength of 101010 seconds.\nAViD 65 was introduced in 2020 as a dataset for anonymized action recognition. It contains 410K videos for training and 40K videos for testing. Each video clip duration is between 3-15 seconds and in total it has 887 action classes. During data collection, the authors tried to collect data from various countries to deal with data bias. They also remove face identities to protect privacy of video makers. Therefore, AViD dataset might not be a proper choice for recognizing face-related actions. Before we dive into the chronological review of methods, we present several visual examples from the above datasets in Figure 4 to show their different characteristics.\nIn the top two rows, we pick action classes from UCF101 90 and Kinetics400 00 datasets. Interestingly, we find that these actions can sometimes be determined by the context or scene alone. For example, the model can predict the action riding a bike as long as it recognizes a bike in the video frame. The model may also predict the action cricket bowling if it recognizes the cricket pitch. Hence for these classes, video action recognition may become an object/scene classification problem without the need of reasoning motion/temporal information.\nIn the middle two rows, we pick action classes from Something-Something dataset [ref]69. This dataset focuses on human-object interaction, thus it is more fine-grained and requires strong temporal modeling. For example, if we only look at the first frame of dropping something and picking something up without looking at other video frames, it is impossible to tell these two actions apart.\nIn the bottom row, we pick action classes from Moments in Time dataset 42. This dataset is different from most video action recognition datasets, and is designed to have large inter-class and intra-class variation that represent dynamical events at different levels of abstraction. For example, the action climbing can have different actors (person or animal) in different environments (stairs or tree).\n\nReference markers found: [69]\n\nContext windows for each [ref]:\n[ref]69: from 400 human action categories. The Kinetics family continues to expand, with Kinetics-600 2 released in 2018 with 480K videos and Kinetics7003 in 2019 with 650K videos. \n20BN-Something-Something [ref]69 V1 was introduced in 2017 and V2 was introduced in 2018. This family is another popular benchmark that consists of 174 action classes that describe humans performing basic actions with\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 69,\n            \"titles\": [\n                \"AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions\",\n                \"YouTube-8M: A Large-Scale Video Classification Benchmark\",\n                \"Self-Supervised MultiModal Versatile Networks\",\n                \"Self-Supervised Learning by Cross-Modal Audio-Video Clustering\",\n                \"NetVLAD: CNN architecture for weakly supervised place recognition\",\n                \"Sequential Deep Learning for Human Action Recognition\",\n                \"SpeedNet: Learning the Speediness in Videos\",\n                \"Understanding and Simplifying One-Shot Architecture Search\",\n                \"Efficient Video Classification Using Fewer Frames\",\n                \"Dynamic Image Networks for Action Recognition\",\n                \"Rethinking Zero-Shot Video Classification: End-to-End Training for Realistic Applications\",\n                \"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware\",\n                \"A Short Note about Kinetics-600\",\n                \"A short note on the kinetics-700 human action dataset\",\n                \"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\",\n                \"Temporal Attentive Alignment for Large-Scale Video Domain Adaptation\",\n                \"RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning\",\n                \"A Simple Framework for Contrastive Learning of Visual Representations\",\n                \"Improved Baselines with Momentum Contrastive Learning\",\n                \"Webly Supervised Learning of Convolutional Networks\",\n                \"Multi-Fiber Networks for Video Recognition\",\n                \"Appending Adversarial Frames for Universal Video Attack\",\n                \"Generalized Rank Pooling for Activity Recognition\",\n                \"P-CNN: Pose-based CNN Features for Action Recognition\",\n                \"Why Can’t I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition\",\n                \"PoTion: Pose MoTion Representation for Action Recognition\",\n                \"MARS: Motion-Augmented RGB Stream for Action Recognition\",\n                \"AutoAugment: Learning Augmentation Strategies from Data\",\n                \"The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines\",\n                \"Rescaling Egocentric Vision\",\n                \"ImageNet: A Large-Scale Hierarchical Image Database\",\n                \"Improved Regularization of Convolutional Neural Networks with Cutout\",\n                \"Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification\",\n                \"Spatio-Temporal Channel Correlation Networks for Action Classification\",\n                \"Large Scale Holistic Video Understanding\",\n                \"Efficient Two-Stream Motion and Appearance 3D CNNs for Video Classification\",\n                \"Deep Temporal Linear Encoding Networks\",\n                \"Long-term Recurrent Convolutional Networks for Visual Recognition and Description\",\n                \"Omni-sourced Webly-supervised Learning for Video Recognition\",\n                \"Temporal Cycle-Consistency Learning\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]69 based on the context and research area of A Comprehensive Study of Deep Video Action Recognition. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}