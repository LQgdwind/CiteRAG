{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Sparse Stream Semantic Registers: A Lightweight ISA Extension Accelerating General Sparse Linear Algebra\n\nPaper Abstract: \n\nPaper Section Text:\nSparse linear algebra (LA) poses a formidable challenge for contemporary processor architectures optimized for highly regular and vectorizable workloads.\nIn sparse tensors, a significant portion of entries is zero. To reduce their memory footprint and eliminate redundant computation, they are usually compressed by storing only the values and positions of nonzero elements.\nHowever, this results in nested indirect data structures that cannot be randomly accessed, irregular memory access patterns, and significant control overheads.\nEven the most fundamental sparse tensor operations, addition and multiplication, are inherently irregular. Worse, their optimal execution strategy depends on operand sparsity: while one sparse operand can be handled through indirect memory accesses, two sparse operands require comparing nonzero positions in both tensors. Consider the multiplication of a sparse matrix A, stored in the widespread compressed sparse rows (CSR) format, with a dense vector b, which can be implemented as shown in Listing 1.\nHere, A_vals and A_idcs store the nonzero values and their column indices, and A_ptrs delimits the rows of A. For each element of the result vector c, we perform a sparse-dense dot product sV×dV111We henceforth use the prefixes s and d to denote sparse and dense tensors, respectively. Analogously, M denotes a matrix and V a vector. as shown.\nCompiling this sV×dV for the RISC-V instruction set yields a loop of nine instructions, of which only one, a fused multiply-accumulate (MAC), performs necessary compute [ref]1.\nThus, an in-order core can achieve at most 11 %/times11dividepercentabsent11\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} floating-point unit (FPU) utilization, while a superscalar core would need at least nine issue slots to keep a single FPU busy, even when ignoring dependencies and control bottlenecks.\nIn both cases, the main efficiency bottleneck is the indirection b[A_idcs[j]], which requires half of the non-compute instructions and is also needed for sparse-dense addition. Now, consider the multiplication of A with a sparse vector b stored in the common compressed sparse fiber (CSF) format. The sparse-sparse dot product sV×sV computed for each result element can be implemented as shown in Listing 1. We intersect the nonzero indices in the two vectors since only positions where both vectors are nonzero contribute to the result.\nCompared to sV×dV, the control overhead is even higher and dynamically depends on the sparsity patterns of both operands. Sparse-sparse tensor addition needs yet another approach: we must form a union of indices, since a nonzero in either operand produces a nonzero in the result. [() Multiplication of a sparse matrix A in CSR format with a dense vector b (sM×dV), iterating sparse-dense dot products (sV×dV).]\n\n\n [() Dot product of two sparse vectors a and b in CSF format (sV×sV).]\n\n Sparse compute is essential in many applications . The SuiteSparse matrix collection  hosts sparse matrices from numerous real-world problems in domains including computational physics, mathematics, economics, and biology.\nIn machine learning (ML), sparsification can significantly reduce the operations and memory required for a given inference accuracy ; while some approaches target sparse-dense compute by sparsifying only weights, others also exploit activation sparsity.\nGraphs, commonly represented as highly sparse matrices, are operands in both sparse-dense and sparse-sparse workloads such as PageRank  and triangle counting . Established superscalar out-of-order architectures struggle with the high control overhead of sparse compute: a Fujitsu A64FX running sM×dV with a highly optimized format  achieves at most 130.9 Gflop/stimes130.9dividegigaflopsecond130.9\\text{\\,}\\mathrm{Gflop}\\text{/}\\mathrm{s} or 4.7 %/times4.7dividepercentabsent4.7\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} of its peak compute.\nWhile recent CPU, GPU, and accelerator works improve sparse workload performance, they are limited by their lack of generality or large microarchitectural impact.\nMost general-purpose instruction set architecture (ISA) extensions consider only one-sided sparsity  , and those targeting sparse-sparse workloads incur large area impacts and cannot efficiently handle the former .\nSparse ML solutions often rely on custom, domain-specific formats and low or structured sparsity  , while more general sparse LA accelerators   incur significant silicon area impacts compared to their host systems and other solutions. To address these shortcomings, we present sparse stream semantic registers (SSSRs), a modular ISA extension significantly accelerating and improving FPU utilization on both sparse-dense and sparse-sparse LA without major microarchitectural impact.\nSSSRs extend stream semantic registers (SSRs) , which enable near-complete FPU utilization in in-order cores for dense linear algebra and other data-oblivious workloads222A workload whose memory access patterns are input-invariant. [ref]15 by mapping memory streams with regular address patterns directly to architectural registers.\nWe extend SSRs to accelerate the indirection, intersection, and union operations necessary for general sparse LA, incurring only a small increase in overall hardware complexity while significantly improving sparse performance. For example, SSSRs allow us to write both sV×dV and sV×sV in RISC-V assembly as in Listing 2, where the registers ft0 and ft1 have stream semantics.\nSince all index processing and comparison is handled in hardware, the loop body consists of only the useful fmadd.d instruction, which we iterate over using a hardware loop. We evaluate SSSRs by integrating them into the RISC-V Snitch  core and compute cluster, in which we evaluate their benefits on various sparse LA workloads using generated vectors and real-world matrices.\nOn a single core, SSSR indirection, intersection, and union enable speedups of up to 7.0×\\times, 7.7×\\times, and 9.8×\\times over an optimized RISC-V baseline on sparse-dense multiply, sparse-sparse multiply, and sparse-sparse addition, respectively, with peak indirection FPU utilizations of up to 80 %/times80dividepercentabsent80\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/}333Full FPU utilization would require separate index memory ports, which we forego to minimize our architectural impact (see Section 2.2).. On an eight-core cluster, sparse-dense sMxdV using SSSRs is up to 4.9×\\times faster and 2.9×\\times more energy efficient, while sparse-sparse sMxsV up to 5.9×\\times faster and 3.0×\\times more energy efficient. SSSRs incur only 1.8% in additional area in an eight-core cluster (128 KiB/times128dividekibibyteabsent128\\text{\\,}\\mathrm{KiB}\\text{/} data memory and 8 KiB/times8dividekibibyteabsent8\\text{\\,}\\mathrm{KiB}\\text{/} instruction cache) over regular SSRs. The synthesizable register transfer level (RTL) description for SSSRs and their integration into Snitch are available free and open-source444https://github.com/pulp-platform/snitch: SSSR IP in hw/ip/\nsnitch_ssr and integration in hw/ip/snitch_cluster.. SSSRs extend our prior work on indirection SSRs accelerating sparse-dense tensor products [ref]1. We include these contributions in this work to provide a holistic, coherent description of our SSR sparsity extensions. All contributions regarding sparse-sparse LA, most parameterizability work, and many minor improvements are original to this work, and we updated all existing evaluation to a newer version of Snitch  and a new technology node.\nIn detail, our contributions are: A lightweight, modular extension to SSRs handling streaming indirection, intersection, and union in hardware to efficiently handle sparse general LA in single-issue cores, with a complete open-source hardware implementation in an open RISC-V core and multi-core cluster. (Section 2). A programming model for SSSRs, efficient sparse-dense and sparse-sparse LA kernels operating on vectors and matrices, and further applications for our hardware (Section 3). Significant performance and energy efficiency benefits on single cores (≤7.0,7.7,9.8absent7.07.79.8\\leq 7.0,7.7,9.8×\\times) and eight-core clusters with realistic interconnect and main memory models (≤4.9,5.9absent4.95.9\\leq{4.9},{5.9}×\\times and 2.9,3.02.93.0{2.9},{3.0}×\\times), incurring a minimal 1.8 %/times1.8dividepercentabsent1.8\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} impact on cluster area (Section 4). A comparison to state-of-the-art CPU, GPU, and accelerator approaches, achieving higher generality, a lower area impact, and 1.7×\\times higher parallel sM×dV FPU utilization than GPUs (Section 5). The paper is organized as follows: Section 2 describes our SSSR architecture and its integration into the Snitch core and cluster. Section 3 presents our SSSR programming model and sparse LA kernels. Section 4 evaluates the performance, timing, area, and energy efficiency benefits of SSSRs. Finally, Section 5 discusses related CPU, GPU, and accelerator work and compares it to our approach.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: and V a vector. as shown.\nCompiling this sV×dV for the RISC-V instruction set yields a loop of nine instructions, of which only one, a fused multiply-accumulate (MAC), performs necessary compute [ref]1.\nThus, an in-order core can achieve at most 11 %/times11dividepercentabsent11\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} floating-point unit (FPU) utilization, while a superscalar core would need\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"A survey of sparse representation: algorithms and applications\",\n                \"Indirection Stream Semantic Register Architecture for Efficient Sparse-Dense Linear Algebra\",\n                \"The university of florida sparse matrix collection\",\n                \"Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks\",\n                \"The pagerank citation ranking : Bringing order to the web\",\n                \"Sparsecore: stream isa and processor specialization for sparse computation\",\n                \"Performance modeling of streaming kernels and sparse matrix-vector multiplication on a64fx\",\n                \"Prodigy: Improving the memory latency of data-indirect irregular workloads using hardware-software co-design\",\n                \"Stream-based memory access specialization for general purpose processors\",\n                \"Dual-side sparse tensor core\",\n                \"Sparten: A sparse tensor accelerator for convolutional neural networks\",\n                \"Extensor: An accelerator for sparse tensor algebra\",\n                \"Towards general purpose acceleration by exploiting common data-dependence forms\",\n                \"Stream semantic registers: A lightweight risc-v isa extension achieving full compute utilization in single-issue cores\",\n                \"Data oblivious algorithms for multicores\",\n                \"Snitch: A tiny Pseudo Dual-Issue Processor for Area and Energy Efficient Execution of Floating-Point Intensive Workloads\",\n                \"NVIDIA A100 Tensor Core GPU Architecture.\",\n                \"Yale Sparse Matrix Package\",\n                \"Sparse Matrix Test Problems\",\n                \"Tensor-matrix products with a compressed sparse tensor\",\n                \"Improving performance of sparse matrix-vector multiplication\",\n                \"Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights\",\n                \"An overview of quantization in jpeg 2000\",\n                \"Compiling stencils in high performance fortran\",\n                \"Sparse matrix transpose unit\",\n                \"Radix sort for vector multiprocessors\",\n                \"Dramsys: A flexible dram subsystem design space exploration framework\",\n                \"Decoupled access/execute computer architectures\",\n                \"The ARM Scalable Vector Extension\",\n                \"Knights Landing: Second-Generation Intel Xeon Phi Product\",\n                \"Unlimited vector extension with data streaming support\",\n                \"Save: Sparsity-aware vector engine for accelerating dnn training and inference on cpus\",\n                \"Cvr: efficient vectorization of spmv on x86 processors\",\n                \"Vectorized parallel sparse matrix-vector multiplication in petsc using avx-512\",\n                \"Regu2d: Accelerating vectorization of spmv on intel processors through 2d-partitioning and regular arrangement\",\n                \"A pattern-based spgemm library for multi-core and many-core architectures\",\n                \"Sparse linear algebra on amd and nvidia gpus – the race is on\",\n                \"Merge-based parallel sparse matrix-vector multiplication\",\n                \"Tilespmv: A tiled algorithm for sparse matrix-vector multiplication on gpus\",\n                \"Cuda Toolkit 10.0.\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of Sparse Stream Semantic Registers: A Lightweight ISA Extension Accelerating General Sparse Linear Algebra. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}