{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A subgradient splitting algorithm for optimization on nonpositively curved metric spaces\n\nPaper Abstract: \n\nPaper Section Text:\nWe consider optimization problems posed over a subset C𝐶C of a complete metric space (X,d)𝑋𝑑(X,d). We assume that X𝑋X is a Hadamard space, meaning that it has nonpositive curvature — the “CAT(0)” property from metric geometry 1. This optimization framework covers the familiar example of Hilbert space, and all complete simply-connected Riemannian manifolds of nonpositive sectional curvature. Examples of such manifolds include Euclidean and hyperbolic spaces, and spaces of positive-definite symmetric matrices with the affine-invariant metric . However, the Hadamard space framework also subsumes interesting examples that are not manifolds, such as the Billera-Holmes-Vogtmann (BHV) tree space  and, more generally, all CAT(0) cubical complexes. A simple example to keep in mind is the tripod, which consists of three copies of the halfline ℝ+subscriptℝ\\mathbb{R}_{+} with its usual metric, glued together by identifying the three copies of 0. Any two points x𝑥x and y𝑦y in the Hadamard space X𝑋X are connected by a unique geodesic [x,y]𝑥𝑦[x,y]. For algorithmic purposes, we assume that we can readily compute the distance d​(x,y)𝑑𝑥𝑦d(x,y) and the geodesic\n[x,y]𝑥𝑦[x,y]. We also assume the extension property: there exists a geodesic ray originating at x𝑥x that contains y𝑦y, and furthermore we can find such a ray computationally. These assumptions hold for each of the examples we have mentioned ( ). For tractability, as in the classical Euclidean case, we focus on convex minimization problems where both the feasible region C𝐶C and the objective f:C→ℝ:𝑓→𝐶ℝf\\colon C\\to\\mathbb{R} are geodesically convex. In general Hadamard spaces, as shown in , such optimization problems are solvable in principle by iterating the proximal update for any constant α>0𝛼0\\alpha>0. However, this update is only implementable in a few special cases. For our current purposes, we refine the expression of the optimization problem (1) to allow for additive structure: where the component functions fi:C→ℝ:subscript𝑓𝑖→𝐶ℝf_{i}\\colon C\\to\\mathbb{R} are geodesically convex. Rather than iterating the proximal update for f𝑓f, we can then apply a splitting approach, cyclically applying proximal updates to each component fisubscript𝑓𝑖f_{i} in turn. Under mild assumptions on\nthe problem data, convergence is proved in  but with no explicit complexity analysis. A closer inspection of the proofs\nreveals that appropriately chosen stepsizes lead to complexity guarantees for certain problems of interest: an\nε𝜀\\varepsilon-approximate minimizer of the objective can be found within O​(ε−2)𝑂superscript𝜀2O(\\varepsilon^{-2}) iterations.\nProximal splitting works, for example, for the p𝑝p-mean problem (for p≥1𝑝1p\\geq 1) associated with given points ai∈Csubscript𝑎𝑖𝐶a_{i}\\in C and nonnegative weights wisubscript𝑤𝑖w_{i}. In that case each component has an implementable proximal update, albeit requiring a one-dimensional minimization problem unless the exponent p𝑝p is 1 or 2. The case when each wi=1subscript𝑤𝑖1w_{i}=1 and p=1𝑝1p=1 is the classical Weber problem [ref]28 in location theory. Unfortunately, proximal updates are rarely implementable in general. Even in the classical Euclidean case we\nmust often rely instead on subgradient-based methods. We therefore consider the following question: Do classical subgradient methods extend to general Hadamard spaces? This question presents an immediate conundrum. Whereas the proximal iteration (2) is primal in nature, simply defined on the underlying Hadamard space X𝑋X, subgradients in the Euclidean setting are dual objects — linear functionals on X𝑋X. In the absence of any linear structure, the analogue in Hadamard space is unclear. To handle this fundamental hurdle when the space X𝑋X is a manifold, the subgradient methods introduced in  and the complexity analysis of  rely on local linearization, much like the theory of smooth optimization in . These works view gradients and subgradients at a point x∈X𝑥𝑋x\\in X as elements of the tangent space Tx​Xsubscript𝑇𝑥𝑋T_{x}X, forcing use of the exponential map Expx:Tx​X→X:subscriptExp𝑥→subscript𝑇𝑥𝑋𝑋\\mbox{Exp}_{x}\\colon T_{x}X\\to X or some approximate version. Moreover, the complexity bound in  depends unavoidably on a lower curvature bound for X𝑋X . This approach is technical, and furthermore fails in general Hadamard spaces X𝑋X because the machinery of local linearization, duality, and lower curvature bounds is unavailable. We argue here for an entirely different approach, one that is simpler, global, primal, and requires no lower curvature bound. We identify subgradients with constant-speed geodesic rays in X𝑋X, and we escape the curvature-related worst-case bounds of  by restricting the class of objectives f𝑓f, arriving at an algorithm with complexity analogous to the Euclidean case. A first step in this direction was the recent development of a horospherical subgradient algorithm in . This method applies in a general Hadamard space X𝑋X, but requires a quasiconvexity property: at any point x∈C𝑥𝐶x\\in C, the level set must be horospherically convex. In non-Euclidean settings, horospherical quasiconvexity is a significant assumption, but one that often holds in practice. The algorithm relies on an oracle that returns a geodesic ray originating from x𝑥x and in some sense normal to Lxsubscript𝐿𝑥L_{x}, mimicking the key property of subgradient directions in the Euclidean case. The algorithm then takes a step from x𝑥x along a “supporting ray”: a geodesic ray opposite to the normal ray. The resulting complexity parallels the Euclidean case. Horospherical ideas had appeared earlier in optimization, in works such as   . Horospheres in Hadamard spaces are limits of spheres: in Euclidean space, they are hyperplanes. Horospheres have the form\n{x∈X:b​(x)=0}conditional-set𝑥𝑋𝑏𝑥0\\{x\\in X:b(x)=0\\} for Busemann functions b𝑏b, the natural generalizations of affine functions on Euclidean spaces. Noticing these analogies, a notion of Fenchel conjugation based on Busemann functions was introduced for manifolds in , and more generally in . Although the supporting ray oracle in  is implementable for some interesting functions, the horospherical subgradient algorithm is difficult to apply in much generality, because unlike Euclidean subgradients, supporting rays have no obvious calculus. Specifically, for structured objectives f=∑ifi𝑓subscript𝑖subscript𝑓𝑖f=\\sum_{i}f_{i}, we cannot easily combine supporting rays for each component fisubscript𝑓𝑖f_{i} into a supporting ray for f𝑓f. We therefore turn our attention instead to splitting subgradient methods that employ steps computed only from individual components fisubscript𝑓𝑖f_{i}. Splitting subgradient methods are appealing for their simplicity. Our approach in Hadamard space is inspired by the complexity analysis for a Euclidean splitting (or “incremental”) algorithm in .\nQuasiconvex versions of that algorithm appear in  , and were considered on manifolds with curvature bounded below in , but these algorithms have the drawback that all the components fisubscript𝑓𝑖f_{i} must share a common minimizer. This restriction rules out many interesting examples, including the p𝑝p-mean problem, but it seems inherent to methods relying only on supporting ray oracles, which cannot distinguish between different component functions having geometrically similar level sets. This drawback suggests the need for a stronger oracle: supporting rays give directional information but that alone does not suffice. Our main contribution — the new splitting algorithm we introduce here — relies instead on a Busemann subgradient oracle. When called for one of the component functions fisubscript𝑓𝑖f_{i} at a point x∈C𝑥𝐶x\\in C, the oracle returns a Busemann function b𝑏b and a “speed” s≥0𝑠0s\\geq 0 such that Analogous subgradient inequalities have appeared previously in  . Indeed, the notion of a Busemann subgradient that we introduce corresponds to subgradients in the sense of the . Starting from x𝑥x, the algorithm uses the information returned by the oracle, following the geodesic ray associated with b𝑏b at speed s𝑠s for a judiciously chosen time interval, before repeating the process. The method generalizes the incremental subgradient algorithm of , and enjoys the same complexity. The structure of this paper is as follows. We begin by reviewing some metric geometry, and consequences of nonpositive curvature. Central to our development is the concept of direction, so we review some large-scale geometric properties of Hadamard spaces. This exploration leads to a new property, called Busemann subdifferentiability, strong enough to support a useful notion of subgradients in nonlinear space. We compare this new notion with earlier ideas about subgradients in Hadamard space, and relate it to horospherical convexity. Remarkably, even in the Euclidean case, Busemann subdifferentiability gives a new perspective on subgradients of convex functions, allowing a fundamentally geometric or primal understanding, without explicit reference to the inner product. We show that many natural functions\ndefined in terms of metric data are Busemann subdifferentiable, and we explore the calculus of Busemann subgradients. We demonstrate that Busemann subdifferentiability is not preserved under addition, motivating the use of splitting in Busemann subgradient-based algorithms. Rather than formal convex analysis, our aim here is the development of implementable algorithms with complexity guarantees for structured convex optimization. To this end, we adapt an incremental subgradient method from the Euclidean setting  to minimize sums of Busemann subdifferentiable functions on a Hadamard space. The complexity result matches that of the\nEuclidean algorithm and the cyclic proximal algorithm appearing in , requiring O​(ε−2)𝑂superscript𝜀2O(\\varepsilon^{-2}) iterations to guarantee an objective function value within\nε𝜀\\varepsilon of the minimum value. To illustrate the approach computationally, we solve the Weber (1-mean) problem for some small examples, from , in BHV tree space .\n\nReference markers found: [28]\n\nContext windows for each [ref]:\n[ref]28: proximal update, albeit requiring a one-dimensional minimization problem unless the exponent p𝑝p is 1 or 2. The case when each wi=1subscript𝑤𝑖1w_{i}=1 and p=1𝑝1p=1 is the classical Weber problem [ref]28 in location theory. Unfortunately, proximal updates are rarely implementable in general. Even in the classical Euclidean case we\nmust often rely instead on subgradient-based methods. We\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 28,\n            \"titles\": [\n                \"First-order Methods for Geodesically Convex Optimization\",\n                \"Convergence analysis of incremental quasi-subgradient method on Riemannian manifolds with lower bounded curvature\",\n                \"The Newick tree format\",\n                \"Computing medians and means in Hadamard spaces\",\n                \"Convex Analysis and Optimization in Hadamard Spaces\",\n                \"First-Order Methods in Optimization\",\n                \"Fenchel conjugate via Busemann function on Hadamard manifolds\",\n                \"A geometric study of Wasserstein spaces: Hadamard spaces\",\n                \"Geometry of the space of phylogenetic trees\",\n                \"Direct theorems in semi-infinite convex programming\",\n                \"An Introduction to Optimization on Smooth Manifolds\",\n                \"Metric Spaces of Non-Positive Curvature\",\n                \"Curvature and complexity: Better lower bounds for geodesically convex optimization\",\n                \"Groups acting on spaces of non-positive curvature, 2016\",\n                \"Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space\",\n                \"Subgradient algorithm on Riemannian manifolds\",\n                \"A polynomial time algorithm to compute geodesics in cat(0) cubical complexes\",\n                \"Convex analysis on Hadamard spaces and scaling problems\",\n                \"Incremental quasi-subgradient methods for minimizing the sum of quasi-convex functions\",\n                \"Sticky central limit theorems at isolated hyperbolic planar singularities\",\n                \"Duality and subdifferential for convex functions on complete CAT(0) metric spaces\",\n                \"Basic convex analysis in metric spaces with bounded curvature\",\n                \"Horoballs and the subgradient method, 2024\",\n                \"Incremental subgradient methods for nondifferentiable optimization\",\n                \"Polyhedral computational geometry for averaging metric phylogenetic trees\",\n                \"A fast algorithm for computing geodesic distances in tree space\",\n                \"Variational Analysis\",\n                \"Normalized incremental subgradient algorithm and its application\",\n                \"The Theory of the Location of Industries\",\n                \"Generated Reference Title 29 for [ref]28\",\n                \"Generated Reference Title 30 for [ref]28\",\n                \"Generated Reference Title 31 for [ref]28\",\n                \"Generated Reference Title 32 for [ref]28\",\n                \"Generated Reference Title 33 for [ref]28\",\n                \"Generated Reference Title 34 for [ref]28\",\n                \"Generated Reference Title 35 for [ref]28\",\n                \"Generated Reference Title 36 for [ref]28\",\n                \"Generated Reference Title 37 for [ref]28\",\n                \"Generated Reference Title 38 for [ref]28\",\n                \"Generated Reference Title 39 for [ref]28\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]28 based on the context and research area of A subgradient splitting algorithm for optimization on nonpositively curved metric spaces. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}