{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Knowledge-Aware Language Model Pretraining\n\nPaper Abstract: \n\nPaper Section Text:\nPretraining with Knowledge Awareness.\nAll models in this study are pretrained on a dataset similar to OpenWebText111https://github.com/NVIDIA/Megatron-LM/tree/master/tools/openwebtext; it contains about 30 billion tokens after filtering and deduplication. The entity tokenizer in KALM uses the entity dictionary from the CMNS Linker¬†[ref]20, which was harvested from the FACC1 and FAKBA entity annotations¬†[ref]32. We keep surface forms with frequency at least 1k, and entities with at least 100 links ‚Äì in total about 1.4M entities. The tokenizer is forward-greedy by mapping the longest (at most 4-grams) surface form to its most popular entity¬†[ref]20.\nWe choose this frequency-based dictionary lookup to favor higher entity coverage, linking speed, and recognition of entity polymorphism, while relying on the language model pretraining to handle any inaccuracy in the linking¬†[ref]26. Evaluations. The language modeling evaluations include perplexity on WikiText-103¬† and accuracy in LAMBADA last-word prediction¬†, implemented the same as Megatron-LM¬†. The LAMA knowledge probes use their official setting¬†https://github.com/facebookresearch/LAMA. Particularly we use their set-up to evaluate auto-regressive LMs such as GPT¬†. Our results are directly comparable with GPT-2 but not BERT as the two use different tokenizers.\nIn zero-shot QA evaluations, the official evaluation splits of Trivia QA¬†, Natural Questions (Short Answer Setting)¬†, and Web Questions¬† are used. Besides exact match (EM), we also show whether the generated answers contains the ground truth answer (cover-EM).\nThe edge probe tasks use their official implementation¬†https://github.com/\njsalt18-sentence-repl/jiant. We probe the last layer of PLMs at every 1k pretraining gradient steps from 5k to 10k, and again at convergence. Compared Methods.\nWe mainly compare with the autoregressive GPT-2 architecture, and pretrain our own version GPT-2 in-house, following the implementation of Megatron-LM¬†.\nAll else remaining equal, we pretrain KALM exactly as in-house GPT-2 networks, except with knowledge awareness.\nThe statics of all the evaluation tasks are listed in Table¬†2. The architecture specification of these models are listed in Table¬†2. We measured that the KALM models had a forward-pass runtime that was a constant added to that of an equivalent GPT-2; doubling the number of layers does not change this constant look up cost. Implementation Details.\nWe trained our models on either 32 or 64 Nvidia v100 GPUs in DGX-2 pods for 3-6 days using the DeepSpeed ZeRO optimizer ¬†. In all cases, we used the same linear learning rate warm-up over 3.2k steps to a maximum of 1.5e-4 with batches of 512 sequences each of length 1024, with cosine learning rate decay to a minimum of 1e-5 over at most 300k steps. Models were trained until convergence around 200-250k steps (4-5 epochs). Dropout of 0.1 was applied on the input tokens and the attention layers; the model had ‚Ñì‚Ñì\\ell-2 regularization of 0.01. We initialized all matrices with a uniform normal. For KALM we added an additional dropout on the linked entities, replacing them with the null entity 10% of the time. We also sampled negatives for the output entity prediction task according to the following regimen: 1% of the time the negative was the null entity, 49% it was a random entity chosen from among all 1.4M, and the remaining 50% was a ‚Äúdifficult‚Äù negative chosen from the 100 nearest Trans-E neighbors of the positive entity. All our models are pretrained from scratch with entity embedding dimension de=300subscriptùëëùëí300d_{e}=300.\nMore details can be found in Appendix.\n\nReference markers found: [20]\n\nContext windows for each [ref]:\n[ref]20: it contains about 30 billion tokens after filtering and deduplication. The entity tokenizer in KALM uses the entity dictionary from the CMNS Linker¬†[ref]20, which was harvested from the FACC1 and FAKBA entity annotations¬†[ref]32. We keep surface forms with frequency at least 1k, and entities with at least 100 links ‚Äì in total about 1.4M\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 20,\n            \"titles\": [\n                \"What do you learn from context? Probing for sentence structure in contextualized word representations\",\n                \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n                \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\",\n                \"Language models are unsupervised multitask learners\",\n                \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\",\n                \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\",\n                \"BERT Rediscovers the Classical NLP Pipeline\",\n                \"Language models as knowledge bases?\",\n                \"Transformers as Soft Reasoners over Language\",\n                \"From‚Äôf‚Äôto‚Äôa‚Äôon the ny regents science exams: An overview of the aristo project\",\n                \"Language Models are Few-Shot Learners\",\n                \"How Much Knowledge Can You Pack Into the Parameters of a Language Model?\",\n                \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n                \"Don‚Äôt Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training\",\n                \"Barack‚Äôs wife hillary: Using knowledge graphs for fact-aware language modeling\",\n                \"The curious case of neural text degeneration\",\n                \"Knowledge enhanced contextual word representations\",\n                \"Scaling Laws for Neural Language Models\",\n                \"Entities as Experts: Sparse Memory Access with Entity Supervision\",\n                \"Latent Relation Language Models\",\n                \"Entity Linking in Queries: Efficiency vs.Effectiveness\",\n                \"A neural probabilistic language model\",\n                \"Interpolated estimation of markov source parameters from sparse data\",\n                \"Megatron-lm: Training multi-billion parameter language models using gpu model parallelism\",\n                \"Turing-nlg: A 17-billion-parameter language model by microsoft\",\n                \"Word-Entity Duet Representations for Document Ranking\",\n                \"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\",\n                \"Momentum Contrast for Unsupervised Visual Representation Learning\",\n                \"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\",\n                \"Natural questions: a benchmark for question answering research\",\n                \"Semantic parsing on freebase from question-answer pairs\",\n                \"Facc1: Freebase annotation of clueweb corpora, version 1\",\n                \"Pointer Sentinel Mixture Models\",\n                \"Entity Tracking Improves Cloze-style Reading Comprehension\",\n                \"Zero: Memory optimizations toward training trillion parameter models\",\n                \"What Does BERT Look At? An Analysis of BERT‚Äôs Attention\",\n                \"StereoSet: Measuring stereotypical bias in pretrained language models\",\n                \"Leading conversational search by suggesting useful questions\",\n                \"Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs\",\n                \"A Neural Knowledge Language Model\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]20 based on the context and research area of Knowledge-Aware Language Model Pretraining. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}