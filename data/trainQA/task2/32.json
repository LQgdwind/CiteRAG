{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: High-Quality Hierarchical Process Mapping1footnote 11footnote 1This work was partially supported by the Austrian Science Fund (FWF, project P 31763-N31) and partially supported by DFG grant FINCA (ME-3619/3-2) within the SPP 1736 Algorithms for Big Data as well as the German Federal Ministry of Education and Research (BMBF) project WAVE (grant 01—H15004B).\n\nPaper Abstract: \n\nPaper Section Text:\nThere has been an immense amount of research on GPP, and the reader is referred to    for extensive material and references.\nThe most successful general-purpose methods to solve GPP for huge real-world graphs are based on the multilevel approach.\nThe basic idea of this approach can be traced back to multigrid solvers for systems of linear equations , and its first application to GPP was by Barnard and Simon .\nThe most commonly used formulation of the multilevel scheme was proposed by\nHendrickson and Leland .\nSimilar multilevel schemes are used for other graph partitioning problem formulations such as DAG partitioning  , hypergraph partitioning  , graph clustering  , graph drawing 9 8 or the node separator problem [ref]39 .\nAmong the most successful multilevel software packages to solve GPP, we mention Jostle , Metis 3, Scotch , and KaHIP . Systems like KaHIP  and Metis 3 typically compute a k𝑘k-partition on the coarsest level through a recursive bisection strategy or a direct k𝑘k-way partitioning scheme. In recursive bisection, the graph is recursively divided into two blocks until the number of blocks is reached, i. e., a bisection algorithm is used to split the graph into two blocks.\nMore precisely, each bisection step itself uses a multilevel algorithm that stops as soon as the number of nodes is below a small threshold.\nTo obtain a bipartition in the coarsest level, KaHIP uses the greedy graph growing algorithm.\nIn KaHIP, if k𝑘k is not even, the graph gets split into two blocks, V1subscript𝑉1V_{1} and V2subscript𝑉2V_{2}, such that c​(V1)≤⌊k2⌋​Lmax𝑐subscript𝑉1𝑘2subscript𝐿c(V_{1})\\leq\\big{\\lfloor}\\frac{k}{2}\\big{\\rfloor}L_{\\max}, c​(V2)≤⌈k2⌉​Lmax𝑐subscript𝑉2𝑘2subscript𝐿c(V_{2})\\leq\\big{\\lceil}\\frac{k}{2}\\big{\\rceil}L_{\\max}.\nBlock V1subscript𝑉1V_{1} will be recursively partitioned in ⌊p2⌋𝑝2\\lfloor\\frac{p}{2}\\rfloor blocks and block V2subscript𝑉2V_{2} will be recursively partitioned in ⌈p2⌉𝑝2\\lceil\\frac{p}{2}\\rceil blocks. In addition to GPP, Jostle and Scotch can also solve GPMP.\nJostle integrates local search into a multilevel scheme to partition the model of\ncomputation and communication.\nIn this scheme, it solves the problem on the coarsest level and afterwards performs refinements based on the user-supplied network communication model.\nOn the other hand, Scotch performs dual recursive bipartitioning to compute a mapping.\nMore precisely, it starts the recursion considering all given processes and PEs.\nAt each recursion level, it bipartitions the communication graph and also the distance graph with a graph bipartitioning algorithm.\nThe first (resp., second) block of the communication graph is then assigned to the first (resp., second) block of the distance graph.\nThe recursion proceeds until the distance graph only contains one vertex. There is likewise a large literature on OPMP, often in the context of scientific applications using the Message Passing Interface (MPI).\nHatazaki  was among the first authors to propose graph partitioning to solve the MPI process mapping of a virtual unweighted topology onto a hardware topology organized in modules and sub-modules.\nTräff 6 used a similar approach to implement one of the first non-trivial mappings designed for the NEC SX-series of parallel vector computers.\nMercier and Clet-Ortega 5 and later Mercier and Jeannot 6 simplified the mapping problem to ignore the whole network topology except that inside each node.\nThey also investigated multiple placement policies to enhance overall system performance.\nYu et al.  discussed and implemented graph embedding heuristics for the BlueGene 3​d3𝑑3d torus systems.\nHoefler and Snir 2 optimized instead the congestion of the mapping, additionally providing a proof that this problem is NP-complete, a heuristic to solve it, and an experimental evaluation based on application data from the Florida Sparse Matrix Collection.\nRouting-aware mapping heuristics taking the hierarchy of specific hardware topologies into account were discussed in .\nVogelstein et al. 7 concentrated on solving OPMP.\nThey proposed a gradient–based heuristic that involves solving assignment problems and gave experimental evidence for better solution quality and speed compared to other heuristics. Müller-Merbach  proposed a greedy construction method to obtain an initial permutation for OPMP.\nThe method roughly works as follows:\nInitially compute the total communication volume for each process and also the sum of distances from each core to all the others.\nNote that this corresponds to the weighted degrees of the nodes in the communication and distance models, respectively.\nAfterwards, the process with the largest communication volume is assigned to the core with the smallest total distance.\nTo build a complete assignment, the algorithm proceeds by looking at unassigned processes and cores.\nFor each of the unassigned processes, the communication load to already assigned vertices is computed.\nFor each core, the total distance to already assigned cores is computed.\nThe process with the largest communication sum is assigned to the core with the smallest distance sum. Glantz et al. 5 noted that the algorithm does not link the choices for the vertices and cores and hence propose a modification of this algorithm called GreedyAllC (the best algorithm in 5).\nGreedyAllC links the mapping choices by scaling the distance with the amount of communication to be done.\nThe algorithm has the same asymptotic complexity and memory requirements as the algorithm by Müller-Merbach. Heider 9 proposed a method to improve an already given solution for OPMP.\nThe method repeatedly tries to perform swaps in the assignment.\nTo do so, the author defines a pair-exchange neighborhood N​(Π)𝑁ΠN(\\Pi) that contains all permutations that can be reached by swapping two elements in ΠΠ\\Pi.\nHere, swapping two elements means that Π−1​(i)superscriptΠ1𝑖\\Pi^{-1}(i) will be assigned to processor j𝑗j and Π−1​(j)superscriptΠ1𝑗\\Pi^{-1}(j) will be assigned to processor i𝑖i after the swap is done.\nThe algorithm then looks at the neighborhood in a cyclic manner.\nMore precisely, in each step the current pair (i,j)𝑖𝑗(i,j) is updated to (i,j+1)𝑖𝑗1(i,j+1) if j<n𝑗𝑛j<n, to (i+1,i+2)𝑖1𝑖2(i+1,i+2) if j=n𝑗𝑛j=n and i<n−1𝑖𝑛1i<n-1, and lastly to (1,2)12(1,2) if j=n𝑗𝑛j=n and i=n−1𝑖𝑛1i=n-1.\nA swap is performed if it yields a positive gain, i. e., the swap reduces the objective.\nThe overall runtime of the algorithm is O​(n3)𝑂superscript𝑛3O(n^{3}).\nWe denote the search space with N2superscript𝑁2N^{2}.\nTo reduce the runtime, Brandfass et al.  introduced a couple of modifications.\nFirst, only symmetric inputs are considered.\nIf the input is not symmetric, it is substituted by a symmetric one such that the output of the algorithm remains the same.\nSecond, pairs (i,j)𝑖𝑗(i,j) for which the objective cannot change are not considered.\nFor example, if two processes reside on the same compute node, swapping them will not change the objective.\nThird, the authors partition the neighborhood search space into s𝑠s consecutive index blocks and only perform swaps inside those blocks.\nThis reduces the number of possible pairs from O​(n2)𝑂superscript𝑛2O(n^{2}) to O​(n​s)𝑂𝑛𝑠O(ns) overall pairs.\nWe denote the search space with 𝒩psubscript𝒩𝑝\\mathcal{N}_{p} (pruned neighborhood).\nIn addition, the authors use the method of Müller-Merbach  to compute an initial solution. Schulz and Träff 3 tackled the GPMP using a two-phase approach.\nFirst, the graph is partitioned using KaHIP (which uses recursive bisection). The quotient graph (communication graph) is then the input to OPMP.\nThis is solved using a construction algorithm called hierarchy top down and a variation of the refinement method proposed by Brandfass et al. .\nHierarchy top down consists of a perfectly balanced multisection partitioning algorithm that partitions the communication graph recursively into blocks specified by the given hierarchy.\nThe applied refinement method is based on the local search proposed by Brandfass et al., but with alternative schemes of swapping neighborhoods and better data structures to improve performance.\nEach of these schemes is represented by N𝒞dsuperscriptsubscript𝑁𝒞𝑑N_{\\mathcal{C}}^{d}, in which a swap of two blocks is only allowed if their theoretical distance in the communication graph G𝒞subscript𝐺𝒞G_{\\mathcal{C}} does not exceed d𝑑d.\nFor instance, in the simplest swapping neighborhood N𝒞1superscriptsubscript𝑁𝒞1N_{\\mathcal{C}}^{1}, assignments can only be switched if the two blocks are connected by an edge in G𝒞subscript𝐺𝒞G_{\\mathcal{C}}.\nThis definition implies a sequence of neighborhoods increasing in size N𝒞1⊆N𝒞2⊆…⊆N𝒞n=N2subscriptsuperscript𝑁1𝒞subscriptsuperscript𝑁2𝒞…subscriptsuperscript𝑁𝑛𝒞superscript𝑁2N^{1}_{\\mathcal{C}}\\subseteq N^{2}_{\\mathcal{C}}\\subseteq\\ldots\\subseteq N^{n}_{\\mathcal{C}}=N^{2} where N2superscript𝑁2N^{2} is the largest neighborhood used by Brandfass et al. .\nSchulz and Träff 3 experimentally showed that N𝒞10subscriptsuperscript𝑁10𝒞N^{10}_{\\mathcal{C}} is an adequate choice to obtain good solutions with a moderate running time. The OPMP algorithm proposed by Glantz et al. 6 requires that the\nhardware topology is a partial cube, i. e. an isometric subgraph of a\nhypercube. This requirement allows to label (i) the PEs as well as (ii)\nthe nodes of the application graph G𝐺G with meaningful bit-strings along\nconvex cuts. These bit-strings facilitate (i) the fast computation of\ndistances between PEs and (ii) an effective hierarchical local search\nmethod to improve the mapping induced by the labels. Subsequently, von Kirchbach et al. 8 modified the graph partitioning step such that is already using hierarchical multisection itself.\nThis yields better communication graphs for the second OPMP mapping step.\nIn particular, all the processes are partitioned among all the available data centers, then the processes of each data center are partitioned among the servers contained in it, and so forth.\nHowever, this approach restricts the movement of nodes to the module in which the local searches of KaHIP are operating at each moment.\nIn our integrated approach, instead, we adapt the local searches of KaHIP for the objective function of GPMP, which allows them to freely move nodes between any two blocks at any time.\nAfter the partitioning step, von Kirchbach et al. 8 test different OPMP algorithms and demonstrate that an identity mapping followed by an N𝒞10subscriptsuperscript𝑁10𝒞N^{10}_{\\mathcal{C}} local search provides a good balance between solution quality and runtime performance.\nHere we use a multisection setup followed by an identity mapping to compute initial solutions in the coarsest level of our multilevel approach, and give more details in Section 3.2.\n\nReference markers found: [39]\n\nContext windows for each [ref]:\n[ref]39: schemes are used for other graph partitioning problem formulations such as DAG partitioning  , hypergraph partitioning  , graph clustering  , graph drawing 9 8 or the node separator problem [ref]39 .\nAmong the most successful multilevel software packages to solve GPP, we mention Jostle , Metis 3, Scotch , and KaHIP . Systems like KaHIP  and Metis 3 typically compute a k𝑘k-partition on\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 39,\n            \"titles\": [\n                \"𝑘-way Hypergraph Partitioning via 𝑛-Level Recursive Bisection\",\n                \"RAHTM: Routing algorithm aware hierarchical task mapping\",\n                \"Memetic Multilevel Hypergraph Partitioning\",\n                \"Benchmarking for graph clustering and partitioning\",\n                \"A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems\",\n                \"What MPI could (and cannot) do for mesh-partitioning on non-homogeneous networks\",\n                \"Graph Partitioning\",\n                \"Fast unfolding of community hierarchies in large networks\",\n                \"Rank reordering for MPI communication optimization\",\n                \"Recent Advances in Graph Partitioning, pages 117–158\",\n                \"The university of florida sparse matrix collection\",\n                \"Orca reduction and contraction graph clustering\",\n                \"Engineering route planning algorithms\",\n                \"A Linear-Time Heuristic for Improving Network Partitions\",\n                \"Some Simplified NP-Complete Problems\",\n                \"Algorithms for mapping parallel processes onto grid and torus architectures\",\n                \"Topology-induced Enhancement of Mappings\",\n                \"A multilevel bilinear programming algorithm for the vertex separator problem\",\n                \"Rank reordering strategy for MPI topology creation functions\",\n                \"A computationally simplified pair-exchange algorithm for the quadratic assignment problem\",\n                \"A Multilevel Algorithm for Partitioning Graphs\",\n                \"Multilevel algorithms for acyclic partitioning of directed acyclic graphs\",\n                \"Generic topology mapping strategies for large-scale parallel architectures\",\n                \"A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs\",\n                \"Multilevel k-way hypergraph partitioning\",\n                \"Towards an efficient process placement policy for MPI applications in multicore environments\",\n                \"Improving MPI applications performance on multicore clusters with rank reordering\",\n                \"Partitioning Complex Networks via Size-constrained Clustering\",\n                \"Drawing Large Graphs by Multilevel Maxent-Stress Optimization\",\n                \"Graph partitioning with acyclicity constraints\",\n                \"Evolutionary multi-level acyclic graph partitioning\",\n                \"Optimale reihenfolgen, volume 15 of Ökonometrie und Unternehmensforschung\",\n                \"Scotch Home Page\",\n                \"SCOTCH: A software package for static mapping by dual recursive bipartitioning of process and architecture graphs\",\n                \"Near linear time algorithm to detect community structures in large-scale networks\",\n                \"P-complete approximation problems\",\n                \"KaHIP – Karlsruhe High Qualtity Partitioning Homepage\",\n                \"Engineering Multilevel Graph Partitioning Algorithms\",\n                \"Think Locally, Act Globally: Highly Balanced Graph Partitioning\",\n                \"Advanced Multilevel Node Separator Algorithms\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]39 based on the context and research area of High-Quality Hierarchical Process Mapping1footnote 11footnote 1This work was partially supported by the Austrian Science Fund (FWF, project P 31763-N31) and partially supported by DFG grant FINCA (ME-3619/3-2) within the SPP 1736 Algorithms for Big Data as well as the German Federal Ministry of Education and Research (BMBF) project WAVE (grant 01—H15004B).. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}