{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese\n\nPaper Abstract: \n\nPaper Section Text:\nSpeech synthesis systems have received a lot of attention in recent years due to the great advance provided by the use of deep learning, which allowed the popularization of virtual assistants, such as Amazon Alexa , Google Home  and Apple Siri . According to [ref]4 traditional Speech Synthesis systems are not easy to develop, because these are typically composed of many specific modules, such as, a text analyzer, a grapheme-to-phoneme converter, a duration estimator and an acoustic model.\nIn summary, given an input text, the text analyzer module converts dates, currency symbols, abbreviations, acronyms and numbers into their standard formats to be pronounced or read by the system, i.e. carries out the text normalization and tackles problems like homographs, then with the normalized text the phonetic analyzer converts the grapheme into phonemes. In turn, the duration estimator estimates the duration of each phoneme. Finally, the acoustic model receives the phoneme representation sequence, the prosodic information about phoneme segments’ length, the F0 contour and computes the speech signal  . Several acoustic models have been proposed such as the classical formant model , Linear Prediction Coefficients (LPC) model, the Pitch Synchronous Overlap and Add (PSOLA) models  widely used in TTS engines like Microsoft Speech API. In addition, Hidden Markov Model (HMM) based synthesis is still a topic of research  0 1, as well as a variety of Unit Selection Models 2 3. Deep Learning 4 allows to integrate all processing steps into a single model and connect them directly from the input text to the synthesized audio output, which is referred to as end-to-end learning. While neural models are sometimes criticized as difficult to interpret, several end-to-end trained speech synthesis systems 5 6 7 [ref]4 8 9 0 were shown to be able to estimate spectrograms from text inputs with promising performances. Due to the sequential characteristic of text and audio data, recurrent units were the standard building blocks for speech synthesis such as in Tacotron 1 and 2 6 7. In addition, convolutional layers showed good performance while reducing computational costs as implemented in DeepVoice3 and Deep Convolutional Text To Speech (DCTTS) methods [ref]4 8. Models based on Deep Learning require a greater amount of data for training, therefore, languages with low available resources are impaired. For this reason, most current TTS models are designed for the English language 7 8 0 9, which is a language with many open resources. In this work we propose to solve this problem for the Portuguese language. Although there are some public datasets of synthesis for European Portuguese 1, due to the small amount of speech, approximately 100 minutes, makes the training of models based on Deep Learning unfeasible. In addition, simultaneously with this work, two datasets for automatic speech recognition for Portuguese, with good quality, were released. The CETUC 2 dataset, which was made publicly available by 3, has approximately 145 hours of 100 speakers. In this dataset, each speaker uttered a thousand phonetically balanced sentences extracted from journalistic texts; on average each speaker spoke 1.45 hours. The Multilingual LibriSpeech (MLS) 4 dataset is derived from LibriVox audiobooks and consists of speech in 8 languages including Portuguese. For Portuguese, the authors provided approximately 130 hours of 54 speakers, an average of 2.40 hours of speech per speaker. Although the quality of both datasets is good, both were made available with a sampling rate of 16Khz and have no punctuation in their texts, making it difficult to apply them for speech synthesis. In addition, the amount of speech per speaker in both datasets is low, thus making it difficult to derive a single-speaker dataset with a large vocabulary for single-speaker speech synthesis. For example, the LJ Speech 5 dataset, which is derived from audiobooks and is one of the most popular open datasets for single-speaker speech synthesis in English, has approximately 24 hours of speech. In this article, we compare models of TTS available in the literature for a language with low available resources for speech synthesis. The experiments were carried out in Brazilian Portuguese and based on a single-speaker TTS. For this, we created a new public dataset, including 10.5 hours of speech. Our contributions are twofold (i) a new publicly available dataset with more than 10 hours of speech recorded by a native speaker of Brazilian Portuguese; (ii) an experimental analysis comparing two publicly available TTS models in Portuguese language.\nIn addition, our results and discussions shed light on the matter of training end-to-end methods for a non-English language, in particular Portuguese, and the first public dataset and trained model for this language are made available. This work is organized as follows. Section 2 presents related work on speech synthesis. Section 3 describes our novel audio dataset. Section 4 details the models and experiments performed. Section 5 compares and discusses the results. Finally, Section 6 presents conclusions of this work and future work.\n\nReference markers found: [4]\n\nContext windows for each [ref]:\n[ref]4: recent years due to the great advance provided by the use of deep learning, which allowed the popularization of virtual assistants, such as Amazon Alexa , Google Home  and Apple Siri . According to [ref]4 traditional Speech Synthesis systems are not easy to develop, because these are typically composed of many specific modules, such as, a text analyzer, a grapheme-to-phoneme converter, a\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 4,\n            \"titles\": [\n                \"Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks With Guided Attention\",\n                \"“” alexa is my new bff” social roles\",\n                \"alexa is my new bff social roles, user satisfaction, and personification of the amazon echo\",\n                \"The teardown: Google home personal assistant\",\n                \"Siri, a virtual personal assistant-bringing intelligence to the interface\",\n                \"Statistical parametric speech synthesis using deep neural networks\",\n                \"Prediction of fujisaki model’s phrase commands\",\n                \"Software for a cascade/parallel formant synthesizer\",\n                \"Diphone synthesis using an overlap-add technique for speech waveforms concatenation\",\n                \"Speech parameter generation algorithms for hmm-based speech synthesis\",\n                \"Template-warping based speech driven head motion synthesis\",\n                \"Statistical parametric speech synthesis: A review\",\n                \"Automatic detection of unnatural word-level segments in unit-selection speech synthesis\",\n                \"Survey on various methods of text to speech synthesis\",\n                \"Deep Learning\",\n                \"Char2wav: End-to-end speech synthesis\",\n                \"Tacotron: A fully end-to-end text-to-speech synthesis model\",\n                \"Natural tts synthesis by conditioning wavenet on mel spectrogram predictions\",\n                \"Deep voice 3: 2000-speaker neural text-to-speech\",\n                \"Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search\",\n                \"Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis\",\n                \"Phonetic events from the labeling the european portuguese database for speech synthesis, feup/ipbdb\",\n                \"Lsf and lpc-derived features for large vocabulary distributed continuous speech recognition in brazilian portuguese\",\n                \"An open-source end-to-end asr system for brazilian portuguese using dnns built from newly assembled corpora\",\n                \"MLS: A Large-Scale Multilingual Dataset for Speech Research\",\n                \"The lj speech dataset\",\n                \"Deep voice: Real-time neural text-to-speech\",\n                \"Deep Voice 2: Multi-Speaker Neural Text-to-Speech\",\n                \"Signal estimation from modified short-time fourier transform\",\n                \"World: a vocoder-based high-quality speech synthesis system for real-time applications\",\n                \"WaveNet: A Generative Model for Raw Audio\",\n                \"Speech enhancement in the STFT domain\",\n                \"Multi-Scale Context Aggregation by Dilated Convolutions\",\n                \"Neural Machine Translation in Linear Time\",\n                \"Real-time signal estimation from modified short-time fourier transform magnitude spectra\",\n                \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n                \"Training very deep networks\",\n                \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\",\n                \"Speaker-dependent wavenet vocoder\",\n                \"Evaluation of deep learning approaches to text-to-speech systems for european portuguese\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]4 based on the context and research area of TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}