{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: From Solitary Directives to Interactive Encouragement! LLM Secure Code Generation by Natural Language Prompting\n\nPaper Abstract: \n\nPaper Section Text:\nSoftware vulnerability analysis has long been a critical aspect of software development [ref]15. This challenge has intensified with the widespread adoption of large language models (LLMs) for code generation. These models include both closed-source solutions (e.g., commercial code completion engines) and open-source ones (e.g., academic projects). For example, the Copilot commercial tool is used by more than 1 million developers and more than 5,000 businesses [ref]12. Similarly, the open source tool CodeGeex boasts tens of thousands of active daily users, each averaging more than 250 API calls per week . This broad acceptance and utilization of LLMs for code generation underscores their role in enhancing programming productivity. It is widely acknowledged that most LLMs do not address security issues during code generation but focus merely on improving their code functionality. Generally, code generation LLMs are pretrained models that use prompts as inputs to produce code from various programming languages (e.g., C or Python) as outputs. This setup lacks consideration of security mechanisms, often resulting in vulnerabilities in the generated code. For instance, a recent study found that 40% of the generated programs contain serious vulnerabilities, including those generated by ChatGPT [ref]12.\nTable 1 presents our pilot experimental results using GPT-3.5 based on our constructed NL prompt datasets .\nThe second last column shows the percentage of vulnerabilities in the generated code, and the last column shows the percentage of vulnerable samples that can be fixed by the basic prompt “Now you are a security expert, please identify the possible MITRE TOP 25 vulnerabilities in the following code snippet and fix them thoroughly.”\nOur experiments highlight that more than half of the generated code samples in both datasets contain MITRE TOP 25 vulnerabilities, with SecEval and Holmes datasets showing vulnerabilities in 59% and 64% of samples, respectively. The results also showed that these insecure codes can not be fixed by using the naive prompt. This underscores the critical importance of addressing vulnerabilities in generated code to ensure software security. Challenges: While numerous techniques have been developed to identify security vulnerabilities in AI-generated code, research specifically focused on securing the code generation process remains limited,\nindicating that secure code generation is still an emerging field. Khoury et al.  noted that LLMs like ChatGPT can potentially identify code vulnerabilities,\nbut code generated by LLMs is often not robust against certain attacks.\nSVEN [ref]12 is proposed to enhance the capability of LLMs in generating secure code. However, SVEN relies on a mixture of programming language and natural language prompts, and there is significant room for improvement in HumanEval performance in terms of functional correctness.\nVery recent studies   \nhighlight that, in terms of security, AI assistants and LLMs struggle to detect and fix insecure code that they themselves generate.\nTherefore, there is a pressing need for novel methodologies to secure LLM-powered code generation using only natural language prompts while satisfying the practical demand for user-friendly, accessible tools within the broader developer community. To achieve this, there are three identified major challenges.\n ■■\\blacksquare Challenge #1: Scarcity of Diverse Natural Language-Only Prompts for Secure Code Generation. Most AI assistants and LLMs     generate code using one of two approaches. Either relying on prompts written in a programming language or alternatively using a mixture of programming language and natural language as prompts, based on the developer’s request.\nHowever, there is a lack of studies focusing on code generation using natural language prompts as input. In addition, to be user-friendly, the requested user-provided prompt must be simple, despite that this seed prompt can be automatically augmented. Moreover, there is a notable scarcity of publicly available datasets featuring prompts designed solely in NL for software code generation. We note that there is only one such dataset of LLMSecEva, which exists , focusing on MITRE’s top 25 CWEs released in 2021. The CWEs are lack of diversity, which is prohibitive for comprehensive evaluations.\nTherefore, the first challenge is to figure out how to utilize simple user-provided seed NL prompts rather than programming language prompts to generate secure software code, compounded by the lack of available benchmark datasets. ■■\\blacksquare Challenge #2: Ineffective Prompt Strategies Undermine LLMs’ Potential for Code Vulnerability Analysis and Remediation.\nEmerging LLMs , including Codex, AlphaCode, CodeGen, DeepSeek Coder, GPT-3.5, and GPT-4 are being developed for code generation. However, research has raised concerns about security vulnerabilities in LLM-produced code, including the use of untrustworthy libraries. Additionally, many researchers   7  find that there are often security issues in the AI assistant generated code.\nRecent studies indicate that LLMs are unreliable in reasoning about software vulnerabilities based on simple prompts, often leading to incorrect and unfaithful conclusions . Another study found errors in code generated by ChatGPT . Wu et al. 7 noted that LLMs themselves are ineffective in detecting software vulnerabilities.\nSpecifically, prompts like chain-of-thought are not optimized for code generation or software vulnerability patching.\nThe research highlights the need for further advancements in developing domain-specific prompts for LLMs, ensuring they can effectively and reliably function as code vulnerability detectors or evaluators.\nTherefore, designing effective interactive prompts for accurate software vulnerability analysis, as well as efficient patching of these vulnerabilities, poses a very challenging problem. ■■\\blacksquare Challenge #3: Inadequate LLM Self-Alignment and the Critical Need for Rigorous Cross-Checking. Alrashedy and Aljasser  emphasized that LLMs alone are insufficient for patching software vulnerabilities. However, performance improvements can be achieved when LLMs are guided by feedback for code refinement . This suggests that relying solely on LLMs for vulnerability detection and software vulnerability patching is not advisable, as they can potentially produce misleading and false information 0. For instance, an LLM might incorrectly overlook potential vulnerabilities in order to produce what appears to be a ‘successful’ code vulnerability detection.\nAs acknowledged by previous studies  7 , there is often a significant improvement in secure code generation when advanced feedback is provided. In this research context, He and Vechev [ref]12, who focused on security issues in generated code, leveraged CodeQL—a rule-based software toolkit for code vulnerability static debugging and detection—to perform security checks, achieving reliable results. However, their work involved a mix of programming language and natural language as input, and lacked specifically designed prompts for secure code generation, primarily focusing on functional correctness rather than fix success rate.\nTherefore, designing a novel pipeline that provides alignment and then cross-checking for LLMs in software vulnerability detention and patching for secure code generation is a challenging problem. Our solution: To tackle the three challenges, we propose the SecCode scheme for secure code generation. SecCode includes three stages to tackle the above three challenges:\n1) Code Generation using simple user-provided NL Prompts: SecCode generates code based on user-friendly NL prompts. These simple prompts are automatically enhanced by the SecCode pipeline. To address the scarcity of datasets containing NL prompt and code pairs covering a wide range of CWEs, we have constructed two additional datasets for comprehensive evaluations (Challenge #1).\n2) Code Vulnerability Detection and Fixing: Using the novel encouragement prompting and interactive feedback with our proposed LLM-Optimized prompts (Challenge #2). This method moves beyond a one-shot, directive approach, by developing the first method for encouragement prompting and interactive feedback. Our approach engages LLMs in a series of interactive iterations, designed to fully leverage their potential for vulnerability detection and remediation.\nSpecficially, we incentivize LLMs to generate secure code by providing vulnerable program code and the relevant vulnerability information. We implement a prompt-based encouragement strategy and propose an interactive iteration strategy to automatically check and fix security issues in the generated code using LLMs.\n3) Vulnerability Cross-Check and Code Security Refinement: SecCode incorporates external feedback from rule-based software vulnerability detection tools, feeding it back into the LLM through dynamically updated prompts to continuously refine code security (Challenge #3). That is, we implement a new strategy that integrates the LLM self-alignment with CodeQL for advanced feedback generation. This process cross-checks any possible existing vulnerabilities and then provides additional information for the LLMs to further correct the security issues in the generated code.\nIn other words, securing code generation is not a single-step task. It involves conducting a comprehensive security check of the generated code, and if vulnerabilities are detected, feeding this information back to the LLM for further refinement. This generation/correction, security check, and feedback process can be executed over multiple interactive iterations rather than stopping after a single iteration.\nIt is worth noting that we are the first to propose encouragement prompting for secure code generation, and we are among the first to propose the use of the interactive iteration prompting and the combination of LLMs with CodeQL to augment\nLLMs for secure code refinement. Our experimental results show that SecCode outperforms the baselines in terms of vulnerability correction rate, as well as functional correctness.\nContribution. Our main contributions are as follows. ∙∙\\bullet We are the first to propose the novel Encouragement Prompting (EP) for secure code generation. Our experiments show that EP can help LLMs achieve significantly better performance in secure code generation. ∙∙\\bullet We are the first to propose the strategy of interactive iteration prompting in providing the vulnerability information and fixing the vulnerability automatically. ∙∙\\bullet We propose a hybrid refinement strategy that integrates both LLMs and CodeQL to provide comprehensive feedback and cross-checking. This approach incorporates CodeQL into the gamification iteration, delivering enriched feedback to the LLMs for enhanced secure code refinement. ∙∙\\bullet We develop and implement the SecCode framework. We have conducted extensive experiments using a range of proprietary LLMs in different scenarios.\nWe have open-sourced the code111Will be available soon and prompts for other researchers to evaluate and further contribute to this research area.\n\nReference markers found: [15]\n\nContext windows for each [ref]:\n[ref]15: Software vulnerability analysis has long been a critical aspect of software development [ref]15. This challenge has intensified with the widespread adoption of large language models (LLMs) for code generation. These models include both closed-source solutions (e.g., commercial code\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 15,\n            \"titles\": [\n                \"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator\",\n                \"Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)\",\n                \"Can LLMs Patch Security Issues?\",\n                \"Programming is hard-or at least it used to be: Educational opportunities and challenges of ai code generation\",\n                \"Evaluating Large Language Models Trained on Code\",\n                \"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\",\n                \"Codeprompt: Task-agnostic prefix tuning for program and language generation\",\n                \"Code generation using machine learning: A systematic review\",\n                \"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language\",\n                \"Self-collaboration code generation via chatgpt\",\n                \"Codeql\",\n                \"DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence\",\n                \"Large language models for code: Security hardening and adversarial testing\",\n                \"A Survey on Large Language Models for Code Generation\",\n                \"How Secure is Code Generated by ChatGPT?\",\n                \"A Survey on Data-driven Software Vulnerability Assessment and Prioritization\",\n                \"StarCoder: may the source be with you!\",\n                \"Evaluating c/c++ vulnerability detectability of query-based static application security testing tools\",\n                \"Software vulnerability detection using deep neural networks: a survey\",\n                \"Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation\",\n                \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\",\n                \"Automatic Programming: Large Language Models and Beyond\",\n                \"Assessing the security of github copilot’s generated code-a targeted replication study\",\n                \"Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities\",\n                \"Show Your Work: Scratchpads for Intermediate Computation with Language Models\",\n                \"Asleep at the keyboard? assessing the security of github copilot’s code contributions\",\n                \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\n                \"Android source code vulnerability detection: a systematic literature review\",\n                \"PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\",\n                \"Large Language Model Alignment: A Survey\",\n                \"Securityeval dataset: Mining vulnerability examples to evaluate machine learning-based code generation techniques\",\n                \"LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations\",\n                \"Llms cannot reliably identify and reason about security vulnerabilities (yet?): A comprehensive evaluation, framework, and benchmarks\",\n                \"Combining structured static code information and dynamic symbolic traces for software vulnerability prediction\",\n                \"Chain-of-thought prompting elicits reasoning in large language models\",\n                \"Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review\",\n                \"How Effective Are Neural Networks for Fixing Security Vulnerabilities\",\n                \"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models\",\n                \"Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x\",\n                \"Large Language Model for Vulnerability Detection: Emerging Results and Future Directions\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]15 based on the context and research area of From Solitary Directives to Interactive Encouragement! LLM Secure Code Generation by Natural Language Prompting. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}