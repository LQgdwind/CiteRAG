{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Evaluatology: The Science and Engineering of Evaluation\n\nPaper Abstract: \n\nPaper Section Text:\nTo further illustrate the limitations of existing evaluation and benchmarking practices, we present Figure 16, which showcases these shortcomings within the evaluatology framework. By examining this figure, we can gain a clearer understanding of the areas where state-of-the-art and state-of-the-practice evaluation and benchmarks fall short. It is evident that a lack of consensus exists regarding concepts and terminologies across different areas of study. This lack of consensus often leads to confusion and misinterpretation, especially when the same terms are used in different disciplines with varying meanings. For example, the term \"benchmark\" is commonly employed in computer science, finance, and business disciplines but without a formal definition. Moreover, even within these fields, the definition of \"benchmark\" can be vague and subject to interpretation. In contrast, psychology may use the term \"scale\" as a concept similar to benchmark, while social science and medicine may not have an analogous concept at all. Recognizing this challenge, our work has aimed to propose universal concepts and terminologies that can bridge these disciplinary gaps. By establishing clear and standardized definitions, we seek to promote a shared understanding and facilitate effective communication and collaboration across different areas of study. Few works discuss the essence of evaluation, let alone reaching a consensus on it. Evaluation is often mistakenly equated with measurement or testing without clear differentiation. For instance, in computer science and psychology, evaluation and measurement are often used interchangeably. In the context of testing, where the goal is to determine whether an individual or a system aligns with the expected behavior defined by test oracles, evaluation is often conflated with testing. For instance, according to the SPEC terminology, a benchmark refers to \"a test, or set of tests, designed to compare the performance of one computer system against the performance of others\"  . SPEC is a highly influential benchmark organization. Our work has revealed the essence of the evaluation. The evaluation theories and methodologies proposed are often domain-specific, with a lack of universally applicable foundational principles and evaluation methodologies that transcend diverse disciplines. Different disciplines do not delve into the underlying principles of evaluation. Instead, they adopt a pragmatic approach and prioritize guidelines for conducting evaluations within specific contexts. For instance, in the medical discipline, the focus is primarily on eliminating confounding variables within the specific groups or cohorts being studied. In the business discipline, efforts are concentrated on searching the state of the practice. The most rigorous theoretical foundation can be found in the field of clinical trials. For instance, Randomized Controlled Trial (RCT) techniques are employed to rule out the effect of confounding variables. However, there is a lack of universal problem formulations or fundamental solutions that fully consider the intricate interactions among the key components of EMs in diverse scenarios. There are two serious drawbacks to the RCT methodology and its variants. Firstly, there is a lack of a stringent hierarchical definition of EC and EEC. The variations in ECs can introduce confounding that may affect the results and make meaningful comparisons difficult. Without ensuring EECs, it becomes an illusion to expect comparable evaluation outcomes. Secondly, when it comes to studying complex systems such as human beings or experimental animals, which we refer to as support systems, the RCT methodology and its variants may struggle to establish an REM. This kind of support system is characterized by a multitude of independent variables, making it difficult to isolate and control all relevant factors in a controlled experimental setting. Consequently, it becomes challenging to eliminate confounding variables and ensure unbiased evaluation outcomes completely. In the realms of business and finance, different observational study methodologies are widely used, and we have revealed their inherent limitations in Section 2.1. An observational study is not even an experiment. Certainly, it can not eliminate confounding variables and reveal the cause-and-effect relationships. In the business discipline, benchmarking assumes the state-of-the-art instantiation of the algorithm-like mechanism and the reference evaluation outcomes.\nIn finance and education disciplines, benchmarks or indexes assume the role of reference evaluation outcomes in an observational study that measures variables of interest but does not attempt to influence the response . Rossi et al. [ref]30 propose a valuable framework for evaluating methodologies in the field of social science. However, they do not provide a universal theory that can be applied to different disciplines. Their limitations stem from their narrow focus on assessing social programs without developing a generalized theory for evaluating other subjects in complex conditions. Rossi et al. indeed utilized or developed some approaches to isolate the social programs’ impacts, e.g., comparison group designs and randomized controlled trials (RCT), but they failed to explicitly state the underlying principles and methodology for universal science and engineering of evaluation. Within the computer science field, there are varying viewpoints and perspectives. For example, Hennessy et al. highlight the significance of benchmarks and define them as programs specifically selected for measuring computer performance. On the other hand, John et al. compile a book on performance evaluation and benchmarking without providing formal definitions for these concepts. Kounev et al. present a formal definition of benchmarks as \"tools coupled with methodologies for evaluating and comparing systems or components based on specific characteristics such as performance, reliability, or security.\" The ACM SIGMETRICS group  considers performance evaluation as the generation of data that displays the frequency and execution times of computer system components, with a preceding orderly and well-defined set of analysis and definition steps. In psychology, social and personality psychologists often utilize scales, such as psychological inventories, tests, or questionnaires, to assess psychometric variables  . While these tools are commonly used, it is important to recognize that they rely on virtual assessments and self-report-style evaluations, which may introduce potential distortions. To overcome this limitation, we suggest implementing a physical application of an EC to the subjects, supplemented with a variety of measurement instruments. This approach aims to provide a more objective and accurate assessment of various aspects, including attitudes, traits, self-concept, self-evaluation, beliefs, abilities, motivations, goals, and social perceptions , by incorporating tangible and observable data. Various disciplines have proposed engineering approaches to evaluations. However, they fail to provide universal benchmark concepts, theories, principles, and methodologies. For instance, benchmarks are commonly utilized in finance, computer science, and business, albeit with inconsistent meanings and practices.\nRegrettably, there have been limited discussions in previous works regarding universal benchmark principles and methodologies that can be applied across different disciplines.\nFrom a computer science standpoint, Kounev et al.  provide a comprehensive foundation for benchmarking, including metrics, statistical techniques, experimental design, and more. Most state-of-the-art and state-of-the-practice benchmarks overlook an essential aspect: the stakeholders’ evaluation requirements. This oversight leads to a failure to consider different and diverse evaluation requirements. For instance, they do not enforce the discrepancy threshold in evaluation outcomes, nor do they consider evaluation confidence, among other crucial factors. As a result, most CPU benchmarks are ill-equipped to meet the evaluation requirements in scenarios involving safety-critical, mission-critical, and business-critical applications. Another issue is the lack of a stringent definition for similar concepts, such as an EEC or LEEC. For example, most CPU or AI (deep learning) benchmarks, like ImageNet, fail to provide a clear definition of an EEC or LEEC. Instead, they jump directly into the implementation of algorithms or a specific dataset labeled with the ground truth without proper justification. Additionally, the support system, which plays a crucial role in some cases, is omitted without any explanation of the condition of simplifying the benchmarks. Furthermore, most of the methodologies fail to discuss the confounding elimination mechanism. This oversight can potentially introduce bias and inaccuracies in the evaluation outcomes. Not surprisingly, the intricate evaluation mechanisms and policies introduced in Section 4.4.2 are not explicitly discussed in the design and implementation of most benchmarks. For instance, it fails to address important aspects such as investigating and characterizing real-world ES, the design and implementation of a perfect EM, the modeling policy and procedure from a real-world ES to an EM, and the sampling policy and procedure from a perfect EC to a pragmatic EC. This omission makes it difficult for the benchmark to adapt to intricate evaluation scenarios. It is crucial to include these mechanisms and policies to ensure the benchmark’s applicability and effectiveness in complex evaluation scenarios.\nWithout explicit discussion of the real-world ES, it is difficult to establish an EC that captures the characteristics and requirements of real-world evaluations.\nFurthermore, exploring different sampling and modeling policies is essential to gain the confidence of the evaluation community in using the benchmark for inferring parameters of real-world ES. By carefully designing these policies, we can strike a balance between achieving high accuracy in evaluation outcomes and managing the associated evaluation costs. There are many widely used AI (deep learning) benchmarks.\nTaking the ImageNet dataset as an illustrative example , we reveal their limitations. Firstly, a specific AI benchmark like ImageNet can not be traced back to an explicit formulation of a problem or task and instead manifests itself in the form of a dataset containing ground truth, which may possess certain biases. In other scenarios, we also encounter challenges in identifying a precise mathematical function that accurately models the chemical and biological activities within the human body (Case Three in Section 3.5.2) or the social dynamics within the target population (Case Four in Section 3.5.2). Secondly, the benchmark relies on an unverified assumption that the data distribution within the real world closely aligns with that of the collected dataset to a considerable extent. Thirdly, in real-world applications, we use the statistic of a sample—a specific benchmark— to infer the parameters of the entire population. However, we do not know their confidence levels and intervals.\n\nReference markers found: [30]\n\nContext windows for each [ref]:\n[ref]30: benchmarks or indexes assume the role of reference evaluation outcomes in an observational study that measures variables of interest but does not attempt to influence the response . Rossi et al. [ref]30 propose a valuable framework for evaluating methodologies in the field of social science. However, they do not provide a universal theory that can be applied to different disciplines. Their\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 30,\n            \"titles\": [\n                \"ImageNet Large Scale Visual Recognition Challenge\",\n                \"system\",\n                \"Benchmarking supply chain management: finding best practices\",\n                \"The landscape of parallel computing research: A view from berkeley\",\n                \"The definition of system\",\n                \"Test oracles\",\n                \"The evolution of cognitive bias\",\n                \"The parsec benchmark suite: Characterization and architectural implications\",\n                \"The international vocabulary of metrology—basic and general concepts and associated terms (vim)\",\n                \"An analysis of measurement procedures for computer systems\",\n                \"Benchmarking: the search for industry best practices that lead to superior performance\",\n                \"The food, drug, and cosmetic act of 1938: its legislative history and its substantive provisions\",\n                \"Software testing\",\n                \"Computability theory\",\n                \"Imagenet: A large-scale hierarchical image database\",\n                \"The science of computing: What is computer science?\",\n                \"The linpack benchmark: past, present and future\",\n                \"Some new stock-market indexes\",\n                \"Scale construction and psychometrics for social and personality psychology\",\n                \"Computer architecture: a quantitative approach\",\n                \"Digitizing clinical trials\",\n                \"History of clinical trials.\",\n                \"Performance evaluation and benchmarking\",\n                \"A catalog of complexity classes\",\n                \"On quantity, value, unit, and other terms in the jcgm international vocabulary of metrology\",\n                \"A performance measurement and system evaluation project plan proposal\",\n                \"Systems Benchmarking\",\n                \"The CONSORT statement: revised recommendations for improving the quality of reports of parallel-group randomized trials\",\n                \"Randomized controlled trials and real-world data: differences and similarities to untangle literature data\",\n                \"Can real-world data really replace randomised clinical trials?\",\n                \"Evaluation: A systematic approach\",\n                \"Wti and brent futures pricing structure\",\n                \"An introduction to the analysis of algorithms\",\n                \"Descriptive analysis of good clinical practice inspection findings from us food and drug administration and european medicines agency\",\n                \"SPEC CPU Benchmark Suite\",\n                \"SPEC CPU2006 Benchmark Suite\",\n                \"SPEC CPU2017 Benchmark Suite\",\n                \"SPEC Glossary\",\n                \"The practice of statistics\",\n                \"On the theory of scales of measurement\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]30 based on the context and research area of Evaluatology: The Science and Engineering of Evaluation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}