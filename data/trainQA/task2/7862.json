{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Accelerating evolutionary exploration through language model-based transfer learning\n\nPaper Abstract: \n\nPaper Section Text:\nThe framework designed to enable a relational knowledge transfer incorporates an encoder-decoder architecture from the field of language modeling, especially the transformer introduced by Vaswani et al. . The first part of the framework is responsible for capturing essential features and relationships of the input values and mapping them to a latent space to yield a fixed-size representation. The decoder, in contrast, is tasked with converting this latent information back into a coherent and interpretable sequence. This is accomplished by first computing a probabilistic vector and then applying a sampling technique to retrieve the subsequent symbol. Both are constructed from multiple transformer blocks that implement functionalities such as self-attention, layer normalization, and feed-forward neural networks. Several hyperparameters govern the size of the language model: the context window (s​i​z​ew𝑠𝑖𝑧subscript𝑒𝑤size_{w}), the quantity and dimensionality of attention heads, the number of transformer blocks, and the dimension of the embedding (de​m​bsubscript𝑑𝑒𝑚𝑏d_{emb}).\n\n\nEncoder: An encoder (ℰ)ℰ(\\mathcal{E}) can be specified as a function that maps an input 𝐗={𝐱1,𝐱2,…,𝐱i},𝐘={y1,y2,…,yi}formulae-sequence𝐗subscript𝐱1subscript𝐱2…subscript𝐱𝑖𝐘subscript𝑦1subscript𝑦2…subscript𝑦𝑖\\mathbf{X}=\\{\\mathbf{x}_{1},\\mathbf{x}_{2},...,\\mathbf{x}_{i}\\},\\,\\mathbf{Y}=\\{y_{1},y_{2},...,y_{i}\\} into latent representation 𝐳~~𝐳\\mathbf{\\tilde{z}}: Here, θℰsubscript𝜃ℰ\\theta_{\\mathcal{E}} denotes learnable parameters governing the encoder. The latent representation (𝐳~)~𝐳(\\mathbf{\\tilde{z}}) encapsulates the essential features and relationships among the elements of (𝐗,𝐘)𝐗𝐘(\\mathbf{X},\\mathbf{Y}) into a vector with a fixed size of de​m​bsubscript𝑑𝑒𝑚𝑏d_{emb}, thus serving as a compressed version of the given task. Considering the input parameters for a generic symbolic regression algorithm and building on previous studies in which the transformer architecture has been employed [ref]20, [ref]21,  we emphasize the requirement of the output of ℰℰ\\mathcal{E} being order-invariant to the input (𝐗,𝐘)𝐗𝐘(\\mathbf{X},\\mathbf{Y}). In addressing this task, the literature provides different approaches like a set transformer [ref]42 (modified transformer block) or the pointNet . We employ the latter mentioned for equation (3), particularly the transformation-net component (T-net) similar to the study [ref]21 since there is no restriction on the input amount. The input to ℰℰ\\mathcal{E} is a matrix of dimension n×m𝑛𝑚n\\times m, where m𝑚m is the sum of the number of input features 𝐗𝐗\\mathbf{X} and the corresponding output values 𝐘𝐘\\mathbf{Y}. The dimension n𝑛n represents the quantity of data points relevant to the problem under investigation. To facilitate the training of an encoder for multivariate tasks, it is requisite that m𝑚m is defined as greater than one; zeros can be conveniently padded during inference where certain features are absent. Algorithm (1) illustrates the mapping of the concatenation of (𝐗,𝐘)𝐗𝐘(\\mathbf{X},\\mathbf{Y}) into (𝐳~)~𝐳(\\mathbf{\\tilde{z}}), while also annotating the dimensions of the resultant vector, with the batch size (b𝑏b), after each step. The components of the T-Net layer involved can be described as follows: Conv1DlsubscriptConv1D𝑙\\text{Conv1D}_{l}: Signifies the application of different 1D-convolution layers with progressively increasing size (de​m​b,2⋅de​m​b,4⋅de​m​bsubscript𝑑𝑒𝑚𝑏⋅2subscript𝑑𝑒𝑚𝑏⋅4subscript𝑑𝑒𝑚𝑏d_{emb},2\\cdot d_{emb},4\\cdot d_{emb}). BNormlsubscriptBNorm𝑙\\text{BNorm}_{l}: This layer conducts batch normalization to enforce the output’s zero mean and unit variance distribution. The function comprises five distinct instances of such units due to learnable parameters that facilitate a shift in the output. ReLU: After calling the particular layer and normalizing the result, the Rectified Linear Unit (ReLU) is applied to introduce sparsity. GlobalMaxPooling: This aggregation determines the maximum values across a dimension, which ensures a reduction and provides invariance. FFNlsubscriptFFN𝑙\\text{FFN}_{l}: Before obtaining the result, the information is compressed from size (1×2⋅de​m​b)⋅12subscript𝑑𝑒𝑚𝑏(1\\times 2\\cdot d_{emb}) to (1×de​m​b)1subscript𝑑𝑒𝑚𝑏(1\\times d_{emb}) by passing through a two-layered feed-forward neural network (FFN). Decoder: As previously stated, the decoder component takes the latent information and previously determined sequence (𝐬𝐬\\mathbf{s}) as input to calculate a matrix (P). This matrix comprises the log probabilities corresponding to each viable candidate for the subsequent symbol at the n𝑛n-th position in the sequence: with θDsubscript𝜃𝐷\\theta_{D} representing the parameters. In contrast to the conventional transformer decoder architecture as described by Vaswani et al. , we utilize a modified version of the open and efficient foundation language model (LLaMA) proposed by Touvron et al. . Including adaptations such as RMSNorm-based pre-normalization , the SwiGLU activation function , and the rotary embedding mechanism , the LLaMA model demonstrates superior performance over models like GPT-3 on various benchmarks . Notably, this is achieved by only using 4% to 30% of the parameter amount of GPT-3. With our adaptation, i.e., implementing a cross-attention layer to infer 𝐳~~𝐳\\mathbf{\\tilde{z}}, the decoder consists of the following elements: Tokenembedding: The function of the token embedding converts the 𝐬𝐬\\mathbf{s} into a continuous floating point representation (𝐬^^𝐬\\mathbf{\\hat{s}}) with a dimension of the context window ×de​m​babsentsubscript𝑑𝑒𝑚𝑏\\times d_{emb}. RMSNorm: Analogous to the encoder’s batch normalization, this component modifies the output to a statistical mean of zero. The specific preference for using RMSNorm over classic layer normalization is due to its computational efficiency . AttnlsubscriptAttn𝑙\\text{Attn}_{l}: Within this function, the multi-head attention layer is applied with respect to the embedded input sequence (Attn1subscriptAttn1\\text{Attn}_{1}) and in terms of the cross-relation (Attn2subscriptAttn2\\text{Attn}_{2}) between the embedded sequence and the z~~𝑧\\tilde{z}, determining the contextual relation between the representation of the symbols and the latent vector. FFN: This component signifies a two-layer feed-forward network with a layer size of 4⋅de​m​b⋅4subscript𝑑𝑒𝑚𝑏4\\cdot d_{emb} and de​m​bsubscript𝑑𝑒𝑚𝑏d_{emb} respectively. Except for the token embedding function, the operations described herein comprise what is commonly referred to as a transformer block. This component consists of several operations, excluding the token embedding function, which enables information propagation through multiple blocks. Algorithm (2) summarizes the steps involved in processing a single instance that takes in the embedded sequence (𝐬^^𝐬\\mathbf{\\hat{s}}) and the latent information. The method starts by invoking normalization (RMSnorm), followed by a self-attention function call (lines 1-2). The result from employing a normalization on the residual connection (res1subscriptres1\\text{res}_{1}) is in addition to 𝐳~~𝐳\\mathbf{\\tilde{z}} an input parameter for the cross-attention operation (lines 3-4). Repeating the procedure of passing the result (att2subscriptatt2\\text{att}_{2}) through a residual connection with the inference of the fully connected network (FFN) provides the transformed variable for the next block or serves as the output (lines 5-7). For a more detailed understanding of the individual elements, please refer to  or .\n\nReference markers found: [20]\n\nContext windows for each [ref]:\n[ref]20: version of the given task. Considering the input parameters for a generic symbolic regression algorithm and building on previous studies in which the transformer architecture has been employed [ref]20, [ref]21,  we emphasize the requirement of the output of ℰℰ\\mathcal{E} being order-invariant to the input (𝐗,𝐘)𝐗𝐘(\\mathbf{X},\\mathbf{Y}). In addressing this task, the literature provides\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 20,\n            \"titles\": [\n                \"SymbolicGPT: A Generative Transformer Model for Symbolic Regression\",\n                \"Genetic Algorithms\",\n                \"Genetic Programming: On the Programming of Computers by Means of Natural Selection\",\n                \"Gene Expression Programming: Mathematical Modeling by an Artificial Intelligence\",\n                \"GAMaterial-A genetic‐algorithm software for material design and discovery\",\n                \"Symbolic regression in materials science\",\n                \"A novel evolutionary algorithm applied to algebraic modifications of the RANS stress–strain relationship\",\n                \"Transition modeling for low pressure turbines using computational fluid dynamics driven machine learning\",\n                \"Modeling Value-Based Decision-Making Policies Using Genetic Programming\",\n                \"A Genetic Programming Approach for Economic Forecasting with Survey Expectations\",\n                \"Switching From Face-to-Face to an Online Teaching Strategy: How Anatomy and Physiology Teaching Transformed Post-Covid-19 for a University Preprofessional Program\",\n                \"A Retrospective Pharmacovigilance Analysis at Tertiary Care Hospital: An Observational Study\",\n                \"Gene Expression Programming: A Survey\",\n                \"Discovering Symbolic Models from Deep Learning with Inductive Biases\",\n                \"Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients\",\n                \"Discovering governing equations from data by sparse identification of nonlinear dynamical systems\",\n                \"Symbolic Regression via Neural-Guided Genetic Programming Population Seeding\",\n                \"A semi-implicit discrepancy model of Reynolds stress in a higher-order tensor basis framework for Reynolds-averaged Navier–Stokes simulations\",\n                \"Turbulence Model Development based on a Novel Method Combining Gene Expression Programming with an Artificial Neural Network\",\n                \"A Seq2Seq approach to Symbolic Regression\",\n                \"End-to-end symbolic regression with transformers\",\n                \"SymFormer: End-to-end symbolic regression using transformer-based architecture\",\n                \"A Survey on Transfer Learning\",\n                \"Continual Lifelong Learning with Neural Networks: A Review\",\n                \"Transfer learning in Genetic Programming\",\n                \"Further investigation on genetic programming with transfer learning for symbolic regression\",\n                \"Transfer learning: A building block selection mechanism in genetic programming for symbolic regression\",\n                \"Transfer learning in constructive induction with Genetic Programming\",\n                \"Common subtrees in related problems: A novel transfer learning approach for genetic programming\",\n                \"Differential evolution for instance based transfer learning in genetic programming for symbolic regression\",\n                \"Genetic Programming for Instance Transfer Learning in Symbolic Regression\",\n                \"Transfer learning based on semantic approximation techniques in genetic programming\",\n                \"Transfer Learning in Genetic Programming Hyper-heuristic for Solving Uncertain Capacitated Arc Routing Problem\",\n                \"A Parametric Framework for Genetic Programming with Transfer Learning for Uncertain Capacitated Arc Routing Problem\",\n                \"Multi-donor Neural Transfer Learning for Genetic Programming\",\n                \"UCI Machine Learning Repository\",\n                \"Discovering Governing Equations by Machine Learning implemented with Invariance\",\n                \"Neural Symbolic Regression that Scales\",\n                \"Bloating Reduction in Symbolic Regression Through Function Frequency-Based Tree Substitution in Genetic Programming\",\n                \"Study of various methods for tokenization\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]20 based on the context and research area of Accelerating evolutionary exploration through language model-based transfer learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}