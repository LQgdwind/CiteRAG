{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Benchmarking a wide range of optimisers for solving the Fermi-Hubbard model using the variational quantum eigensolver\n\nPaper Abstract: \n\nPaper Section Text:\nThe variational quantum eigensolver (VQE) has emerged as a leading approach to simulating ground states of many-body quantum systems using a quantum computer [ref]1 . The primary appeal of this method is that it can be implemented on current and near-term quantum hardware , without the need to wait for an error-corrected, universal quantum computer, which could still be many years away . Ultimately, VQE provides a cost function to optimise, mapping a list of parameters to a real number. If the cost function is sufficiently expressive, then its minimum argument will correspond to preparation of the ground state of a quantum system of interest on a quantum computer – a task which is difficult to perform classically. Several studies have demonstrated that VQE could yield an advantage over classical methods in the immediate future [ref]1   . In practice, a VQE cost function arises from preparing an initial quantum state, applying a parametrised quantum circuit, and undertaking an energy measurement of the final state. Due to the statistical nature of quantum measurements, the resulting cost function is a random variable and hence for a fixed set of parameters, one must run the quantum circuit many times in order to gain a reliable estimate of the expectation value of the final energy. Much research has been done on understanding how to design good VQE cost functions (or ansätze) – i.e. which initial state to use, the precise form of the parametrised circuit, and how best to perform the energy measurements    . From the point of view of optimisation however, one can also simply take the cost function as a black box function and study which optimisers may be well suited to finding the minimum of the search space in question. The field of optimisation algorithms is vast and expansive   . There are many different flavours of optimiser: hill climbing, interpolation and model fitting approaches, evolutionary algorithms, and a wealth of gradient-based approaches, particularly inspired by the field of machine learning . In the context of VQE, there are also several more bespoke optimisation techniques that have been developed, such as coordinate descent (also known as rotosolve or sequential minimal optimization)     , quantum analytic descent [ref]16, quantum natural gradient , and model gradient descent  . Our goal here is to study a variety of optimisation techniques for solving the Fermi-Hubbard model (i.e. preparing the ground state) using VQE, focusing on a particular circuit ansatz known as the Hamiltonian variational (HV) ansatz  . After fixing a selection of problem instances, we numerically benchmark several promising algorithms and analyse their performance by considering both final accuracy with the ground energy achieved, and number of cost function calls to get within a certain tolerance of the ground energy. We also consider the quantum natural gradient descent method , considering it separately to our main numerical study due to ansatz considerations (see Appendix B). Although there exist several numerical studies comparing optimisers for VQE (see below), to the best of our knowledge our work is the first to do so specifically in the context of the Hamiltonian variational ansatz for solving Fermi-Hubbard. Our contribution also includes a much larger range of problem instances and optimisers than prior works. It is our hope that this work will serve to inform optimiser choice for near term implementations of VQE on actual quantum hardware. Our main findings can be summarised as follows (see also Section 4): Momentum or Adam with finite differences are good optimiser choices when considering overall final accuracy with ground energy – see Fig. 7. SPSA, CMAES or BayesMGD are good optimiser choices for using a low number of calls – see Fig. 9. The simultaneous perturbation gradient subroutine can be beneficial when seeking to minimise number of calls, whereas using finite difference is more effective for final accuracy. We discuss the importance of hyperparameter selection, including choosing an appropriate step size for gradient subroutines (we found that for our purposes 0.4 seemed to be a good choice for finite differences). Any improvements offered by the quantum natural gradient or imaginary time evolution methods seem to be lost when considering the total amount of calls taken (for the systems and implementation considered here). This study is organised as follows. After providing a brief overview of relevant literature and some background on VQE for the Fermi-Hubbard model, we lay out our main method and approach to the bulk of our numerical study in Section 2. We give a brief overview of the optimisers considered in Table 3, Section 2.7, but provide more extensive notes in Appendix C. We present our main results and figures in Section 3. A brief discussion and conclusion appears in Section 4. The appendices contain additional figures, some analysis of the quantum natural gradient method, and notes on the optimisers considered in the main text. Data used to generate results for this paper are available at Zenodo .\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: The variational quantum eigensolver (VQE) has emerged as a leading approach to simulating ground states of many-body quantum systems using a quantum computer [ref]1 . The primary appeal of this method is that it can be implemented on current and near-term quantum hardware , without the need to wait for an error-corrected, universal quantum computer, which\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"Variational quantum algorithms\",\n                \"Noisy intermediate-scale quantum algorithms\",\n                \"Observing ground-state properties of the Fermi-Hubbard model using a scalable algorithm on a quantum computer\",\n                \"Quantum Threat Timeline Report 2022\",\n                \"Progress towards practical quantum variational algorithms\",\n                \"Strategies for solving the Fermi-Hubbard model on near-term quantum computers\",\n                \"Efficient quantum measurement of Pauli operators in the presence of finite sampling error\",\n                \"Algorithms for optimization\",\n                \"Essentials of Metaheuristics\",\n                \"Numerical optimization\",\n                \"An overview of gradient descent optimization algorithms\",\n                \"Sequential minimal optimization for quantum-classical hybrid algorithms\",\n                \"A Jacobi diagonalization and Anderson acceleration algorithm for variational quantum algorithm parameter optimization\",\n                \"Convergence of a block coordinate descent method for nondifferentiable minimization\",\n                \"Structure optimization for parameterized quantum circuits\",\n                \"Quantum analytic descent\",\n                \"Quantum Natural Gradient\",\n                \"Using models to improve optimizers for variational quantum algorithms\",\n                \"Dataset for “Benchmarking a wide range of optimisers for solving the Fermi-Hubbard model using the variational quantum eigensolver”\",\n                \"The variational quantum eigensolver: a review of methods and best practices\",\n                \"VQE method: a short survey and recent developments\",\n                \"Variational ansatz-based quantum simulation of imaginary time evolution\",\n                \"Classical optimizers for noisy intermediate-scale quantum devices\",\n                \"Scikit-Quant\",\n                \"Performance comparison of optimization methods on variational quantum algorithms\",\n                \"Optimizing quantum heuristics with meta-learning\",\n                \"Robust and efficient algorithms for high-dimensional black-box quantum optimization\",\n                \"Performance of hybrid quantum/classical variational heuristics for combinatorial optimization\",\n                \"Practical optimization for hybrid quantum-classical algorithms\",\n                \"Strategies for quantum computing molecular energies using the unitary coupled cluster ansatz\",\n                \"Avoiding local minima in variational quantum eigensolvers with the natural gradient optimizer\",\n                \"Learning to learn with quantum neural networks via classical neural networks\",\n                \"SciPy 1.0—Fundamental Algorithms for Scientific Computing in Python\",\n                \"General parameter-shift rules for quantum gradients\",\n                \"Estimating the gradient and higher-order derivatives on quantum hardware\",\n                \"Optuna: A Next-generation Hyperparameter Optimization Framework\",\n                \"An overview of the simultaneous perturbation method for efficient optimization\",\n                \"Implementation of the simultaneous perturbation algorithm for stochastic optimization\",\n                \"DEAP: Evolutionary Algorithms Made Easy\",\n                \"Completely derandomized self-adaptation in evolution strategies\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of Benchmarking a wide range of optimisers for solving the Fermi-Hubbard model using the variational quantum eigensolver. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}