{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: LPC-AD: Fast and Accurate Multivariate Time Series Anomaly Detection via Latent Predictive Coding\n\nPaper Abstract: \n\nPaper Section Text:\nDevice’s Information.\nWe run algorithms on a server,\nwhich has a CPU (Intel(R) Xeon(R) Platinum 8151 CPU @ 3.40GHz, 96GB memory)\nand a GPU (Quadro GV100, 32GB memory). Public Datasets.\nWe use four public datasets which are extensively used in previous works\n[ref]36   [ref]4  8 .\nEach dataset contains a train set and a testing set.\nEach data point in the testing dataset has a binary label\nindicating whether it is an anomaly (1) or not (0).\n1 summarizes the overall statistics of\neach dataset. More details of each dataset are described as follows. Server Machine Dataset (SMD111https://github.com/NetManAIOps/OmniAnomaly/tree/master/ServerMachineDataset).\nThe SMD[ref]36 time series dataset is collected from a large Internet company.\nIts dimension is 38 and each dimension corresponds to\none performance metric of a server.\nIn total, there are 28 time series and\neach time series corresponds to one server.\nThe horizon of each time series is five weeks.\nSMD is divided into two subsets of equal size,\nwhere the first half is the training set, and the second half with anomaly labels is the testing set. Application Server Dataset (ASD222https://github.com/zhhlee/InterFusion/tree/main/data/processed).\nThis dataset is also collected from a large Internet company, publiced by 8.\nIts dimension is 19 and each dimension corresponds to\none performance metric that characterizes the status of a server (CPU-related metrics and memory-related metrics, etc.).\nIn total, there are 12 time series and\neach time series corresponds to one server.\nEach time series records a server’s status data in 45 days, with a fixed rate 5 minites.\nThe first 30-day-long data without anomaly labels are used for training,\nand the last 15-day-long data with anomaly label are used for anomaly testing. Secure Water Treatment(SWaT333https://github.com/JulienAu/Anomaly_Detection_Tuto).\nThis dataset is from a real-world industrial water treatment plant producing filtered water .\nThe SWaT dataset is a scaled-down version of the original data.\nIts dimension is 51, and each dimension corresponds to one performance metric of the\nindustrial water treatment plant.\nThe time horizon of this time series is 11 days.\nThe dataset in the first seven days is collected under normal operations, and that in the last four days is collected with attacks. Water Distribution Dataset(WADI444https://itrust.sutd.edu.sg/itrust-labs_datasets/dataset_info/#wadi).\nThis dataset is collected from the WADI testbed,\nand it is an extension of the SWaT dataset.\nIts dimension is 123, and each dimension corresponds to one performance metric of the\nindustrial water treatment plant.\nIt only contains one time series corresponding to one server.\nThe time horizon of this time series is 16 days.\nThe dataset in the first 14 days is collected under normal operations,\nand that in the last two days is collected with attacks. Comparison baselines.\nTo show the merit of our proposed algorithms,\nwe consider the following SOTA baselines. OmniAnomaly555OmniAnomaly open source link: https://github.com/NetManAIOps/OmniAnomaly[ref]36.\nThis method is built from a variational autoencoder.\nIts sophisticated model\nto capture temporal dependence of time series\nenhances the detection accuracy,\nbut leads to a slow training speed. InterFusion666InterFusion open source link: https://github.com/zhhlee/InterFusion8.\nThis method is an improved variant of OmniAnomaly.\nThe improvement is on the detection accuracy of OmniAnomaly.\nThe model is still sophisticated and the training speed is slow. USAD777USAD open source link: https://github.com/manigalati/usad[ref]4.\nThis method achieves a fast training speed\nby trading-off the detection accuracy.\nIt is built on autoencoder and uses adversarial training. TranAD888TranAD open source link: https://github.com/imperial-qore/TranAD.\nSimilar to USAD, this method focuses on a fast training speed by trading-off detection accuracy.\nIt is built on the transformer architecture. We do not compare with a number of other notable baselines such as\n   ,\ndue to that they are shown inferior to TranAD and\nwe omit then for brevity and simplicity of presentation. To reveal a fundamental understanding of our proposed LPC-AD framework,\nwe consider the following instances of it,\nwhere different instances have differet predictors. LPC-AD-SA\nsets the predictor function Predic​(⋅;𝚯PD)Predic⋅subscript𝚯PD\\texttt{Predic}(\\cdot;\\bm{\\Theta}_{\\text{PD}})\nto be an attention enabled Seq2Seq shown in Figure 5. LPC-AD-S\nsets the predictor function Predic​(⋅;𝚯PD)Predic⋅subscript𝚯PD\\texttt{Predic}(\\cdot;\\bm{\\Theta}_{\\text{PD}})\nto be a LSTM enabled Seq2Seq shown in Figure 4. LPC-AD-L\nsets the predictor function Predic​(⋅;𝚯PD)Predic⋅subscript𝚯PD\\texttt{Predic}(\\cdot;\\bm{\\Theta}_{\\text{PD}})\nas a linear function shown in Equation (4). Evaluation Metrics.\nFollowing previous works [ref]36   [ref]4  8 ,\nwe use precision, recall, area under the receiver operating characteristic curve (AUROC), and F1 score\nto evaluate the detection accuracy of all algorithms.\nWe use these metrics that are designed for binary classification because\nanomaly detection has a binary output, i.e., anomaly (1) or not (0). In real-world applications,\nanomalous data points often occur consecutively as an anomalous segment.\nThe purpose of anomaly detection is to notify system operators about the possible issues.\nThus, it is acceptable that an anomaly detection algorithm can\ntrigger the anomaly alarm at any subset of a segment of anomalies,\ninstead of correctly classifying each anomalous data points.\nFor this practical consideration,\nwe use a point-adjustment strategy proposed by  to calculate the performance metrics.\nThis point-adjustment strategy was widely recognized and applied in previous works [ref]36  [ref]4  8 .\nAccording to this strategy, if at least one data point of an anomalous segment is\nclassified as an anomaly, all the data points of this anomalous segment is\nconsidered to be correctly classified as an anomaly. There are randomness in the training process of these algorithms.\nThe randomness comes from random sampling and\nrandom initialization.\nFaced with such randomness, we repeat the training and testing of each algorithm for\nD∈ℕ+𝐷subscriptℕD\\in\\mathbb{N}_{+} times,\nand take the average results.\nIn some datasets, the number of time series N𝑁N is more than one.\nFor example, SMD contains 28 time series.\nTo simplify the presentation, we consider the average of the performance metric\nacross all time series in a dataset.\nFormally, let Pi,j,Ri,j,F1,i,j,A​U​R​O​Ci,jsubscript𝑃𝑖𝑗subscript𝑅𝑖𝑗subscript𝐹1𝑖𝑗𝐴𝑈𝑅𝑂subscript𝐶𝑖𝑗P_{i,j},R_{i,j},F_{1,i,j},AUROC_{i,j} denote the\nprecision, recall, F1subscript𝐹1F_{1} score and receiver operating characteristic curve\nof the an algorithm on the i𝑖i-th time series of a dataset\nin the j𝑗j-th repeated training and testing. Notably, F1subscript𝐹1F_{1} is called the micro F1subscript𝐹1F_{1} score\nand F1∗subscriptsuperscript𝐹∗1F^{\\ast}_{1} is called the macro F1subscript𝐹1F_{1} score.\nIn our experiments, we conduct D=8𝐷8D=8 rounds of repeated\ntraining and testing, because some baseline algorithms is time-consuming. Hyperparameter setting & implementation details.\nFor the baseline algorithms (OmniAnomaly, InterFusion, USAD and TranAD),\nwe set their hyperparameters such as window size, embedding size, etc.,\naccording to\ntheir paper or\ntheir open-source code\n(when it is not stated in the paper).\nFor three instances of LPC-AD, i.e., LPC-AD-SA, LPC-AD-S and LPC-L,\nwe use the following default hyper parameters:\nhistorical window size ℓh=10subscriptℓℎ10\\ell_{h}=10,\nfuture window size ℓ=2ℓ2\\ell=2,\nhidden layer dimension of LSTM = M/2𝑀2M/2,\nembedding dimension N=8​(16​ for WADI dataset)𝑁816 for WADI datasetN=8(16\\text{ for WADI dataset}),\ncovariance matrix 𝚺=𝑰𝚺𝑰\\bm{\\Sigma}=\\bm{I},\nlearning rate =0.001absent0.001=0.001,\nmaximum training epoch M​a​x​E​p​o​c​h=40​(25​ for WADI dataset)𝑀𝑎𝑥𝐸𝑝𝑜𝑐ℎ4025 for WADI datasetMaxEpoch=40(25\\text{ for WADI dataset}),\ntraining batch size=64absent64=64.\nWe also vary the hyperparameters to study their\nimpact on the detection accuracy and training time.\nBy default, we use 100% training dataset for all the algorithms.We implemented instances of LPC-AD with\nPyTorch-1.9.0 library,\ntrained with Adam optimizer. All algorithms in consideration detect anomalies based on the alert threshold λ𝜆\\lambda.\nSimilar with previous works [ref]36  [ref]4  8 ,\nwe use exhaustive search to select the optimal alert threshold.\nIn the search, the objective is to achieve the highest F1subscript𝐹1F_{1} metric.\nFor instances of LPC-AD,\nwe exhaustively search in   with a step size of 0.0001.\nFor other baseline algorithms, we use the exhaustive search method stated in their paper.\n\nReference markers found: [36]\n\nContext windows for each [ref]:\n[ref]36: has a CPU (Intel(R) Xeon(R) Platinum 8151 CPU @ 3.40GHz, 96GB memory)\nand a GPU (Quadro GV100, 32GB memory). Public Datasets.\nWe use four public datasets which are extensively used in previous works\n[ref]36   [ref]4  8 .\nEach dataset contains a train set and a testing set.\nEach data point in the testing dataset has a binary label\nindicating whether it is an anomaly (1) or not (0).\n1 summarizes\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 36,\n            \"titles\": [\n                \"TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\",\n                \"Perturbation techniques in online learning and optimization\",\n                \"Wadi: a water distribution testbed for research in the design of secure cyber physical systems\",\n                \"Adaptive predictive coding of speech signals\",\n                \"Usad: unsupervised anomaly detection on multivariate time series\",\n                \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n                \"Efficient anomaly detection by isolation using nearest neighbour ensemble\",\n                \"Collective anomaly detection based on long short-term memory recurrent neural networks\",\n                \"Anomaly detection in ecg time signals via deep long short-term memory networks\",\n                \"Sdfvae: Static and dynamic factorized vae for anomaly detection of multivariate cdn kpis\",\n                \"Graph Neural Network-Based Anomaly Detection in Multivariate Time Series\",\n                \"Unsupervised Visual Representation Learning by Context Prediction\",\n                \"Hitanomaly: Hierarchical transformers for anomaly detection in system log\",\n                \"Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding\",\n                \"Elle: Inferring Isolation Anomalies from Experimental Observations\",\n                \"Linear gaussian state space modeling\",\n                \"MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks\",\n                \"opengauss: An autonomous database system\",\n                \"Multivariate time series anomaly detection and interpretation using hierarchical inter-metric and temporal embedding\",\n                \"MicroHECL: High-Efficient Root Cause Localization in Large-Scale Microservice Systems\",\n                \"Isolation forest\",\n                \"Deep learning for universal linear embeddings of nonlinear dynamics\",\n                \"Diagnosing root causes of intermittent slow queries in cloud databases\",\n                \"Lstm-based encoder-decoder for multi-sensor anomaly detection\",\n                \"Swat: A water treatment testbed for research and training on ics security\",\n                \"Anomaly detection in video using predictive convolutional long short-term memory networks\",\n                \"Localizing failure root causes in a microservice through causality inference\",\n                \"Efficient estimation of word representations in vector space\",\n                \"A Multimodal Anomaly Detector for Robot-Assisted Feeding Using an LSTM-based Variational Autoencoder\",\n                \"An overview of anomaly detection techniques: Existing solutions and latest technological trends\",\n                \"A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction\",\n                \"Estimate the implicit likelihoods of gans with application to anomaly detection\",\n                \"Gaussian mixture models\",\n                \"Variational inference with normalizing flows\",\n                \"Anomaly detection using autoencoders with nonlinear dimensionality reduction\",\n                \"Unsupervised learning of video representations using lstms\",\n                \"Robust anomaly detection for multivariate time series through stochastic recurrent neural network\",\n                \"Attention Is All You Need\",\n                \"Real-time sensor anomaly detection and recovery in connected automated vehicle sensors\",\n                \"Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]36 based on the context and research area of LPC-AD: Fast and Accurate Multivariate Time Series Anomaly Detection via Latent Predictive Coding. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}