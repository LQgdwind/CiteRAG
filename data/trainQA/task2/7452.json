{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Systematic Review of Data-to-Text NLG.\n\nPaper Abstract: \n\nPaper Section Text:\nOur analysis indicates that the most commonly employed automatic metric is BLEU , with a substantial presence in 80 papers. It is closely followed by METEOR , which is employed in 40 papers, demonstrating its continued relevance. ROUGE , with 17 papers utilizing it, has also been a consistent choice for assessing generated text quality. Furthermore, CHrF++ , used in 10 papers, and NIST , applied in 9 papers, have offered valuable insights into the evaluation of data-to-text outputs. These metrics, while non-semantic in nature, have played a role in understanding word or character count and n-gram overlap between generated text and reference texts. In seven studies, TER  was employed as an edit distance metric to evaluate machine translation, quantifying the human-level editing required to align system output with a reference . Additionally, fifteen studies utilized CIDEr , a widely adopted metric for image captioning, which assesses the similarity between generated and reference captions, considering both linguistic and content aspects, and applying TF-IDF-based n-gram weighting . Furthermore, two studies incorporated SPICE  metrics, which calculates the semantic propositional content overlap between generated and reference captions using scene graphs. Four (4) studies incorporated the use of the SER (Slot-Error Rate) metrics, which are appropriate for assessing the presence of named entities, with SER being computed through exact matching of slot values in the candidate texts . A significant debate persists among researchers regarding the appropriateness of these non-semantic n-gram metrics for evaluating data-to-text outputs. These metrics primarily rely on word count and n-gram overlap between the generated text and reference texts, often showing limited correlation with human judgment . They were originally developed and applied in other natural language generation (NLG) domains, such as translation for BLEU, NIST and METEOR and summarization for ROUGE.\nIn response to the unique challenges posed by table-to-text generation, task-specific metrics like PARENT  and its variant, PARENT-T [ref]73, have emerged. These metrics assess the quality of table-to-text outputs by comparing the generated information with the entries in the source table. The research also analyzes the sensitivity of the metrics to divergence by collecting labels for cases where references only contain information already present in the tables. The study shows that PARENT maintains a high correlation as the number of such examples varies . These task-specific metrics are gaining prominence, with nine (9) papers considering PARENT and one (1) paper exploring PARENT-T. To enhance the semantic alignment between generated texts and their references, embedding-based and pretrained-based metrics have been introduced. These metrics utilize contextualized embeddings and Transformer-based models to assess the quality and similarity of generated text to reference sentences. BERTScore , featured in 7 papers, calculates the cosine similarity between generated texts and the ground truth, offering a more nuanced understanding of text quality. MoverScore , a metric that allows many-to-one matching, is used in 2 papers and computes the Euclidean distances between words or n-grams. It enhances the evaluation process by considering partial alignments and offering insights into text quality. Notably, BLEURT [ref]137, a Transformer-based trained metric, has been employed in 5 papers. This approach pretrains BERT with synthetically generated sentence pairs by mask-filling with BERT, back-translation, or randomly dropping words to assess NLG system performance . In data-to-text evaluation, extractive evaluation methods were introduced in Wiseman et al. [ref]46 to assess the performance of the alignment of the information extraction model in the content selection and text planning in the generation process. A total of 11 studies adopted these metrics to rate their model’s performance in content selection and planning tasks. This approach employs metrics like content selection (CS), content ordering (CO), and relational generation (RG). An Information Extraction (IE) system is used to extract content plans, identify candidate entities and value pairs present in the generated text, and predict their types. CS evaluates how accurately the system’s extracted records match those in the reference output, considering precision and recall. RG assesses factuality by measuring the proportion of system-extracted records that also appear in the input table. CO evaluates the system’s record ordering by computing the normalized Damerau-Levenshtein Distance between the sequence of extracted records and the reference output [ref]46. The Table 6 illustrate the distribution of these evaluation metrics identified in various research papers.\n\nReference markers found: [73]\n\nContext windows for each [ref]:\n[ref]73: for BLEU, NIST and METEOR and summarization for ROUGE.\nIn response to the unique challenges posed by table-to-text generation, task-specific metrics like PARENT  and its variant, PARENT-T [ref]73, have emerged. These metrics assess the quality of table-to-text outputs by comparing the generated information with the entries in the source table. The research also analyzes the\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 73,\n            \"titles\": [\n                \"Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints\",\n                \"인공지능 기반 공격 그래프 생성\",\n                \"Building applied natural language generation systems\",\n                \"Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation\",\n                \"Automatic and Human-AI Interactive Text Generation\",\n                \"Semantic Neural Machine Translation using AMR\",\n                \"ClipCap: CLIP Prefix for Image Captioning\",\n                \"Maximizing stylistic control and semantic accuracy in nlg: Personality variation and discourse contrast.\",\n                \"Neural natural language generation: A survey on multilinguality, multimodality, controllability and learning.\",\n                \"Transformer based natural language generation for question-answering.\",\n                \"Learning Neural Templates for Text Generation\",\n                \"ToTTo: A Controlled Table-To-Text Generation Dataset\",\n                \"Scalable Micro-planned Generation of Discourse from Structured Data.\",\n                \"Damata: A robot-journalist covering the brazilian amazon deforestation.\",\n                \"Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model\",\n                \"SQL-to-Text Generation with Graph-to-Sequence Model\",\n                \"Survey of the state of the art in natural language generation: Core tasks, applications and evaluation.\",\n                \"PAN: Pipeline assisted neural networks model for data-to-text generation in social internet of things.\",\n                \"Dealing with hallucination and omission in neural Natural Language Generation: A use case on meteorology.\",\n                \"Towards constructing sports news from live text commentary,\",\n                \"Interacting with financial data using natural language.\",\n                \"Personalizing Weekly Diet Reports\",\n                \"Neural Text Generation from Structured Data with Application to the Biography Domain.\",\n                \"Getting to Production with Few-shot Natural Language Generation Models.\",\n                \"Automated learning of templates for data-to-text generation: comparing rule-based, statistical and neural methods.\",\n                \"AGGGEN: Ordering and aggregating while generating.\",\n                \"Learning Semantic Correspondences with Less Supervision\",\n                \"Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization\",\n                \"Handling rare items in data-to-text generation.\",\n                \"ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation\",\n                \"Logic2Text: High-Fidelity Natural Language Generation from Logical Forms\",\n                \"Survey of Hallucination in Natural Language Generation\",\n                \"A Systematic Literature Review on Text Generation Using Deep Neural Network Models.\",\n                \"Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods,\",\n                \"A Survey of Evaluation Metrics Used for NLG Systems.\",\n                \"Neural Text Generation: Past, Present and Beyond\",\n                \"Innovations in Neural Data-to-text Generation,\",\n                \"The WebNLG challenge: Generating text from RDF data.\",\n                \"Investigating Pretrained Language Models for Graph-to-Text Generation\",\n                \"Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text Generation\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]73 based on the context and research area of A Systematic Review of Data-to-Text NLG.. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}