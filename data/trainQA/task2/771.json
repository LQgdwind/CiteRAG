{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Assessing Safety-Critical Systems from Operational Testing: A Study on Autonomous Vehicles\n\nPaper Abstract: \n\nPaper Section Text:\nFor the sake of public safety, it is usually required that AVs being tested on public roads be supervised by human drivers (sometimes called “safety drivers”) who are responsible for monitoring the safe operation of the vehicles at all times, and must take over control (this is a “disengagement” of the autonomous driving function) in the event of a failure of the autonomous technology or other emergency. It is also mandated that disengagements during road testing be reported, and records made available\nto the public555E.g., www.dmv.ca.gov/portal/dmv/detail/vr/autonomous/testing/ (at the time of writing).. As AV technology evolves, one would expect a decreasing trend in the frequency of disengagements. Indeed, Banerjee and co-authors, using large-scale AV road testing data, show negative correlation between dpm and cumulative miles driven over three years, but still not reaching AV manufacturers’ targets despite millions of miles driven . Disengagements are much more frequent than serious accidents. So, studying the trend of dpm, as done in several statistical papers   , is appealing. Studying these trends is a useful tool for planning future road testing. To this end, in our previous paper  we showed how past AV disengagement data can be used to predict future disengagement via Software Reliability Growth Models (SRGMs)  – a family of statistical, point-process models representing the occurrence of software failures in continuous time. Fitting these models to Waymo’s publicly available disengagement data over 51 months, we evaluated the accuracy of their reliability forecasts, and showed how the models’ predictions can be improved by “recalibration” – a model improvement technique that utilizes statistical data on how the models’ past predictions fall short of observed outcomes . SRGMs, together with the forecast accuracy evaluation and recalibration methods introduced by  , have been shown to be a powerful tool for extrapolating the trend of AV disengagements in . However, we argued briefly in  that SRGMs are not suitable for deciding whether the AV is safe enough, even if a SRGM’s forecast accuracy and calibration properties have been proved good for a data series. We give here some more detailed and general arguments against using statistical predictors of dpm as indicators of trends in safety (as measured e.g. by pfm). It is true that many disengagements represent safety-relevant events – “near misses” – and studying near misses is a powerful aid for learning about the safety of a system: to gauge how far from the safety target a system is that has not yet had any accidents, and to correct flaws before they can cause accidents. However: a) The disengagement rate is not a safety indicator. Generally, there are two types of disengagement – passive disengagement and active disengagement. Passive disengagements are when subsystems in the AV detect a failure of the autonomous technology, or detect a dangerous situation, and initiate the disengagement. Active disengagements are when the AV does not detect any problem, but the driver monitoring the situation actively disengages the autonomous mode . In both cases, ensuring safety relies on the human driver’s supervision ability – the ability to react quickly and correctly. Imperfect human supervision causes noise in the disengagement data – both false positives and false negatives. Example false positives include unnecessary interventions due to a lack of confidence of the human driver in the AV’s ability to negotiate some new pattern of traffic, or due to non-hazardous “failures” like uncomfortable riding. Example false negatives include dangerous situations that are not recognised by either the AV or the human driver, but bring no noticeable consequence due to “good luck”. Moreover, the human supervision ability will vary in complex ways as the autonomy capacity evolves over time , which makes it harder to filter out for safety assessment this noise in the disengagement data. Thus, interpreting dpm as an indicator of AV safety is wrong  and potentially dangerous, through both being misleading and creating incentives to improve dpm rather than safety. Proper use of disengagement data in arguing safety would require assessing the interplay between (i) the evolution of ML functions, (ii) that of the safety drivers’ supervision ability, and (iii) the safety subsystems. For instance, we need to consider that an improvement of ML-based functions most likely reduces drivers’ ability to trigger disengagements when needed.\nIndeed, it is likely to affect their situation awareness (and thus their ability to detect potentially dangerous situations) and/or their trust in the AV (and thus their readiness to believe that their intervention is needed to resolve that situation).\nAlso, the probability of a safety subsystem (like a human driver) taking successful action depends on the probability distribution of the demands on it created by the ML-based functions’ failures   , which will vary as the ML-based system evolves. b) No guarantee of monotonicity.\nSome statistical properties of AVs, such as dpm, exhibit a reliability growth trend, in a statistical sense. However, there is a general reason for not relying on detected trends for safety assessment – a growth trend does not imply that every update is an improvement; some updates can reduce reliability. This is true for all systems, and more so for AVs with their substantial ML components. Thus any statistical predictor like SRGMs, that extrapolates such a trend, will be untrustworthy. That is, we cannot give high confidence in the one prediction that matters for current safety, i.e. the prediction made after the latest change. That change could have departed from the previous trend – even radically increasing the failure rate – but the predictor would not “notice”, until the next failure occurs, surprisingly early in view of that last prediction. To support the use of\ntrend extrapolation techniques for safety, a possible sufficient condition could be a sound empirical argument (yet to be invented, perhaps infeasible), that the AV is evolving in such a way that it indeed becomes more and more reliable (in terms of hazardous failures) with every update. This requires more understanding of the online learning mechanism of the ML component and how the dynamic distribution of failures created by it interacts with the safety subsystems. Perhaps such an argument could be supported if the training process for the ML were restricted in some appropriate way. All this relates to activities that accumulate confidence in “no worse than existing systems” arguments  (which we also used to answer Q4 and Q5 in Section 4). For example, accelerated testing in simulators before the release of a new version, attempts to build libraries of regression tests, and verification on the learning with safety constraints [ref]49. One may wish to, instead, use an SRGM as a way of obtaining a prior belief to plug into a CBI argument. Indeed this would be quite an appropriate use of extrapolation from previous trends. Care would be needed to choose SRGMs that produce predictions in terms of failure rates, or to rigorously translate predictions that are in terms of distribution of time-to-failure into distributions of failure rates. Apart from this, we see two kinds of practical difficulties that need to be overcome: obtaining a believable data series as input to the SRGM. One would typically look for a prediction of the frequency of hazardous situations due to AV behaviour. Predicting an accident rate directly would seem inappropriate anyway, and the log of accidents would be too sparse to give a basis for the SRGM to “learn” a trend; logs of disengagements would need to be “cleaned” from all the spurious events (or missing events: undetected hazardous situations) discussed in item (a) earlier on. However, targeted improvements in monitoring and log analysis could possibly overcome these difficulties. our CBI arguments have used strong beliefs in some very small upper bound on the failure rate. To use an SRGM’s prediction as this belief, one would need the SRGM to actually produce such an output, and to have proved well-calibrated regarding that level of belief: e.g., for its “99% confidence” statements to have been generally correct (or conservative, in the sense we use here for “conservatism”: conservative in the error they produce on posterior probabilities) for a series of data points. SRGMs have been rarely evaluated for good calibration for these high confidence levels. However, this may be worth doing, including dealing with the complexities in translating between predictions of rates and of time to next event (only the latter being observable).\n\nReference markers found: [49]\n\nContext windows for each [ref]:\n[ref]49: 4). For example, accelerated testing in simulators before the release of a new version, attempts to build libraries of regression tests, and verification on the learning with safety constraints [ref]49. One may wish to, instead, use an SRGM as a way of obtaining a prior belief to plug into a CBI argument. Indeed this would be quite an appropriate use of extrapolation from previous trends.\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 49,\n            \"titles\": [\n                \"Probabilistic Model Checking of Robots Deployed in Extreme Environments\",\n                \"Validation of ultra-high dependability for software-based systems\",\n                \"The infeasibility of quantifying the reliability of life-critical real-time software\",\n                \"Disruptive innovations and disruptive assurance: Assuring machine learning and autonomy\",\n                \"Considerations in assuring safety of increasingly autonomous systems\",\n                \"Mind the gaps: Assuring the safety of autonomous systems from an engineering, ethical, and legal perspective\",\n                \"Hands off: The future of self-driving cars\",\n                \"Hands off the wheel in autonomous vehicles?: A systems perspective on over a million miles of field data\",\n                \"Driving to safety: How many miles of driving would it take to demonstrate autonomous vehicle reliability?\",\n                \"Autonomous vehicles’ disengagements: Trends, triggers, and regulatory limitations\",\n                \"Autonomous vehicles: Disengagements, accidents and reaction times\",\n                \"Analysis of autopilot disengagements occurring during autonomous vehicle testing\",\n                \"Autonomous vehicle technology: A guide for policymakers\",\n                \"Safety first for automated driving\",\n                \"Waymo safety report: On the road to fully self-driving\",\n                \"A plan to develop safe autonomous vehicles. And prove it\",\n                \"DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars\",\n                \"Safety verification of deep neural networks\",\n                \"Verifying autonomous systems\",\n                \"Verifiable self-certifying autonomous systems\",\n                \"Credible autonomy safety argumentation\",\n                \"Assessing the Safety and Reliability of Autonomous Vehicles from Road Testing\",\n                \"Toward a formalism for conservative claims about the dependability of software-based systems\",\n                \"Software fault-freeness and reliability predictions\",\n                \"Modeling the probability of failure on demand (pfd) of a 1-out-of-2 system in which one channel is “quasi-perfect”\",\n                \"Conservative claims about the probability of perfection of software-based systems\",\n                \"Conservative claims for the probability of perfection of a software-based system using operational experience of previous similar systems\",\n                \"Exponential order statistic models of software reliability growth\",\n                \"IEC61508, Functional Safety of Electrical/ Electronic/Programmable Electronic Safety Related Systems\",\n                \"EN 50129:2018: Railway applications - Communication, signalling and processing systems - Safety related electronic systems for signalling\",\n                \"Handbook of parameter estimation for probabilistic risk assessment\",\n                \"Guidelines for Statistical Testing\",\n                \"Reliability estimation from appropriate testing of plant protection software\",\n                \"Bayesian nonparametric system reliability using sets of priors\",\n                \"Deriving a frequentist conservative confidence bound for probability of failure per demand for systems with different operational and test profiles\",\n                \"Imprecise probabilistic inference for software run reliability growth models.\",\n                \"Assessing asymmetric fault-tolerant software\",\n                \"Examining accident reports involving autonomous vehicles in California\",\n                \"How safe is safe enough for self-driving vehicles?\",\n                \"Assessing the risk due to software faults: Estimates of failure rate vs evidence of perfection\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]49 based on the context and research area of Assessing Safety-Critical Systems from Operational Testing: A Study on Autonomous Vehicles. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}