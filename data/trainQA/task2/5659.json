{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: From Copilot to Pilot: Towards AI Supported Software Development\n\nPaper Abstract: \n\nPaper Section Text:\nCopilot suggested the best practice from the AirBNB JavaScript coding style guide  for 3 out of the 25 coding standards we tested, i.e., in 3 out of 25 instances Copilot had the recommended best practice as its top suggestion.\nMoreover, only 5 of the remaining 22 coding scenarios had the best practice in Copilot’s top 10 suggestions currently viewable.\nCopilot failed, i.e., did not have the best practice in its top 10 suggestions for 17 scenarios out of 25 coding scenarios we tested. The results show that Copilot did not suggest the recommended best practice as its first suggestion in the majority (88%) of the best practices we tested.\nAs Copilot is closed source, we cannot find the reason behind this, but one could argue that lack of data for JavaScript compared to Python could be a reason for this behavior. We did not test Copilot for suggesting project-specific coding styles because Copilot does not have the feature to customize its suggestions based on preferences [ref]5.\nHowever, AI-supported code completion tools like Copilot should follow coding style guides and adapt their code suggestions to match the coding style used in the project.\nFor example, if a user is working on a project where one of the coding style guidelines says to leave a blank line after blocks and before the next statement , Copilot should respect this.\nAs a productivity tool, the ideal behavior for AI-supported code completion tools like Copilot is to detect the coding style guideline from existing code or a coding style document in the project and always suggest code that follows the guidelines. Figure 2 shows the Best Practice for Copying Array Contents, showing user input (i.e., Human Input), the top suggestion by Copilot, and the ideal way suggested by AirBNB JavaScript coding style guide .\nThe code suggested by Copilot uses a for loop to iterate through all the contents of the array and copy them, which is a lot slower than the spread approach ([..items]) shown as the best practice to copy array in the AirBNB JavaScript coding style guide . This eliminates the need to iterate through all the elements of the array to copy each one. Table II shows the complete list of all the best practices we tested on Copilot sampled from the AirBNB Coding Style guide  and the ranking of the best practice in Copilot suggestions (if it exists). All the best practices shown in Table II can be found in the replication package [ref]10 including the code used as input (i.e., human input), the top suggestion by Copilot, and the best practice from AirBNB JavaScript coding style guide . The results show that Copilot performed worse on coding style than with the language idioms. This indicates that current AI-supported\ncode completion tools like Copilot are not yet capable of suggesting the best practices in their suggestions, even though the best practices are sampled from a widely accepted coding style guide. There could be many reasons for this performance. For one, the public repositories do not always follow coding standards, and Copilot cannot detect coding styles from repositories with contribution guides, including the coding standards followed in the project.\nCopilot being closed source, we cannot investigate the potential reasons behind this behavior and recommend ways to fix this issue, improving the performance of Copilot. However, improving the frequency of best practice usage in training data and including metrics such as repository popularity in ranking of code suggestions could be some potential areas to explore for improving performance of Copilot. Based on the results shown in Table II, Copilot is more likely to have the recommended best practice in its top 10 suggestions when it is a common beginner programming task like finding ‘sum of numbers’ or ‘importing a module from a file.’\nWe also observed that Copilot did not always generate all 10 suggestions like in the case of Pythonic idioms, and it struggled to come up with 10 suggestions to solve a programming task.\nThis shows that Copilot does not have enough training data compared to Python to create more relevant suggestions, which may include the recommended best practices in JavaScript. The ideal behavior for AI-supported code completion tools like Copilot is to suggest best practices extracted from public code repositories (training data) to avoid code smells.\nAdditionally, AI-supported code completion tools like Copilot should detect the project’s coding style and adapt its code suggestions to be helpful for a user as a productivity tool.\n\nReference markers found: [5]\n\nContext windows for each [ref]:\n[ref]5: could be a reason for this behavior. We did not test Copilot for suggesting project-specific coding styles because Copilot does not have the feature to customize its suggestions based on preferences [ref]5.\nHowever, AI-supported code completion tools like Copilot should follow coding style guides and adapt their code suggestions to match the coding style used in the project.\nFor example, if a\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 5,\n            \"titles\": [\n                \"Evaluating Large Language Models Trained on Code\",\n                \"Towards a theory of software developer job satisfaction and perceived productivity\",\n                \"What predicts software developers’ productivity?\",\n                \"A Systematic Review of Productivity Factors in Software Development\",\n                \"How are java software developers using the eclipse ide?\",\n                \"Github copilot\",\n                \"Language Models are Few-Shot Learners\",\n                \"Asleep at the keyboard? assessing the security of github copilot’s code contributions\",\n                \"To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set?\",\n                \"Replication package\",\n                \"What to Fix? Distinguishing between design and non-design rules in automated tools\",\n                \"On the usage of pythonic idioms\",\n                \"There is more than one way to zen your python\",\n                \"Airbnb javascript style guide\",\n                \"Jungloid mining: Helping to navigate the api jungle\",\n                \"An Empirical Study on the Usage of BERT Models for Code Completion\",\n                \"An empirical evaluation of GitHub Copilot’s code suggestions\",\n                \"Deep code search\",\n                \"On the naturalness of software\",\n                \"CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning\",\n                \"Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models\",\n                \"A Program Synthesis Approach for Adding Architectural Tactics to An Existing Code Base\",\n                \"Jigsaw: Large Language Models meet Program Synthesis\",\n                \"What do pre-trained code models know about code?\",\n                \"Taxonomy and definitions for terms related to driving automation systems for on-road motor vehicles\",\n                \"Maturity levels for autonomous vehicle safety\",\n                \"A theory of human motivation.\",\n                \"SOTorrent: Reconstructing and Analyzing the Evolution of Stack Overflow Posts\",\n                \"Architecture knowledge for evaluating scalable databases\",\n                \"Competition-Level Code Generation with AlphaCode\",\n                \"Amazon codewhisperer\",\n                \"Self-Consistency Improves Chain of Thought Reasoning in Language Models\",\n                \"The architecture tradeoff analysis method\",\n                \"An exploratory study of sharing strategic programming knowledge\",\n                \"Generated Reference Title 34 for [ref]5\",\n                \"Generated Reference Title 35 for [ref]5\",\n                \"Generated Reference Title 36 for [ref]5\",\n                \"Generated Reference Title 37 for [ref]5\",\n                \"Generated Reference Title 38 for [ref]5\",\n                \"Generated Reference Title 39 for [ref]5\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]5 based on the context and research area of From Copilot to Pilot: Towards AI Supported Software Development. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}