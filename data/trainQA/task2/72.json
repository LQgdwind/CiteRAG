{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: An Analysis of Adversarial Attacks and Defenses on Autonomous Driving Models\n\nPaper Abstract: \n\nPaper Section Text:\nMany pervasive computing applications now use regression neural network models. For instance, a CNN-based regression model is capable of predicting the distance to collision for unmanned aerial vehicles to accomplish collision-free navigation. A stacked autoencoder regression model is deployed at the edge of simulated sensor networks to predict values of QoS metrics (response time and throughput) for each service .\nIn this paper, we focus\non autonomous driving, which extensively uses CNN-based regression models.\n Nowadays, technology companies such as Tesla, Uber, and Waymo have made a huge investment in autonomous vehicles. Waymo recently launched the first self-driving car service in Phoenix, making one of the first steps towards commercializing autonomous vehicles .\nIn an autonomous driving system, cameras and LiDARs are deployed to collect information about the driving scene, which is then fed into a CNN-based driving model to make decisions such as adjusting the speed and steering angle. Unfortunately, CNNs can be easily fooled by adversarial examples, which are constructed by applying small, pixel-level perturbations to input images 6 [ref]7. Despite imperceptible to human eyes, such adversarial examples cause CNNs\nto make completely wrong decisions. Recently, Tencent Keen Security Lab demonstrated an adversarial attack on Tesla Autopilot by generating adversarial examples to turn on rain wipers when there is no rain . Many adversarial attacks have been proposed and demonstrated effective on image classification models 6    2. To defend adversarial attacks, several techniques have been proposed to harden neural networks [ref]7 0  .\nHowever, previous research mainly focuses on image classification models. It is unclear to what extent these adversarial attacks and defenses are effective on regression models (e.g., autonomous driving models). This uncertainty exposes potential security risks and raises research opportunities. If adversarial attacks could be successfully applied to autonomous driving systems, attackers could easily cause traffic accidents and jeopardize personal\nsafety. If existing defense methods cannot be adapted to defend against attacks on regression models, it is imperative to identity a novel defense mechanism suitable for autonomous driving. This paper presents a comprehensive analysis of five adversarial attack methods and four defense methods on autonomous driving models. By conducting systematic experiments on three driving models, we find that except IT-FGSM   (36% attack success rate), all other attacks, including Opt 6, Opt_uni , AdvGAN 2, and AdvGAN_uni 2), could effectively generate adversarial examples with an average of 98% success rate in the white-box setting. Therefore, similar to classification models, CNN-based regression models are also highly vulnerable to adversarial attacks. On the other hand, the attack success rate of all attack methods is significantly lower in the black-box setting (4% only on average). This implies that, if neural network architecture and hyperparameters are not known, a driving model is much less vulnerable to adversarial attacks. Therefore, in practice, systems and middleware builders should keep their neural networks confidential. It may also be beneficial to apply model obfuscation or model privacy protection techniques 8 . In terms of defense, none of the four defense methods can effectively detect all five kinds of attacks. Adversarial training [ref]7 and defensive distillation 0 are only effective to reduce the success rate of two attacks: IT-FGSM and an optimization based approach. A method that detects abnormal hardware state (e.g., GPU memory usage, GPU utilization rate) can effectively detect these two attacks and to some extent detect AdvGan. Feature squeezing , on the other hand, can detect all five attacks with more than 78% recall under a specific setting but with a high false positive rate up to 40%. This indicates that, when building a defense component in a system or middleware, it is necessary to deploy multiple defense methods in tandem to be robust to various attacks. Overall, this paper makes the following contributions: Implementation.\nWe replicate five adversarial attack methods and four defense techniques with proper adaptations to cater to regression-based driving models. We release our implementations, models, and datasets for future research and validation.111Our dataset and models are available at https://github.com/ITSEG-MQ/Adv-attack-and-defense-on-driving-model Evaluation.\nWe comprehensively experiment with five adversarial attacks and four defenses and summarize results from experiments. Implications.\nWe propose three system building implications for future research in adversarial attacks and defenses on autonomous driving models.\n\nReference markers found: [7]\n\nContext windows for each [ref]:\n[ref]7: such as adjusting the speed and steering angle. Unfortunately, CNNs can be easily fooled by adversarial examples, which are constructed by applying small, pixel-level perturbations to input images 6 [ref]7. Despite imperceptible to human eyes, such adversarial examples cause CNNs\nto make completely wrong decisions. Recently, Tencent Keen Security Lab demonstrated an adversarial attack on Tesla\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 7,\n            \"titles\": [\n                \"Countering Adversarial Images using Input Transformations\",\n                \"Autoencoders, unsupervised learning, and deep architectures\",\n                \"End to End Learning for Self-Driving Cars\",\n                \"Towards evaluating the robustness of neural networks\",\n                \"cg23\",\n                \"Waymo launches its first commercial self-driving car service\",\n                \"Generative adversarial nets\",\n                \"Explaining and Harnessing Adversarial Examples\",\n                \"Convergence of Edge Computing and Deep Learning: A Comprehensive Survey\",\n                \"Adversarial example defense: Ensembles of weak defenses are not strong\",\n                \"Reducing the dimensionality of data with neural networks\",\n                \"Safety verification of deep neural networks\",\n                \"APE-GAN: Adversarial Perturbation Elimination with GAN\",\n                \"PRADA: Protecting Against DNN Model Stealing Attacks\",\n                \"Adam: A method for stochastic optimization\",\n                \"Adversarial examples in the physical world\",\n                \"Adversarial Machine Learning at Scale\",\n                \"Tencent keen security lab: Experimental security research of tesla autopilot\",\n                \"A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks\",\n                \"Learning iot in edge: Deep learning for the internet of things with edge computing\",\n                \"Creating autonomous vehicle systems\",\n                \"Delving into Transferable Adversarial Examples and Black-box Attacks\",\n                \"Universal adversarial perturbations\",\n                \"DeepFool: a simple and accurate method to fool deep neural networks\",\n                \"Towards Reverse-Engineering Black-Box Neural Networks\",\n                \"A survey of security and privacy in connected vehicles\",\n                \"cleverhans v2. 0.0: an adversarial machine learning library\",\n                \"Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples\",\n                \"Practical Black-Box Attacks against Machine Learning\",\n                \"Distillation as a defense to adversarial perturbations against deep neural networks\",\n                \"Perception, planning, control, and coordination for autonomous vehicles\",\n                \"Generative adversarial perturbations\",\n                \"Foolbox: A Python toolbox to benchmark the robustness of machine learning models\",\n                \"Learning a Driving Simulator\",\n                \"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\n                \"Intriguing properties of neural networks\",\n                \"DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars\",\n                \"Stealing Machine Learning Models via Prediction APIs\",\n                \"Udacity challenge 2: Steering angle prediction\",\n                \"Runtime monitoring for safety of intelligent vehicles\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]7 based on the context and research area of An Analysis of Adversarial Attacks and Defenses on Autonomous Driving Models. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}