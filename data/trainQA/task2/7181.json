{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management\n\nPaper Abstract: \n\nPaper Section Text:\nDeep learning, another subset of AI, has emerged as a transformative and highly influential field in recent years. It encompasses a class of algorithms and neural network architectures that have displayed remarkable capabilities in handling complex and large-scale tasks. In the realm of wildfire management, DL techniques have gained prominence for their prowess in tackling various challenges, especially in the context of wildfire detection, classification, and segmentation. DL models have the capacity to automatically learn inherent patterns and representations from diverse data sources, making them well-suited for tasks like image recognition, which is crucial in identifying wildfire occurrences. They can process a wide array of inputs, including high-resolution satellite images, aerial photographs, UAV-based imagery, and even real-time video streams from surveillance cameras. Taking advantage of these capabilities, DL techniques contribute to enhancing wildfire management by providing accurate and timely information not only for active-fire detection but also for real-time fire monitoring and post-fire analysis. In the following, we provide a comprehensive review of recent and powerful DL-based wildfire detection approaches, including those related to classification, segmentation, and object detection tasks. Wildfire Classification Approaches\nWildfire classification approaches aim to accurately categorize different types and severity levels of wildfires. These approaches leverage deep learning techniques to analyze various features such as flame color, smoke density, and temperature patterns to identify and classify wildfires. Through precise wildfire classification, these methods aid in assessing the potential risk, determining appropriate response strategies, and allocating resources efficiently for the control and administration of wildfires. The general architecture of the wildfire classification framework, which is primarily based on the deep CNN network, is illustrated in Figure 10. Wildfire classification based on deep CNN networks consists of two main components: feature learning and feature classification. Feature learning block uses a set of convolutional layers and pooling layers, while feature classification block uses a series of fully connected layers and a single flattening layer. Convolutional Layer is responsible for feature extraction tasks from input data. It performs convolution operations on the image data to identify specific patterns, such as edges, shapes, and textures in images. Each convolution operation comprises a small filter (also known as a kernel) with learnable parameters (weights) that are fine-tuned during the training process. Afterward, an appropriate activation function (transfer function) is used to introduce non-linearity into the network, enabling deep CNN models to learn and approximate complex data relationships. rectified linear unit (ReLU) , dynamic ReLU (DY-ReLU) , Swish , and Elastic exponential linear unit (EELU)  are a few recent and powerful activation functions specifically dealing with classification and detection tasks. Lastly, the result of this layer is a feature map, which presents information about the features found in the input data. Pooling Layer down-samples the spatial dimensions of the feature maps to minimize the number of parameters within the network while preserving the most essential information. Some of the latest and most popular pooling operations commonly used in image classification tasks are max and average pooling, compact bilinear pooling (CBP) , and spatial pyramid matching (SPM) . Flattened Layer converts the dimensions of the feature maps into a one-dimensional array for the dense layers. It is an essential task during the transition from convolutional layers to fully connected layers. Fully Connected Layer is responsible for making predictions or classifications based on the features extracted from previous layers. In fully connected layers, the neuron applies a linear transformation to the input vector through the learnable weights. The output of this layer is used for classification tasks in the final stage of the network. 1∗{{}^{*}}^{1} The related paper utilized data from various sources, including Kaggle, DFire, and Flame1 datasets.\n2∗{{}^{*}}^{2} The related paper consists of 2165 images from the Google, Kaggle, Korean Tourist Spot (KTS), and Day–Night Image Matching (DNIM) datasets.\n3∗{{}^{*}}^{3} The related paper used a Cycle-consistent Generative Adversarial Network (CycleGAN) to create their custom wildfire dataset.\n4∗{{}^{*}}^{4} The related paper collected images from internet copyright-free websites and public wildfire datasets.\n5∗{{}^{*}}^{5} The related paper used the UAV (DJI900) equipped with a SONY A7 camera to collect forest fire images. In Table 5, we explore the DL methods employed by wildfire classification approaches throughout the period from 2018 to 2023. A comprehensive understanding of these methodologies is essential for evaluating the strengths and limitations of each approach while assessing their effectiveness in active-fire management. Many works have utilized variants of famous previously proposed models and have redesigned the architecture following the framework of transfer learning.  first converts RGB images into grayscale images and extracts intensity, texture and shape features through 3 stages. Intensity features are extracted using the mean and standard deviation of brightness and the probability of gray value. To extract the texture features, the gray co-occurrence matrix and seven invariant moments based on the co-occurrence matrix are used, and lastly the shape features, the area, roundness, boundary circumference and boundary the roughness of fire region are extracted. After normalizing features, the authors relied on an SVM, and compare the performance with a Reduce-VGGNet. VGGNet-based models are here proposed in a modified version of the fine-tuned VGGNet . The proposed model aims to reduce the original training time, and transfers optimal parameters in the first 13 layers, and uses 2 fully connected layers with a softmax, instead of the original 3 fully connected layers afterward.  uses the pre-trained VGG19 model and uses transfer learning by freezing the weights of its convolutions base and adding fully connected dense layers with ReLU and sigmoid activations.  uses both VGG16 and Inception-v3. They freeze the weights of the feature extraction layers of both models, while the newly added classifiers are trained with an Adam optimizer and using binary cross-entropy loss. ResNet architectures have been one of the most profound models introduced in deep learning. Introducing deep residual connections helped the problem of vanishing gradients, and improved generalization and feature reuse.  uses the ResNet-50 as a rival model against their proposed DenseNet-121 modified model used as the backbone feature extractor.  uses an architecture called ForestResNet, which is essentially the ResNet-50 model trained on 175 forest fire images obtained from the internet. [ref]155 uses DSA-ResNet50 in which DSA stands for dual semantic attention. In this method, they use two streams of features extracted from an intermediate feature map passed through two transformations (each consisting of a series of convolutional, batch normalization, and ReLU activation units). Next, they simply fuse the output of these two output feature streams. After performing global average pooling on the width and height dimensions, they use a fully connected layer to embed all the fused information in a compact tensor. Finally, they compute attention weights for the two feature streams using two independent fully connected layers and output a combined attentive feature map.  applies transfer learning to ResNet50 as well by considering it as the backbone network, and fine-tune the last layers to optimize the network for the target UAV forest fire images.  uses deep ensemble learning and combines ResNet18, ResNet50, ResNet101, and InceptionResNetV2, through eight feature vectors extracted from their last layers. They propose two ensemble methods, with feature fusion, neighbor component analysis (NCA) selection, and binary SVM classification outputting fire/no-fire labels. The second architecture differs by implementing an iterative hard majority voting (IHMV) layer operating on eight prediction vectors generated by parallel SVM classifiers, instead of using feature fusion. DenseNet was proposed following the skip connection concept introduced in ResNet. [ref]170 introduced DenseNet that connects each layer to all preceding layers to create very diversified feature maps. contributing to feature reuse and propagation, and prevention of vanishing gradients, and a reduction in the number of parameters.  uses DenseNet-121 as the backbone feature extractor, and feeds the generated features to a multi-label classifier consisting of fully-connected, batch normalization, ReLU activations and a sigmoid classifier.  on the other hand, does not change the architecture substantially and focuses on improving the performance with augmenting new data using CycleGANs. EfficientNet follows a concept called architecture scaling, a common practice in neural network design to enhance efficiency.  employs a technique known as the “compound coefficient” to uniformly scale all the dimensions of the network (width, depth, and resolution) using a constant ratio. with the advantage of improved efficiency in training time.  uses EfficientNetB7, one of the variants of the popularly-known network and fine-tune it on forest fire datasets by unfreezing the final convolutional layer and adding a classifier, while keeping the backbone feature extractor frozen during fine-tuning. Moreover, they use an attention connected module (ACM) along the main architecture to boost the model’s performance.  combines the EfficientNet-B5 and the DenseNet-201 [ref]170 in a deep ensemble learning fashion. Next, they simply add them to an average pooling layer, a dropout layer, and a sigmoid function for binary classification. MobileNet networks were originally proposed to be deployed on edge devices due to low computational burden. While , uses a transfer learning approach with a pure MobileNetV2,  uses a variation of the original MobileNetV2 with a binary cross entropy loss, and calls it the BCN-MobileNetV2.  also adds a global average pooling layer and modifies some output layers of the original MobileNet, by explaining the fact that all the previous layers are acting as a powerful feature extractor and the output layers can be tuned for the needed classification task. MobileNet itself  is a lightweight model, initially designed to be trained and tuned on mobile devices considering its low power consumption, fast execution, and low memory usage.  uses MobileNetV2 as it’s backbone and applies transfer learning likewise, with adding a sigmoid and ReLU activation layer afterwards, proposing FFireNet. Some other works use MobileNets ideas on other conventional object detection models for fire detection. For instance,  uses the deep separable convolutional structures present in MobileNetV3 to replace YoloV4’s original backbone network (CSPDarknet53) . Their proposed model, Yolo-Edge aims to reduce the model size and parameters for enhanced adaptation to edge devices and multi-scale prediction. Moreover, feature fusion is performed through a feature pyramid to improve the detection accuracy of small targets. Inception, initially introduced in , and its variations are used in many works.  uses Inceptionv3, an improved version of Inception (less computation power), along with factorized convolutions, regularization, dimension reduction, and parallel computations to make the network more efficient. They also modify the Xception architecture that uses depth-wise separable convolutions, with the same context of holding on to the feature extractor and changing the output layers. Depth-wise separable convolutions have the upside of less computation and parameters compared to separable convolutions but can be slower than them. For this work they use a fine-tuning technique on InceptionV3 and Xception, which unfreezes a few last layers of the transferred model (to learn task-specific features) and adds a classifier as needed. The other technique used is Learning without Forgetting (LwF), where a network is trained with new images while keeping its previous capabilities. Among other works following the same family of architectures,  uses a variant of Inception called Inception-ResNet-v2, a combination of grid reduction modules following residual inception modules. They transfer the weights directly without architecture modification and use the Adam optimizer for fine-tuning the network on the new data.  simply modifies Inceptionv3 by retaining the classifier and substituting a soft-max function. Classic CNN-based Models build upon a simple architecture, from scratch, rather than a famous backbone model. Although many models start with a well-known deep learning model and apply transfer learning for the specific task of wildfire detection, some start with a basic model designed by their own.  for instance starts with a 9-layer CNN, and then builds their deep CNN-17, capable of accurate detection after preprocessing raw images with histogram matching and image smoothing.  proposes a new saliency detection algorithm for fast location and segmentation of core fire area in aerial images. A 15-layered self-learning DCNN architecture named ‘Fire_Net’ is then presented as a self-learning fire feature exactor and classifier.  uses a radial bases function network after pre-processing multi-spectral images of wildfires. The authors argue that due to its simplicity, ease of implementation, and good approximation behavior, the radial basis function is a popular alternative when generating a geometric model from multivariate scattered data such as wildfires. They next feed the output to a super-resolution module, and next classify the image as fire/no-fire.  first performs data augmentation and transforms such as rotation, width and height shift and zoom on the ForestFire dataset to increase the generalization of their approach. Their proposed model, FFireXnet, consists of three convolutional blocks followed by a global average pooling layer and a classifier head. They also use an X-AI (explainable-AI) tool named SHAP (Shapely additive explanations) which makes the extracted features interpretable, infers the positive/negative contribution of features towards fire likelihood, identifies more important features and model biases.  proposes an unsupervised method for labelling sub-images to extend the original dataset and enhancing the supervised core framework. Next, they discuss the design and implementation of LW-FIRE (lightweight wildfire image classification), comprising multiple convolutional blocks to extract global features followed by a fully connected layer with ReLU activation and a final sigmoid activation. Like many Inception-based works addressed above,  uses depth-wise separable convolutions as a tool. The authors here aim to enhance the small SqeezeNet model. Moreover, a manual design algorithm is implemented beforehand to extract suspected smoke areas. SqueezeNet was originally proposed to tackle the problem of model and parameter size, shrinking down the computational complexity of DNNs . Federated Learning and distributed machine learning architectures have became popular as a result of both data dissipation and advances in network communication, security, edge computing. Authors in  have taken a federated learning approach towards wildfire classification accounting for the heterogeneity of UAV specifications and capabilities in a collaborative wildfire detection team. They showcase the accuracy improvement with increasing the number of participating UAVs on 3 different datasets. Spatio Temporal Classifiers are a new subset of methods that take the temporal dependency of consecutive frames into account, along with the spatial features extracted from each. Authors in  propose an interesting hybrid approach including both CNN and RNN for feature extraction. They claim to be the first to use such hybrid method for forest fire detection. Two fully connected layers are responsible for aggregating the extracted features of the two networks. While the CNN extracts high- and low-level spatial features, the RNN focuses on dependencies of frames and sequences, while taking the flattened version of the CNNs final output map as its input. A similar aptio-temporal model is proposed in . They claim to achieve real-time accurate fire detection by utilizing the static and dynamic characteristics of the fire. They first convert fire RGB images to optical flow images in real-time, next use a convolutional neural network for spatial learning, and finally a class of recurrent convolutional architectures for sequence learning. After the concept of visual attention was introduced, many detection models attempted to improve their performance by taking advantage of the dynamic focus it provides towards learning important features.  proposes an attention enhanced bidirectional LSTM (ABi-LSTM) for video-based forest fire smoke recognition. The model consists of three main parts: the spatial features extraction network, the Bidirectional Long Short-Term Memory Network (LSTM), and the temporal attention sub-network. This design helps the model to pay different levels of attention to different patches. Wildfire Segmentation Approaches\nSegmentation or pixel-wise classification is another important task in the context of wildfire management. Accurately categorizing wildfire-affected areas has remained an ongoing challenge. For this purpose, DL-based segmentation methods offer an advanced solution to facilitate wildfire management and mitigate their impacts. They can automatically identify the boundaries of flame or smoke within various remote sensing technologies, such as satellite or UAV-based imagery. Generally, these approaches rely on the ability of DNN networks to determine complex patterns and spatial relationships in the data, which allows them to precisely classify each pixel in the image based on their respective object classes. Utilizing wildfire segmentation methods significantly enhances early detection performance, allowing for more effective wildfire management. Figure 11 illustrates the fundamental architecture of a wildfire segmentation framework based on the deep CNN network. Wildfire segmentation based on deep CNN networks consists of two main components: feature extractor and segmentation decoder. Feature extractor block uses a set of convolutional layers and pooling layers, while segmentation decoder block utilizes a group of upsampling layers, convolutional layers, and a softmax layer. Upsampling Layer, also known as the deconvolution layer, is responsible for enhancing the spatial resolution of the feature maps by increasing the dimension of every single pixel in an image. This layer is particularly critical for wildfire segmentation since it retrieves details that might have been lost during the downsampling operation. This layer plays a crucial role in precisely identifying and characterizing the boundary of fire/smoke regions. The Softmax Layer serves as the final layer in the segmentation decoder block. It receives real values of various classes and then converts the network’s raw output into a probability distribution. In the context of wildfire segmentation tasks, the softmax layer assigns probabilities to each pixel for various classes, including fire, no-fire, smoke, and no-smoke. These probabilities help to detect and classify different regions within the images. In Table 6, we investigate the DL methods used in wildfire segmentation approaches between 2018 and 2023. A comprehensive understanding of these methodologies is essential for evaluating the strengths and limitations of each approach while assessing their effectiveness in active-fire management. 1∗{{}^{*}}^{1} The related paper creates its own datasets from Turkey’s wildfires using Sentinel-2 multiband images.\n2∗{{}^{*}}^{2} This article collected images from the Flame1 dataset (500 Images) and other online sources (500 images).\n3∗{{}^{*}}^{3} The related article collected images from Kaggle (1,900 images), Github’s DFireDataset, and Flame2 dataset.\n4∗{{}^{*}}^{4} This paper used three public datasets, including Corsican (1,775 images), Firefront-Gestosa (238 images), and Flame1 (2003 images) datasets.\n5∗{{}^{*}}^{5} The related article gathered the images from the Corsican dataset and online resources.\n6∗{{}^{*}}^{6} This paper collected data from three different video resources, including the NTUST dataset (1,033 videos), online sources (300 videos), and the Foggia dataset (31 videos).\n7∗{{}^{*}}^{7} The related paper utilized six different datasets consisting of 4,000 images for the training, including Corsican, FireNet, two private wildfires, and two public smoke datasets. For the test, they used a 360-dataset consisting of 150 images.\n U-Net-based Models are the main backbone model used for feature extraction in wildfire segmentation tasks. By comparing the works listed in Tables 5 and 6, we observe that U-Net architectures are significantly more present in segmentation, then classification. In fact, the U-Net architecture consists of an encoder-decoder structure with skip connections, allowing for precise pixel-wise segmentation, making the model commonly used for semantic segmentation tasks in various fields. U-Net models are often combined with other famous backbones for feature extraction.  employed uni-temporal Sentinel-2 images and deep learning models, specifically U-Net and ResNet, for wildfire detection. The U-Net model, utilizing different encoders like ResNet50, ResNet101, and ResNet152, demonstrated effectiveness in wildfire mapping. The authors introduce the attention ResU-Net model, incorporating an attention mechanism for enhanced wildfire detection performance. In the Flame1 dataset, presented in , a customized U-Net architecture is used for fire segmentation. The U-Net comprises a contracting path and an expanding path forming a U shape. The input layer matches the size of input images with three RGB channels. The contracting path consists of fully convolutional layers using the ELU activation function, dropout layers, and max pooling layers, repeated four times to shape the left side of the U. The right side mirrors the left with up-convolutional layers replacing max pooling layers. Concatenation occurs between the current and peer blocks from the contracting path, and the final layer employs the Sigmoid activation function for binary classification.  introduces advancements in multiclass fire segmentation using the UNet architecture, presenting the UUNet-concatenative architecture and the wUUNet model. The UUNet-concatenative architecture incorporates two UNet models, with the first performing binary flame segmentation and the second indicating specific fire classes through concatenation. Skip connections between the binary model decoder and multiclass model encoder enhance results across model levels. The wUUNet model, an advanced UNet version, employs a loss function with cross-entropy and soft-Jaccard for both binary and multiclass segmentation. Demonstrating superior performance, the wUUNet outperforms the baseline UNet model in binary and multiclass segmentation accuracy by 2% and 3%, respectively, contributing to the improved capabilities of fully convolutional neural networks for multiclass fire segmentation. Among works utilizing different variations of U-net, some focus on input shape adaptation.  for instance, explores active fire detection in Landsat-8 imagery using three variations of the U-Net architecture: U-Net (10c), U-Net (3c), and U-Net-Light (3c). U-Net (10c) processes a 10-channel image with all Landsat-8 bands, while U-Net (3c) and U-Net-Light (3c) use a reduced 3-channel input with specific bands. Additionally, the architectures are evaluated using a ”best-of-three” voting scheme, where a pixel is identified as active fire if at least two sets of conditions align on its classification.  introduces a novel spatiotemporal feature fusion technique called FuseNet combining temporal and spatial features extracted by TemporalNet and SpatioNet. TemporalNet focuses on learning temporal features using 3D convolutions, employing semi-supervised learning with ground truth masks for one frame, and incorporating VGG blocks with 3D for temporal behavior. SpatioNet processes single frames, integrating skip connections, U-Net-inspired architecture, and attention mechanisms to capture spatial features. FuseNet combines outputs from TemporalNet and SpatioNet, using a self-attention mechanism for spatial-temporal dependencies. Modifications include multi-stage training, a two-stage pipeline for fire detection, and sensitivity to fires of varying sizes. Notable novelties include spatio-temporal self-attention, semi-supervised learning in TemporalNet, and a two-stage detection pipeline, making it effective for real-world wildfire surveillance. This work is also reviewed in the detection section following this section, where the pipeline differences for segmentation and detection are discussed.\nAuthors in  also modify the original UNet architecture for wildfire image segmentation. Specific adaptations, including adding batch normalization layers and reducing the number of filters in the deepest layers, were made to optimize the U-Net for efficient processing of drone-captured wildfire imagery. These modifications aim to enhance both efficiency and effectiveness, considering the constraints of drone-based processing. Further optimization techniques, such as quantization and pruning, were applied to achieve reduced inference times while maintaining segmentation performance.  use U-Net for the segmentation network they propose along classification, specifically trained for fire and smoke segmentation. Besides integrating classification and segmentation, the study introduces novel elements the use of a Quad-Tree search algorithm for scalable segmentation, and a comprehensive evaluation of different model configurations. DenseNet is used for segmentation as well as classification (mentioned in previous section) due to its efficiency and feature reuse properties.  uses DenseNet while emphasizing a segmentation strategy relying on the YUV color space. The paper introduces a formula for the segmentation process and presents segmentation results in the YUV color space, showcasing the RGB image, Y, U, and V components, along with identified candidate smoke regions. SqueezeNet  has emerged into a popular backbone for segmentation on edge devices for its lightweight design, striking a balance between model size and accuracy, making it efficient for real-time applications on resource-constrained devices. Its impact extends to embedded systems and IoT, influencing subsequent architectures that prioritize efficiency without sacrificing performance. In this vein, fire segmentation applications for real-time operating systems are no exception.  present the Squeezed Fire Binary Segmentation Network (SFBSNet) is a novel architecture designed for binary semantic segmentation of fire images, based on the lightweight SqueezeNet model. They add depthwise separable convolution and 1 × 1 convolution to reduce parameters and model size while maintaining high performance. Depth-wise separable convolutions, first introduced in Xception , provide cross-channel feature extraction capabilities while maintaining spatial feature extraction with lighter design.  utilizes these separable convolutions in the proposed FBC-ANet model. Xception, from which these convolutions are inherited, is the encoder in the FBC-ANet architecture. The authors use of separable convolutions helps to reduce the number of parameters and the computational complexity of the model, making it more efficient while maintaining strong performance in semantic segmentation tasks.  utilizes a modified SqueezeNet architecture for dense prediction in forest fire segmentation. The network includes a front-end prediction module without pooling layers and intermediate map padding, enhancing segmentation accuracy. A context module, employing dilated convolutions for multi-scale contextual information aggregation, further improves accuracy. ResNet-based Models have been a popular backbone for segmentation tasks, as well as classification, that was reviewed in the previous part. These models rely on the famous ResNet proposed in  and build on its architecture through transfer learning, adapting it to wildfire scenarios. Lately, some works have integrated attention mechanisms with a famous backbone model. The work in [ref]155 introduces two solutions for forest fire image classification and segmentation. The first solution, DSA module, is a novel attention mechanism enhancing feature channel representation for improved accuracy in incipient forest fire classification. The second solution, MaskSU R-CNN, is an enhanced instance segmentation model combining Mask Scoring R-CNN and a U-shaped network to reduce segmentation errors and accurately distinguish fire regions. The model utilizes DSA-ResNet50 as its backbone, incorporating the DSA module to improve feature extraction. The architecture includes the feature pyramid network for multi-scale fusion. Collectively, these solutions provide a flexible model, MaskSU R-CNN, for efficient unmanned fire monitoring across large forest areas.  uses residual blocks in its proposed encoder-decoder for wildfire segmentation, inspired by FusionNet model . The residual connections are integral for feature extraction and information propagation. Beside the main purpose of them, addressing the vanishing gradient problem, In the encoder-decoder architecture, the residual block contributes to complex feature extraction in the encoder and aids in refining the segmented output in the decoder, enhancing the model’s ability to capture intricate details and spatial information for accurate wildfire segmentation. DeepLabV3 was introduced in 2017  with the primary motivation to enhance the model’s ability to capture contextual information and generate more accurate, finer‐grained segmentation maps . The breakthroughs of DeepLabv3 included the addition of an improved encoder‐decoder architecture, batch normalization, and regularization, which aimed to improve the model’s performance in semantic image segmentation . Authors in  use DeepLabv3+, an extension of DeepLabv3, for wildfire segmentation. It employs an encoder-decoder structure, utilizing a ResNet backbone as the encoder, and incorporates an Atrous Spatial Pyramid Pooling (ASPP) module. The ASPP module employs parallel atrous convolutions with different dilation rates to capture multi-scale contextual information, enabling accurate segmentation of objects with fine details and small sizes. The ResNet backbone, pre-trained on a large dataset, extracts high-level features. DeepLabv3+ demonstrates proficiency at handling images of varying sizes and aspect ratios through a spatial pyramid pooling module. DeepLabv3+ is also used in  but with backbone differences. The authors applied the Deeplabv3+ architecture to the French Corsican dataset with a modified Xception backbone for wildfire segmentation. The modifications to the Xception model include adjustments to the entry flow path for faster computation and higher memory efficiency, replacing max pooling layers with depthwise separable convolutions connected to an atrous separable convolution for random resolution feature extraction. The Deeplabv3+ model implements Atrous Spatial Pyramid Pooling (ASPP) with depthwise separable convolutions at atrous rates of 6, 12, and 18, enhancing multi-scale feature extraction. The decoder module involves bilinear upsampling, concatenation with encoder features, and subsequent convolutions for refined segmentation.  enhance the original DeepLabv3+ with some modifications. The encoder network combines a deep convolutional neural network with atrous spatial pyramid pooling, producing feature maps at four different resolutions. To optimize segmentation speed, the authors here replace the original deep convolutional neural network with the lightweight MobileNetV3. However, to address potential accuracy loss due to the absence of atrous convolution, two additional shallow features are incorporated into the original decoder network, ensuring a wealth of fire feature information.  focuses on refining the DeeplabV3+ model for precise fire segmentation in aerial images. Modifications involved fine-tuning with various backbones, including ResNet-50, and experimenting with different loss functions to optimize the model for detecting fire pixels. The study considered the impact of diverse loss functions, highlighting a tailored approach to the model’s training for the unique characteristics of aerial fire images. Leveraging the inherent encoder-decoder architecture of DeeplabV3+, the study potentially further optimized this structure to meet the demands of fire segmentation. Attention-based Models are further used in DeepLabV3+ backbones to improve feature extraction with a focus on flame-related areas. FlameTransNet , designed for wildfire segmentation, follows an encoder-decoder architecture, leveraging MobileNetV2 for feature extraction and integrating a transformer module for global feature capture. It utilizes the DeepLabV3+ decoder, enhancing spatial context preservation. Notably, the CBAM attention mechanism refines lower-level features during fusion, prioritizing flame regions. Modifications include adaptive Copy-Paste data augmentation to handle class imbalance, dice loss for flame emphasis, and addressing CNN’s limited receptive field. Novelties involve global feature extraction via transformers, attention mechanisms for detail refinement, and innovative data augmentation and loss functions. The network architecture proposed by  for joint fire classification and segmentation incorporates several key modifications and innovations, with a focus on leveraging the DeepLab-v3+ framework for segmentation. Notable adaptations include the introduction of a spatial self-attention mechanism for capturing long-range dependencies, a channel attention module to enhance feature relevance, and joint training for simultaneous segmentation and classification tasks. The utilization of the DeepLab-v3+ encoder backbone, initialized with pre-trained ImageNet weights, further contributes to the network’s capabilities. Transformers have shown significant success leading the state-of-the-art models for attentive segmentation and detection, and wildfire segmentation has been no exception.  discusses the integration of two vision transformers, TransUNet and TransFire, into wildfire segmentation in aerial images. Vision transformers utilize self-attention mechanisms to effectively capture long-range dependencies in images. Notably, TransUNet combines U-Net architecture with vision transformers, enabling the incorporation of self-attention for capturing global dependencies between inputs and outputs. This integration proves beneficial for tasks like wildfire segmentation by extracting fine details and long-range interactions in input features. Additionally, TransFire, based on the Medical Transformer (MedT) architecture , utilizes gated position-sensitive axial attention and a LoGo (Local-Global) training methodology to enhance segmentation performance.\nLike classification and detection, temporal information can be beneficial for segmentation as well.  introduces the FFS-UNet model, a spatiotemporal architecture for forest fire segmentation, integrating a temporal transformer module (TTM) into a modified lightweight U-Net model. The TTM assesses temporal relevance and indications across a sequence of frames, addressing challenges posed by fast-moving UAVs, irregular fire shapes, and cluttered backgrounds. The TTM employs patch embedding and a temporal REST-block encoder to extract CNN feature maps, enhancing feature learning and extraction in UAV video semantic segmentation. Additionally, the model incorporates long-skip connections between the encoder and decoder layers to improve precision in fire region detection. By explicitly learning fire features from the temporal transformer, the FFS-UNet achieves promising results in forest fire segmentation, showcasing the efficacy of integrating temporal information for robust feature extraction in the context of U-Net architecture. Transformers have also shown their substantial capabilities in combination with U-net architectures in other works. Authors in  employ an ImageNet pre-trained InceptionResNet v2 as the primary feature extractor, recognized for its robust performance in image recognition. Two DeepLab V3+ networks are trained in the study, and a modified loss function is implemented to better suit the task of detecting candidate fire regions. The modified loss function incorporates weighting factors to emphasize the importance of fire pixels, aiming to enhance the model’s effectiveness in detecting fire events. Other Approaches are based on modifying a traditional CN structure with elements specializing in the network for wildfire segmentation. Deep-RegSeg , a novel deep learning-based method for wildfire segmentation, achieves a high F1-score of 94.46%, outperforming recent state-of-the-art techniques. It excels in accurately detecting and segmenting fire pixels in challenging, non-structured environments, including conditions like smoke and changing luminosity. Notably, Deep-RegSeg proves effective in identifying small fire areas under diverse weather and brightness conditions, crucial for early wildfire detection and management. The method offers adaptability with various backbone options, termed RegNet models (RegNetX800MF, RegNetX200MF, RegNetX400MF, RegNetX16GF, and RegNetX32GF), and two loss functions (Dice loss and Binary Cross Entropy Dice loss), providing flexibility for optimizing performance in wildfire segmentation tasks. RegNet models, known for scalability and efficient feature extraction, serve as the backbone in Deep-RegSeg, contributing to its ability to accurately segment wildfire pixels and detect fire areas under varying environmental conditions. The study conducted in  is comprehensively described in the next subsection for wildfire detection models, but they also propose a segmentation pipeline utilizing their saliency detection-based method to efficiently locate and isolate core fire regions in aerial images. Region-based CNNs are used for segmentation as well as classification (described in the previous section). , for instance, designs a segmentation network called LS-Net, specifically to perform pixel-wise segmentation for fire and smoke regions. The segmentation results obtained from LS-Net are then utilized in the decision network (AD-Net) to predict the probability of fire smoke existing in an image, contributing to the classification objective.\nThe proposed framework enhances the efficiency of a baseline R-CNN for forest fire and smoke detection by employing knowledge distillation, reducing computational complexity through a teacher-student model. This approach combines a complex CNN feature extractor with a simplified student network, striking a balance between computational efficiency and detection performance, resulting in faster inference times for real-world applications. As discussed above, there are many options to choose when it comes to applying transfer learning from a backbone network and the choice depends on the hardware and software requirements. However,  compares four widely used fully convolutional network models (FCN, U-Net, PSPNet, and DeepLabV3+), evaluating their performance in forest fire image segmentation. The study employs different backbone networks, such as VGG16 and ResNet50, and finds that the U-Net model with ResNet50 as a backbone exhibits the highest segmentation accuracy for forest fires. Additionally, DeepLabV3+ with ResNet50 demonstrates satisfactory segmentation performance with faster running speed. Wildfire Object Detection Approaches\nDetecting and locating specific objects within a wildfire-affected area, such as individual flames, smoke plumes, or structures in danger, is a crucial aspect of wildfire management. By accurately identifying and tracking specific objects related to wildfires, firefighters and emergency responders can make informed decisions about managing and responding to the wildfires. To this aim, DL-based wildfire object detection approaches are at the forefront of addressing the challenges within efficient wildfire detection. DL models can identify objects of interest within images or video streams, such as fire fronts, burned areas, and potential ignition. Like wildfire classification and segmentation, object detection methods employ deep neural networks, primarily built upon the foundation of CNNs. Their goal is to enable automated recognition and delineation of wildfire-related objects, providing valuable information for Precise situational awareness and timely decision-making. Figure 12 shows the general architecture of a wildfire object detection framework based on deep CNN networks. The network is trained to recognize distinctive patterns associated with different wildfire-related objects, ensuring a comprehensive understanding of the evolving situation. They commonly analyze multi-spectral or high-resolution imagery from diverse sources, such as satellites, UAVs, or ground-based sensors. In Table 7, we explore the DL methods employed by wildfire classification approaches throughout the period from 2018 to 2023. A comprehensive understanding of these methodologies is essential for evaluating the strengths and limitations of each approach while assessing their effectiveness in active-fire management. ∗∗ This paper evaluated the different versions of Yolo architectures, including YoloV5, YoloV6, YoloV7, and YoloV8. Each one of these models has different variants, and the best one is highlighted in this table.\n1∗{{}^{*}}^{1} The related article collected images from Kaggle (1,900 images), Github’s DFireDataset, and Flame2 dataset.\n2∗{{}^{*}}^{2} This paper gathered images from different public forest fire datasets and online sources such as Google.\n3∗{{}^{*}}^{3} This paper customized their dataset, namely the FireDetn dataset, where the data have been selected from five various resources, including the FireClips, BoWFire, FireNet, Fire-Detection-Image-Dataset, and Paddle Fire datasets. \n4∗{{}^{*}}^{4} This paper collected data from three different video resources, including the NTUST dataset (1,033 videos), online sources (300 videos), and the Foggia dataset (31 videos).\n5∗{{}^{*}}^{5} This paper used four public datasets, including BoWFire, FD-dataset, ForestryImages, and VisiFire datasets.\n6∗{{}^{*}}^{6} The related article collected images from Kaggle (1,900 images), Github’s DFireDataset, and Flame2 dataset.\n7∗{{}^{*}}^{7} The related paper collected data from the Corsican Dataset and other online sources.\n YOLO (You Only Look Once) has pioneered the realm of object detection in computer vision for many years [ref]226. Many works listed in Table 7 use the popular Yolo object detection model. The Yolo model is a pioneering approach in the field of object detection, known for its real-time processing capabilities and unified architecture [ref]226. The main distinguishing factor of Yolo models from previous object detection models lies in their unified approach to object detection. Unlike traditional models that rely on region proposal algorithms to hypothesize object locations , Yolo frames object detection as a regression problem, directly predicting bounding boxes and class probabilities in a single step [ref]226. This unified architecture allows Yolo models to process images in real-time, making them significantly faster than previous methods . Additionally, Yolo models can detect a wide range of object categories, with Yolo9000 predicting detections for over 9000 different object categories . The original Yolo model was introduced in 2016, aiming to provide a unified solution for real-time object detection. Since then, several variants of the Yolo model have been developed, each with its own improvements in terms of accuracy, speed, and model size . YoloV2, an improved model, has been reported to be state-of-the-art on standard detection tasks such as PASCAL VOC and COCO . YoloV3 and YoloV4 are further enhanced versions of the original Yolo algorithm, offering improvements in both accuracy and speed. Additionally, Tiny Yolo is a simplified architecture derived from YoloV3, designed to be more lightweight and suitable for deployment on embedded devices . YoloV5, presented in , shares a foundational architecture with YoloV4 but incorporates numerous enhancements in terms of speed, precision, and user-friendliness. The inclusion of ”SPP” (Spatial Pyramid Pooling) is a notable innovation, effectively diminishing the computational load necessary for object detection. Additionally, YoloV5 introduces a novel backbone architecture referred to as ”CSPNet” (Cross-Stage Partial Network), which refines the feature extraction phase, contributing to heightened model accuracy. The proposed YoloV6 model  incorporates several modifications to the YoloV5 architecture, including a novel anchor-free detection approach and a new feature pyramid network. The anchor-free approach eliminates the need for predefined anchors, which makes the model more flexible and robust to object size variations. The feature pyramid network enhances the model’s ability to detect objects of different sizes and resolutions. The YoloV7  model is based on a single-shot detector architecture and is trained end-to-end on a large dataset of annotated images. It incorporates several advanced features, including a backbone network based on the EfficientNet architecture, an SPP (Spatial Pyramid Pooling) module for capturing multi-scale features, and a PAN (Path Aggregation Network) module for integrating features from different scales. YoloV8 offers improved speed and accuracy and is suitable for real-time applications. However, the performance of the aforementioned models may vary from one dataset to the other. Thus, the choice of the Yolo version for an object detection framework should be based on the necessities of the application. Following the discussion on Yolo object detection models, several works as listed in Table 7 have used a backbone Yolo model as the main architecture of their detection model and have applied little modifications to make it suitable for wildfire detection application.  explores three object detection methods for real-time forest fire detection: Faster R-CNN, YoloV3, and SSD. Faster R-CNN uses a Region Proposal Network but has a low frame rate, limiting its real-time applicability. YoloV3 employs anchor boxes and logistic regression, introducing an improved structure for enhanced fire detection. SSD eliminates the need for bounding box proposals, using small convolutional filters for efficient category score predictions. Each method has unique strengths and trade-offs, providing options for real-time forest fire detection based on specific requirements.  proposes a model adapting YoloV3 to the task of wildfire detection by implementing a small-scale CNN with the help of YoloV3, resulting in improved detection speed and reliable accuracy. Additionally,  uses YoloV3 to develop a coarse-to-fine framework for auto-detecting wildfires in high-resolution aerial images acquired by UAS. This framework involves a two-phase learning process that significantly reduces time overhead while maintaining high accuracy. By combining a coarse detector for adaptive sub-region selection and a fine detector for detailed scrutiny, the model improves the mean average precision (mAP) from while achieving, surpassing real-time one-stage YoloV3 in average inference speed. Specifically designed for high-resolution aerial images from Unmanned Aerial Systems (UAS), it effectively addresses challenges in detecting sparse, small, and irregularly shaped wildfires. The proposed method provides a speed-accuracy trade-off, outperforming the baseline Yolo-crop (a modification of the Yolo specifically tailored for processing high-resolution images) in real-world wildfire detection applications. Authors in  present multiple key features in their UAV-FFD platform, including real-time image transmission, high-performance computing, edge computing architecture, and integration with big data analysis, which all contribute to the efficient and accurate detection of forest fires using the YoloV3. Some works have used different ideas proposed in deep learning to handle sequential detection and segmentation.  for instance, relies on YoloV5 for detection and a U-net for segmentation. The YoloV5 model utilizes Cross Stage Partial Networks (CSPN) as a backbone for fire detection and localization, generating bounding boxes with class scores. A Crop Layer is subsequently applied to extract regions limited by the bounding boxes, containing the localized fire area. The cropped images are fed into a U-Net model for pixel-level segmentation, confirming the presence of flames and producing a binary mask representing fire pixels, offering precise location detection. This integrated approach enhances fire detection accuracy by combining object localization and pixel-level segmentation. Authors in  follow the same hybrid approach, but here to improve the challenges present in YoloV4. The study initially employs YoloV4 for object detection but replaces it with a more lightweight MobileNetV3 model due to computational and memory constraints. Further compression is achieved by removing redundant parts, leading to the creation of a Pruned + KD model through knowledge distillation. Redundancies in weights, channels, and layers are addressed by pruning, with a focus on channel-level sparsity-induced regularization. This regularization involves scaling factors inserted into each channel of MobileNetV3, subsequently serving as L1 regularization for training. The resulting model exhibits reduced size and improved efficiency compared to the initial network. In another work, ensemble learning is used to integrate YoloV5 and EfficientDet models together. Ensemble learning enhances forest fire detection accuracy by integrating multiple learners, such as YoloV5 and EfficientDet, to improve model robustness and performance. EfficientDet , a highly efficient object detection model by Google, is renowned for its performance under resource constraints. Leveraging the EfficientNet backbone, Bi-FPN neck, and compound scaling method, it excels in detecting diverse forest fires. Individual learners may exhibit limitations, focusing too much on local information and generating false positives. The integration of YoloV5 and EfficientDet in parallel within the system synergistically improves the detection accuracy of various forest fire types. Despite the substantial amount of work deploying Yolo variants for forest fire detection, there are limitations in detecting small fires in particular which authors in  point out. To address this, the authors enhance YoloV5 by introducing a very-small-target detection layer, a CBAM attention module, and refining the SPPF module into SPPFP(Spatial Pyramid Pooling-Fast-Plus). These modifications aim to improve the model’s focus on global information and mitigate the issue of missing details in small-target forest fires. The resulting YoloV5 Improvement model is designed to adaptively extract features, particularly for small-target forest fires, enhancing overall detection performance. Authors in  also tackle the problem of small target object detection and propose their fire variant of Yolo. The Fire-Yolo deep learning method enhances small target object detection in fire inspection through expanded three-dimensional feature extraction, improving network performance. It outperforms other models like Faster R-CNN and unimproved YoloV3 in terms of detection efficiency for very small target objects. Achieving real-time detection with an average time of 0.04 s per frame at 416 × 416 resolution, the model adapts dimensions for small target images, strengthening information interaction and enhancing detection accuracy in fire scenarios—proving valuable for public safety and forest fire management.  mentions the crucial role of sensing technologies such as LiDAR (Light Detection and Ranging) in improving fire detection accuracy through various methods. In their work, LiDAR enables the creation of high-resolution 3D models of forested areas, serving as input for YoloV4 tiny. Additionally, it aids in feature extraction, providing information on tree height and canopy density for enhanced fire and smoke detection. Integration with visual and thermal imagery further enhances the overall accuracy of forest fire detection and prediction. Among all the work done with Yolo for wildfire detection, one work has studied how different variants of Yolo perform under fire detection tasks.  proposes a model demonstrating a decreased sensitivity level and improved anomaly identification speed on these original Yolo models. Utilizing a dataset with three detection zones, the model outperforms the gold-standard detection approach for forest fires by 96.8%, achieving an mAP of 50 and FPS of 122 on a multi-oriented dataset. Comparative analysis indicates superior performance over advanced object-detection algorithms, especially in detecting smoke from wildfires under challenging environmental conditions. (Detailed results are shown in terms of accuracy in Table 7) Classic CNN-based Models have been utilized by several works, such that a popular backbone network has been transferred for the task of wildfire detection, working with a modified classification/detection head. Some of the models listed in Table 7 share their detection task with a classification/segmentation and are also listed in Tables 5 and 6. Here we will present other models, not mainly based on Yolo object detection models.  investigates feature fusion in handling RGB and IR image pairs using two approaches: Early Fusion, concatenating images and modifying the network, and Late Fusion, training separate networks for each modality and merging features later. The Flame network, a lightweight CNN with less than 1000 parameters, serves as a baseline, featuring three convolutional layers, max-pooling, and two fully connected layers. Transfer learning with pre-trained models is employed, substantially expediting training. Overall, the study compares Early and Late Fusion in conjunction with the Flame network and underscores the efficacy of transfer learning for improved training efficiency.  designs a hierarchical approach to wildfire detection, comprising two modules: wildfire image classification and wildfire region detection. The first module employs traditional machine learning (SVM) and Reduce-VGGNet to classify extracted video frames based on normalized shape, texture, and color features. In the second module, the Vibe algorithm identifies candidate fire regions, and an optimized CNN extracts temporal and spatial features to enhance detection accuracy. This approach, aimed at improving wildfire detection precision, showcases high accuracy in the experiment. The Reduce-VGGNet model and the optimized CNN contribute to reduced parameters and effective combination of spatial and temporal features for accurate wildfire image classification and region detection.  employs an ensemble CNN architecture, incorporating MobileNetV2, XceptionNet, and ResNet-50, to enhance overall performance in wildfire detection. Through transfer learning and data augmentation, the models are trained on a dataset containing smoke and fire images. The ensemble CNN is integrated with a staged Yolo model, forming a two-stage detection system. The ensemble CNN identifies abnormalities, and if detected, the staged Yolo model is employed to localize smoke or fire, providing a comprehensive approach to improve the accuracy of smoke and fire detection.  proposes FireXnet; developed to address limitations in conventional wildfire detection methods, leveraging data-driven deep learning solutions. Its tailored lightweight architecture, with reduced trainable parameters, allows for efficient deployment on resource-constrained devices like drones.Notably, FireXnet incorporates explainable AI using the SHapley Additive Explanations (SHAP) tool. This enhances interpretability, allowing for a detailed understanding of the features contributing to wildfire predictions, thus improving accuracy and reliability. Furthermore, FireXnet is compared with five pre-trained models (VGG16, InceptionResNetV2, InceptionV3, DenseNet201, and MobileNetV2) through transfer learning, providing insights into their respective performances for wildfire segmentation.  presents a saliency detection algorithm for rapid identification and segmentation of core fire areas in UAV aerial images, addressing the challenge of wildfire detection. The 15-layered self-learning DCNN architecture, ’FireNet,’ efficiently extracts wildfire features and serves as a classifier, achieving an impressive 98% overall accuracy. The combination of saliency detection and the proposed DCNN proves effective in localizing and recognizing wildfires, preventing feature loss and enriching the image database. The practical utility of ’FireNet’ is demonstrated through accurate wildfire identification in sampled images from news reports, showcasing its real-time inspection capabilities.  introduces an energy-efficient and computationally efficient CNN architecture for fire detection and localization, inspired by the SqueezeNet model, utilizing smaller convolutional kernels to minimize computational requirements. The model differs from complex models by excluding dense, fully connected layers, further reducing computational needs. Despite its simplicity, the model achieves comparable accuracies to more complex counterparts, primarily due to increased depth. Notably, the proposed model is significantly smaller in size, making it more feasible for implementation in resource-constrained equipment.  proposes Detectron2, a model utilizing the Mask R-CNN (Region-based Convolutional Neural Network) for fire detection. The Mask R-CNN is a popular deep learning model for instance segmentation, which can identify and locate objects at the pixel level. It combines the Faster R-CNN object detection framework with a semantic segmentation task, allowing it to not only detect objects but also precisely outline their shapes within an image. The use of Mask R-CNN in the Detectron2 model enables accurate and detailed detection of fire regions, even in challenging conditions such as varying lighting, motion, and different fire characteristics.  proposes a novel dual-channel Convolutional Neural Network (CNN) for forest fire detection, designed to handle different-sized fire scenes. The model comprises two single-channel networks with distinct input sizes, fused to create a novel two-channel network. Two feature fusion approaches are employed to combine the results of the two networks, enhancing feature characterization. An attention mechanism focuses on key details in the fused features for improved efficiency. Transfer learning is utilized to mitigate overfitting and reduce training time. Experimental results, shown in table 7 demonstrate the model’s superior performance in fire recognition, surpassing the single-channel network. Attention-based Models and transformers have also been used in some works. Authors in  utilize modern computer vision techniques such as visual attention to improve wildfire detection performance. , specifically uses attention to fuse spatial and temporal extracted features together. The proposed Spatio-Temporal Self-Attention Network consists of three main components: TemporalNet, SpatioNet, and FuseNet. TemporalNet focuses on learning temporal features from a sequence of frames, while SpatioNet processes a single frame. Both networks produce 64-channel feature maps, which are concatenated and passed through a 1x1 convolution layer for size reduction. FuseNet then employs a self-attention mechanism to capture spatial-temporal dependencies crucial for fire detection and segmentation. The network undergoes multi-stage training, with independent training for SpatioNet and TemporalNet to extract fire segmentation masks for individual frames, followed by training FuseNet to integrate their outputs. The approach also incorporates transfer learning and a two-stage pipeline for fire detection and verification, enhancing the network’s effectiveness. While this work is also highlighted in the segmentation section, the segmentation and detection pipelines include subtle differences. The segmentation architecture focuses on identifying fire regions in single frames using SpatioNet, TemporalNet, and FuseNet, while the detection architecture processes frame sequences, employing the Spatio-Temporal Network for high-quality segmentation maps in the region proposal stage and a subsequent classifier for fire presence determination. Segmentation targets individual frames, while detection assesses fire existence across sequential frames, with segmentation playing a vital role in the region proposal for detection. Transformers Models have recently been used as a powerful tool for wildfire detection. The FireDetn model is designed by authors in  for efficient real-time wildfire detection in complex scenarios, featuring key elements to enhance accuracy. It employs four detection heads for flames of various sizes, improving overall model accuracy. The integration of Transformer Encoder blocks with multi-head attention allows the model to capture global and contextual features, enabling better detection in complex scenarios. This integration facilitates learning from relationships between different image features, enhancing the model’s contextual understanding and prediction accuracy. The multi-head attention mechanism enables simultaneous focus on different image regions, further improving the model’s ability to capture global and contextual information. Notably, the model also integrates a spatial pyramid pooling fast structure into the smallest detection head, efficiently capturing multi-scale flame objects with lower computational cost. Overall, the FireDetn model’s features collectively contribute to its accuracy in detecting wildfires in real-time and complex environments. The STPM_SAHI forest fire detection model proposed by  incorporates the Swin Transformer for enhanced global information capture, leveraging its self-attention mechanism for improved context understanding. It replaces the traditional Feature Pyramid NEtwork (FPN) with the proposed Path Aggregation FPN (PAFPN) for more effective feature fusion, reducing the impact of down-sampling. The model also introduces Slicing Aided Hyper Inference (SAHI) technology to address small-target fire detection challenges, providing a slice-aided reasoning pipeline and significantly improving accuracy for such fires. These innovations distinguish the model from traditional CNN models, offering improved feature extraction and small-target detection capabilities. Following this path, one other work upraged the idea of path aggregation networks. The Forest Fire Classification and Detection Model (FCDM) first optimizes the loss function by switching to SIoU Loss in the YoloV5 bounding box, incorporating directionality for faster convergence during training and inference. FCDM introduces the Convolutional Block Attention Module (CBAM) to fuse channel and spatial attention, improving classification recognition accuracy. The model, next, advances feature fusion by upgrading the Path Aggregation Network (PANet) layer to Bidirectional Feature Pyramid Network (BiFPN), preventing feature loss and enhancing forest fire detection across different scales.\n\nReference markers found: [155]\n\nContext windows for each [ref]:\n[ref]155: model used as the backbone feature extractor.  uses an architecture called ForestResNet, which is essentially the ResNet-50 model trained on 175 forest fire images obtained from the internet. [ref]155 uses DSA-ResNet50 in which DSA stands for dual semantic attention. In this method, they use two streams of features extracted from an intermediate feature map passed through two\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 155,\n            \"titles\": [\n                \"Aerial Imagery Pile burn detection using Deep Learning: the FLAME dataset\",\n                \"How natural disasters affect energy innovation? the perspective of environmental sustainability\",\n                \"Identifying the influence of natural disasters on technological innovation\",\n                \"Naturally negative: The growth effects of natural disasters\",\n                \"The economic impacts of direct natural disaster exposure\",\n                \"Wildfire-grazing impact on forage quality assessed with near-infrared spectroscopy and generalized partial least squares regression\",\n                \"Forest service large fire area burned and suppression expenditure trends, 1970–2002\",\n                \"A spatial database of wildfires in the united states, 1992-2011\",\n                \"Exploring the onset of high-impact mega-fires through a forest land management prism\",\n                \"Wildfire management in canada: Review, challenges and opportunities\",\n                \"Short-term effect of wildfires and prescribed fires on ecosystem services\",\n                \"Short-term impacts of wildfire and post-fire mulching on ecosystem multifunctionality in a semi-arid pine forest\",\n                \"Long-term health effects of wildfire exposure: a scoping review\",\n                \"Agency and resilience: teachings of pikangikum first nation elders, northwestern ontario\",\n                \"A burning story: the role of fire in the history of life\",\n                \"The human dimension of fire regimes on earth\",\n                \"Towards a comprehensive wildfire management strategy for mediterranean areas: Framework development and implementation in catalonia, spain\",\n                \"A model weighting scheme for fire weather projections simulated by cmip6 climate model ensembles\",\n                \"Human-started wildfires expand the fire niche across the united states\",\n                \"In the line of fire: consequences of human-ignited wildfires to homes in the us (1992–2015)\",\n                \"Computational modeling of extreme wildland fire events: A synthesis of scientific understanding with applications to forecasting, land management, and firefighter safety\",\n                \"Latest trends on tree classification and segmentation using uav data—a review of agroforestry applications\",\n                \"Advances in the study of global forest wildfires\",\n                \"A review on early wildfire detection from unmanned aerial vehicles using deep learning-based computer vision algorithms\",\n                \"Computer vision for fire detection on uavs—from software to hardware\",\n                \"K. Dimitropoulos\",\n                \"Intelligent and vision-based fire detection systems: A survey\",\n                \"A review on forest fire detection techniques: A decadal perspective\",\n                \"Advances in the remote sensing of active fires: A review\",\n                \"Airborne optical and thermal remote sensing for wildfire detection and monitoring\",\n                \"A survey on technologies for automatic forest fire monitoring\",\n                \"P. Gonzalez-Moreno\",\n                \"Environmental hazards posed by mine dust\",\n                \"The q-rung fuzzy lopcow-vikor model to assess the role of unmanned aerial vehicles for precision agriculture realization in the agri-food 4.0 era\",\n                \"Green internet of things using uavs in b5g networks: A review of applications and strategies\",\n                \"UAV-Assisted Communication in Remote Disaster Areas using Imitation Learning\",\n                \"A. de la Escalera\",\n                \"A review of ai-enabled routing protocols for uav networks: Trends\",\n                \"A Solution for Dynamic Spectrum Management in Mission-Critical UAV Networks\",\n                \"Marine environmental monitoring with unmanned vehicle platforms: Present applications and future prospects\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]155 based on the context and research area of A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}