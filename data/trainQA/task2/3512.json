{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Learning Stochastic Dynamics with Statistics-Informed Neural Network\n\nPaper Abstract: \n\nPaper Section Text:\nThe use of machine learning (ML) techniques to model stochastic processes and time series data has seen many contributions in the past years. Two common strategies are to utilize neural networks (NNs) to either solve or learn the associated differential equations. In the â€˜solverâ€™ strategy, the differential equation governing a dynamical system is assumed to be known a priori and an NN is used to construct a numerical solver for the equation.\nA representative approach using the solver strategy is the physics-informed neural network (PINN)Â  . In the PINN approach, an NN serves as a solver that transforms initial and boundary conditions into approximate solutions. Specifically, an NN is trained using a specialized loss function that is defined in terms of the underlying differential equation. Thus, minimizing this specialized loss function steers the solver towards producing outputs that conform to the target differential equation. More recent members within the PINN family include sparse physics-informed neural network (SPINN)Â , parareal physics-informed neural network (PPINN)Â [ref]4, and so onÂ [ref]5. The â€˜learningâ€™ strategy, on the other hand, aims to learn the hidden dynamics from data using an NN.\nThis strategy is adopted by the neural ordinary differential equation (NeuralODE) approachÂ . In this approach, an NN is not used to directly solve an equation, but rather to compute the gradient of the state variables. To train such a network, an adjoint dynamic system and a reverse-time ODE solver are adopted to facilitate backpropagation. After updating the parameters of the augmented dynamics, solutions to the differential equations can be found to reconstruct and extrapolate the hidden dynamics. Recent developments on top of NeuralODE include NeuralSDEÂ  , Neural Jump SDEÂ , NeuralSPDEÂ 0, Neural OperatorsÂ 1, infinitely deep Bayesian neural networksÂ 2, etc. Generally speaking, learning an unknown dynamics, which is an â€˜inverseâ€™ problem, is more challenging than solving or simulating a known dynamics. This is particularly so for stochastic systems. The ever-growing abundance of data necessitates methods that can learn stochastic dynamics in a wide range of scientific disciplines such as, for example, molecular dynamicsÂ 3 4 5, computational chemistryÂ 6, and ecologyÂ 7. Aside from the aforementioned work based on ML methodologies, recent progress in this direction includes the work of Lu et al. 8 9 0 on the learning of interaction kernel of multi-particle systems with non-parametric methods, the kernel-based method 1 2 for learning discrete non-Markovian time series, and various approximations of the Mori-Zwanzig equation for the learning of non-Markovian stochastic dynamics 3 4 3 4 5 6 in molecular dynamics. Typically, these non-ML methods use sophisticated series expansion and regression techniques to learn and construct the desired stochastic model.\nAlthough being theoretically complete and numerically successful within their own applicability, such modeling processes are often complex and too difficult to be extended to high-dimensional or highly heterogeneous systems. The success of NN models in dealing with high-dimensional problems for complex systems inspired us to use it to build a simple data-driven, extensible framework for modeling stochastic dynamics, i.e. the statistics-informed neural network (SINN). We now detail SINNâ€™s construction and the main rationale behind it, as well as its major differences from the existing ML frameworks. To learn stochastic dynamics with NNs, we first consider how to use deterministic architectures such as the recurrent neural network (RNN) to generate randomness. Being built on top of a deterministic RNN architecture, SINN does not generate randomness per se, but rather transforms an input stream of discrete independent and identically distributed (i.i.d.) random numbers to produce realizations of stochastic trajectories. The modeling capacity of this simple construction can be examined using the universal approximation theorem (UAT) for RNNs. Specifically, by mimicking the proof of the UAT for a one-layer RNN for arbitrary deterministic, open dynamical systems 7, we obtain a similar result stating that a one-layer RNN with Gaussian white-noise input can universally approximate arbitrary stochastic systems. We then use the long short-term memory (LSTM)Â 8 architecture as the building blocks for SINN in order to capture non-Markovian and memory effects that the underlying stochastic system may contain. Contrary to PINNs and the NeuralSDE, SINN is trained on the statistics, such as probability density function and time autocorrelation functions, of an ensemble of trajectories. Briefly speaking, SINN is mainly different from other ML frameworks in the following three aspects: (I) SINN is entirely equation-free â€” training and modeling do not rely on knowledge about the underlying stochastic differential equation; (II) an RNN, instead of fully connected or convolutional layers, is used as the primary architecture of the model; (III) instead of seeking a pathwise approximation to the stochastic dynamics, SINN constructs simulated trajectories that converge to the example trajectories in the sense of probability measure and nğ‘›n-th moment. The computational and modeling merit brought by these three features will be elaborated later with numerical supports. Â Â Â Â Â Â Â Â Â Â \nEquation-based modelingÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \nEquation-free modeling In a greater picture, SINN can be categorized as an equation-free ML architecture for discovering the hidden dynamics of a physical system at different spatial-temporal scales. From a modeling perspective, the classical equation-based modeling approach, as shown in FigureÂ 1, normally starts with a microscopic model for the molecular/quantum dynamics. A certain coarse-grained procedure, such as the projection operator method or the mean-field approximation, is then applied to reduce the dimensionality of the system in order to generate mesoscopic and macroscopic models. At different scales, the established physical models have a general form: where uğ‘¢u is the unknown (vector) variable or a (vector) field uâ€‹(x,t)ğ‘¢ğ‘¥ğ‘¡u(x,t), Fâ€‹(u,k,t)ğ¹ğ‘¢ğ‘˜ğ‘¡F(u,k,t) is the combination of functions, stochastic processes, and operators with modeling parameters kğ‘˜k. In contrast to the equation-based approach, ML approaches rely on equation-free models based on the convolutional neural network (CNN)Â 9 0, RNNÂ 1, or graph neural network (GNN)Â 2 to model a physical process. These NNs need to be trained on data, such as the sample trajectories of an observable in the phase space. The general form of the modeling ansatz for the unknown function uâ€‹(x,t)ğ‘¢ğ‘¥ğ‘¡u(x,t) relies on the multi-fold function composition: where fisubscriptğ‘“ğ‘–f_{i} is the activation function of iğ‘–i-th NN layer and kisubscriptğ‘˜ğ‘–k_{i} represents the corresponding weights and parameters. ML models such as PINNs use the underlying equations to define the loss function while the modeling anstaz is of the form (2). Other examples such as the NeuralODE/SDE/SPDE use (2) to model the derivatives function, i.e. the right-hand side of equation (1). In comparison, SINN is completely equation-free during the training and the modeling process. Specifically for this paper, numerical experiments will be provided to demonstrate the capability of SINN in approximating Gaussian and non-Gaussian stochastic dynamics as well as its ability to capture the memory effect for non-Markovian systems. We also use SINN to discover surrogate models for transition dynamics that often appears in computational chemistry. Our SINN model can be trained using temporally coarse-grained trajectories. This feature makes it an efficient simulator for rare events. In addition to the application in physics, SINN provides a simple and flexible framework to model arbitrary stationary stochastic processes, hence is generally applicable in the areas of uncertainty quantification and time series modeling. Several simple test examples presented in SectionÂ 4.2 promisingly show its numerical advantages over established stochastic process modeling tools such as the transformed Karhunen-LoÃ©ve or polynomial chaos expansion 3 4 3 4. This paper is organized as follows. In SectionÂ 2, we review the established universal approximation theorem for a single-layer RNN model and show that there is a natural extension of this theorem for stochastic systems driven by Gaussian and non-Gaussian white noise. Inspired by this theoretical result, in SectionÂ 3, we propose a statistics-informed neural network (SINN) and introduce different types of loss functions. The training method of SINN is provided in SectionÂ 4.1. Three simple test examples are presented in SectionÂ 4.2 to demonstrate that SINN can well approximate both Gaussian and non-Gaussian stochastic dynamics. In SectionÂ 5, we apply SINN to a coarse-graining problem and also use it as an effective rare-event simulator to evaluate transition rates. Several assessments of SINN as a tool for learning stochastic dynamics are also provided. Lastly, the main findings of this paper are summarized in SectionÂ 6. The proofs for the main theorems are given in AppendixÂ A.\n\nReference markers found: [4]\n\nContext windows for each [ref]:\n[ref]4: that conform to the target differential equation. More recent members within the PINN family include sparse physics-informed neural network (SPINN)Â , parareal physics-informed neural network (PPINN)Â [ref]4, and so onÂ [ref]5. The â€˜learningâ€™ strategy, on the other hand, aims to learn the hidden dynamics from data using an NN.\nThis strategy is adopted by the neural ordinary differential equation\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 4,\n            \"titles\": [\n                \"Learning in Modal Space: Solving Time-Dependent Stochastic PDEs Using Physics-Informed Neural Networks\",\n                \"Learning and meta-learning of stochastic advectionâ€“diffusionâ€“reaction systems from sparse measurements\",\n                \"Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems\",\n                \"SPINN: Sparse, physics-based, and partially interpretable neural networks for PDEs\",\n                \"PPINN: Parareal physics-informed neural network for time-dependent PDEs\",\n                \"Neural Ordinary Differential Equations\",\n                \"Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise\",\n                \"Scalable gradients and variational inference for stochastic differential equations\",\n                \"Neural Jump Stochastic Differential Equations\",\n                \"Neural stochastic partial differential equations\",\n                \"Neural operator: Learning maps between function spaces\",\n                \"Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations\",\n                \"Data-driven parameterization of the generalized Langevin equation\",\n                \"Effective Mori-Zwanzig equation for the reduced-order modeling of stochastic systems\",\n                \"The Mori-Zwanzig formalism for the derivation of a fluctuating heat conduction model from molecular dynamics\",\n                \"Statistical mechanics: theory and molecular simulation\",\n                \"Inferring the structure and dynamics of interactions in schooling fish\",\n                \"Learning interaction kernels in heterogeneous systems of agents from multiple trajectories\",\n                \"Learning interaction kernels in mean-field equations of 1st-order systems of interacting particles\",\n                \"Learning interaction kernels in stochastic systems of interacting particles from multiple trajectories\",\n                \"Kernel-based prediction of non-Markovian time series\",\n                \"Machine Learning for Prediction with Missing Dynamics\",\n                \"Generalized Langevin equations for systems with local interactions\",\n                \"Incorporation of memory effects in coarse-grained modeling via the Mori-Zwanzig formalism\",\n                \"Coarse-graining of overdamped langevin dynamics via the Moriâ€“Zwanzig formalism\",\n                \"Recurrent neural networks are universal approximators\",\n                \"Long short-term memory\",\n                \"Understanding of a convolutional neural network\",\n                \"Cross-time and orientation-invariant overhead image geolocalization using deep local features\",\n                \"Wave physics as an analog recurrent neural network\",\n                \"The graph neural network model\",\n                \"Simulation of strongly non-Gaussian processes using Karhunenâ€“Loeve expansion\",\n                \"Polynomial chaos decomposition for the simulation of non-Gaussian nonstationary stochastic processes\",\n                \"A note on eulerâ€™s approximations\",\n                \"The pathwise convergence of approximation schemes for stochastic differential equations\",\n                \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\n                \"JAX: composable transformations of Python+NumPy programs\",\n                \"Density estimation for statistics and data analysis\",\n                \"Fokker-planck equation\",\n                \"Hypoellipticity and the Moriâ€“Zwanzig formulation of stochastic differential equations\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]4 based on the context and research area of Learning Stochastic Dynamics with Statistics-Informed Neural Network. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}