{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Sketching Merge Trees for Scientific Data Visualization\n\nPaper Abstract: \n\nPaper Section Text:\nWe begin by reviewing the notion of a merge tree that arises from a scalar field.\nWe then introduce the technical background needed to map a merge tree to a column vector in the data matrix.\nOur framework utilizes the probabilistic matching from the Gromov-Wasserstein (GW) framework of Chowdhury and NeedhamÂ , with a few ingredients from Peyre et al.Â . Merge trees.Â Â Let f:ğ•„â†’â„:ğ‘“â†’ğ•„â„f:{\\mathbb{M}}\\to{\\mathbb{R}} be a scalar field defined on the domain of interest ğ•„ğ•„{\\mathbb{M}}, where ğ•„ğ•„{\\mathbb{M}} can be a manifold or a subset of â„dsuperscriptâ„ğ‘‘{\\mathbb{R}}^{d}.\nFor our experiments inÂ sectionÂ 5, ğ•„âŠ‚â„2ğ•„superscriptâ„2{\\mathbb{M}}\\subset{\\mathbb{R}}^{2}.\nMerge trees capture the connectivity among the sublevel sets of fğ‘“f, i.e., ğ•„a=fâˆ’1â€‹(âˆ’âˆ,a]subscriptğ•„ğ‘superscriptğ‘“1ğ‘{\\mathbb{M}}_{a}=f^{-1}(-\\infty,a].\nFormally, two points x,yâˆˆğ•„ğ‘¥ğ‘¦ğ•„x,y\\in{\\mathbb{M}} are equivalent, denoted by xâˆ¼ysimilar-toğ‘¥ğ‘¦x\\sim y, if fâ€‹(x)=fâ€‹(y)=ağ‘“ğ‘¥ğ‘“ğ‘¦ğ‘f(x)=f(y)=a, and xğ‘¥x and yğ‘¦y belong to the same connected component of a sublevel set ğ•„asubscriptğ•„ğ‘{\\mathbb{M}}_{a}.\nThe merge tree, T(ğ•„,f)=ğ•„/âˆ¼T({\\mathbb{M}},f)={\\mathbb{M}}/{\\sim}, is the quotient space obtained by gluing together points in ğ•„ğ•„{\\mathbb{M}} that are equivalent under the relation âˆ¼similar-to\\sim.\nTo describe a merge tree procedurally, as we sweep the function value ağ‘a from âˆ’âˆ-\\infty to âˆ\\infty, we create a new branch originating at a leaf node for each local minimum of fğ‘“f.\nAs ağ‘a increases, such a branch is extended as its corresponding component in ğ•„asubscriptğ•„ğ‘{\\mathbb{M}}_{a} grows until it merges with another branch at a saddle point.\nIf ğ•„ğ•„{\\mathbb{M}} is connected, all branches eventually merge into a single component at the global maximum of fğ‘“f, which corresponds to the root of the tree.\nFor a given merge tree, leaves, internal nodes, and root node represent the minima, merging saddles, and global maximum of fğ‘“f, respectively.\nFigureÂ 2 displays a scalar field with its corresponding merge tree embedded in the graph of the scalar field.\nAbstractly, a merge tree Tğ‘‡T is a rooted tree equipped with a scalar function defined on its node set, f:Vâ†’â„:ğ‘“â†’ğ‘‰â„f:V\\to{\\mathbb{R}}. Gromov-Wasserstein distance for measure networks.Â  The GW distance was proposed by MemoliÂ   for metric measure spaces.\nPeyre et al.Â  introduced the notion of a measure network and defined the GW distance between such networks.\nThe key idea is to find a probabilistic matching between a pair of networks by searching over the convex set of couplings of the probability measures defined on the networks. A finite, weighted graph GğºG can be represented as a measure network using a triple (V,W,p)ğ‘‰ğ‘Šğ‘(V,W,p), where Vğ‘‰V is the set of nğ‘›n nodes, Wğ‘ŠW is a weighted adjacency matrix, and pğ‘p is a probability measure supported on the nodes of GğºG.\nFor our experiments, pğ‘p is taken to be uniform, that is, p=1nâ€‹ğŸnğ‘1ğ‘›subscript1ğ‘›p=\\frac{1}{n}{\\mathbf{1}_{n}}, where ğŸn=(1,1,â€¦,1)Tâˆˆâ„nsubscript1ğ‘›superscript11â€¦1ğ‘‡superscriptâ„ğ‘›{\\mathbf{1}_{n}}=(1,1,\\dots,1)^{T}\\in{\\mathbb{R}}^{n}. Let G1â€‹(V1,W1,p1)subscriptğº1subscriptğ‘‰1subscriptğ‘Š1subscriptğ‘1G_{1}(V_{1},W_{1},p_{1}) and G2â€‹(V2,W2,p2)subscriptğº2subscriptğ‘‰2subscriptğ‘Š2subscriptğ‘2G_{2}(V_{2},W_{2},p_{2}) be a pair of graphs with n1subscriptğ‘›1n_{1} and n2subscriptğ‘›2n_{2} nodes, respectively.\nLet [n]delimited-[]ğ‘›[n] denote the set {1,2,â€¦,n}12â€¦ğ‘›\\{1,2,\\dots,n\\}.\nV1={xi}iâˆˆ[n1]subscriptğ‘‰1subscriptsubscriptğ‘¥ğ‘–ğ‘–delimited-[]subscriptğ‘›1V_{1}=\\{x_{i}\\}_{i\\in[n_{1}]} and V2={yj}jâˆˆ[n2]subscriptğ‘‰2subscriptsubscriptğ‘¦ğ‘—ğ‘—delimited-[]subscriptğ‘›2V_{2}=\\{y_{j}\\}_{j\\in[n_{2}]}.\nA coupling between probability measures p1subscriptğ‘1p_{1} and p2subscriptğ‘2p_{2} is a joint probability measure on V1Ã—V2subscriptğ‘‰1subscriptğ‘‰2V_{1}\\times V_{2} whose marginals agree with p1subscriptğ‘1p_{1} and p2subscriptğ‘2p_{2}.\nThat is, a coupling is represented as an n1Ã—n2subscriptğ‘›1subscriptğ‘›2n_{1}\\times n_{2} non-negative matrix Cğ¶C such that Câ€‹ğŸn2=p1ğ¶subscript1subscriptğ‘›2subscriptğ‘1C{\\mathbf{1}_{n_{2}}}=p_{1} and CTâ€‹ğŸn1=p2superscriptğ¶ğ‘‡subscript1subscriptğ‘›1subscriptğ‘2C^{T}{\\mathbf{1}_{n_{1}}}=p_{2}.\nGiven matrix Cğ¶C, its binarization is an n1Ã—n2subscriptğ‘›1subscriptğ‘›2n_{1}\\times n_{2} binary matrix, denoted as ğŸC>0subscript1ğ¶0{\\mathbf{1}_{C>0}}: this matrix has 111 where C>0ğ¶0C>0, and 00 elsewhere. The distortion of a coupling Cğ¶C with an arbitrary loss function Lğ¿L is defined asÂ  Let ğ’=ğ’â€‹(p1,p2)ğ’ğ’subscriptğ‘1subscriptğ‘2{\\mathcal{C}}={\\mathcal{C}}(p_{1},p_{2}) denote the collection of all couplings between p1subscriptğ‘1p_{1} and p2subscriptğ‘2p_{2}.\nThe Gromov-Wasserstein discrepancyÂ  is defined as In this paper, we consider the quadratic loss function Lâ€‹(a,b)=12â€‹|aâˆ’b|2ğ¿ğ‘ğ‘12superscriptğ‘ğ‘2L(a,b)=\\frac{1}{2}|a-b|^{2}.\nThe Gromov-Wasserstein distanceÂ    dGâ€‹Wsubscriptğ‘‘ğºğ‘Šd_{GW} between G1subscriptğº1G_{1} and G2subscriptğº2G_{2} is defined as It follows from the work of SturmÂ [ref]55 that such minimizers always exist and are referred to as optimal couplings. Alignment and blowup.Â Â Given a pair of graphs G1=(V1,W1,p1)subscriptğº1subscriptğ‘‰1subscriptğ‘Š1subscriptğ‘1G_{1}=(V_{1},W_{1},p_{1}) and G2=(V2,W2,p2)subscriptğº2subscriptğ‘‰2subscriptğ‘Š2subscriptğ‘2G_{2}=(V_{2},W_{2},p_{2}) with n1subscriptğ‘›1n_{1} and n2subscriptğ‘›2n_{2} nodes respectively, a coupling Câˆˆğ’â€‹(p1,p2)ğ¶ğ’subscriptğ‘1subscriptğ‘2C\\in{\\mathcal{C}}(p_{1},p_{2}) can be used to align their nodes.\nIn order to do this, we will need to increase the size of G1subscriptğº1G_{1} and G2subscriptğº2G_{2} appropriately into their respective blowup graphs G1â€²subscriptsuperscriptğºâ€²1G^{\\prime}_{1} and G2â€²subscriptsuperscriptğºâ€²2G^{\\prime}_{2}, such that G1â€²subscriptsuperscriptğºâ€²1G^{\\prime}_{1} and G2â€²subscriptsuperscriptğºâ€²2G^{\\prime}_{2} contain the same nğ‘›n number of nodes (where n1,n2â‰¤nsubscriptğ‘›1subscriptğ‘›2ğ‘›n_{1},n_{2}\\leq n).\nRoughly speaking, let xğ‘¥x be a node in G1subscriptğº1G_{1}, and let nxsubscriptğ‘›ğ‘¥n_{x} be the number of nodes in G2subscriptğº2G_{2} that have a nonzero coupling probability with xğ‘¥x.\nThe blowup graph G1â€²=(V1â€²,W1â€²,p1â€²)subscriptsuperscriptğºâ€²1subscriptsuperscriptğ‘‰â€²1subscriptsuperscriptğ‘Šâ€²1subscriptsuperscriptğ‘â€²1G^{\\prime}_{1}=(V^{\\prime}_{1},W^{\\prime}_{1},p^{\\prime}_{1}) is created by making nxsubscriptğ‘›ğ‘¥n_{x} copies of node xğ‘¥x for each node in G1subscriptğº1G_{1}, generating a new node set V1â€²subscriptsuperscriptğ‘‰â€²1V^{\\prime}_{1}. The probability distribution p1â€²subscriptsuperscriptğ‘â€²1p^{\\prime}_{1} and the weight matrix W1â€²subscriptsuperscriptğ‘Šâ€²1W^{\\prime}_{1} are updated from p1subscriptğ‘1p_{1} and W1subscriptğ‘Š1W_{1} accordingly.\nSimilarly, we can construct the blowup G2â€²=(V2â€²,W2â€²,p2â€²)subscriptsuperscriptğºâ€²2subscriptsuperscriptğ‘‰â€²2subscriptsuperscriptğ‘Šâ€²2subscriptsuperscriptğ‘â€²2G^{\\prime}_{2}=(V^{\\prime}_{2},W^{\\prime}_{2},p^{\\prime}_{2}) of G2subscriptğº2G_{2}. An optimal coupling Cğ¶C expands naturally to a coupling Câ€²superscriptğ¶â€²C^{\\prime} between p1â€²subscriptsuperscriptğ‘â€²1p^{\\prime}_{1} and p2â€²subscriptsuperscriptğ‘â€²2p^{\\prime}_{2}.\nAfter taking appropriate blowups, Câ€²superscriptğ¶â€²C^{\\prime} can be binarized to be an nÃ—nğ‘›ğ‘›n\\times n permutation matrix, and used to align the nodes of the two blown-up graphs.\nThe GW distance is given by a formulation equivalent toÂ EquationÂ 3 based on an optimal coupling, FrÃ©chet mean.Â Â Given a collection of graphs ğ’¢={G1,G2,â€¦,GN}ğ’¢subscriptğº1subscriptğº2â€¦subscriptğºğ‘{\\mathcal{G}}=\\{G_{1},G_{2},\\dots,G_{N}\\}, a FrÃ©chet meanÂ  GÂ¯Â¯ğº\\overline{G} of ğ’¢ğ’¢{\\mathcal{G}} is a minimizer of the functional Fâ€‹(H,ğ’¢)=1Nâ€‹âˆ‘i=1NdGâ€‹Wâ€‹(Gi,H)ğ¹ğ»ğ’¢1ğ‘superscriptsubscriptğ‘–1ğ‘subscriptğ‘‘ğºğ‘Šsubscriptğºğ‘–ğ»F(H,{\\mathcal{G}})=\\frac{1}{N}\\sum_{i=1}^{N}d_{GW}(G_{i},H) over the space ğ’©ğ’©{\\mathcal{N}} of measure networks, Chowdhury and NeedhamÂ  defined the directional derivative and the gradient of the functional Fâ€‹(H,ğ’¢)ğ¹ğ»ğ’¢F(H,{\\mathcal{G}}) at Hğ»H and provided a gradient descent algorithm to compute the FrÃ©chet mean.\nTheir iterative optimization begins with an initial guess H0subscriptğ»0H_{0} of the FrÃ©chet mean.\nAt the ktâ€‹hsuperscriptğ‘˜ğ‘¡â„k^{th} iteration, there is a two-step process: each Gisubscriptğºğ‘–G_{i} is first blown-up and aligned to the current FrÃ©chet mean, Hksubscriptğ»ğ‘˜H_{k}; then Hksubscriptğ»ğ‘˜H_{k} is updated using the gradient of the functional Fâ€‹(Hk,ğ’¢)ğ¹subscriptğ»ğ‘˜ğ’¢F(H_{k},{\\mathcal{G}}) at Hksubscriptğ»ğ‘˜H_{k}.\nSuch a two-step process is repeated until convergence where the gradient vanishes.\nFor the complete algorithmic and implementational details, seeÂ .\nIf GÂ¯=(VÂ¯,WÂ¯,pÂ¯)Â¯ğºÂ¯ğ‘‰Â¯ğ‘ŠÂ¯ğ‘\\overline{G}=(\\overline{V},\\overline{W},\\overline{p}) is the FrÃ©chet mean, then we have where Wkâ€²subscriptsuperscriptğ‘Šâ€²ğ‘˜W^{\\prime}_{k} is the weight matrix obtained by blowing-up and aligning Gkâˆˆğ’¢subscriptğºğ‘˜ğ’¢G_{k}\\in{\\mathcal{G}} to GÂ¯Â¯ğº\\overline{G}.\nThat is, when all the graphs in ğ’¢ğ’¢{\\mathcal{G}} are blown-up and aligned to GÂ¯Â¯ğº\\overline{G}, the weight matrix of GÂ¯Â¯ğº\\overline{G} is given by a simple element-wise average of the weight matrices of the graphs. A simple example.Â Â We give a simple example involving a pair of merge trees inÂ FigureÂ 3.\nT1subscriptğ‘‡1T_{1} and T2subscriptğ‘‡2T_{2} contain 8 and 6 nodes, respectively (nodes are labeled starting with a 00 index).\nThe optimal coupling Cğ¶C obtained by the gradient descent algorithm is visualized in FigureÂ 3(A).\nCğ¶C is an 8Ã—6868\\times 6 matrix, and it shows that node 0 in T1subscriptğ‘‡1T_{1} is matched to node 5 in T2subscriptğ‘‡2T_{2} with the highest probability (0.120.120.12, red stars).\nNode 4 in T1subscriptğ‘‡1T_{1} is coupled with both node 0 (with a probability 0.080.080.08) and node 1 (with a probability 0.040.040.04) in T2subscriptğ‘‡2T_{2} (green stars). Now, we compute the FrÃ©chet mean TÂ¯Â¯ğ‘‡\\overline{T} of T1subscriptğ‘‡1T_{1} and T2subscriptğ‘‡2T_{2}, which has 12 nodes.\nWe align both T1subscriptğ‘‡1T_{1} and T2subscriptğ‘‡2T_{2} to TÂ¯Â¯ğ‘‡\\overline{T} via their blowup graphs.\nThis gives rise to a coupling matrix between TÂ¯Â¯ğ‘‡\\overline{T} and T1subscriptğ‘‡1T_{1} (of size 12Ã—812812\\times 8) in FigureÂ 3(B), and a coupling matrix between TÂ¯Â¯ğ‘‡\\overline{T} and T2subscriptğ‘‡2T_{2} (of size 12Ã—612612\\times 6) in FigureÂ 3(C), respectively.\nAs shown inÂ FigureÂ 3, root node 10 of TÂ¯Â¯ğ‘‡\\overline{T} is matched with root node 0 of T1subscriptğ‘‡1T_{1} and root node 5 of T2subscriptğ‘‡2T_{2} (red stars).\nNode 0 of TÂ¯Â¯ğ‘‡\\overline{T} is matched probabilistically with node 4 in T1subscriptğ‘‡1T_{1} and nodes 00 and 111 in T2subscriptğ‘‡2T_{2} (green stars).\nNow both trees T1subscriptğ‘‡1T_{1} and T2subscriptğ‘‡2T_{2} are blown-up to be T1â€²subscriptsuperscriptğ‘‡â€²1T^{\\prime}_{1} and T2â€²subscriptsuperscriptğ‘‡â€²2T^{\\prime}_{2}, each with 12 nodes, and can be vectorized into column vectors of the same size.\n\nReference markers found: [55]\n\nContext windows for each [ref]:\n[ref]55: Gromov-Wasserstein distanceÂ    dGâ€‹Wsubscriptğ‘‘ğºğ‘Šd_{GW} between G1subscriptğº1G_{1} and G2subscriptğº2G_{2} is defined as It follows from the work of SturmÂ [ref]55 that such minimizers always exist and are referred to as optimal couplings. Alignment and blowup.Â Â Given a pair of graphs\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 55,\n            \"titles\": [\n                \"Optimal Transport for structured data with application on graphs\",\n                \"Embedding metrics into ultrametrics and graphs into spanning trees with constant average distortion\",\n                \"Nearly tight low stretch spanning trees\",\n                \"Using petal-decompositions to build a low stretch spanning tree\",\n                \"Barycenters in the Wasserstein space\",\n                \"A graph-theoretic game and its application to the k-server problem\",\n                \"Gromov-Wasserstein Alignment of Word Embedding Spaces\",\n                \"Vector field topology of time-dependent flows in a steady reference frame\",\n                \"Measuring the distance between merge trees\",\n                \"Iterative Bregman projections for regularized transportation problems\",\n                \"Residual based sampling for online low rank approximation\",\n                \"SVD based initialization: A head start for nonnegative matrix factorization\",\n                \"An improved approximation algorithm for the column subset selection problem\",\n                \"Efficient computation of isometry-invariant distances between surfaces\",\n                \"Learning Generative Models across Incomparable Spaces\",\n                \"Computing contour trees in all dimensions\",\n                \"Moduli spaces of Morse functions for persistence\",\n                \"Gromov-Wasserstein Averaging in a Riemannian Framework\",\n                \"Fast local algorithms for large scale nonnegative matrix and tensor factorizations\",\n                \"Fast computation of Wasserstein barycenters\",\n                \"Adaptive sampling and fast low-rank matrix approximation\",\n                \"Fast Monte Carlo algorithms for matrices ii: Computing a low-rank approximation to a matrix\",\n                \"Fast approximation of matrix coherence and statistical leverage\",\n                \"Non-Euclidean statistics for covariance matrices, with applications to diffusion tensor imaging\",\n                \"Lower-stretch spanning trees\",\n                \"Fifty years of graph matching, network alignment and network comparison\",\n                \"GWCNN: A metric alignment layer for deep shape analysis\",\n                \"Algorithms for nonnegative matrix factorization with the Î²-divergence\",\n                \"Intrinsic Interleaving Distance for Merge Trees\",\n                \"Frequent Directions : Simple and Deterministic Matrix Sketching\",\n                \"Metric Structures for Riemannian and Non-Riemannian Spaces, volume 152 of Progress in mathematics\",\n                \"Data-driven network alignment\",\n                \"Generic objective vortices for flow visualization\",\n                \"A survey of topology-based methods in visualization\",\n                \"Using Gromov-Wasserstein distance to explore sets of networks\",\n                \"Data assimilation in oceanography: Current status and new directions\",\n                \"Extensions of Lipschitz mappings into a Hilbert space\",\n                \"A nearly-m log n time solver for SDD linear systems\",\n                \"Simple and deterministic matrix sketching\",\n                \"Visualizing high-dimensional data: Advances in the past decade\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]55 based on the context and research area of Sketching Merge Trees for Scientific Data Visualization. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}