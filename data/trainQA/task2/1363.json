{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Applying Deutsch’s concept of good explanations to artificial intelligence and neuroscience - an initial exploration\n\nPaper Abstract: \n\nPaper Section Text:\nIn our view, the way the term “generalization” is used across both statistics and machine learning can often be misleading. In both fields,“generalization ability” is defined as the gap between test set error and training set error, where the training set and test set are drawn from the same distribution.  What we are interested in here is the ability to generalize across distributions, not within them, so for sake of clarity we will refer to this as “extrapolation”. Deep learning systems are notoriously bad at extrapolation, often failing spectacularly when small distributional shifts occur. To give a few high profile examples, a deep learning system for diagnosing retinopathy developed by Google’s Verily Life Sciences which reached human-level diagnostic ability in the lab performed very poorly in field trials in Thailand due to poor lighting conditions, lower resolution images, and images which had been stitched together. A trained diagnostician would have been able to adapt to these conditions, but deep learning could not. Another high profile example occurred in 2015 when a deep learning system embedded in Google’s Photos service was found to be tagging African American people as “gorillas”. The quick-fix solution was to remove the category of gorillas. Three years later, WIRED magazine reported that the category was still missing, suggesting that the problem was not easy to solve. In 2017 a much-lauded DeepMind deep reinforcement learning system[ref]9 which achieved super-human performance on several Atari games was shown to fail if minor changes are made, such as moving the paddle 3% higher in the game of Breakout. This shows that the system has no grasp of basic physics and is built on top of a highly brittle fitting procedure. Another example is DeepMind’s AlphaStar system for playing the computer game StarCraft. The system plays at the above-human level for a given character and map but cannot generalize to other characters and maps (it must be retrained separately for each one). The range of input space within which a model, theory, or explanation makes accurate predictions has different names in different disciplines. The physicist David Deutsch calls it the “reach”, Popper calls it “range”, philosophers sometimes call it the “explanatory power”, and in some sub-fields of machine learning it is called the “applicability domain”. As discussed in detail by Hasson et al.,[ref]13 and by this author in a prior work,[ref]14 the double descent phenomena indicates that machine learning models operate largely by local interpolation over a dense set of training data rather than by extracting global trends or general principles. In light of these findings, the failures noted above are not so surprising. There are couple of approaches that may help with these failure modes. In a prior work we advocated adding some self-awareness to such systems so they know the bounds of their expertise and can provide a warning to users when they are asked to perform outside their domain of applicability.[ref]14 Another possible avenue is to develop unsupervised learning algorithms which can adapt on-the-fly as the distribution of their input data changes without forgetting what they’ve already learned. The influential deep learning scientist Yann Lecun has touted this approach, calling unsupervised learning the key missing ingredient of AI in his famous “layer-cake” analogy for AI.111https://medium.com/syncedreview/yann-lecun-cake-analogy-2-0-a361da560dae However, a constant stream of new data is still required for unsupervised learning to work. What interests us here is the ability to generalize beyond training data distributions without having to take in any new data first. A related line of work is “zero-shot” learning and more recently, “less than one shot” learning, which might be thought of as a type of dynamic clustering where spaces between clusters are left as a sort of terra incognita where additional clusters may appear. This is an important line of work, but so far such systems have only been demonstrated on carefully curated toy problems and even on such problems perform poorly, with average accuracies typically less than 50% . What is missing from the aforementioned systems? We believe we can gain insight into this question by turning our attention away from the types of statistical modeling done in mathematics and computer science departments to the type of modeling done in science departments, which is based on theories developed using the scientific method. Physicists in particular have an impressive track record of being able to generate models that predict phenomena their creators had never seen or imagined before. For instance, Isaac Newton developed his laws of motion which have been applied successfully to make predictions in many domains Newton had no direct experience in. In 1915 Einstein predicted that given the initial conditions of a light ray traveling close to the sun, the light ray would follow a curved trajectory, a finding that was confirmed experimentally a five years later during a solar eclipse. Many radical and surprising phenomena have been predicted by physicists in advance, before any empirical confirmation - black holes, grRavitational waves, antimatter, the Higgs boson, quantum computation, and metamaterials. With regards to invention and innovation, much can be done with trial and error in the lab, but explanatory theories can be applied to hypothetical configurations of matter which have never been created before and suggest which configurations would be the most useful for some end. Edison developed the light bulb through trial and error, but it is doubtful the first nuclear weapon or quantum computer could have been developed through trial and error alone - deep explanatory theories were needed to guide such advances. To avoid trial-and-error in the lab, many technological artifacts such as novel drugs or spacecraft are first developed in-silico using physics-based simulation. Machine learning (ML) based prediction is much more computationally expedient than physics-based simulation so much work has been done to see if ML can replace physics simulation. However, ML stops performing outside of domain of data it was trained on, and thus fails precisely in the domains where simulations provide the most value (ie going beyond the domain of experimental data collected so far). To give another example, in the course of our prior work on trying to predict the properties of molecules,  a system trained on explosive and propellant molecules predicted that a sugar molecule would have a high explosive energy. Similarly, deep generative models based on deep learning fail to yield truly novel designs.\nTo give an example, a deep reinforcement learning system for constructing molecules developed by a team from Insilico Medicine and Harvard received much media fanfare after one of the molecules it generated was synthesized and shown to be active against cancer in a mouse model. However, Walters & Murcko quickly pointed out that the molecule it generated is very similar to two existing drug molecules which were in the system’s training database. In general deep generative models trained on molecules generate highly nonphysical nonsense molecules when pushed to generate molecules outside their training set distribution. It is not our intention to wade into the philosophical debate about what denotes an “explanation” versus a “pure prediction”, although we think there is an important distinction there to be made. In what follows we treat scientific explanations and predictive models as elements of the same class. This may seem odd, but both are scientific explanations and models can be operationalized as functions which take in inputs (observations/data) and output predictions. Our latest physical theories can make accurate predictions for an enormous range of phenomena and are only known to break down in exotic situations where quantum gravity is important. The question we are interested in is why models derived via the scientific method can extrapolate while models derived from the methods of deep learning and statistics cannot. More generally we are interested in if there are any general characteristics of models/theories/explanations which correlate with their reach (how far they can extrapolate). According to David Deutsch, one such characteristic is that models/theories/explanations with large reach are “hard-to-vary” (HTV) while those with small reach are “easy-to-vary”. If true this is a profound insight, but to our knowledge it has not yet been formalized or applied to artificial intelligence.\n\nReference markers found: [9]\n\nContext windows for each [ref]:\n[ref]9: Three years later, WIRED magazine reported that the category was still missing, suggesting that the problem was not easy to solve. In 2017 a much-lauded DeepMind deep reinforcement learning system[ref]9 which achieved super-human performance on several Atari games was shown to fail if minor changes are made, such as moving the paddle 3% higher in the game of Breakout. This shows that the\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 9,\n            \"titles\": [\n                \"Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics\",\n                \"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification\",\n                \"A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis\",\n                \"Mastering the game of Go with deep neural networks and tree search\",\n                \"The Beginning of Infinity: Explanations That Transform the World\",\n                \"The Elements of Statistical Learning\",\n                \"Fantastic generalization measures and where to find them\",\n                \"A human-centered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy\",\n                \"When it comes to gorillas, google photos remains blind\",\n                \"Human-level control through deep reinforcement learning\",\n                \"Deepmind’s losses and the future of artificial intelligence\",\n                \"The Logic of Scientific Discovery\",\n                \"Direct fit to nature: An evolutionary perspective on biological and artificial neural networks\",\n                \"Self-explaining AI as an alternative to interpretable AI\",\n                \"‘Less Than One’-Shot Learning: Learning N Classes From M<N Samples\",\n                \"A determination of the deflection of light by the sun’s gravitational field, from observations made at the total eclipse of May 29, 1919\",\n                \"Applying machine learning techniques to predict the properties of energetic materials\",\n                \"Machine learning of energetic material properties\",\n                \"Deep learning enables rapid identification of potent DDR1 kinase inhibitors\",\n                \"Assessing the impact of generative AI on medicinal chemistry\",\n                \"Deep learning for molecular design—a review of the state of the art\",\n                \"Creative blocks\",\n                \"Conjectures and Refutations: The Growth of Scientific Knowledge\",\n                \"Pattern Recognition and Machine Learning\",\n                \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\",\n                \"A theory of universal artificial intelligence based on algorithmic complexity\",\n                \"Novum Organum\",\n                \"Objective Knowledge: An Evolutionary Approach\",\n                \"The Myth of the Framework: In Defence of Science and Rationality, In Defence of Science and Rationality\",\n                \"A new way to explain explanation\",\n                \"Reconciling modern machine-learning practice and the classical bias–variance trade-off\",\n                \"Deep Double Descent: Where Bigger Models and More Data Hurt\",\n                \"Summa totius logicae\",\n                \"Quaestiones et decisiones in quattuor libros Sententiarum Petri Lombardi: Centilogium theologicum\",\n                \"Modeling by shortest data description\",\n                \"Keeping the neural networks simple by minimizing the description length of the weights\",\n                \"A philosophical treatise of universal induction\",\n                \"Flat minima\",\n                \"Sharp Minima Can Generalize For Deep Nets\",\n                \"On the Measure of Intelligence\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]9 based on the context and research area of Applying Deutsch’s concept of good explanations to artificial intelligence and neuroscience - an initial exploration. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}