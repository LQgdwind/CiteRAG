{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Inexact bilevel stochastic gradient methods for constrained and unconstrained lower-level problems\n\nPaper Abstract: \n\nPaper Section Text:\nIn bilevel stochastic optimization, fusubscriptğ‘“ğ‘¢f_{u} and fâ„“subscriptğ‘“â„“f_{\\ell} can be interpreted as expected values, namely, fu=ğ”¼â€‹[fuâ€‹(x,y,Ï‘u)]subscriptğ‘“ğ‘¢ğ”¼delimited-[]subscriptğ‘“ğ‘¢ğ‘¥ğ‘¦superscriptitalic-Ï‘ğ‘¢f_{u}=\\mathbb{E}[f_{u}(x,y,\\vartheta^{u})] and fâ„“=ğ”¼â€‹[fâ„“â€‹(x,y,Ï‘â„“)]subscriptğ‘“â„“ğ”¼delimited-[]subscriptğ‘“â„“ğ‘¥ğ‘¦superscriptitalic-Ï‘â„“f_{\\ell}=\\mathbb{E}[f_{\\ell}(x,y,\\vartheta^{\\ell})], where Ï‘usuperscriptitalic-Ï‘ğ‘¢\\vartheta^{u} and Ï‘â„“superscriptitalic-Ï‘â„“\\vartheta^{\\ell} are random variables defined in a probability space (with probability measure independent from xğ‘¥x and yğ‘¦y) such that i.i.d. samples can be observed or generated.\nThe same applies to the functions possibly defining Yâ€‹(x)ğ‘Œğ‘¥Y(x).\n(To keep the notation simple, we are using the same fusubscriptğ‘“ğ‘¢f_{u} and fâ„“subscriptğ‘“â„“f_{\\ell} for deterministic and random variants.) Having in mindÂ ML applications, the methods that we are considering are Stochastic Approximation (SA) techniques, of the type of the stochastic gradient (SG) method   [ref]53 for single-objective optimization. In fact, the bilevel stochastic gradient (BSG) method can be seen as anÂ SG method applied toÂ (1.1), which leads to xk+1=xkâˆ’Î±kâ€‹gkBSGsubscriptğ‘¥ğ‘˜1subscriptğ‘¥ğ‘˜subscriptğ›¼ğ‘˜superscriptsubscriptğ‘”ğ‘˜BSGx_{k+1}=x_{k}-\\alpha_{k}g_{k}^{\\operatorname{BSG}}, where\nÎ±ksubscriptğ›¼ğ‘˜\\alpha_{k} is the step size or learning rate and gkBSGsuperscriptsubscriptğ‘”ğ‘˜BSGg_{k}^{\\operatorname{BSG}} is a stochastic gradient ofÂ fğ‘“f. Such a stochastic gradient is obtained by sampling the gradients and Hessians inÂ (1.2) at (xk,y~k)subscriptğ‘¥ğ‘˜subscript~ğ‘¦ğ‘˜(x_{k},\\tilde{y}_{k}), withÂ y~ksubscript~ğ‘¦ğ‘˜\\tilde{y}_{k} denoting an approximation to theÂ LL optimal solution. The stochastic gradientÂ gkBSGsuperscriptsubscriptğ‘”ğ‘˜BSGg_{k}^{\\operatorname{BSG}} is inexact when y~kâ‰ yâ€‹(xk)subscript~ğ‘¦ğ‘˜ğ‘¦subscriptğ‘¥ğ‘˜\\tilde{y}_{k}\\neq y(x_{k}), even in the full-batch (deterministic) case. In general,Â BSG methods have mainly been considered for theÂ LL unconstrained caseÂ (i.e., Yâ€‹(x)=â„mğ‘Œğ‘¥superscriptâ„ğ‘šY(x)=\\mathbb{R}^{m}) and are commonly classified according to the approach used to compute theÂ BSG directionÂ gkBSGsuperscriptsubscriptğ‘”ğ‘˜BSGg_{k}^{\\operatorname{BSG}}Â [ref]35. In particular, a first category, referred to as implicit differentiation, is composed of algorithms that compute theÂ BSG direction by applying the implicit function theorem and either solving the adjoint equationÂ  or using a truncated Neumann series to approximate the inverse HessianÂ âˆ‡yâ€‹y2fâ„“âˆ’1superscriptsubscriptâˆ‡ğ‘¦ğ‘¦2superscriptsubscriptğ‘“â„“1\\nabla_{yy}^{2}f_{\\ell}^{-1}Â . Note that using a truncated Neumann series requiresÂ â€–âˆ‡yâ€‹y2fâ„“â€–2<1subscriptnormsuperscriptsubscriptâˆ‡ğ‘¦ğ‘¦2subscriptğ‘“â„“21\\|\\nabla_{yy}^{2}f_{\\ell}\\|_{2}<1, which is a strong assumption. Therefore, a common approach is to first assumeÂ âˆ‡yfâ„“subscriptâˆ‡ğ‘¦subscriptğ‘“â„“\\nabla_{y}f_{\\ell} Lipschitz continuous inÂ yğ‘¦y with constantÂ C0subscriptğ¶0C_{0}, and then apply the truncated Neumann series to approximateÂ [(1/C0)â€‹âˆ‡yâ€‹y2fâ„“]âˆ’1superscriptdelimited-[]1subscriptğ¶0superscriptsubscriptâˆ‡ğ‘¦ğ‘¦2subscriptğ‘“â„“1[(1/C_{0})\\nabla_{yy}^{2}f_{\\ell}]^{-1}. However, this requires the knowledge ofÂ C0subscriptğ¶0C_{0}, which is typically unknown in practice. A second category, referred to as iterative differentiation, includes all the approaches based on automatic differentiation through dynamic systemsÂ   .\nA general convergence theory for the two classes of algorithms was proposed inÂ [ref]29, which also shows that computing theÂ BSG direction by using automatic differentiation can be less computationally efficient than using an implicit differentiation method. Therefore, our paper focuses on the first category, which has been promoted inÂ       (see alsoÂ [ref]35  for recent reviews).\nThese existing approaches either focus on a specific problem structure or require the LL problem to be solved to optimality at each iteration or rely on a truncated Neumann series for the inverse Hessian approximation in the adjoint gradient, which has the issues mentioned before. Among all the algorithms proposed in the papers cited above, we emphasize StocBiOÂ [ref]29. StocBiO employs a truncated Neumann series with automatic differentiation and a double-loop iterative scheme, which means that multiple iterations are required at theÂ LL problem in order for the algorithm to converge, similar to the algorithms developed in our paper. DARTSÂ  is an optimization technique related to the BSG method and has enjoyed great popularity in NAS.\nIt always considers an inexact solution to the LL problem, and it starts an iteration by applying one step of SG to the LL problem,\ny~k=ykâˆ’Î·â€‹âˆ‡yfâ„“â€‹(xk,yk)subscript~ğ‘¦ğ‘˜subscriptğ‘¦ğ‘˜ğœ‚subscriptâˆ‡ğ‘¦subscriptğ‘“â„“subscriptğ‘¥ğ‘˜subscriptğ‘¦ğ‘˜\\tilde{y}_{k}=y_{k}-\\eta\\nabla_{y}f_{\\ell}(x_{k},y_{k}), where Î·ğœ‚\\eta is a fixed step size. Then,\nit displaces the UL variables using\nxk+1=xkâˆ’Î·â€‹gkDARTSsubscriptğ‘¥ğ‘˜1subscriptğ‘¥ğ‘˜ğœ‚superscriptsubscriptğ‘”ğ‘˜DARTSx_{k+1}=x_{k}-\\eta\\,g_{k}^{\\operatorname{DARTS}}, where gkDARTSsuperscriptsubscriptğ‘”ğ‘˜DARTSg_{k}^{\\operatorname{DARTS}} is computed by applying the chain rule to âˆ‡xfuâ€‹(x,yâˆ’Î·â€‹âˆ‡yfâ„“â€‹(x,y))subscriptâˆ‡ğ‘¥subscriptğ‘“ğ‘¢ğ‘¥ğ‘¦ğœ‚subscriptâˆ‡ğ‘¦subscriptğ‘“â„“ğ‘¥ğ‘¦\\nabla_{x}f_{u}(x,y-\\eta\\nabla_{y}f_{\\ell}(x,y)), leading to (when using full-batch gradients) However, in the full-batch (deterministic) case,Â gkDARTSsuperscriptsubscriptğ‘”ğ‘˜DARTSg_{k}^{\\operatorname{DARTS}} may not be a descent direction.\nThe matrix-vector product inÂ (1.5) is approximated by finite differences, rendering DARTS free of both second-order derivatives and matrix-vector products (see SectionÂ 5.2). All the papers cited above focus on theÂ LL unconstrained case. A few approaches dealing with theÂ LL constrained case have been very recently proposed to tackle bilevel problems withÂ LL constraints. However, all these approaches are only applicable to linear constraints or constraints only depending onÂ yğ‘¦yÂ     . Among the algorithms proposed, we highlightÂ SIGDÂ , which determines a direction by applying the implicit function theorem (similar to the approach that we use) but can only be applied to bilevel problems with linearÂ LL constraints inÂ yğ‘¦y.\n\nReference markers found: [53]\n\nContext windows for each [ref]:\n[ref]53: and random variants.) Having in mindÂ ML applications, the methods that we are considering are Stochastic Approximation (SA) techniques, of the type of the stochastic gradient (SG) method   [ref]53 for single-objective optimization. In fact, the bilevel stochastic gradient (BSG) method can be seen as anÂ SG method applied toÂ (1.1), which leads to\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 53,\n            \"titles\": [\n                \"Constrained Optimization to Train Neural Networks on Critical and Under-Represented Classes\",\n                \"Practical Bilevel Optimization: Algorithms and Applications\",\n                \"First-Order Methods in Optimization\",\n                \"Bilevel Optimization and Machine Learning, pages 25â€“47\",\n                \"Practical Gauss-Newton optimisation for deep learning\",\n                \"Optimization Methods for Large-Scale Machine Learning\",\n                \"Generalized Data Weighting via Class-level Gradient Manipulation\",\n                \"Gradient-based Bi-level Optimization for Deep Learning: A Survey\",\n                \"A Single-Timescale Stochastic Bilevel Optimization Method\",\n                \"Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems\",\n                \"On a stochastic approximation method\",\n                \"An overview of bilevel optimization\",\n                \"Bi-level stochastic gradient for large scale support vector machine\",\n                \"On the convergence of stochastic bi-level gradient methods\",\n                \"Foundations of bilevel programming, volume 61 of Nonconvex Optimization and its Applications\",\n                \"Bilevel Optimization: Advances and Next Challenges\",\n                \"Stabilizing Bi-Level Hyperparameter Optimization using Moreau-Yosida Regularization\",\n                \"A proposal on machine learning via dynamical systems\",\n                \"Sensitivity analysis for nonlinear programming using penalty methods\",\n                \"Introduction to sensitivity and stability analysis in nonlinear programming, volume 165 of Mathematics in Science and Engineering\",\n                \"Nonlinear programming: Sequential unconstrained minimization techniques\",\n                \"Bilevel Programming for Hyperparameter Optimization and Meta-Learning\",\n                \"Catastrophic forgetting in connectionist networks\",\n                \"On the Promise of the Stochastic Generalized Gauss-Newton Method for Training DNNs\",\n                \"Approximation Methods for Bilevel Programming\",\n                \"Explaining and Harnessing Adversarial Examples\",\n                \"An empirical investigation of catastrophic forgetting in gradient-based neural networks\",\n                \"A Two-Timescale Framework for Bilevel Optimization: Complexity Analysis and Application to Actor-Critic\",\n                \"Meta-Learning in Neural Networks: A Survey\",\n                \"Bilevel Optimization: Convergence Analysis and Enhanced Design\",\n                \"Learning to Defend by Learning to Attack\",\n                \"Linearly constrained bilevel optimization: A smoothed implicit gradient approach\",\n                \"Learning multiple layers of features from tiny images\",\n                \"On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation\",\n                \"DARTS: Differentiable architecture search\",\n                \"Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond\",\n                \"The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning\",\n                \"Gradient Episodic Memory for Continual Learning\",\n                \"Optimizing Millions of Hyperparameters by Implicit Differentiation\",\n                \"Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]53 based on the context and research area of Inexact bilevel stochastic gradient methods for constrained and unconstrained lower-level problems. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}