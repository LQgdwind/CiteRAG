{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Heuristic-Guided Reinforcement Learning\n\nPaper Abstract: \n\nPaper Section Text:\nThe horizon-truncation idea can be traced back to Blackwell optimality in the known MDP setting¬†.\nReducing the discount factor amounts to running HuRL with a zero heuristic.\nPetrik and Scherrer , Jiang et¬†al.   study the MDP setting; Chen et¬†al.  study POMDPs. Amit et¬†al.  focus on discount regularization for Temporal Difference (TD) methods, while Van¬†Seijen et¬†al. [ref]6 use a logarithmic mapping to lower the discount for online RL. Reward shaping has a long history in RL, from the seminal PBRS work¬† to recent bilevel-optimization approaches¬†. Tessler and Mannor  consider a complementary problem to HuRL: given a discount Œ≥‚Ä≤superscriptùõæ‚Ä≤\\gamma^{\\prime}, they find a reward r‚Ä≤superscriptùëü‚Ä≤r^{\\prime} that preserves trajectory ordering in the original MDP.\nMeanwhile there is a vast literature on bias-variance trade-off for online RL with horizon truncation. TD(ŒªùúÜ\\lambda)¬†[ref]39 [ref]40 and Generalized Advantage Estimates¬† blend value estimates across discount factors, while¬†Sherstan et¬†al.  use the discount factor as an input to the value function estimator. TD(ŒîŒî\\Delta)¬† computes differences between value functions across discount factors. Classic uses of heuristics include A*¬†, Monte-Carlo Tree Search (MCTS)¬†, and Model Predictive Control (MPC)¬†.\nZhong et¬†al.  shorten the horizon in MPC using a value function approximator.\nHoeller et¬†al.  additionally use an estimate for the running cost to trade off solution quality and amount of computation.\nBejjani et¬†al.  show heuristic-accelerated truncated-horizon MPC on actual robots and tune the value function throughout learning.\nBhardwaj et¬†al.  augment MPC with a terminal value heuristic, which can be viewed as an instance of HuRL where the base algorithm is MPC.\nAsai and Muise  learn an MDP expressible in the STRIPS formalism that can benefit from relaxation-based planning heuristics.\nBut HuRL is more general, as it does not assume model knowledge and can work in unknown environments. Offline RL techniques employ pessimistic extrapolation for robustness¬†, and their learned value functions can be used as heuristics in HuRL.\nKumar et¬†al.  penalize out-of-distribution actions in off-policy optimization while Liu et¬†al.  additionally use a variational auto-encoder (VAE) to detect out-of-distribution states. We experimented with VAE-filtered pessimistic heuristics in Appendix¬†C.\nEven pessimistic offline evaluation techniques¬† can be useful in HuRL, since function approximation often induces extrapolation errors¬†. Our concept of heuristic pessimism can be easily confused for the well-established notion of admissibility¬†, but in fact they are opposites. Namely, an admissible heuristic never underestimates V‚àósuperscriptùëâV^{*} (in the return-maximization setting), while a pessimistic one never overestimates V‚àósuperscriptùëâV^{*}. Similarly, our notion of improvability is distinct from consistency: they express related ideas, but with regards to pessimistic and admissible value functions, respectively. Thus, counter-intuitively from the planning perspective, our work shows that for policy learning, inadmissible heuristics are desirable. Pearl  is one of the few works that has analyzed desirable implications of heuristic inadmissibility in planning.\n HuRL is a new way to warm-start online RL methods.\nBianchi et¬†al. 0 use a heuristic policy to initialize agents‚Äô policies.\nHester et¬†al. , Vinyals et¬†al.  train a value function and policy using batch IL and then used them as regularization in online RL. Nair et¬†al.  use off-policy RL on batch data and fine-tune the learned policy. Recent approaches of hybrid IL-RL have strong connections to HuRL¬†1  2.\nIn particular, Cheng et¬†al.  is a special case of HuRL with a max-aggregation heuristic.\nFarahmand et¬†al.  use several related tasks to learn a task-dependent heuristic\nand perform shorter-horizon planning or RL. Knowledge distillation approaches¬†3 can also be used to warm-start learning, but in contrast to them, HuRL¬†expects prior knowledge in the form of state value estimates, not features, and doesn‚Äôt attempt to make the agent internalize this knowledge. A\nHuRL¬†agent learns from its own environment interactions, using prior knowledge only as guidance. Reverse Curriculum approaches¬†4 create short horizon RL problems by initializing the agent close to the goal, and moving it further away as the agent improves. This gradual increase in the horizon inspires the HuRL approach. However, HuRL does not require the agent to be initialized on expert states and can work with many different base RL algorithms.\n\nReference markers found: [6]\n\nContext windows for each [ref]:\n[ref]6: and Scherrer , Jiang et¬†al.   study the MDP setting; Chen et¬†al.  study POMDPs. Amit et¬†al.  focus on discount regularization for Temporal Difference (TD) methods, while Van¬†Seijen et¬†al. [ref]6 use a logarithmic mapping to lower the discount for online RL. Reward shaping has a long history in RL, from the seminal PBRS work¬† to recent bilevel-optimization approaches¬†. Tessler and\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 6,\n            \"titles\": [\n                \"Blending MPC & Value Function Approximation for Efficient Reinforcement Learning\",\n                \"Dota 2 with Large Scale Deep Reinforcement Learning\",\n                \"Grandmaster level in Starcraft II using multi-agent reinforcement learning\",\n                \"Sample complexity of episodic fixed-horizon reinforcement learning\",\n                \"Near-optimal time and sample complexities for solving Markov decision processes with a generative model\",\n                \"Maximizing the total reward via reward tweaking\",\n                \"Using a logarithmic mapping to enable lower discount factors in reinforcement learning\",\n                \"Truncated approximate dynamic programming with task-dependent terminal value\",\n                \"Accelerating online reinforcement learning with offline datasets\",\n                \"Deep Q-learning from Demonstrations\",\n                \"Planning with Markov decision processes: An AI perspective\",\n                \"Beyond UCB: Optimal and efficient contextual bandits with regression oracles\",\n                \"The FF planning system: Fast plan generation through heuristic search\",\n                \"The LAMA planner: Guiding cost-based anytime planning with landmarks\",\n                \"Classical planning in MDP heuristics: With a little help from generalization\",\n                \"Regularized Behavior Value Estimation\",\n                \"Policy Improvement via Imitation of Multiple Oracles\",\n                \"Discrete dynamic programming\",\n                \"Biasing approximate dynamic programming with a lower discount factor\",\n                \"The dependence of effective planning horizon on model accuracy\",\n                \"On structural properties of mdps that bound loss due to shallow planning\",\n                \"Improving offline value-function approximations for pomdps by reducing discount factors\",\n                \"Discount factor as a regularizer in reinforcement learning\",\n                \"A formal basis for the heuristic determination of minimum cost paths\",\n                \"A survey of monte carlo tree search methods\",\n                \"Value function approximation and model predictive control\",\n                \"Deep Value Model Predictive Control\",\n                \"Planning with a Receding Horizon for Manipulation in Clutter using a Learned Value Function\",\n                \"Policy invariance under reward transformations: Theory and application to reward shaping\",\n                \"Is pessimism provably efficient for offline RL?\",\n                \"Provably good batch off-policy reinforcement learning without great exploration\",\n                \"Mujoco: A physics engine for model-based control\",\n                \"Measuring Sample Efficiency and Generalization in Reinforcement Learning Benchmarks: NeurIPS 2020 Procgen Benchmark\",\n                \"Is Q-learning provably efficient?\",\n                \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\n                \"Proximal policy optimization algorithms\",\n                \"Garage: A toolkit for reproducible reinforcement learning research\",\n                \"Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping\",\n                \"True online td (lambda)\",\n                \"Beyond the One-Step Greedy Approach in Reinforcement Learning\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]6 based on the context and research area of Heuristic-Guided Reinforcement Learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}