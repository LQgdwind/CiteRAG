{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Kraken: An Efficient Engine with a Uniform Dataflow for Deep Neural Networks\n\nPaper Abstract: \n\nPaper Section Text:\nDeep Neural Networks (DNNs) have been widely adopted in modern automation systems that require accurate classification and detection due to their remarkable performance in complex pattern recognition tasks. DNNs have been growing deeper and deeper in the past few years, empowering them with beyond human-level capabilities [ref]1 . However, this has been achieved at the expense of increased computational complexity, while many mobile and edge processing applications require fast inference of such DNNs with low power and low chip area. In order to address this rising demand for efficient inference, application-specific integrated circuit (ASIC) architectures are developed as arrays of hundreds of processing elements (PEs), where each PE performs a multiply-accumulate (MAC) operation. Since convolutional layers, fully connected layers, and matrix products do not impose an order of performing the MACs, the design space for such hardware architectures and their corresponding spatio-temporal orchestrations of data (called dataflows) is quite large. The amount of data required for the computation of each layer in modern DNNs is in the order of several megabytes [ref]1, which cannot fit the on-chip memories. While this makes repeated DRAM accesses inevitable, they are primarily responsible for the energy consumption in hardware accelerators. For instance, a 32-bit DRAM access consumes 200 times the energy required for a MAC operation in 45 nm technology . In addition, a PE array can only perform a few hundred operations in parallel, a tiny subset of the hundreds of millions of operations required for a layer in a DNN. Naively mapping the operations of layers of varying shapes to the fixed hardware architecture would result in PEs idling without work. Therefore, it is a challenging task of paramount importance to design a generic dataflow that maximizes the overall performance efficiency by optimally utilizing the fixed PE array architecture, while reducing the number of memory accesses by exploiting data reuse opportunities, over varying shapes and types of DNN layers. Several architectures and their corresponding dataflows have been introduced in the literature over the past few years to accelerate the inference of DNNs   . Weight-stationary dataflows such as NVDLA , TPU , neuFlow , Sankaradas et al. , Park et al. , Chakradhar et al. , Sriram et al. , Cambricorn-X  and Origami  hold the weights in register files or SRAMs inside PEs over multiple operations. Input-stationary dataflows such as SCNN  hold input pixels inside PEs while changing weights. Output-stationary dataflows such as DaDianNao , DianNao , Zhang et al. , Moons et al. 0, ShiDianNao 1, and Gupta et al. 2 are designed to minimize the energy consumption of reading and writing partial sums. The DianNao family of accelerators 3   1 minimize memory accesses by storing the entire neural network within their eDRAM buffers and are hence evaluated only on older, smaller networks. DNA 4 supports three different dataflows to individually optimize the reuse of inputs, outputs, and weights, resulting in a complex 3-level PE structure with large multiplexers. Chen et al. introduced a row-stationary dataflow with an architecture named Eyeriss 5 6, which maximizes the reuse of weights, inputs and partial sums using scratchpads inside its 168 PEs. The scratchpads take 46.8% of the total area and 47.9% of the total power, in addition to the global buffers that take 18.8% of area. The 2-D array of PEs is controlled through a network-on-chip structure that orchestrates the proposed dataflow to process both convolutional and fully-connected layers. It was benchmarked on AlexNet and VGG-16 among state-of-the-art convolutional neural networks (CNNs) with high arithmetic intensity. However, its dataflow does not efficiently utilize the PE array due to long reconfiguration times and inability to perform computations when transferring data, resulting in low performance efficiency. Jo et al. 7 introduced DSIP with a master-slave ISA to overlap data transfer and computation. DSIP also employs scratchpads that take 45.9% of total area and 35.6% of total power. Ardakani et al. introduced the Fully-Connected Inspired Dataflow (FID) 8 for VGG-like CNNs, where 3√ó3333{\\times}3 and 1√ó1111{\\times}1 convolutional layers are treated as special cases of fully-connected layers. FID was then generalized into GFID with corresponding architectures named Multi-Mode Inference Engine (MMIE) 9 and ZASCAD 0 with the ability to process larger filter sizes needed for AlexNet and ResNet-50. While MMIE/ZASCAD reports a high utilization factor (percentage of PEs active in a computational clock cycle), its overall performance efficiency is relatively low due to clock cycles wasted during weight passing and data transfer. Ahmadi et al. introduced an architecture 1 for the convolutional layers of VGG-like CNNs and then generalized it into CARLA 2, implemented as an array of 196 PEs. CARLA can process only the convolutional layers of CNNs, and it is tailored for 3√ó3333{\\times}3 and 1√ó1111{\\times}1 convolutional layers where the number of output channels is a multiple of 64. This results in a low performance efficiency for convolutional layers with larger filter sizes. CARLA also uses four different dataflows to maximize its utilization, requiring large multiplexers, resulting in mostly idle datapaths in its architecture. Another parallel area of research is exploiting sparsity in compressed DNNs. The first published versions of Eyeriss 6, MMIE 9, and Ahmadi et al. 1 focused on dense DNNs. They were later extended to exploit sparsity as Eyeriss v2 3, ZASCAS 0 and CARLA 2, respectively. Eyeriss v2 and EIE  use the Compressed Sparse Column (CSC) scheme, while Cnvlutin 4 uses Compressed Sparse Row (CSR) to exploit the zeros resulting from the Rectified Linear Unit (ReLU) activation function, which is commonly found in early CNNs. ZASCAS 0 and EIE are designed to skip such null activations. Cambricorn-X  exploits sparsity in pruned weights, whereas SCNN  exploits sparsity in both weights and activations. Moons et al. explored the effects of quantization on CNNs 5 to implement EEPS 6, a precision-scalable processor to exploit sparsity. It was then extended into Envision 7, with dynamic scaling of voltage, accuracy and frequency. DNPU 8 also supports precision scaling, but it has a much larger chip area of 16 mm2. There is also some research in designing analog circuits to accelerate smaller CNNs 9. The architectures presented in literature have traditionally relied on scratchpads (SRAMs inside each of the hundreds of PEs), in addition to large global SRAMs, for data reuse. This typically results in over half of the overall chip area being utilized, and a significant amount of energy being consumed for scratchpads 6 7. Furthermore, most architectures focus on improving either performance efficiency or arithmetic intensity. Ones that report high PE utilization factors demonstrate lower overall performance efficiencies when considering their wasted clock cycles 9 0. Some architectures employ multiple dataflows requiring large multiplexers and idle datapaths 2 7. This paper presents the first-generation design and implementation of the Kraken architecture and its corresponding uniform dataflow. Kraken is able to accelerate the convolutional and fully-connected layers of CNNs [ref]1, along with matrix products required for other DNN types such as the attention-based transformers 0 that are rising in popularity. The key contributions of this paper are as follows: A spatial architecture where PEs are arranged into R=7ùëÖ7R{=}7{} rows and C=96ùê∂96C{=}96{} cores. Each PE consists of just a multiplier, accumulator, and a 2-way multiplexer, allowing 672 PEs to be packed in an area of 7.3 mm2superscriptmm2\\mathrm{m}\\mathrm{m}^{2}. A uniform dataflow that treats fully-connected layers and matrix products as special cases of convolutional layers, outperforming the state-of-the-art in overall performance efficiency, arithmetic intensity, and memory accesses. Multiple levels of data reuse without requiring scratchpads: Output partial sums are reused within accumulators, weights are rotated in a global buffer, input activations are reused in horizontal and vertical convolutions. Elastic grouping, where C=96ùê∂96C{=}96{} cores dynamically reconfigure within one clock, using a header of just 64 bits, to process convolutional layers of different filter sizes while maintaining a high PE utilization. Decentralized control: The configuration propagates with data, reconfiguring each part on the fly, without stalling the engine, making Kraken the first accelerator to the best of our knowledge to achieve such decentralized, dynamic reconfiguration. Detailed performance analysis: The performance efficiency, number of memory accesses, and arithmetic intensity of Kraken are derived as accurate functions of RùëÖR and Cùê∂C, and optimized to find the best static configuration R√óC=7√ó96ùëÖùê∂796R{\\times}C=7{}{\\times}96{} for common DNNs. Thorough comparison of performance with prior works, benchmarked on AlexNet, VGG-16, and ResNet-50. Implementation in TSMC 65-nm, which outperforms the state-of-the-art 2 with 5.8√ó{\\times} more Gops/mm2 and 1.6√ó{\\times} more Gops/W in just 1.2√ó{\\times} the area.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: due to their remarkable performance in complex pattern recognition tasks. DNNs have been growing deeper and deeper in the past few years, empowering them with beyond human-level capabilities [ref]1 . However, this has been achieved at the expense of increased computational complexity, while many mobile and edge processing applications require fast inference of such DNNs with low power\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"ImageNet Large Scale Visual Recognition Challenge\",\n                \"A survey of the recent architectures of deep convolutional neural networks\",\n                \"EIE: Efficient Inference Engine on Compressed Deep Neural Network\",\n                \"Survey of Machine Learning Accelerators\",\n                \"Efficient Processing of Deep Neural Networks\",\n                \"An Updated Survey of Efficient Hardware Architectures for Accelerating Deep Convolutional Neural Networks\",\n                \"Research on NVIDIA Deep Learning Accelerator\",\n                \"In-datacenter performance analysis of a tensor processing unit\",\n                \"NeuFlow: A runtime reconfigurable dataflow processor for vision\",\n                \"A massively parallel coprocessor for convolutional neural networks\",\n                \"A 1 . 93TOPS/W Scalable Deep Learning / Inference Processor with Tetra-Parallel MIMD Architecture for Big-Data Applications\",\n                \"A dynamically configurable coprocessor for convolutional neural networks\",\n                \"Towards an embedded biologically-inspired machine vision processor\",\n                \"Cambricon-X: An accelerator for sparse neural networks\",\n                \"Origami: A 803-GOp/s/W Convolutional Network Accelerator\",\n                \"SCNN: An accelerator for compressed-sparse convolutional neural networks\",\n                \"DaDianNao: A Machine-Learning Supercomputer\",\n                \"DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning\",\n                \"Optimizing FPGA-based accelerator design for deep convolutional neural networks\",\n                \"A 0.3-2.6 TOPS/W precision-scalable processor for real-time large-scale ConvNets\",\n                \"ShiDianNao: Shifting vision processing closer to the sensor\",\n                \"Deep Learning with Limited Numerical Precision\",\n                \"DianNao family: Energy-efficient hardware accelerators for machine learning\",\n                \"Deep Convolutional Neural Network Architecture with Reconfigurable Computation Patterns\",\n                \"Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks\",\n                \"Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks\",\n                \"DSIP: A Scalable Inference Accelerator for Convolutional Neural Networks\",\n                \"An Architecture to Accelerate Convolution in Deep Neural Networks\",\n                \"Multi-Mode Inference Engine for Convolutional Neural Networks\",\n                \"Fast and Efficient Convolutional Accelerator for Edge Computing\",\n                \"An Energy-Efficient Accelerator Architecture with Serial Accumulation Dataflow for Deep CNNs\",\n                \"CARLA: A Convolution Accelerator with a Reconfigurable and Low-Energy Architecture\",\n                \"Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices\",\n                \"Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing\",\n                \"Energy-Efficient ConvNets Through Approximate Computing\",\n                \"An Energy-Efficient Precision-Scalable ConvNet Processor in 40-nm CMOS\",\n                \"ENVISION: A 0.26-to-10TOPS/W Subword-Parallel Dynamic-Voltage-Accuracy-Frequency-Scalable Convolutional Neural Network Processor in 28nm FDSOI\",\n                \"DNPU: An 8.1TOPS/W reconfigurable CNN-RNN processor for general-purpose deep neural networks\",\n                \"RF-rate hybrid CNN accelerator based on analog-CMOS and Xilinx RFSoC\",\n                \"Attention Is All You Need\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of Kraken: An Efficient Engine with a Uniform Dataflow for Deep Neural Networks. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}