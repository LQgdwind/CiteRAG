{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning\n\nPaper Abstract: \n\nPaper Section Text:\nThe training dataset for the 2D Multimodal Contextual Bandit Environment, D={(si,ai)}i=1n𝐷subscriptsuperscriptsubscript𝑠𝑖subscript𝑎𝑖𝑛𝑖1D=\\{(s_{i},a_{i})\\}^{n}_{i=1}, consists of n=10000𝑛10000n=10000 state-action tuples with horizon H=1𝐻1H=1. To examine the policy in OOD states, the states generated for training are sampled from the uniform distribution st​r​a​i​n∼U​n​i​f​o​r​m​([−0.05,0.05]×[−0.05,0.05])similar-tosubscript𝑠𝑡𝑟𝑎𝑖𝑛𝑈𝑛𝑖𝑓𝑜𝑟𝑚0.050.050.050.05s_{train}\\sim Uniform([-0.05,0.05]\\times[-0.05,0.05]) in Fig. 1(a), and st​r​a​i​n∼U​n​i​f​o​r​m​([−0.08,0.08]×[−0.08,0.08])similar-tosubscript𝑠𝑡𝑟𝑎𝑖𝑛𝑈𝑛𝑖𝑓𝑜𝑟𝑚0.080.080.080.08s_{train}\\sim Uniform([-0.08,0.08]\\times[-0.08,0.08]) in Fig. 1(b). The states used for the evaluation are sampled from the uniform distribution of st​e​s​t∼U​n​i​f​o​r​m​([−1,1]×[−1,1])similar-tosubscript𝑠𝑡𝑒𝑠𝑡𝑈𝑛𝑖𝑓𝑜𝑟𝑚1111s_{test}\\sim Uniform([-1,1]\\times[-1,1]). Ground truth for OOD state generalization corresponding to the training datasets in Fig. 1(a) and (b) is illustrated in Fig. 1(c). The black points in Fig. 2(a) show the x and y-coordinates of the actions in the dataset, constituting two pairs of clusters located close to the diagonal edges of the 2D plane. Similarly, the black points in Fig. 2(a) and (b) show the x and y-coordinates of the actions generated by the SRDP and BC-Diffusion policies, respectively. To distinguish which states are mapped to which quadrants the actions reside in, we illustrate states in four different colors. More specifically, if a state generates an action in the first, second, third, and fourth quadrants, the state is colored red, green, dark yellow, and blue, respectively. Therefore, visually distinguishable quadrants with respective coloring in Fig. 2(a) (SRDP) indicate that the reverse diffusion process can generate actions accurately for OOD states. In contrast, Fig. 2(b) shows that BC-Diffusion memorizes actions without using state information. Subsequently, results in Fig. 2 show that the representation learning with state reconstruction loss promotes finding expert skills in multimodal data, achieving faster convergence. Although a larger portion of the state space is not included in the training set distribution ( st​r​a​i​n∼U​n​i​f​o​r​m​([−0.05,0.05]×[−0.05,0.05])similar-tosubscript𝑠𝑡𝑟𝑎𝑖𝑛𝑈𝑛𝑖𝑓𝑜𝑟𝑚0.050.050.050.05s_{train}\\sim Uniform([-0.05,0.05]\\times[-0.05,0.05])) in Fig. 2, SRDP can generalize well to OOD states and learn at least one of the two expert behaviors. Conversely, BC-Diffusion finds the action distribution of the dataset, yet it cannot assign the correct action distribution for the given state. Hence, we observe that the state space is not partitioned correctly. In the next set of experiments, the training dataset includes (st​r​a​i​n,at​r​a​i​nsubscript𝑠𝑡𝑟𝑎𝑖𝑛subscript𝑎𝑡𝑟𝑎𝑖𝑛s_{train},a_{train}) tuples where st​r​a​i​n∼Uniform([−0.08,0.08]×[−0.08,0.08]s_{train}\\sim Uniform([-0.08,0.08]\\times[-0.08,0.08]). The comparison results with this setting show that only SRDP can represent multimodal expert distributions and partition the state space into visibly clear quadrants at earlier iterations, improving training stability and reducing computation costs. The last three rows in Fig. 3 show the results for BC-CVAE ), BC-MLE), BC-MMD [ref]14 which were used in Diffusion-QL 2D bandit experiments . BC-CVAE, the policy model used in , represents the policy by a conditional variational encoder (CVAE) trained with the evidence lower bound \n\n𝔼(𝒔,𝒂)∼𝒟​[𝔼𝒛∼q(⋅∣𝒔,𝒂)​[log⁡p​(𝒂∣𝒔,𝒛)]−KL⁡(q​(𝒛∣𝒔,𝒂)∥p​(𝒛))]\\displaystyle\\mathbb{E}_{(\\boldsymbol{s},\\boldsymbol{a})\\sim\\mathcal{D}}\\left[\\mathbb{E}_{\\boldsymbol{z}\\sim q(\\cdot\\mid\\boldsymbol{s},\\boldsymbol{a})}[\\log p(\\boldsymbol{a}\\mid\\boldsymbol{s},\\boldsymbol{z})]-\\operatorname{KL}(q(\\boldsymbol{z}\\mid\\boldsymbol{s},\\boldsymbol{a})\\|p(\\boldsymbol{z}))\\right]\n. BC-MLE  uses a Multivariate Gaussian to model the policy where a diagonal matrix represents the covariance. Finally, BC-MMD [ref]14 uses a CVAE that learns the behavior policy to constrain a Gaussian policy. Our experiments show that although the bottleneck layer used in BC-CVAE increases the performance, the proposed model, SRDP, which uses state reconstruction loss, yields far better performance with faster convergence. Consistent with the experiments in Fig. 2, BC-Diffusion only learns the action distributions in the dataset, excluding the state information in OOD states. Hence, SRDP is the only method that can represent both expert modalities in this setting with the fastest convergence.\n\nReference markers found: [14]\n\nContext windows for each [ref]:\n[ref]14: space into visibly clear quadrants at earlier iterations, improving training stability and reducing computation costs. The last three rows in Fig. 3 show the results for BC-CVAE ), BC-MLE), BC-MMD [ref]14 which were used in Diffusion-QL 2D bandit experiments . BC-CVAE, the policy model used in , represents the policy by a conditional variational encoder (CVAE) trained with the evidence lower\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 14,\n            \"titles\": [\n                \"Behavior Regularized Offline Reinforcement Learning\",\n                \"D4RL: Datasets for Deep Data-Driven Reinforcement Learning\",\n                \"Improving language understanding by generative pre-training\",\n                \"Language models are unsupervised multitask learners\",\n                \"Language Models are Few-Shot Learners\",\n                \"Training language models to follow instructions with human feedback\",\n                \"Summary of chatgpt/gpt-4 research and perspective towards the future of large language models\",\n                \"Offline reinforcement learning for autonomous driving with real world driving data\",\n                \"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\",\n                \"Dealing with the Unknown: Pessimistic Offline Reinforcement Learning\",\n                \"Diffusion policies as an expressive policy class for offline reinforcement learning\",\n                \"Planning with diffusion for flexible behavior synthesis\",\n                \"Improving Reconstruction Autoencoder Out-of-distribution Detection with Mahalanobis Distance\",\n                \"Training bidirectional generative adversarial networks with hints\",\n                \"Stabilizing off-policy q-learning via bootstrapping error reduction\",\n                \"Accelerating online reinforcement learning with offline datasets\",\n                \"Offline Reinforcement Learning with Implicit Q-Learning\",\n                \"Conservative Q-Learning for Offline Reinforcement Learning\",\n                \"GradientDICE: Rethinking Generalized Offline Estimation of Stationary Values\",\n                \"Doubly robust off-policy evaluation for reinforcement learning\",\n                \"MOPO: Model-based Offline Policy Optimization\",\n                \"COMBO: Conservative Offline Model-Based Policy Optimization\",\n                \"Reinforcement learning as one big sequence modeling problem\",\n                \"Decision Transformer: Reinforcement Learning via Sequence Modeling\",\n                \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\",\n                \"Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models\",\n                \"RainDiffusion: When Unsupervised Learning Meets Diffusion Models for Real-world Image Deraining\",\n                \"Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Datasets\",\n                \"All are worth words: a vit backbone for score-based diffusion models\",\n                \"Schrödinger’s Bat: Diffusion Models Sometimes Generate Polysemous Words in Superposition\",\n                \"Denoising Diffusion Probabilistic Models\",\n                \"Diffusion models: A comprehensive survey of methods and applications\",\n                \"Reinforcement learning: An introduction\",\n                \"Double q-learning\",\n                \"Off-Policy Deep Reinforcement Learning without Exploration\",\n                \"Continuous control with deep reinforcement learning\",\n                \"A Minimalist Approach to Offline Reinforcement Learning\",\n                \"Diffusion-policies-for-offline-rl\",\n                \"Generalization in transfer learning: robust control of robot locomotion\",\n                \"Mujoco: A physics engine for model-based control\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]14 based on the context and research area of Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}