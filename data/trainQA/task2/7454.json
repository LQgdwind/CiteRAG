{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation\n\nPaper Abstract: \n\nPaper Section Text:\nProtein representation learning involves learning embeddings to convert raw protein data into latent space representations, extracting meaningful features and chemical attributes. More specifically, given a protein x=[o1,o2,…​oL]𝑥subscript𝑜1subscript𝑜2…subscript𝑜𝐿x=[o_{1},o_{2},\\dots o_{L}], where each o1subscript𝑜1o_{1} represents an amino acid (sequence-based) or atom coordinate (structure-based), learn an embedding z=[h1,h2,…​hL]𝑧subscriptℎ1subscriptℎ2…subscriptℎ𝐿z=[h_{1},h_{2},\\dots h_{L}], where each hi∈ℝdsubscriptℎ𝑖superscriptℝ𝑑h_{i}\\in\\mathbb{R}^{d} represents a d−limit-from𝑑d-dimensional token representation for amino acid oisubscript𝑜𝑖o_{i}. This general framework can be extended to a multitude of tasks— for token property prediction tasks, additionally allocate a prediction p​(yi|oi)𝑝conditionalsubscript𝑦𝑖subscript𝑜𝑖p(y_{i}|o_{i}) in the embedding, and for pairwise prediction tasks, allocate a prediction p​(yi​j|oi,oj)𝑝conditionalsubscript𝑦𝑖𝑗subscript𝑜𝑖subscript𝑜𝑗p(y_{ij}|o_{i},o_{j}). For overall protein property prediction, allocate predictions p​(y|x)𝑝conditional𝑦𝑥p(y|x) or p​(y|x1,x2)𝑝conditional𝑦subscript𝑥1subscript𝑥2p(y|x_{1},x_{2}) for individual and protein-protein properties, respectively. These embeddings can be seen as creating “richer” data spaces for various models to work on; whereas raw atom graphs and amino acid sequences are entirely arbitrary, these embeddings are aimed to capture various chemical traits and properties (i.e. two proteins with similar properties should be represented with similar embeddings, even if their raw forms are very different). Thus, representation learning models do not directly perform any tasks on their own, but instead are incorporated in other generative models to improve their ability to model these important chemical features. Note that a plethora of datasets exist for all various downstream tasks; we do not describe all such datasets. We focus on datasets used for pre-training in protein representation learning, described below. Note that UniRef, UniParc, and ProteinKG are typically used for sequence-based learning, and PDB and AlphafoldDB are primarily used for structure-based learing. Pfam has been used in both types of models. UniRef  - A clustered version of the Unified Protein KnowledgeBase (UniProtKB), part of the central resource UniProt, which is a curated and labeled set of protein sequences and their functions UniParc  - A larger dataset of protein sequences, part of the central resource Uniprot, which includes UniProtKB and adds proteins from a variety of other sources ProteinKG [ref]38 - A dataset created by Zhang et al. (OntoProtein), aligning biological knowledge with protein sequences with knowledge graphs (KG), used for directly injecting biological knowledge in the representation learning process PDB  - Protein Data Bank, a central archive for all experimentally determined protein structures, widely used in almost all protein structure-related tasks. AlphaFoldDB  - AlphaFold Protein Structure Database, a large dataset created by using AlphaFold2 to predict structures from sequence datasets, such as UniProt and Swiss-Prot. Pfam [ref]161 - A collection of protein families, used in multiple sequence alignments (MSA) Representation learning models do not directly address tasks on their own but rather supplement a wide range of tasks by refining a latent space. Thus, the protein representation learning field in particular lacks uniformity in testing and training methods, making it hard to make generalized evaluations. Some methods like Tasks Assessing Protein Embeddings (TAPE) [ref]162 and, more recently, Protein sEquence undERstanding (PEER)  seek to address this by compiling various testing metrics into a standardized benchmark for evaluating these language models. A few of the most prominent tasks include: Contact Prediction - Given two amino acid residues, predict the probability that they “contact”, or are within some 3D distance threshold. Measured using precision of top L/5𝐿5L/5 medium-range and long-range contacts (contact between sequentially distant amino acids) according to CASP standards . Fold Classification - Also known as remote homology prediction. Given an amino acid sequence, predict the corresponding fold class . Used for finding structural similarities for distantly related inputs. Tested using the dataset proposed by Hou et al. , which comprises three versions of increasing difficulty: Family (test set in the same family as the training set), Superfamily (same superfamily as training) and Fold (same fold as training). Measured using accuracy. Stability Prediction - Given a protein, output a label y∈ℝ𝑦ℝy\\in\\mathbb{R} representing the most extreme circumstances under which a protein maintains its fold above a concentration threshold. Measured using Spearman’s ρ𝜌\\rho. PPI - Protein-Protein Interaction, given two proteins, predict whether or not they interact (binary classification). Measured using accuracy. The following metrics are used to evaluate the above tasks: Accuracy - Ratio between correct classifications to total classifications. Precision - Accuracy within the set of selected elements; in contact prediction, this refers to the following: of the top L/5𝐿5L/5 contacts with the highest predicted probability, find the percentage of these which are ground-truth contacts. L𝐿L refers to the length of a protein sequence. Spearman’s ρ𝜌\\mathbf{\\rho} -  Spearman’s rank correlation coefficient, found by sorting the ground-truth and predicted output values into ranked order (so each protein has a true rank yi∈ℕsubscript𝑦𝑖ℕy_{i}\\in\\mathbb{N} and predicted rank xi∈ℕsubscript𝑥𝑖ℕx_{i}\\in\\mathbb{N}) and computing: where disubscript𝑑𝑖d_{i} refers to the numerical difference in ranks for each yi,xisubscript𝑦𝑖subscript𝑥𝑖y_{i},x_{i}, and n𝑛n refers to the total number of elements in each set. As mentioned above, protein representation learning models can be divided into two broad categories: sequence-based and structure-based. This distinction simply refers to the type of input these models learn representations for; we begin by discussing sequence-based learning models. Both amino acid sequences and protein structures carry valuable information about a protein’s function and binding properties, and thus these two types of learning can be seen as complements to each other. Unified Representation (UniRep)  used a multiplicative long-short-term memory recurrent neural network (mLSTM RNN) to approach the task of representation learning. By iterating through each amino acid and comparing its prediction with the true amino acid type, adjusting its parameters at each step, UniRep gradually learns a better representation of the sequence. While the LSTM model has been proven inferior to newer transformer and natural language models, the basic concepts laid an important groundwork for future representation learning models. ProtBERT  improved upon performance by applying a BERT model to amino acid sequences, treating each residue as a word. BERT stands for Bidirectional Encoder Representations from Transformers, which is a pre-trained language model for neural language processing. Several variations, such as RoBERTa and DeBERTa, improve upon BERT by enhancing parameters and attention mechanisms and are used in models discussed later. ESM-1B  trained a deep transformer with 33 layers and 650 million parameters, using the masked language modeling (MLM) objective for training. ESM-1b considered residue contexts across the entire sequence through its stacked attention layers, and its complexity and size allowed it to outperform other SOTA models on a wide range of tasks, as shown below. ESM-1b’s massive size also resulted in expensive training costs, however, and smaller research groups would had to find more unique approaches to compete with ESM-1b on representation learning. Following ESM-1b’s breakthrough, a series of models applied the concept of direct knowledge injection. Previous models had simply treated these sequences as generic sequences of tokens, failing to take advantage of the vast biological knowledge in existence today. One avenue for knowledge injection comes from the analysis of Multiple Sequence Alignments (MSA), which groups proteins concerning their evolutionary families. Because inherent patterns appear as a result of the evolutionary process, analyzing MSAs can result in a richer understanding of protein sequences. MSA Transformer  was the first model to intersect a protein language model with MSA analysis, pretraining a transformer model on MSA input. While MSAs can provide large benefits, the alignment process is computationally costly. Retrieved Sequence Augmentation (RSA)  provides a solution to this, using a dense sequence retriever to directly augment input proteins with corresponding evolutionary information from the Pfam database. This allows RSA to incorporate similar information as traditional MSA methods, without the computationally costly MSA alignment process. OntoProtein [ref]38 incorporates direct knowledge injection by pretraining on gene ontology (GO) knowledge graphs (KG). OntoProtein uses the ProtBERT architecture for encoding protein sequences but introduces a second encoder for training on the GO input. To perform this hybrid encoding, Zhang et al. [ref]38 create a new dataset, ProteinKG25, which aligns protein sequences with their respective GO annotations in text format. Knowledge-exploited Auto-encoder for Protein (KeAP)  extends upon this by exploring protein and annotated text on a more granular level; while OntoProtein models relationships over proteins and texts as a whole, KeAP uses a cross-attention mechanism to perform token-level exploration on individual amino acids and words. Now, we discuss a few structure-based learning models. IEConv  incorporates both intrinsic distance (based on connectivity in a node-graph representation) and extrinsic distance (based on physical 3D distance) information in a convolutional neural network (CNN), using contrastive learning to pre-train. Additionally, IEConv uses protein-specific pooling methods, like reducing each amino acid to its alpha carbon, to reduce computational complexity and allow for more learned features per atom under memory constraints. DeepFRI  combines the idea of sequence representation learning with structural representation learning, incorporating both a pre-trained LSTM to learn sequence data and a GCN to learn from contact map input (matrix representing 3D distances between residues). GearNET  applies a GNN to a graph of residues, with three types of directed edges defined for sequential/physical distance and K-nearest neighbors. While previous models had only considered message-passing between residues, GearNET applied direct message-passing between edges in its variant model GearNET-Edge, which led to even further improved performance on various metrics. In addition, GearNET-IEConv added an intrinsic-extrinsic convolutional layer inspired by Hermosilla et al.  to improve performance, with GearNET-Edge-IEConv combining both of these features.\n\nReference markers found: [38]\n\nContext windows for each [ref]:\n[ref]38: and their functions UniParc  - A larger dataset of protein sequences, part of the central resource Uniprot, which includes UniProtKB and adds proteins from a variety of other sources ProteinKG [ref]38 - A dataset created by Zhang et al. (OntoProtein), aligning biological knowledge with protein sequences with knowledge graphs (KG), used for directly injecting biological knowledge in the\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling\",\n                \"Drug discovery: a historical perspective\",\n                \"Rational drug design\",\n                \"Statistical and machine learning approaches to predicting protein–ligand interactions\",\n                \"Comparison of preclinical development programs for small molecules (drugs/pharmaceuticals) and large molecules (biologics/biopharmaceuticals): studies, timing, materials, and costs\",\n                \"Computational methods in drug discovery\",\n                \"Rethinking drug design in the artificial intelligence era\",\n                \"Deep learning for drug design: an artificial intelligence paradigm for drug discovery in the big data era\",\n                \"Interpretation of quantitative structure–activity relationship models: past, present, and future\",\n                \"A practical overview of quantitative structure-activity relationship\",\n                \"High-throughput screening of bimetallic catalysts enabled by machine learning\",\n                \"Machine-learning scoring functions for structure-based virtual screening\",\n                \"Machine-learning-guided directed evolution for protein engineering\",\n                \"Machine learning-assisted directed protein evolution with combinatorial libraries\",\n                \"De novo drug design\",\n                \"Advances in de novo drug design: from conventional to machine learning methods\",\n                \"Use of machine learning approaches for novel drug discovery\",\n                \"Molecular trapping in two-dimensional chiral organic kagomé nanoarchitectures composed of baravelle spiral triangle enantiomers\",\n                \"What are diffusion models?\",\n                \"Crystal and molecular structure of 4, 6-dimethyl-9-phenyl-8, 12-dioxa-4, 6-diazatetracyclo [8.8. 0.02, 7.013, 18] octadeca-2 (7), 13, 15, 17-tetraene-3, 5, 11-trione 2-ethoxyphenyl (2e)-but-2-enoate\",\n                \"A few words on representation learning\",\n                \"Protein structure prediction.\",\n                \"Levels of protein structure – quaternary.\",\n                \"In silico methods in antibody design\",\n                \"Deep learning approaches for de novo drug design: An overview\",\n                \"De novo design: balancing novelty and confined chemical space\",\n                \"Computational approaches for de novo drug design: past, present, and future\",\n                \"The cost of drug development\",\n                \"Progress in computational protein design\",\n                \"Systems pharmacology in small molecular drug discovery\",\n                \"Quantifying the chemical beauty of drugs\",\n                \"Understanding drug-likeness\",\n                \"Estimation of the size of drug-like chemical space based on gdb-17 data\",\n                \"Innovation in the pharmaceutical industry: new estimates of r&d costs\",\n                \"Ai in small-molecule drug discovery: A coming wave\",\n                \"Protein design via deep learning\",\n                \"Deep Learning in Protein Structural Modeling and Design\",\n                \"The coming of age of de novo protein design\",\n                \"OntoProtein: Protein Pretraining With Gene Ontology Embedding\",\n                \"Retrieved Sequence Augmentation for Protein Representation Learning\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]38 based on the context and research area of A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}