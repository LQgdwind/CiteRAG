{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Towards Improving Exploration in Self-Imitation Learning using Intrinsic Motivation\n\nPaper Abstract: \n\nPaper Section Text:\nAnalyzing the performance of self-IL and IM techniques independently and when being combined: to begin with, Figure 1 analyzes the actual impact on the performance of the agent when using IM and self-IL techniques either independently or in combination. We observe that BeBold shows a good behavior only in 2 out of the 4 environments under consideration (namely, MN7S8 and KS4R3). However, it completely fails when dealing with the challenging scenarios of MultiRoom and ObstructedMaze series (corr. MN12S10 and O2Dlh). When using just SIL, it performs poorly in all scenarios333As mentioned in Section II and Section IV-A, other works have analyzed how complementary SIL and some IM techniques were, but at sparse reward problems that are not so hard as the ones herein considered .. With reference to RAPID, it is capable of solving MultiRoom environments (as expected), but struggles over KS4R3 and O2Dlh, which are assumed to have larger state spaces and an increasing difficulty from the perspective of exploration. On top of the self-IL approaches, BeBold fosters the exploration and consequently renders some valuable learning when using SIL. However, results are worse than using BeBold in isolation, which suggests that the SIL prioritization mechanisms are not working properly. Contrarily, results are outstanding when combined with RAPID, reducing drastically the number of samples to achieve the same performance and attaining a better overall learning when compared to using RAPID in its na√Øve version. Besides these improvements, it is interesting to notice that the benefits of using IM remain even when the latter is not enough to learn in isolation: BeBold does not capture any knowledge over MN12S10 and O2Dlh, but it augments the capabilities of RAPID when used in those scenarios. Evaluation of RAPID with other IM strategies: A key aspect of this work is the capacity of IM to enhance the agent‚Äôs exploration while learning. Therefore, it is of utmost importance to assess the sensitivity of the proposed self-IL+IM combination with respect to the selection of the IM approach. With that in mind, and considering that the current implementation is based on BeBold‚Äôs tabular version (see Section III-B), we now evaluate the agent‚Äôs performance with other two visitation counts strategies: counts (i.e. ri=1/N‚Äã(s‚Ä≤)subscriptùëüùëñ1ùëÅsuperscriptùë†‚Ä≤r_{i}=1/\\sqrt{N(s^{\\prime})}) and counts1st, which is the same as counts but with episodic restriction. This second set of experiments allows comparing very similar IM strategies that have proven to have different results due to their intrinsic reward generation schema [ref]15 [ref]31. The results provided in Figure 2 suggest that there is a high relationship between what the agent can learn with IM (without self-IL) and what it actually does by combining them altogether. This can be regarded as a measure of the effectiveness of IM methods when implemented in isolation, where their base functionality of exploring is not wide-spread with the self-IL counterpart. At this point, by just inspecting the results reported in [ref]15 [ref]31, it is clear that counts is the worst method, followed by counts1st and BeBold. Differences between counts1st and BeBold are unclear: most of the contribution seems to be related to the episodic restriction part. However, going beyond the boundaries of already explored regions seems to be promising as well, as it yields better results when compared to RND with episodic restriction [ref]31. The same comparative performance between IM methods holds when combining them with the ranking replay strategy, where RAPID+counts performs slightly better or equal to RAPID in isolation yet being the worst out of the 3 IM options. What is more, the choice of one IM strategy over another can actually deteriorate the performance of the agent, as observed in KS4R3. Nevertheless, when selecting demonstrably good IM strategies, the agent combining self-IL+IM improves its performance even when it was not able to do it just with the IM strategy in isolation. Exploration-exploitation parameters in self-IL+IM: by introducing IM into the on-policy loss, the agent has to deal with multiple objectives (exploration-exploitation) in various stages: 1) on-policy by balancing the extrinsic and intrinsic rewards; and 2) off-policy by keeping in the buffer the most promising experiences parameterized by the extrinsic, local and global scores. In this regard, Figure 3 depicts the evolution of some representative values concerning how the exploration is carried out during a training run. Initially Gi‚Äã(s)>Ge‚Äã(s)subscriptùê∫ùëñùë†subscriptùê∫ùëíùë†G_{i}(s)>G_{e}(s)444Refers to the episodic discounted intrinsic and extrinsic returns calculated as described in Section III-C. shows how the agent explores in the absence of extrinsic signals from the environment, yet eventually this gains more importance for the agent‚Äôs ability to complete the task. Similarly, the impact of the w0subscriptùë§0w_{0} score in Expression (1) ‚Äì that promotes the exploitation of highly extrinsic rewarded episodes ‚Äì quickly increases. However, the selection criterion is also subject to the w1subscriptùë§1w_{1} local score, which aims to maximize the diversity of observations inside the episode. In a much lower scale, the global scale w2subscriptùë§2w_{2} also plays its role in the selection criterion, which can be helpful during the initial learning stages, when there are no success episodes to complete the task, and also to untie when two episodes require the same amount of steps for the task completion. We proceed by evincing how the on-policy:off-policy ratio Œæùúâ\\xi between the number of on-policy and off-policy updates change over the curse of training. On-policy optimization steps are executed once a trajectory has been finished, and it remains equal during the whole training. Off-policy updates are instead applied once an episode finishes, which varies depending on the maximum steps per episode configured for each environment, and also on the optimality of the agent‚Äôs policy at that moment. The decision to execute off-policy updates at the end of the episode was retrieved from the original paper where RAPID was proposed . Such ratio Œæùúâ\\xi can change from 1:1 to 1:3 in MultiRoom environments, and more dramatically in other scenarios like KS4R3, which initially implies a ratio of 4:1 and can evolve up to a 4:13 relation. In words, the off-policy loss can undergo a modification in its schedule that makes it update more than 10√ó10\\times at its initial frequency (Table I). Such a balance has a critical importance in the agent‚Äôs learning process, as it would turn to optimize what is stored in the buffer rather than what is actually experiencing (or vice versa). This generates in turn a big shift between both methods. In fact, in IL this ratio is usually balanced by either using a weight when combining both losses or by selecting the frequency update [ref]19 . Scheduling the self-IL updates. To shed further light on the importance of the aforementioned ratio, we now fix the off-policy loss to be constant and subject directly to the on-policy updates. We then analyze how the performance varies under several values for this ratio. Figure 4 summarizes the results obtained for this study. In the family of MultiRoom scenarios, the agent is very sensitive to a reduction of the frequency of the off-policy updates, which can eventually make the agent fail when increasing their complexity (10:1 in MN12S10). Contrarily, in KS4R3 the original adopted schema with a ratio of 4:1 performs much better than a more frequent update of the off-policy part (1:1). This fact is also observed when using a more conservative ratio of 10:1, suggesting that, although a higher off-policy update frequency can be beneficial at initial stages to bootstrap the learning process in hard exploration tasks, it can eventually degrade the learned knowledge in the long term. These conclusions can also be inferred when using BeBold, but with a better sample-efficiency and optimal solutions. Similar conclusions hold when analyzing O2Dlh. Addressing inter-episode variance. So far, the selected ratio seems to be decisive for the success and sample-efficiency of the training process. However, the obtained outcomes are very noisy and barely close to optimal results, which can be due to one of the two losses being unstable. While the seminal work presenting RAPID used PPO with a rollout size555The rollout size is directly related with the number and minibatch size. The increment of the first implies that the minibatch size is also incremented (for the same number of minibatches). of n‚Äãs‚Äãt‚Äãe‚Äãp=T=128ùëõùë†ùë°ùëíùëùùëá128nstep=T=128, other similar works considering the same environment used a larger time horizon n‚Äãs‚Äãt‚Äãe‚Äãp=T=2048ùëõùë†ùë°ùëíùëùùëá2048nstep=T=2048, with better and more stable results [ref]15 . As mentioned in Section IV-A, each episode is configured differently depending on the selected seed. Consequently, by training the agent with less episodes in a single update, it might get biased to learn specific features present in that subset of episodes, rather than getting the required high level skills to solve the desired task in the whole possible episode distribution. Hence, the increment of the rollout size implies that the agent will be trained (in the on-policy update) with a larger set of episodes (see Table I to check for episode lengths) which forces the algorithm to extract generalizable knowledge in this wider set of slightly different environments, preventing from a by-heart learning.\nFurthermore, this also reduces the variance of the on-policy updates through the neural network, as the minibatch size itself will be larger. However, the agent will perform less optimization steps during the training process for the same amount of steps/frames. In this context, how does the use of larger rollout size impact on the on-policy update regarding the performance and the stabilization of the learned knowledge? The answer to this question can be found by analyzing Figure 5. The on-policy update is substantially improved, which can be seen in how BeBold performs without being corrupted by off-policy updates, being such IM solution able to solve all the environments with the expected optimal steps and obtaining the best result in both KS4R3 and O2Dlh. Contrarily, RAPID performs worse, and its contribution when combined with BeBold is also not as good as it has been observed in the previous analysis. The reason for such bad results is also connected to what we have previously highlighted: the ratio Œæùúâ\\xi. By increasing the rollout size and making the off-policy updates be subject to the episode completion, the off-policy loss grows up to be 14√ó14\\times, 8√ó8\\times, 4√ó4\\times and 4√ó4\\times more frequent than the on-policy counterpart in MN7S8, MN12S10, KS4R3 and O2Dlh, respectively, just at the start of the training process (Table I). As we have already seen in Figure 4, those kind of ratios does not necessarily mean to foster a better learning. Thus, when adjusting the ratio again with the new rollout size the performance of both RAPID and RAPID+BeBold drastically changes, as seen in Figure 6. In first instance, it can be seen a better sample-efficiency when using a more conservative ratio (1:1) in both KS4R3 and O2Dlh. This also happens when decreasing the off-policy updates up to a 10:1 ratio, where the convergence speed can be affected although it manages to achieve the optimal policy in less steps (the 1:1 ratio struggles more to finally achieve it). In contrast, when using the criteria of applying those updates at the end of the episode, which corresponds with approximately a 1:4 ratio initially in KS4R3 and O2Dlh (Table I), the results get worse, just surpassed by the BeBold approach. These behaviors strengthen our claim: the off-policy loss can help to improve the learning process, although using it in excess can be counter-productive. This is related to what is actually aiming to replay and if it is worth the value that update for the agent at that moment. Concerning MultiRoom environments, increasing the number of off-policy updates seems to be a good strategy, which is difficult to be beaten even by other state-of-the-art solutions. In fact, decreasing the frequency of the replayed experiences has a negative impact that can lead the agent to not learning in the absence of intrinsic rewards.\n\nReference markers found: [15]\n\nContext windows for each [ref]:\n[ref]15: but with episodic restriction. This second set of experiments allows comparing very similar IM strategies that have proven to have different results due to their intrinsic reward generation schema [ref]15 [ref]31. The results provided in Figure 2 suggest that there is a high relationship between what the agent can learn with IM (without self-IL) and what it actually does by combining them\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 15,\n            \"titles\": [\n                \"Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments\",\n                \"Human-level control through deep reinforcement learning\",\n                \"Guidelines for reinforcement learning in healthcare\",\n                \"A review on reinforcement learning: Introduction and applications in industrial process control\",\n                \"Reinforcement learning in robotics: A survey\",\n                \"Mastering the game of go without human knowledge\",\n                \"Grandmaster level in starcraft ii using multi-agent reinforcement learning\",\n                \"A Generalist Agent\",\n                \"Learning for a robot: Deep reinforcement learning, imitation learning, transfer learning\",\n                \"Awac: Accelerating online reinforcement learning with offline datasets\",\n                \"Learning Montezuma‚Äôs Revenge from a Single Demonstration\",\n                \"Self-Imitation Learning\",\n                \"Combining Experience Replay with Exploration by Random Network Distillation\",\n                \"Co-Imitation Learning without Expert Demonstration\",\n                \"Minimalistic gridworld environment for OpenAI Gym\",\n                \"An Evaluation Study of Intrinsic Motivation Techniques applied to Reinforcement Learning over Hard Exploration Environments\",\n                \"Prioritized experience replay\",\n                \"Simplifying deep reinforcement learning via self-supervision\",\n                \"Deep Q-learning from Demonstrations\",\n                \"Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards\",\n                \"Overcoming Exploration in Reinforcement Learning with Demonstrations\",\n                \"Reinforcement Learning with Supervision from Noisy Demonstrations\",\n                \"Neural Program Synthesis with Priority Queue Training\",\n                \"Self-imitation learning in sparse reward settings\",\n                \"Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards\",\n                \"First return, then explore\",\n                \"Self-Imitation Learning from Demonstrations\",\n                \"Unifying count-based exploration and intrinsic motivation\",\n                \"Exploration by Random Network Distillation\",\n                \"Curiosity-driven Exploration by Self-supervised Prediction\",\n                \"Noveld: A simple yet effective exploration criterion\",\n                \"MADE: Exploration via Maximizing Deviation from Explored Regions\",\n                \"RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments\",\n                \"Adversarially Guided Actor-Critic\",\n                \"Solving hard-exploration problems with counting and replay approach\",\n                \"Understanding the World Through Action\",\n                \"An algorithmic perspective on imitation learning\",\n                \"Generated Reference Title 37 for [ref]15\",\n                \"Generated Reference Title 38 for [ref]15\",\n                \"Generated Reference Title 39 for [ref]15\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]15 based on the context and research area of Towards Improving Exploration in Self-Imitation Learning using Intrinsic Motivation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}