{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects1footnote 11footnote 1Minor Revision, AIAA Journal of Guidance, Control, and Dynamics (YouTube video: https://youtu.be/AhDPE-R5GZ4)\n\nPaper Abstract: \n\nPaper Section Text:\nSince an SN-DNN is Lipschitz bounded by design and robust to perturbation, the optimality gap of the guidance framework uℓsubscript𝑢ℓu_{\\ell} introduced in Sec. 3.2 can be bounded as in the following theorem, where the notations are summarized in Table 2 and the proof concept is illustrated in Fig. 5. Suppose that umpcsubscript𝑢mpcu_{\\mathrm{mpc}} is Lipschitz with its 222-norm Lipschitz constant Lmpc∈ℝ>0subscript𝐿mpcsubscriptℝabsent0L_{\\mathrm{mpc}}\\in\\mathbb{R}_{>0}. If uℓsubscript𝑢ℓu_{\\ell} is trained using the empirical loss function (9) of Remark 3 to have ∃ϵtrain∈ℝ≥0subscriptitalic-ϵtrainsubscriptℝabsent0\\exists\\epsilon_{\\mathrm{train}}\\in\\mathbb{R}_{\\geq 0} s.t. then we have the following bound: where r​(x,œ,t,ρ)=infi∈ℕ∪[1,Nd]‖x¯i−x‖2+‖œ¯i−œ‖2+(t¯i−t)2+‖ρ¯i−ρ‖2𝑟𝑥œ𝑡𝜌subscriptinfimum𝑖ℕ1subscript𝑁𝑑superscriptnormsubscript¯𝑥𝑖𝑥2superscriptnormsubscript¯œ𝑖œ2superscriptsubscript¯𝑡𝑖𝑡2superscriptnormsubscript¯𝜌𝑖𝜌2r(x,\\textit{\\oe},t,\\rho)=\\inf_{i\\in\\mathbb{N}\\cup[1,N_{d}]}\\sqrt{\\|\\bar{x}_{i}-x\\|^{2}+\\|\\bar{\\textit{\\oe}}_{i}-\\textit{\\oe}\\|^{2}+(\\bar{t}_{i}-t)^{2}+\\|\\bar{\\rho}_{i}-\\rho\\|^{2}}. Let η=(x,œ,t,ρ)𝜂𝑥œ𝑡𝜌\\eta=(x,\\textit{\\oe},t,\\rho) be a test element in 𝒮testsubscript𝒮test\\mathcal{S}_{\\mathrm{test}} (i.e., η∈𝒮test𝜂subscript𝒮test\\eta\\in\\mathcal{S}_{\\mathrm{test}}) and let ζ¯j​(η)subscript¯𝜁𝑗𝜂\\bar{\\zeta}_{j}(\\eta) be the training data point in ΠtrainsubscriptΠtrain\\Pi_{\\mathrm{train}} (i.e., ζ¯j​(η)∈Πtrainsubscript¯𝜁𝑗𝜂subscriptΠtrain\\bar{\\zeta}_{j}(\\eta)\\in\\Pi_{\\mathrm{train}}) that achieves the infimum of r​(η)𝑟𝜂r(\\eta) with i=j𝑖𝑗i=j for a given η∈𝒮test𝜂subscript𝒮test\\eta\\in\\mathcal{S}_{\\mathrm{test}}. Since uℓsubscript𝑢ℓu_{\\ell} and umpcsubscript𝑢mpcu_{\\mathrm{mpc}} are Lipschitz by design and by assumption, respectively, we have for any η∈𝒮test𝜂subscript𝒮test\\eta\\in\\mathcal{S}_{\\mathrm{test}} that where the second inequality follows from the definition of ζ¯j​(η)subscript¯𝜁𝑗𝜂\\bar{\\zeta}_{j}(\\eta) and the third inequality follows from (10) and the fact that ζ¯j​(η)∈Πtrainsubscript¯𝜁𝑗𝜂subscriptΠtrain\\bar{\\zeta}_{j}(\\eta)\\in\\Pi_{\\mathrm{train}}. This relation leads to the desired result (11) as it holds for any η∈𝒮test𝜂subscript𝒮test\\eta\\in\\mathcal{S}_{\\mathrm{test}}.\n∎ The SN-DNN provides a verifiable optimality gap even for data points not in its training set, which indicates that it still benefits from the near-optimal guarantee of the MPC in terms of dynamic regret as discussed below (5). As illustrated in Fig. 5, each term in the optimality gap (11) can be interpreted as follows: ϵtrainsubscriptitalic-ϵtrain\\epsilon_{\\mathrm{train}} of (10) is the training error of the SN-DNN, which is expected to decrease as the learning proceeds using SGD. In general, the value of ϵtrainsubscriptitalic-ϵtrain\\epsilon_{\\mathrm{train}} is affected by the choice of Lipschitz constant Lℓsubscript𝐿ℓL_{\\ell} (see [ref]27 for more details). r​(η)𝑟𝜂r(\\eta) is the closest distance from the test element η∈𝒮test𝜂subscript𝒮test\\eta\\in\\mathcal{S}_{\\mathrm{test}} to a training data point in ΠtrainsubscriptΠtrain\\Pi_{\\mathrm{train}}, expected to decrease as the number of data points Ndsubscript𝑁𝑑N_{d} in ΠtrainsubscriptΠtrain\\Pi_{\\mathrm{train}} increases and as the training set 𝒮trainsubscript𝒮train\\mathcal{S}_{\\mathrm{train}} gets larger. The Lipschitz constant Lℓsubscript𝐿ℓL_{\\ell} is a design parameter we could arbitrarily choose when constructing the SN-DNN. Note that the SN-DNN is Lipschitz by design by spectrally normalizing the weights of the neural network , and thus we can freely choose Lℓsubscript𝐿ℓL_{\\ell} independently of the choice of the neural net parameters. We could also treat Lmpcsubscript𝐿mpcL_{\\mathrm{mpc}} as a design parameter by adding a Lipschitz constraint, ‖∂umpc/∂η‖≤Lmpcnormsubscript𝑢mpc𝜂subscript𝐿mpc\\|\\partial u_{\\mathrm{mpc}}/\\partial\\eta\\|\\leq L_{\\mathrm{mpc}}, in solving (3). Note that ϵtrainsubscriptitalic-ϵtrain\\epsilon_{\\mathrm{train}} and r​(η)𝑟𝜂r(\\eta) can always be computed numerically for a given η𝜂\\eta as the dataset ΠtrainsubscriptΠtrain\\Pi_{\\mathrm{train}} only has a finite number of data points. The obvious downsides of the optimality gap (11) are that (i) it still suffers the generalization issue, which is common in the field of machine learning, and (ii) we may have to empirically find the region that the MPC is locally Lipschitz. Also, in practice, although the SN-DNN is originally designed for a better generalization property, a smaller Lipschitz constant Lℓsubscript𝐿ℓL_{\\ell} could lead to a larger training error ϵtrainsubscriptitalic-ϵtrain\\epsilon_{\\mathrm{train}}, requiring some tuning by trading-off the training error and test error in real-world simulations (see Sec. 6 and Sec. 7) [ref]27. The benefit of having Lemma 1 is to explicitly describe all the assumptions we need to mathematically obtain the bound on the learning error, without treating it like a black box. For controllers whose optimal solutions are not necessarily Lipsthiz (e.g., impulsive-type controllers), the SN-DNN is not a suitable representation due to its Lipschitz assumption, which is one of the limitations of the bound in Lemma 1. Although we have demonstrated the practicality of this assumption for our cases in Sec. 7 (and in  with the comparison of our method with impulsive maneuvers), improving the expressibility of our method is required for its more general applications with more general mathematical guarantees. This is left as one of our future works. Obtaining a tighter and more general optimality gap of the learned control and state trajectories has been an active field of research. Particularly for a neural network equipped with a dynamical structure as in our proposed approach, we could refer to generalization bounds for the neural ordinary differential equations [ref]6, or use systems and control theoretical methods to augment it with stability and robustness properties [ref]28 . We could also consider combining the SN-DNN with neural networks that have enhanced structural guarantees of robustness, including robust implicit networks  and robust equilibrium networks . The optimality gap discussed in Lemma 1 and in Remark 5 is useful in that they provide mathematical guarantees for learning-based frameworks only with the Lipschitz assumption (e.g., it could prove safety in the learning-based MPC framework ). However, when the system is perturbed by the state uncertainty as in (1), we could only guarantee that the distance between the state trajectories controlled by umpcsubscript𝑢mpcu_{\\mathrm{mpc}} of (5) and uℓsubscript𝑢ℓu_{\\ell} of Lemma 1 is bounded by a function that increases exponentially with time . In Sec. 4 and 5, we will see how such a conservative bound can be replaced by a decreasing counterpart.\n\nReference markers found: [27]\n\nContext windows for each [ref]:\n[ref]27: decrease as the learning proceeds using SGD. In general, the value of ϵtrainsubscriptitalic-ϵtrain\\epsilon_{\\mathrm{train}} is affected by the choice of Lipschitz constant Lℓsubscript𝐿ℓL_{\\ell} (see [ref]27 for more details). r​(η)𝑟𝜂r(\\eta) is the closest distance from the test element η∈𝒮test𝜂subscript𝒮test\\eta\\in\\mathcal{S}_{\\mathrm{test}} to a training data point in\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 27,\n            \"titles\": [\n                \"Robust Classification using Contractive Hamiltonian Neural ODEs\",\n                \"A Brief Visit From a Red and Extremely Elongated Interstellar Asteroid\",\n                \"Initial Characterization of Interstellar Comet 2I/Borisov\",\n                \"Mission Design Study for Objects on Oort Cloud Comet Orbits\",\n                \"Spectral Normalization for Generative Adversarial Networks\",\n                \"The Power of Predictions in Online Control\",\n                \"Neural Ordinary Differential Equations\",\n                \"Adaptive-Control-Oriented Meta-Learning for Nonlinear Systems\",\n                \"Learning-based Model Predictive Control: Toward Safe Learning in Control\",\n                \"On Contraction Analysis for Nonlinear Systems\",\n                \"Applied Nonlinear Control\",\n                \"Interstellar Object Accessibility and Mission Design\",\n                \"Model Predictive Control: Past, Present and Future\",\n                \"A Six Degree-of-freedom Spacecraft Dynamics Simulator for Formation Control Research\",\n                \"Adaptive Dynamic Programming: An Introduction\",\n                \"Differential Dynamic Programming with Nonlinear Constraints\",\n                \"Neural-Fly Enables Rapid Learning for Agile Flight in Strong Winds\",\n                \"Autonomous Optical Navigation (AutoNav) DS1 Technology Validation Report\",\n                \"Navigation Evaluation for Fast Interstellar Object Flybys\",\n                \"Robust Model Predictive Control: A Survey\",\n                \"Contraction Theory for Nonlinear Stability Analysis and Learning-based Control: A Tutorial Overview\",\n                \"Swarm-keeping Strategies for Spacecraft under J2subscript𝐽2J_{2} and Atmospheric Drag Perturbations\",\n                \"Analytical Mechanics of Space Systems\",\n                \"Application of Synchronization to Formation Flying Spacecraft: Lagrangian Approach\",\n                \"A Contraction Theory-based Analysis of the Stability of the Deterministic Extended Kalman Filter\",\n                \"Information-based Guidance and Control Architecture for Multi-Spacecraft On-Orbit Inspection\",\n                \"Nonautonomous Dynamical Systems\",\n                \"Spectrally-normalized margin bounds for neural networks\",\n                \"LyaNet: A Lyapunov Framework for Training Neural ODEs\",\n                \"Robust Implicit Networks via Non-Euclidean Contractions\",\n                \"Recurrent Equilibrium Networks: Unconstrained Learning of Stable and Robust Dynamical Models\",\n                \"A Review of Space Robotics Technologies for On-orbit Servicing\",\n                \"Trajectory Generation and Control for Precise Aggressive Maneuvers with Quadrotors\",\n                \"Rapidly Exponentially Stabilizing Control Lyapunov Functions and Hybrid Zero Dynamics\",\n                \"Stochastic Stability and Control\",\n                \"Control Barrier Function Based Quadratic Programs with Application to Adaptive Cruise Control\",\n                \"Robust Controller Design for Stochastic Nonlinear Systems via Convex Optimization\",\n                \"Trajectory Optimization of Chance-Constrained Nonlinear Stochastic Systems for Motion Planning Under Uncertainty\",\n                \"GLAS: Global-to-Local Safe Autonomy Synthesis for Multi-Robot Motion Planning with End-to-End Learning\",\n                \"Swarm Assignment and Trajectory Optimization using Variable-swarm, Distributed Auction Assignment and Sequential Convex Programming\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]27 based on the context and research area of Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects1footnote 11footnote 1Minor Revision, AIAA Journal of Guidance, Control, and Dynamics (YouTube video: https://youtu.be/AhDPE-R5GZ4). The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}