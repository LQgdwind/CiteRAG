{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Revealing Unfair Models by Mining Interpretable Evidence\n\nPaper Abstract: \n\nPaper Section Text:\nIn this subsection, we first define the RUMIE task. Then we propose a probabilistic framework to model the RUMIE task as a continuous optimization problem. Last, we discuss the key idea to incorporate some other group fairness measurements into our framework. Given a classification model g𝑔g, a set of data instances 𝒟𝒟\\mathcal{D}, and a set of sensitive attributes 𝒜𝒜\\mathcal{A} that may be potentially related to the discrimination of g𝑔g on groups of instances in 𝒟𝒟\\mathcal{D}, the task of revealing unfair models by mining interpretable evidence (RUMIE) is to find the group of data instances, denoted by 𝒮⊂𝒟𝒮𝒟\\mathcal{S}\\subset\\mathcal{D}, that is the most discriminated by the model g𝑔g; and interpret the discrimination on the data instances in 𝒮𝒮\\mathcal{S} by finding a small set of key attributes, denoted by 𝒬⊆𝒜𝒬𝒜\\mathcal{Q}\\subseteq\\mathcal{A}, such that the data instances in 𝒮𝒮\\mathcal{S} are separated from those in 𝒟∖𝒮𝒟𝒮\\mathcal{D}\\setminus\\mathcal{S} in the feature space of 𝒬𝒬\\mathcal{Q}. Here, the feature space of 𝒬𝒬\\mathcal{Q}, denoted by 𝚂𝚙𝚊𝚌𝚎𝒬superscript𝚂𝚙𝚊𝚌𝚎𝒬\\mathtt{Space}^{\\mathcal{Q}}, is the space where a data instance is represented by the attributes in 𝒬𝒬\\mathcal{Q}.\nThe discriminated group 𝒮𝒮\\mathcal{S} is said to be separated from 𝒟∖𝒮𝒟𝒮\\mathcal{D}\\setminus\\mathcal{S} in 𝚂𝚙𝚊𝚌𝚎𝒬superscript𝚂𝚙𝚊𝚌𝚎𝒬\\mathtt{Space}^{\\mathcal{Q}} if 𝒮𝒮\\mathcal{S} and 𝒟∖𝒮𝒟𝒮\\mathcal{D}\\setminus\\mathcal{S} are classified as two different classes by the classification boundary of a binary classifier in 𝚂𝚙𝚊𝚌𝚎𝒬superscript𝚂𝚙𝚊𝚌𝚎𝒬\\mathtt{Space}^{\\mathcal{Q}}. To clearly explain to an end user why and how 𝒮𝒮\\mathcal{S} is separated from 𝒟∖𝒮𝒟𝒮\\mathcal{D}\\setminus\\mathcal{S},\nwe also want the classification boundary separating 𝒮𝒮\\mathcal{S} and 𝒟∖𝒮𝒟𝒮\\mathcal{D}\\setminus\\mathcal{S} to be simple.\nHere, a classification boundary is said to be simple if the partitions of data set 𝒟𝒟\\mathcal{D} induced by the classification boundary have a small minimum description length.\nFor example, a linear classification boundary of a logistic regression classifier is said to be simpler than a non-linear classification boundary of a deep neural network. We refer to (𝒮,𝒬)𝒮𝒬(\\mathcal{S},\\mathcal{Q}) as the interpretable evidence to demonstrate the unfairness of g𝑔g.\nThe discriminated group 𝒮𝒮\\mathcal{S} is the evidence to demonstrate the unfairness of g𝑔g because 𝒮𝒮\\mathcal{S} is discriminated the most by g𝑔g.\nA higher discrimination score on 𝒮𝒮\\mathcal{S} means more unfairness exists in g𝑔g.\nThe set of key attributes 𝒬𝒬\\mathcal{Q} allows us to interpret the reason behind the discrimination of 𝒮𝒮\\mathcal{S} because these attributes are used to separate the discriminated group 𝒮𝒮\\mathcal{S} from the other group 𝒟∖𝒮𝒟𝒮\\mathcal{D}\\setminus\\mathcal{S}. A simpler classification boundary to separate 𝒮𝒮\\mathcal{S} and 𝒟∖𝒮𝒟𝒮\\mathcal{D}\\setminus\\mathcal{S} in 𝚂𝚙𝚊𝚌𝚎𝒬superscript𝚂𝚙𝚊𝚌𝚎𝒬\\mathtt{Space}^{\\mathcal{Q}} indicates the existence of the discriminated group is less possible by chance, and thus is more significant. Tackling the RUMIE task requires to\nfind the optimal set of 𝒮𝒮\\mathcal{S} that maximizes the discrimination score D​S​c​o​r​e​(𝒮)𝐷𝑆𝑐𝑜𝑟𝑒𝒮DScore(\\mathcal{S}); and it also requires to find a small set of sensitive attributes denoted by 𝒬𝒬\\mathcal{Q} such that 𝒮𝒮\\mathcal{S} and 𝒟∖𝒮𝒟𝒮\\mathcal{D}\\setminus\\mathcal{S} are well separated by a simple decision boundary in 𝚂𝚙𝚊𝚌𝚎𝒬superscript𝚂𝚙𝚊𝚌𝚎𝒬\\mathtt{Space}^{\\mathcal{Q}}.\nThis can be done by a brute-force search approach that enumerates all values of 𝒮𝒮\\mathcal{S} and 𝒬𝒬\\mathcal{Q} to find their optimal values.\nHowever, the brute-force search approach does not scale up well on large data sets, because the time cost grows exponentially with respect to the size of 𝒟𝒟\\mathcal{D} and 𝒜𝒜\\mathcal{A}. To efficiently tackle the RUMIE task, we propose a probabilistic framework to model it as a continuous optimization problem, which can be efficiently solved by a gradient-based optimization method . The key idea is to model the discrete variables 𝒮𝒮\\mathcal{S} and 𝒬𝒬\\mathcal{Q} by functions of continuous variables, and then formulate the optimization objective of RUMIE as a continuous function. First, we model 𝒮𝒮\\mathcal{S} as a random set of data instances, where the probability of a data instance 𝐱∈𝒟𝐱𝒟\\mathbf{x}\\in\\mathcal{D} belonging to 𝒮𝒮\\mathcal{S} is modelled by\n Here, σ𝒜​(𝐱)subscript𝜎𝒜𝐱\\sigma_{\\mathcal{A}}(\\mathbf{x}) is the projection of 𝐱𝐱\\mathbf{x} to the feature space 𝚂𝚙𝚊𝚌𝚎𝒜superscript𝚂𝚙𝚊𝚌𝚎𝒜\\mathtt{Space}^{\\mathcal{A}} of the attributes in 𝒜𝒜\\mathcal{A}, f𝜽:𝚂𝚙𝚊𝚌𝚎𝒜→ :subscript𝑓𝜽→superscript𝚂𝚙𝚊𝚌𝚎𝒜01f_{\\bm{\\theta}}:\\mathtt{Space}^{\\mathcal{A}}\\rightarrow  is a continuous function parametrized by a vector 𝜽𝜽\\bm{\\theta} with |𝒜|𝒜{|\\mathcal{A}|} entries, and |𝒜|𝒜|\\mathcal{A}| is the number of attributes in 𝒜𝒜\\mathcal{A}. Every entry in 𝜽𝜽\\bm{\\theta} is a continuous real-valued variable. For a given set of data instances 𝒟𝒟\\mathcal{D}, the distribution of 𝒮𝒮\\mathcal{S} is determined by 𝜽𝜽\\bm{\\theta}.\nTherefore, 𝒮𝒮\\mathcal{S} is a function of the continuous variables in 𝜽𝜽\\bm{\\theta}. Second, we model 𝒬𝒬\\mathcal{Q} as a function of continuous variables by formulating f𝜽subscript𝑓𝜽f_{\\bm{\\theta}} as a logistic regression classifier, that is, where the none-zero entries in 𝜽𝜽\\bm{\\theta} correspond to the set of key attributes 𝒬∈𝒜𝒬𝒜\\mathcal{Q}\\in\\mathcal{A} because the data instances in 𝒮𝒮\\mathcal{S} and 𝒟∖𝒮𝒟𝒮\\mathcal{D}\\setminus\\mathcal{S} are well separated by the simple linear decision boundary induced by f𝜽subscript𝑓𝜽f_{\\bm{\\theta}} in 𝚂𝚙𝚊𝚌𝚎𝒬superscript𝚂𝚙𝚊𝚌𝚎𝒬\\mathtt{Space}^{\\mathcal{Q}}.\nSince the value of 𝒬𝒬\\mathcal{Q} is determined by 𝜽𝜽\\bm{\\theta}, 𝒬𝒬\\mathcal{Q} is a function of continuous variables. As illustrated later, we will enhance the interpretability of 𝒬𝒬\\mathcal{Q} by reducing the size of 𝒬𝒬\\mathcal{Q} with a sparsity constraint on 𝜽𝜽{\\bm{\\theta}} and generate a set of more interpretable rules by distilling f𝜽subscript𝑓𝜽f_{\\bm{\\theta}} into a simple decision tree. Third, we introduce how to formulate the optimization objective of the RUMIE task as a continuous function of 𝜽𝜽\\bm{\\theta}. Since the RUMIE task requires to find the group of data instances that is the most discriminated by the model g𝑔g, we need to find a discriminated group 𝒮𝒮\\mathcal{S} that maximizes the discrimination score D​S​c​o​r​e​(𝒮)𝐷𝑆𝑐𝑜𝑟𝑒𝒮DScore(\\mathcal{S}) in Equation (2). Since we model 𝒮𝒮\\mathcal{S} as a random set of data instances parametrized by 𝜽𝜽\\bm{\\theta}, the discrimination score D​S​c​o​r​e​(𝒮)𝐷𝑆𝑐𝑜𝑟𝑒𝒮DScore(\\mathcal{S}) is a real-valued random variable, which cannot be straight-forwardly maximized.\nTherefore, instead of directly maximizing the random variable D​S​c​o​r​e​(𝒮)𝐷𝑆𝑐𝑜𝑟𝑒𝒮DScore(\\mathcal{S}), we seek to maximize its expectation where 𝔼​[ℙ​(𝚏𝚊𝚟∣𝒟∖𝒮)]𝔼delimited-[]ℙconditional𝚏𝚊𝚟𝒟𝒮\\mathbb{E}[\\mathbb{P}(\\mathtt{fav}\\mid\\mathcal{D}\\setminus\\mathcal{S})] and 𝔼​[ℙ​(𝚏𝚊𝚟∣𝒮)]𝔼delimited-[]ℙconditional𝚏𝚊𝚟𝒮\\mathbb{E}[\\mathbb{P}(\\mathtt{fav}\\mid\\mathcal{S})] are the expectations of ℙ​(𝚏𝚊𝚟∣𝒮)ℙconditional𝚏𝚊𝚟𝒮\\mathbb{P}(\\mathtt{fav}\\mid\\mathcal{S}) and ℙ​(𝚏𝚊𝚟∣𝒟∖𝒮)ℙconditional𝚏𝚊𝚟𝒟𝒮\\mathbb{P}(\\mathtt{fav}\\mid\\mathcal{D}\\setminus\\mathcal{S}), respectively.\nHere, ℙ​(𝚏𝚊𝚟∣𝒮)ℙconditional𝚏𝚊𝚟𝒮\\mathbb{P}(\\mathtt{fav}\\mid\\mathcal{S}) and ℙ​(𝚏𝚊𝚟∣𝒟∖𝒮)ℙconditional𝚏𝚊𝚟𝒟𝒮\\mathbb{P}(\\mathtt{fav}\\mid\\mathcal{D}\\setminus\\mathcal{S}) are random variables because 𝒮𝒮\\mathcal{S} is a random set of data instances. Recall that ℙ​(𝚏𝚊𝚟∣𝒮)ℙconditional𝚏𝚊𝚟𝒮\\mathbb{P}(\\mathtt{fav}\\mid\\mathcal{S}) is the probability of a data instance 𝐱𝐱\\mathbf{x} receiving a favorable prediction from g𝑔g when it is uniformly sampled from 𝒮𝒮\\mathcal{S}. We write ℙ​(𝚏𝚊𝚟∣𝒮)ℙconditional𝚏𝚊𝚟𝒮\\mathbb{P}(\\mathtt{fav}\\mid\\mathcal{S}) as where 𝟙𝚏𝚊𝚟​(𝐱)∈{0,1}subscript1𝚏𝚊𝚟𝐱01\\mathbbm{1}_{\\mathtt{fav}}(\\mathbf{x})\\in\\{0,1\\} is an indicator function that is equal to 1 only if g𝑔g makes a favorable prediction on 𝐱𝐱\\mathbf{x}; and 𝟙​(𝐱∈𝒮)∈{0,1}1𝐱𝒮01\\mathbbm{1}(\\mathbf{x}\\in\\mathcal{S})\\in\\{0,1\\} is an indicator function that is equal to 1 only if 𝐱∈𝒮𝐱𝒮\\mathbf{x}\\in\\mathcal{S}. Since every data instance 𝐱𝐱\\mathbf{x} has a probability of ℙ​(𝐱∈𝒮)ℙ𝐱𝒮\\mathbb{P}(\\mathbf{x}\\in\\mathcal{S}) to belong to 𝒮𝒮\\mathcal{S}, the indicator function\n𝟙​(𝐱∈𝒮)1𝐱𝒮\\mathbbm{1}(\\mathbf{x}\\in\\mathcal{S}) is a random variable following a Bernoulli distribution, and its expectation is Since the numerator and denominator in Equation (6) are conditionally independent given 𝜽𝜽{\\bm{\\theta}}, we have Similarly, we can derive\n where ℙ​(𝐱∈𝒟∖𝒮)=1−ℙ​(𝐱∈𝒮)ℙ𝐱𝒟𝒮1ℙ𝐱𝒮\\mathbb{P}(\\mathbf{x}\\in\\mathcal{D}\\setminus\\mathcal{S})=1-\\mathbb{P}(\\mathbf{x}\\in\\mathcal{S}) is the probability of an instance 𝐱𝐱\\mathbf{x} not belonging to 𝒮𝒮\\mathcal{S}. Recall that ℙ​(𝐱∈𝒮)=f𝜽​(σ𝒜​(𝐱))ℙ𝐱𝒮subscript𝑓𝜽subscript𝜎𝒜𝐱\\mathbb{P}(\\mathbf{x}\\in\\mathcal{S})=f_{\\bm{\\theta}}(\\sigma_{\\mathcal{A}}(\\mathbf{x})) is a function of 𝜽𝜽\\bm{\\theta}. Thus both 𝔼​[ℙ​(𝚏𝚊𝚟∣𝒮)]𝔼delimited-[]ℙconditional𝚏𝚊𝚟𝒮\\mathbb{E}[\\mathbb{P}(\\mathtt{fav}\\mid\\mathcal{S})] and 𝔼​[ℙ​(𝚏𝚊𝚟∣𝒟∖𝒮)]𝔼delimited-[]ℙconditional𝚏𝚊𝚟𝒟𝒮\\mathbb{E}[\\mathbb{P}(\\mathtt{fav}\\mid\\mathcal{D}\\setminus\\mathcal{S})] are real-valued functions of 𝜽𝜽\\bm{\\theta}. Therefore, 𝔼​[D​S​c​o​r​e​(𝒮)]𝔼delimited-[]𝐷𝑆𝑐𝑜𝑟𝑒𝒮\\mathbb{E}[DScore(\\mathcal{S})] is a rea-valued function of 𝜽𝜽\\bm{\\theta}, which can be maximized with respect to 𝜽𝜽\\bm{\\theta}. Last, we model the RUMIE task as the following continuous optimization problem, which aims to maximize the expected discrimination score on 𝒮𝒮\\mathcal{S}. where λ>0𝜆0\\lambda>0, α∈ 𝛼01\\alpha\\in  and β∈ 𝛽01\\beta\\in , α≤β𝛼𝛽\\alpha\\leq\\beta, are positive real-valued hyper-parameters, 𝔼​[|𝒮|]𝔼delimited-[]𝒮\\mathbb{E}[|\\mathcal{S}|] is the expected size of 𝒮𝒮\\mathcal{S} and |𝒟|𝒟|\\mathcal{D}| is the size of 𝒟𝒟\\mathcal{D}. The second term λ∗𝒞​(k,𝜽)𝜆𝒞𝑘𝜽\\lambda*\\mathcal{C}(k,\\bm{\\theta}) in the objective of the optimization problem is a regularization term that reduces the size of 𝒬𝒬\\mathcal{Q} to enhance the interpretability of the discrimination evidence. Here k𝑘k refers to size of 𝒬𝒬\\mathcal{Q} and implies the maximum number of sensitive attributes to be considered for distinguishing 𝒮𝒮\\mathcal{S} from 𝒟∖𝒮𝒟𝒮\\mathcal{D}\\setminus\\mathcal{S}. Choosing k𝑘k provides the user with the flexibility to adjust the interpretability as per their needs.\nThe term is the L-1 reduced penalization term adopted from    . The function s​m​a​l​l​e​s​t​(𝜽,j)𝑠𝑚𝑎𝑙𝑙𝑒𝑠𝑡𝜽𝑗smallest(\\bm{\\theta},j) returns the j𝑗j-th smallest absolute value of the entries in 𝜽𝜽{\\bm{\\theta}}. Maximizing the objective minimizes the term 𝒞​(k,𝜽)𝒞𝑘𝜽\\mathcal{C}(k,\\bm{\\theta}), which enforces the vector 𝜽𝜽{\\bm{\\theta}} to contain k𝑘k effective non-zero entries while squeezing the rest of the entries to be very small or zero  .\nAfter finding a solution 𝜽∗superscript𝜽\\bm{\\theta}^{*} to the continuous optimization problem, we strictly enforce the maximum limit of k𝑘k sensitive attributes by keeping only the k𝑘k entries with the largest absolute values in 𝜽∗superscript𝜽\\bm{\\theta}^{*} and setting the other entries in 𝜽∗superscript𝜽\\bm{\\theta}^{*} to zero. Comparing to using the L1-norm of 𝜽𝜽\\bm{\\theta} as a regularization term to induce the sparsity of 𝜽𝜽\\bm{\\theta}, 𝒞​(k,𝜽)𝒞𝑘𝜽\\mathcal{C}(k,\\bm{\\theta}) provides a better control on the number of non-zero entries in 𝜽𝜽\\bm{\\theta}   .\nA small value of k𝑘k leads to a small set of 𝒬𝒬\\mathcal{Q} thus produces a clear interpretation for the reason behind discrimination of a group. In the constraint of the optimization problem,\nα𝛼\\alpha and β𝛽\\beta constrain the expected proportion taken by 𝒮𝒮\\mathcal{S} in 𝒟𝒟\\mathcal{D}. Since different end users may be interested in different sizes of discriminated instance groups for different applications, the thresholds α𝛼\\alpha and β𝛽\\beta provide a good flexibility to suit the needs of different end users.\nFor example, a bank’s fairness policy may only be violated if at least 20% of its clients are being discriminated by a deployed AI lender model. Next, we discuss how to incorporate some other group fairness measurements, such as disparate impact , difference of F1-scores , equal odds , and equal opportunities  into the proposed framework. The key idea is to extend a fairness measurement into a discrimination score D​S​c​o​r​e​(𝒮)𝐷𝑆𝑐𝑜𝑟𝑒𝒮DScore(\\mathcal{S}), and then compute the expected discrimination score 𝔼​[D​S​c​o​r​e​(𝒮)]𝔼delimited-[]𝐷𝑆𝑐𝑜𝑟𝑒𝒮\\mathbb{E}[DScore(\\mathcal{S})] on the distribution of 𝒮𝒮\\mathcal{S}.\nSince the distribution of 𝒮𝒮\\mathcal{S} is parameterized by 𝜽𝜽\\bm{\\theta}, 𝔼​[D​S​c​o​r​e​(𝒮)]𝔼delimited-[]𝐷𝑆𝑐𝑜𝑟𝑒𝒮\\mathbb{E}[DScore(\\mathcal{S})] is a continuous function of 𝜽𝜽\\bm{\\theta}, and it can be directly plugged in Equation (11) to formulate a continuous optimization problem for the RUMIE task. It is straight-forward to extend a fairness measurement to a discrimination score.\nThe real-valued measurements such as difference of F1-scores  and disparate impact  can be directly used as discrimination scores and can be optimized with respect to 𝜽𝜽\\bm{\\theta}.\nThe boolean measurements such as equal odds , and equal opportunities  can be extended to a real-valued discrimination score in a similar way as we extend demographic parity  4 [ref]22.\nDeriving the expectation of a discriminated score on the distribution of 𝒮𝒮\\mathcal{S} is straight-forward, thus we skip the details.\n\nReference markers found: [22]\n\nContext windows for each [ref]:\n[ref]22: to 𝜽𝜽\\bm{\\theta}.\nThe boolean measurements such as equal odds , and equal opportunities  can be extended to a real-valued discrimination score in a similar way as we extend demographic parity  4 [ref]22.\nDeriving the expectation of a discriminated score on the distribution of 𝒮𝒮\\mathcal{S} is straight-forward, thus we skip the details.\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 22,\n            \"titles\": [\n                \"Satisfying Real-world Goals with Dataset Constraints\",\n                \"Criminal justice forecasts of risk: A machine learning approach\",\n                \"Interpretable machine learning in healthcare\",\n                \"Applications of machine learning in drug discovery and development\",\n                \"Credit risk assessment using statistical and machine learning: basic methodology and risk modeling applications\",\n                \"Machine bias\",\n                \"Amazon just showed us that ‘unbiased’ algorithms can be inadvertently racist.\",\n                \"Data preprocessing techniques for classification without discrimination\",\n                \"Bias in Machine Learning Software: Why? How? What to Do?\",\n                \"Fairness-aware classifier with prejudice remover regularizer\",\n                \"Exploiting reject option in classification for social discrimination control\",\n                \"Fair Decision Rules for Binary Classification\",\n                \"Machine learning interpretability: A survey on methods and metrics\",\n                \"Statistical hypothesis testing: theory and methods\",\n                \"Fairness through awareness\",\n                \"A multidisciplinary survey on discrimination analysis\",\n                \"k-nn as an implementation of situation testing for discrimination discovery and prevention\",\n                \"Situation testing-based discrimination discovery: A causal inference approach.\",\n                \"Data mining for discrimination discovery\",\n                \"Fair-db: Functional dependencies to discover data bias.\",\n                \"Fairness testing: testing software for discrimination\",\n                \"Three naive bayes approaches for discrimination-free classification\",\n                \"Fairness in criminal justice risk assessments: The state of the art\",\n                \"Weapons of math destruction: How big data increases inequality and threatens democracy\",\n                \"Automating inequality: How high-tech tools profile, police, and punish the poor\",\n                \"The Frontiers of Fairness in Machine Learning\",\n                \"A comparative study of fairness-enhancing interventions in machine learning\",\n                \"Fairness definitions explained\",\n                \"‘It’s Reducing a Human Being to a Percentage’; Perceptions of Justice in Algorithmic Decisions\",\n                \"Inherent Trade-Offs in the Fair Determination of Risk Scores\",\n                \"Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment\",\n                \"Equality of Opportunity in Supervised Learning\",\n                \"Ai fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias\",\n                \"The What-If Tool: Interactive Probing of Machine Learning Models\",\n                \"Automated Directed Fairness Testing\",\n                \"Certifying and removing disparate impact\",\n                \"Building classifiers with independency constraints\",\n                \"FairFL: A fair federated learning approach to reducing demographic bias in privacy-sensitive classification models\",\n                \"Fairness in learning: Classic and contextual bandits\",\n                \"Empirical Risk Minimization Under Fairness Constraints\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]22 based on the context and research area of Revealing Unfair Models by Mining Interpretable Evidence. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}