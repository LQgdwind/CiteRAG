{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Unifying Relational Sentence Generation and Retrieval for Medical Image Report Composition\n\nPaper Abstract: \n\nPaper Section Text:\nEvaluation metrics.\nWe conduct experiments to evaluate our method from three aspects: 1) Area Under the Curve (AUC) measures the performance of abnormality classification; 2) Automatic evaluation metrics consist of BLEU , ROUGE  and CIDEr . BLEU is defined as the geometric mean of n-gram precision scores multiplied by a brevity penalty for short sentences. ROUGE-L basically measures the longest common subsequences between a pair of sentences and CIDEr measures the consensus between candidate image description and the reference sentences; 3) Human evaluation is performed by the radiologists with professional experiments. We invite 5 doctors to select the better generated reports between the baseline method CoAtt  and oursâ€™ proposed model. The selection criteria consists of abnormal findings, language fluency, and content coverage. Baselines.\nWe compare our Relation-paraNet with 4 state-of-the-art image captioning methods. They are CNN-RNN , LRCN , AdaAtt , and Att2in  respectively. Besides the above methods, we also compare with state-of-the-art methods of medical report composition, including AoA , Transformer [ref]35, BUTD 0, CoAtt , HRGR-Agent , and KERP 9. For the fair comparison, we employ VGG-19  and DenseNet121  to extract visual features on IU X-ray and CX-CHR dataset respectively, in step with baselines. Further, we conduct additional experiments on both datasets to illustrate how each component contributes to the final results. Abnormality classification.\nThe comparison of multi-label abnormality classification in terms of AUC metric is shown in Table I. Superior to the baselines that simply apply VGG-19 or DenseNet-121 as backbone network with binary cross entropy loss, our method adds relationship constraint loss to exploit the relationships among abnormal medical terms, which outperforms baselines by 0.9% of AUC on IU X-Ray dataset and 1.9% on CX-CHR dataset. This demonstrates that our method is able to obtain useful visual features for correct abnormality classification via relationship constraint of medical terms. Its prominent capacity of detecting abnormalities is helpful for radiologists. Automatic evaluation. The automatic evaluation results under several metrics are shown in the Table II. Obviously, our Relation-paraNet improves all metrics by large margins, demonstrating its effectiveness and extensiveness. CIDEr score represents inverse document frequency (IDF) of each vocabulary in the whole evaluated dataset, while BLEU-n matches n-grams within the evaluated sentences and ground truth sentences for each testing sample. In fact, improving the performance of BLEU-n with shorter n-gram (e.g., BLEU-1) is easier through generating common and seemingly correct words. On the contrary, CIDEr metric pays more attention to the critical but rare words under the consideration of IDF. Thus the CIDEr metric is more important on medical report composition, especially for the small dataset with unbalanced data. On IU X-Ray dataset (i.e., a relatively small dataset including unbalanced data), the Relation-paraNet outperforms all baselines models (based on VGG-19 ) on BLEU-1,2,3,4 scores, showing that the Relational-Topic Encoder and the Adaptive Generator contribute to generating more professional reports.\nHRGR-Agent  combines both the retrieval method and the generative model with reinforcement learning. Thus, it is reasonable that HRGR-Agent achieves the higher CIDEr score. However, our Ralation-paraNet still achieves the best CIDEr score, outperforming HRGR-Agent by 2.2% due to the semantic consistency of abnormal medical terms and final reports encourage to generate rare abnormal descriptions.\nIn addition, when employing the DenseNet-121  backbone as the recent work KERP 9, we can achieve much higher performance on all automatic metrics. On CX-CHR dataset, Relation-paraNet achieves state-of-the-art performance. Our model improves CIDEr score by 0.399, ROUGE-L score by 5.7%, BLEU-1 score by 3.8%, BLEU-2 score by 4.9%, BLEU-3 score by 5.4% and BLEU-4 score by 7.5%, compared to KERP 9. Incorporating the semantic consistency of medical terms and considering abnormal sentences as templates, our Relation-paraNet encourages to produce reasonable and meaningful reports and achieves the best performance on automatic evaluation metrics. To verify the effectiveness of our Relation-paraNet for medical report generation, we conduct extensive comparisons against several state-of-the-art visual captioning methods, such as AoA , Transformer [ref]35 and BUTD 0. As we discussed in Sec. II, the above attention-based methods are inapplicable to medical reports, which consists of multiple sentences or paragraphs. And the results show that our Relation-paraNet outperforms these methods on all automatic evaluation metrics, demonstrating the superiority in retrieving abnormal sentences and generation normal ones. Human evaluation. Since Hybrid-agent  and KERP 9 have not released codes and models, our method is also compared with CoAtt  in human evaluation, similar to them. We randomly select 100 generated medical reports of baseline model CoAtt  and our methods in the test set. We invite five doctors to choose which one is more similar to ground truth under the consideration of abnormal findings, language fluency, and content coverage. We calculate the average preference percentages by excluding default choices (A default choice is provided in case of no or both reports are preferred), which is shown in Table IV. Obviously, our Relation-paraNet outperforms baseline a lot, indicating that it is capable of generating more accurate and reasonable reports. Conclusion. Compared with Hybrid-agent , the proposed Relation-paraNet firstly preserves the superiority of bottom-up and top-down attention 0 for better semantic alignment between visual features and report descriptions. In addition, it is easier to generate abnormal descriptions by establishing the abnormal template datasets. Based on multi-layer and multi-head attention(Transformer) [ref]35, KERP 9 built abnormality graph to learn relationships between abnormalities implicitly, the attention weights in which are lack of consistency and explanation 9. In contrast, the Relation-paraNet contains a relationship constraint loss function to learn label co-occurrence dependencies directly. Since the popular visual captioning methods  [ref]35 0 is limited for paragraph generation, our Relation-paraNet derived the hierarchical structure from 7 and achieved excellent performance in terms of paragraph generation.\n\nReference markers found: [35]\n\nContext windows for each [ref]:\n[ref]35: They are CNN-RNN , LRCN , AdaAtt , and Att2in  respectively. Besides the above methods, we also compare with state-of-the-art methods of medical report composition, including AoA , Transformer [ref]35, BUTD 0, CoAtt , HRGR-Agent , and KERP 9. For the fair comparison, we employ VGG-19  and DenseNet121  to extract visual features on IU X-ray and CX-CHR dataset respectively, in step with\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 35,\n            \"titles\": [\n                \"Attention-Based Multimodal Fusion for Video Description\",\n                \"Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation\",\n                \"On the Automatic Generation of Medical Imaging Reports\",\n                \"An inception convolutional autoencoder model for chinese healthcare question clustering\",\n                \"Densely connected convolutional networks\",\n                \"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\n                \"Weakly supervised deep learning for whole slide lung cancer image analysis\",\n                \"Ultrasound standard plane detection using a composite neural network framework\",\n                \"Incorporating Copying Mechanism in Sequence-to-Sequence Learning\",\n                \"A reinforced topic-aware convolutional sequence-to-sequence model for abstractive text summarization\",\n                \"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\",\n                \"Multimodal Dual Attention Memory for Video Story Question Answering.\",\n                \"Imagenet classification with deep convolutional neural networks\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Long short-term memory\",\n                \"Learning phrase representations using rnn encoder-decoder for statistical machine translation\",\n                \"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\n                \"Rich feature hierarchies for accurate object detection and semantic segmentation\",\n                \"Show and Tell: A Neural Image Caption Generator\",\n                \"Show, attend and tell: Neural image caption generation with visual attention\",\n                \"Dual-stream recurrent neural network for video captioning\",\n                \"Know more say less: Image captioning based on scene graphs\",\n                \"Video captioning with attention-based lstm and semantic consistency\",\n                \"High-quality image captioning with fine-grained and semantic-guided visual attention\",\n                \"Deep hierarchical encoder-decoder network for image captioning\",\n                \"Generating video descriptions with latent topic guidance\",\n                \"Hierarchy parsing for image captioning\",\n                \"A Hierarchical Approach for Generating Descriptive Image Paragraphs\",\n                \"Multi-attention and incorporating background information model for chest x-ray image report generation\",\n                \"Knowledge-driven encode, retrieve, paraphrase for medical image report generation\",\n                \"Graph Attention Networks\",\n                \"Retrieve, rerank and rewrite: Soft template based neural summarization\",\n                \"Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting\",\n                \"Bridging the vocabulary gap between health seekers and healthcare knowledge\",\n                \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n                \"Attention Is All You Need\",\n                \"Aligning where to see and what to tell: image caption with region-based attention and scene factorization\",\n                \"Learning multimodal attention lstm networks for video captioning\",\n                \"X-linear attention networks for image captioning\",\n                \"Attention on attention for image captioning\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]35 based on the context and research area of Unifying Relational Sentence Generation and Retrieval for Medical Image Report Composition. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}