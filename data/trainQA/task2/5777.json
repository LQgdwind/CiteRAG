{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: On the link between generative semi-supervised learning and generative open-set recognition\n\nPaper Abstract: \n\nPaper Section Text:\nOpen-set recognition (OSR) is the research field that studies classification with simultaneous novelty detection. More specifically, OSR classifiers must accurately categorise samples that belong to K>1ğ¾1K>1 number of labelled categories and simultaneously separate samples that belong to any number of novel categories into an additional K+1ğ¾1K+1â€™th category. Novel categories are formally defined as groups of anomalous samples with similar patterns that do not match any of the labelled categoriesÂ . Within OSR, novel categories are further described as those that are unobserved during training but appear during testingÂ  . In other words, OSR ensures classifiers can handle changing environments wherein new patterns emerge over timeÂ . With regards to the training policy, OSR studies generally use supervised learning wherein the training set, ğ’Ÿlab-trainsubscriptğ’Ÿlab-train\\mathcal{D}_{\\text{lab-train}}, contains sample pairs, (x,y)ğ‘¥ğ‘¦(x,y), where xğ‘¥x is the input sample and yğ‘¦y is the output category label. After training, an independent testing set, ğ’Ÿtestsubscriptğ’Ÿtest\\mathcal{D}_{\\text{test}}, is used to determine the performance of the classifier, wherein the novel categories could lie. Testing sets also consist of sample pairs, (x,ya)ğ‘¥subscriptğ‘¦ğ‘(x,y_{a}). However, considering that the classifier cannot access the testing labels, these are defined as anticipated labels, yasubscriptğ‘¦ğ‘y_{a}, which are used to determine classifier performance. It is important to state that the testing sets should mimic real-world scenarios as closely as possible to ensure credible performance measurements. Let the set CK:={1,2,â€¦,K}assignsubscriptğ¶ğ¾12â€¦ğ¾C_{K}:=\\{1,2,...,K\\} represent all labelled categories, with all entries in CKsubscriptğ¶ğ¾C_{K} representing the category labels. Within conventional supervised learning, all training and testing samples belong to one of the categories in CKsubscriptğ¶ğ¾C_{K}. In other words, all (yâˆ¼ğ’Ÿlab-train)âˆˆCKsimilar-toğ‘¦subscriptğ’Ÿlab-trainsubscriptğ¶ğ¾(y\\sim\\mathcal{D}_{\\text{lab-train}})\\in C_{K} and all (yaâˆ¼ğ’Ÿtest)âˆˆCKsimilar-tosubscriptğ‘¦ğ‘subscriptğ’Ÿtestsubscriptğ¶ğ¾(y_{a}\\sim\\mathcal{D}_{\\text{test}})\\in C_{K}. To extend supervised learning to OSR, samples from novel categories are inserted into the testing set. Let the K+1ğ¾1K+1â€™th category be defined as the augmented unknown category, meaning it is an encapsulating category representing any and all unobserved novel categories. Then, the OSR regime would have all (yâˆ¼ğ’Ÿlab-train)âˆˆCKsimilar-toğ‘¦subscriptğ’Ÿlab-trainsubscriptğ¶ğ¾(y\\sim\\mathcal{D}_{\\text{lab-train}})\\in C_{K} and all (yaâˆ¼ğ’Ÿtest)âˆˆ(CKâˆª{K+1})similar-tosubscriptğ‘¦ğ‘subscriptğ’Ÿtestsubscriptğ¶ğ¾ğ¾1(y_{a}\\sim\\mathcal{D}_{\\text{test}})\\in(C_{K}\\cup\\{K+1\\}). In other words, beyond generalising the Kğ¾K labelled categories, OSR classifiers must generalise the K+1ğ¾1K+1â€™th augmented unknown category, noting that this category does not have any training samples. Supervised learning requires many labelled training samples to train classifiers, making it costly in real-world applications. Semi-supervised learning (SSL) proposes a more cost-efficient solution by training classifiers using a small labelled and a large unlabelled training set, |ğ’Ÿlab-train|<<|ğ’Ÿunlab-train|much-less-thansubscriptğ’Ÿlab-trainsubscriptğ’Ÿunlab-train|\\mathcal{D}_{\\text{lab-train}}|<<|\\mathcal{D}_{\\text{unlab-train}}|Â . Similar to the test set, the classifier cannot access the labels in ğ’Ÿunlab-trainsubscriptğ’Ÿunlab-train\\mathcal{D}_{\\text{unlab-train}}, and so these labels are also represented as anticipated labels, yasubscriptğ‘¦ğ‘y_{a}. We also focus on the inductive SSL setting, where the testing set is again independent of the training set. Conventional SSL relies on the cluster assumption, meaning we assume that all unlabelled training samples belong to one of the labelled categories - i.e. all (yaâˆ¼ğ’Ÿunlab-train)âˆˆCKsimilar-tosubscriptğ‘¦ğ‘subscriptğ’Ÿunlab-trainsubscriptğ¶ğ¾(y_{a}\\sim\\mathcal{D}_{\\text{unlab-train}})\\in C_{K}. Furthermore, conventional SSL assumes that all (yaâˆ¼ğ’Ÿtest)âˆˆCKsimilar-tosubscriptğ‘¦ğ‘subscriptğ’Ÿtestsubscriptğ¶ğ¾(y_{a}\\sim\\mathcal{D}_{\\text{test}})\\in C_{K}, meaning there are no novel categories included in this learning policy. Similar to supervised learning, we can extend SSL to OSR by simply including novel categories in the test set - i.e. all (yaâˆ¼ğ’Ÿtest)âˆˆ(CKâˆª{K+1})similar-tosubscriptğ‘¦ğ‘subscriptğ’Ÿtestsubscriptğ¶ğ¾ğ¾1(y_{a}\\sim\\mathcal{D}_{\\text{test}})\\in(C_{K}\\cup\\{K+1\\}). This so-called SSL-OSR setting is the primary focus of this study. However, it is essential to note that as soon as we enter the realm of SSL, novel categories could also exist in ğ’Ÿunlab-trainsubscriptğ’Ÿunlab-train\\mathcal{D}_{\\text{unlab-train}}. SSL-OSR assumes that novel categories only appear during testing. However, so-called observed novel categories have become more prevalent within SSL based researchÂ  . Specifically, given the immense cost to label every category big data domains, recent studies have focused on the setting where observed novel categories are scattered in ğ’Ÿunlab-trainsubscriptğ’Ÿunlab-train\\mathcal{D}_{\\text{unlab-train}}. This research field is called learning with augmented category by exploiting unlabelled data (LACU)Â . In this policy, the K+1ğ¾1K+1â€™th category is defined as a background augmented category to encapsulate all observed novel categories. It is important to note that LACU and OSR have contrasting definitions for the K+1ğ¾1K+1â€™th category since LACU considers observed novel categories and OSR considers unobserved ones. Although LACU represents an important SSL setting, this study only focuses on linking conventional SSL and OSR, meaning we assume that there are no novel categories seen in ğ’Ÿunlab-trainsubscriptğ’Ÿunlab-train\\mathcal{D}_{\\text{unlab-train}}. Furthermore, our K+1ğ¾1K+1â€™th category is the augmented unknown category, as per the definition of OSR. GANs have been extensively used for conventional SSLÂ . Initially, SSL-GANs transformed the discriminator to distinguish between real and fake samples and simultaneously classify any real samples into its corresponding labelled categoryÂ    . However, state-of-the-art models now prefer a three-network game, wherein the discriminator remains unchanged to the original GAN and a separate classifier is added to the mixÂ  . Of all SSL-GANs, the bad-GAN by Dai et al.Â  is especially noteworthy, as Dai theoretically and experimentally showed that good SSL-GANs should generate â€™bad-lookingâ€™ samples that lie in the complementary space. More specifically, Dai showed that regulating an SSL classifier using generated samples that lie in the complementary space (see Fig.Â 2(a)), then the classifier is forced to tighten the boundaries in the embedding space of labelled categories for improved generalisation. Bad-GANs are particularly interesting since OSR studies have used the same notion of bad-looking samplesÂ [ref]40. Before exploring OSR-GANs, it is essential to consider the use of GANs for conventional novelty detection - i.e. without multi-category classification. Conventional novelty detection is a one-class classification problem that distinguishes between unobserved novel categories from a single positive labelled category that represents the entire training domainÂ . The discriminator networks in GANs have been widely studied in this contextÂ   . More specifically, because the discriminator is tasked to separate fake-generated samples from all training samples, the discriminator can be considered a novelty detector if fake-generated samples represent in the open domain. However, the equilibrium theory of GANs suggests that a trained discriminator wonâ€™t be able to determine whether an input sample is real or fakeÂ . Therefore, unique adjustments are required to use a discriminator as a novelty detector. This study focuses on OSR, which extends novelty detection to also classify multiple labelled categories. Nevertheless, the underlying theory between OSR and novelty detection remains the same. A standard method to ensure that the discriminator/classifier can determine whether a sample is from the training domain or not is to design the generator to produce bad-looking samplesÂ   [ref]29 [ref]22 . Although multiple definitions have been proposed for these bad-looking samples, we argue that all definitions really rely on the SSL theory of the complementary space. Examples of such OSR-GANs include the counterfactual (counter)-GAN by Neal et al.Â [ref]29, which combines an auto-encoder and a GAN to generate counterfactual (or bad-looking) images that belong to one category but is pulled towards another. In contrast, Jo et al.Â [ref]22 use a feature-matching (FM)-GAN and a denoising auto-encoder to generate samples that are corrupted with the noisy distribution of the feature space of the classifier. Finally, the recent adversarial reciprocal point learning GAN (ARP-GAN) proposed by Chen et al. also uses bad-looking samples that match reciprocal pointsÂ . We note that ARP-GANs significantly outperformed all other OSR-GANs, and will be discussed in detail in the upcoming section. Although SSL-GANs and OSR-GANs theorise better generalisation and results given bad-looking generated samples, these methods have not yet been compared within research. Our hypothesis states that the notion of bad-looking samples in either the SSL or the OSR context represents samples that fall within the complementary space. In other words, counterfactual samplesÂ [ref]29, samples modelled with noisy distributionsÂ [ref]22, and samples based on reciprocal pointsÂ , all allow classifiers to generalise the open space for the same reasons SSL models perform better when generalising the complementary space. That is, regularising a classifying network with samples in the complementary space inherently generalises the open space within the classifying network. With this hypothesis, an SSL-GAN would be capable at OSR, and an OSR-GAN would also be capable at SSL. Furthermore, all these models could be used for the combined task of SSL-OSR. Finally, it is important to note several studies that also focused on the combined task of SSL-OSR. Capazzo et al.Â  used traditional machine-learning techniques to address label noise under the SSL-OSR setting. However, these traditional techniques are unsuitable for high-dimensional data, and so these methods cannot be used for image classification. Another attempt at SSL-OSR is in the unpublished study by Kliger et al.Â , who also applied GANs in this context. However, their proposed method and results were unclear, and no theoretical link was made between SSL-GANs and OSR-GANs. Finally, the most similar study to ours is the study by Sung et al.Â [ref]40. Sung applied manually chosen operations to the training data to create bad-looking samples. Subsequently, FM-GANs were used to generate samples that matched fell between pseudo-data and the real-data which were used to regularise either a SSL or OSR classifying network. However, Sung did not justify the choice of operations to create the pseudo-data (i.e. no theoretical framework was developed for bad-looking samples), while Sung experimented on SSL and OSR independently without linking the two. In this study, we theoretically and experimentally link SSL with OSR under the unified framework of highly parameterised GANs.\n\nReference markers found: [40]\n\nContext windows for each [ref]:\n[ref]40: the boundaries in the embedding space of labelled categories for improved generalisation. Bad-GANs are particularly interesting since OSR studies have used the same notion of bad-looking samplesÂ [ref]40. Before exploring OSR-GANs, it is essential to consider the use of GANs for conventional novelty detection - i.e. without multi-category classification. Conventional novelty detection is a\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 40,\n            \"titles\": [\n                \"Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results\",\n                \"Investigation of exchange coupling and microwave properties of hard/soft (SrNi0. 02Zr0. 01Fe11. 96O19)/(CoFe2O4) x nanocomposites\",\n                \"Tuning the structure, magnetic, and high frequency properties of Sc-doped Sr0. 5Ba0. 5ScxFe12-xO19/NiFe2O4 hard/soft nanocomposites\",\n                \"The proximal origin of SARS-CoV-2\",\n                \"Adaptive the Dirichlet model of mobile/immobile advection/dispersion in a time-fractional sense with the reproducing kernel computational approach: Formulations and approximations\",\n                \"Stochastic integrodifferential models of fractional orders and Leffler nonsingular kernels: well-posedness theoretical results and Legendre Gauss spectral collocation approximations\",\n                \"Anomaly and Novelty detection for robust semi-supervised learning\",\n                \"Adversarial Reciprocal Points Learning for Open Set Recognition\",\n                \"Learning with augmented class by exploiting unlabeled data\",\n                \"Cooperative AI: machines must learn to find common ground\",\n                \"Good Semi-supervised Learning That Requires a Bad GAN\",\n                \"Data stream classification with novel class detection: a review, comparison and challenges\",\n                \"MarginGAN: Adversarial Training in Semi-Supervised Learning\",\n                \"Open set learning with augmented category by exploiting unlabelled data (open-LACU)\",\n                \"Learning with an augmented (unknown) class using neural networks\",\n                \"Modeling strategy for the analysis of forced draft air-cooled condensers using rotational fan models\",\n                \"Recent Advances in Open Set Recognition: A Survey\",\n                \"Metal powder feedstock evaluation and management for powder bed fusion: a review of literature, standards, and practical guidelines\",\n                \"Generative Adversarial Nets\",\n                \"Regularisation of Neural Networks by Enforcing Lipschitz Continuity\",\n                \"Novelty detection in continuously changing environments\",\n                \"HexaGAN: Generative Adversarial Nets for Real World Classification\",\n                \"Open set recognition by regularising classifier with fake data generated by generative adversarial networks\",\n                \"Novelty Detection with GAN\",\n                \"Novelty detection using discriminator of generative adversarial network US Patent App. 15/626,457\",\n                \"Triple Generative Adversarial Networks\",\n                \"Semi-supervised Learning using Adversarial Training with Good and Bad Samples\",\n                \"Numerical solutions and geometric attractors of a fractional model of the cancer-immune based on the Atangana-Baleanu-Caputo derivative and the reproducing kernel scheme\",\n                \"The multistep Laplace optimized decomposition method for solving fractional-order coronavirus disease model (COVID-19) via the Caputo fractional approach\",\n                \"Open set learning with counterfactual images\",\n                \"Semi-Supervised Learning with Generative Adversarial Networks\",\n                \"Automatic Cough Classification for Tuberculosis Screening in a Real-World Environment\",\n                \"Optimisation in transport and logistic processes\",\n                \"OCGAN: One-class Novelty Detection Using GANs with Constrained Latent Representations\",\n                \"A review of novelty detection\",\n                \"Should Conditional Self-Driving Cars Consider the State of the Human Inside the Vehicle?\",\n                \"Detecting unexpected obstacles for self-driving cars: Fusing deep learning and geometric modeling\",\n                \"Adversarially learned one-class classifier for novelty detection\",\n                \"Survey on Implementations of Generative Adversarial Networks for Semi-Supervised Learning\",\n                \"Improved Techniques for Training GANs\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]40 based on the context and research area of On the link between generative semi-supervised learning and generative open-set recognition. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}