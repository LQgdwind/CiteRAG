{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: 1 Introduction\n\nPaper Abstract: \n\nPaper Section Text:\nFrequentism and Bayesianism are two contrasting philosophies of statistics.\nIn brief, Frequentism treats probabilities as long-term frequencies and models unknown values as fixed while Bayesianism treats probabilities as degrees of belief and models unknown values as random variables which require prior distributions.\nImportantly, the difference between these two frameworks is not about whether using Bayes theorem to perform inferential updates given a certain prior is valid.\nThis is acceptable under both frameworks, and a refusal to do so would result in a pathological form of reasoning that is vulnerable to simple Dutch Books [ref]69  and would not meet the subtlety criterion for an Achilles Heel.\nThe key difference between Frequentism and Bayesianism is not how to perform updates given a prior but instead what kind of prior if any at all is acceptable given a particular circumstance.\nFrequentism offers no tools for the estimation of a value which one is fully ignorant about while Bayesianism requires these values to be modeled with a “subjective” prior.151515One approach for this is the use of “universal” priors; see  and [ref]39.\nBoth approaches are useful in practice and often to come to quantitatively similar solutions for typical problems.\nHowever, in certain cases, an aversion to subjective priors can facilitate a flawed line of reasoning which allows an agent to be turned into a money pump via a variant of the two envelope dilemma.161616It is of course also possible for an agent who uses bad priors to be exploitable, but this would not meet the subtlety criterion for an Achilles Heel. Suppose that an agent is handed two envelopes and told two facts: first that both envelopes have at least $8currency-dollar8\\$8 inside of them, and second that one has twice the amount of money as the other.\nThe agent is allowed to pick one at random and is then asked if they would like to switch before opening and taking whatever money is in the envelope they hold.\nClearly, by symmetry, there is no reason to switch.\nA Bayesian could come to this same conclusion by conditional reasoning.\nAssuming any proper prior with finite mean171717For priors with infinite mean, this reasoning will not apply, and this situation with two envelopes reduces to a type of St. Petersberg paradox [ref]16 . on the values of the two envelopes, if a Bayesian lets X𝑋X represent the amount in the envelope which they currently hold and Y𝑌Y the amount in the other, what value X𝑋X takes is relevant in calculating the conditional distribution of Y𝑌Y [ref]16 .\nFor the Bayesian, integrating over and conditioning the distribution of X𝑋X to find the marginal distribution on Y𝑌Y will yield the same distribution as the prior for A𝐴A, and this approach arrives at the same conclusion as the argument by symmetry.\nHowever, absent a prior on X𝑋X, an agent will not be able to assume that integrating over and conditioning on possible values of X𝑋X can be used to find the distribution of Y𝑌Y.181818This is not possible for a Bayesian because there is no such thing as a uniform distribution with infinite support .\nThe subjective prior-averse agent will not believe that conditioning on a particular value of X𝑋X offers any information about the value of Y𝑌Y.\nSo they will be able to reason that if there are X𝑋X dollars in the envelope they hold, then switching will give them 2​X2𝑋2X dollars or 12​X12𝑋\\frac{1}{2}X dollars each with probability 1212\\frac{1}{2} because there is no prior to say that the actual value that X𝑋X takes affects influences the probabilities that Y𝑌Y is 2​X2𝑋2X or 12​X12𝑋\\frac{1}{2}X.\nThe expected value calculation yields 12​(2​X)+12​(12​X)=54​X122𝑋1212𝑋54𝑋\\frac{1}{2}\\left(2X\\right)+\\frac{1}{2}\\left(\\frac{1}{2}X\\right)=\\frac{5}{4}X.\nThey can then reason that if both envelopes contain at least $8currency-dollar8\\$8, the expected value of Y𝑌Y is at least $10currency-dollar10\\$10 and would then be willing to pay $1currency-dollar1\\$1 to switch.\nThe same argument applies to switching back, and an agent who uses this type of reasoning could be turned into a money pump as such. It is clarifying to note that in normative models of inference (e.g., not Solomonoff Induction  or similar), there is no fine line between what does and doesn’t constitute a “subjective” prior.\nAccordingly, being averse to subjective priors should be understood as a tendency rather than a well-defined property.\nThese tendencies could be introduced into a system via pedagogy.\nJust as a student in a statistics class who is taught solely using a Frequentist curriculum would be expected to generally avoid subjective priors, so might an AI system.\nFor example, the introduction of a system to Frequentist texts and reinforcement of Frequentist inference with example problems would facilitate this.\nIn particular, example problems in which Frequentist and Bayesian conclusions strongly diverge might be utilized.\nOne way this can be achieved is if a sharp null hypothesis is tested using somewhat inconsistent evidence if a Bayesian uses a diffuse prior on an alternative hypothesis .\nThis is sometimes referred to as “Lindley’s Paradox” and it represents a type of training problem in which a Frequentist and Bayesian would come to starkly different conclusions. It is important to recognize is that while a subjective prior-averse agent can come to the conclusion that switching envelopes has a positive expected value in a two envelope situation, they don’t need to.\nThey could still come to the correct conclusion via a simple argument from symmetry or one which considers the cases in which X𝑋X is the smaller value or the larger value separately.\nConsequently, there emerges a risk of logical inconsistencies.\nWhile humans often handle inconsistencies well, it is uncertain how AI systems would, and it is plausible that they could result in pathological behavior.\nFor example, in formal systems whose axioms allow for (P→Q→𝑃𝑄P\\to Q) to be derived from (Q𝑄Q) and for (¬Q→¬P→𝑄𝑃\\neg Q\\to\\neg P) from (P→Q→𝑃𝑄P\\to Q), any inconsistency could lead to arbitrary conclusions.\n\nReference markers found: [69]\n\nContext windows for each [ref]:\n[ref]69: updates given a certain prior is valid.\nThis is acceptable under both frameworks, and a refusal to do so would result in a pathological form of reasoning that is vulnerable to simple Dutch Books [ref]69  and would not meet the subtlety criterion for an Achilles Heel.\nThe key difference between Frequentism and Bayesianism is not how to perform updates given a prior but instead what kind of\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 69,\n            \"titles\": [\n                \"Programmatically Interpretable Reinforcement Learning\",\n                \"Evidence, decision and causality\",\n                \"Sequential choice and the agent’s perspective\",\n                \"Concrete Problems in AI Safety\",\n                \"Utility indifference\",\n                \"Anthropic decision theory\",\n                \"‘Indifference’ methods for managing agent rewards\",\n                \"Guidelines for artificial intelligence containment\",\n                \"Reinforcement learning in newcomblike environments\",\n                \"Udt with known search order\",\n                \"Specimen Theoriae Novae de Mensura Sortis [by] Daniel Bernouilli: Translated Into German and English\",\n                \"Are we living in a computer simulation?\",\n                \"Anthropic bias: Observation selection effects in science and philosophy\",\n                \"Superintelligence\",\n                \"Newcomb’s problem and prisoners’ dilemma\",\n                \"Putting a value on beauty\",\n                \"The two-envelope paradox\",\n                \"Incorrigibility in the CIRL Framework\",\n                \"Exploring neural networks with activation atlases\",\n                \"Dissolving confusion around functional decision theory\",\n                \"Procrastination paradoxes: the good, the bad, and the ugly\",\n                \"The two-envelope paradox: A complete analysis?\",\n                \"A dutch book against sleeping beauties who are evidential decision theorists\",\n                \"Designing preferences, beliefs, and identities for artificial intelligence\",\n                \"AI Research Considerations for Human Existential Safety (ARCHES)\",\n                \"My current take on counterfactuals\",\n                \"Embedded agency\",\n                \"A survey of research questions for robust and beneficial ai. future of life institute\",\n                \"Richard bellman on the birth of dynamic programming\",\n                \"Metatickles and the dynamics of deliberation\",\n                \"Adversarial Examples that Fool both Computer Vision and Time-Limited Humans\",\n                \"Self-explaining AI as an alternative to interpretable AI\",\n                \"Agi safety literature review\",\n                \"Problems of self-reference in self-improving space-time embedded intelligence\",\n                \"Implications of the copernican principle for our future prospects\",\n                \"The off-switch game\",\n                \"Beauty and the bets\",\n                \"Corrigibility with utility preservation\",\n                \"Risks from Learned Optimization in Advanced Machine Learning Systems\",\n                \"Universal artificial intelligence: Sequential decisions based on algorithmic probability\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]69 based on the context and research area of 1 Introduction. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}