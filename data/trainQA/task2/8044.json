{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Size Should not Matter: Scale-invariant Stress Metrics\n\nPaper Abstract: \n\nPaper Section Text:\nRaw Stress (RS):\nRaw stress (also known as unweighted stress) is the earliest defined stress metric, which appears in Shepard’s original MDS definition [ref]39.\nRaw stress is named as such as it contains no extra terms or normalizing factors, but is exactly the sum of squared differences of ideal and realized differences.\nIt has been widely used in various studies  . However, a large problem with raw stress is that it over-penalizes large distances in the drawing and under-penalizes short distances .\nIt is defined formally as: Raw stress is clearly a scale-sensitive metric as can be seen in Figure 3(a).\nIn fact, we can rewrite it as a quadratic polynomial with respect to α𝛼\\alpha: Another way to think about the effect of scaling is, the scaling is changing the domain of the coordinates. Since raw stress is basically the square error with respect to the graph distance, a scaling that changes the coordinate domain closer to the domain of the graph distance will make the value of raw stress smaller. On the other hand, if the scaling changes the coordinates such that the coordinate domain is significantly different from the graph distance domain, then raw stress will be larger. Since raw stress is a quadratic function of α𝛼\\alpha, we are easily able to find the α𝛼\\alpha values where raw stress of two drawings will be equal.\nSetting R​S​(α​X,D)=R​S​(α​X′,D)𝑅𝑆𝛼𝑋𝐷𝑅𝑆𝛼superscript𝑋′𝐷RS(\\alpha X,D)=RS(\\alpha X^{\\prime},D), where X𝑋X and X′superscript𝑋′X^{\\prime} are two drawings for the same graph, allows us to solve for the scale factor αi​n​t​e​r​s​e​c​t​i​o​nsubscript𝛼𝑖𝑛𝑡𝑒𝑟𝑠𝑒𝑐𝑡𝑖𝑜𝑛\\alpha_{intersection} where the functions intersect: While αi​n​t​e​r​s​e​c​t​i​o​nsubscript𝛼𝑖𝑛𝑡𝑒𝑟𝑠𝑒𝑐𝑡𝑖𝑜𝑛\\alpha_{intersection} allows us to find the points at which a change in orderings of raw stress occurs, the quadratic nature of R​S𝑅𝑆RS also enables us to determine the scale factor with minimum raw stress, αm​i​nsubscript𝛼𝑚𝑖𝑛\\alpha_{min}.\nTo determine αm​i​nsubscript𝛼𝑚𝑖𝑛\\alpha_{min} for one drawing, we calculate ∂∂α​R​S​(α​X,D)𝛼𝑅𝑆𝛼𝑋𝐷\\frac{\\partial}{\\partial\\alpha}RS(\\alpha X,D) and solve for α𝛼\\alpha: Finding the minimum value of raw stress allows us to achieve a single value for each drawing, potentially yielding a scale-invariant metric. (a) (b) (c) Kamada-Kawai Stress (KKS):\nThe principles of MDS were first applied to graph drawing by Kamada and Kawai , who base their formulation of stress on the display area available. They describe that a good graph drawing is one in which the “desirable geometric (Euclidean) distance between two vertices in the drawing [are] the graph-theoretic distance between them” . However, they perform a unit conversion of the graph-theoretic distances into the units of the display area: Here, the parameter L=L0/max⁡di​j𝐿subscript𝐿0subscript𝑑𝑖𝑗L=L_{0}/\\max d_{ij} with L0subscript𝐿0L_{0} as min⁡(W,H)𝑊𝐻\\min(W,H) where W𝑊W and H𝐻H are the width and height of the display area, respectively. The other factor, di​j−2superscriptsubscript𝑑𝑖𝑗2d_{ij}^{-2}, is the main ingredient in normalized stress and its impact is discussed in the next subsection. When this definition is used as a quality metric, it does not always make sense to define the display area as a fixed constant, since a drawing algorithm may produce drawings at different scales. However, any drawing will give us a bounding box, which can be used to define a display area; L0=max​‖Xi−Xj‖subscript𝐿0normsubscript𝑋𝑖subscript𝑋𝑗L_{0}=\\max||X_{i}-X_{j}||. Even with this generous definition, K​K​S𝐾𝐾𝑆KKS is still scale-sensitive as can be seen analytically below, as well as in Figure 3(b): Additionally, since K​K​S𝐾𝐾𝑆KKS equates to one quadratic term with respect to α𝛼\\alpha, the minimum values will always be computed at α=0𝛼0\\alpha=0. The only intersection points between different drawings also always occur at α=0𝛼0\\alpha=0. Thus, Kamada-Kawai stress cannot be made scale-invariant using some αm​i​nsubscript𝛼𝑚𝑖𝑛\\alpha_{min} as raw stress and normalized stress can. Normalized Stress (NS):\nThe definition of normalized stress (N​S𝑁𝑆NS) is provided in Equation 1.\nThis metric comes from a desire to “normalize” the penalty of short and long distances by the factor wi​jsubscript𝑤𝑖𝑗w_{ij} which is usually set to 1di​j21superscriptsubscript𝑑𝑖𝑗2\\frac{1}{d_{ij}^{2}}. This ensures distance is weighted analogously to an inverse-square law. Normalized stress is also independent of the drawing area, unlike Kamada-Kawai stress.\nDespite being the most popular formulation of stress to evaluate graph drawings, N​S𝑁𝑆NS makes for a poor metric to compare the output of two different algorithms without accounting for scale. This holds true even for very small graphs, such as the grid graph shown in Figure 2. When used as an optimization function, normalized stress produces nice drawings, as has been shown in many papers e.g. [ref]24 [ref]49. However, when used as a quality metric, one can achieve nearly any order desired when comparing drawing algorithms depending on the scale they are evaluated with. A bad actor could quietly change the scale of drawings to achieve a desirable outcome for an algorithm.\n Normalized stress, as shown in Figure 3(c)\nis scale-sensitive. We observe it as a function of α𝛼\\alpha, for convenience later: Clearly, the value of N​S𝑁𝑆NS changes with respect to α𝛼\\alpha, and it can be rewritten as a quadratic polynomial of α𝛼\\alpha using a similar derivation as Equation 2,\nas follows: Using this property, one can manipulate the order of the quality of different layouts as described in Figure 1.\nWe study the behavior of Equation 6 more carefully, to show just how much normalized stress varies with scale. Suppose we have two drawings of the same graph, X𝑋X and X′superscript𝑋′X^{\\prime}.\nBoth drawings define a quadratic curve in α𝛼\\alpha, so they can intersect (and thus, the order of evaluation metrics can change) at most twice.\nOne of these intersections is trivially at α=0𝛼0\\alpha=0, and the scaling factor must strictly be positive. The other intersection can be computed as follows.\nLet N​S​(α​X,D)𝑁𝑆𝛼𝑋𝐷NS(\\alpha X,D) and N​S​(α​X′,D)𝑁𝑆𝛼superscript𝑋′𝐷NS(\\alpha X^{\\prime},D) be the quadratic N​S𝑁𝑆NS functions computed for two fixed layouts X𝑋X and X′superscript𝑋′X^{\\prime}, respectively. The intersection occurs where N​S​(α​X,D)=N​S​(α​X′,D)𝑁𝑆𝛼𝑋𝐷𝑁𝑆𝛼superscript𝑋′𝐷NS(\\alpha X,D)=NS(\\alpha X^{\\prime},D), so by solving for α𝛼\\alpha as with raw stress in Equation 3, one can find where the order changes: Clearly, this point can exist and is shown by Figure 1. Furthermore, for k𝑘k drawings of the same graph, each drawing defines a quadratic equation of N​S𝑁𝑆NS in terms of α𝛼\\alpha. Hence, 2​(k2)2binomial𝑘22\\binom{k}{2} intersections are generally possible; however, half of them occur at α=0𝛼0\\alpha=0. The remaining (k2)binomial𝑘2\\binom{k}{2} intersections are all non-zero. Hence, at most (k2)binomial𝑘2\\binom{k}{2} effective intersections are possible. This means that when comparing k𝑘k algorithms of a single graph, there are (k2)+1binomial𝑘21\\binom{k}{2}+1 different orderings of normalized stress that can be obtained. This inconsistency is a clearly undesirable property of an evaluation metric. Based on that, one can easily change the order of layouts based on the stress score by modifying the scale, which is illustrated in Figure 1.\nOne can also further manipulate the orderings of layouts by scaling each layout differently. For example, a random graph drawing of a graph scaled such that the max drawing distance is 1 will have a better normalized stress score than a stress-optimized graph with a max distance of 1000. In addition to αi​n​t​e​r​s​e​c​t​i​o​nsubscript𝛼𝑖𝑛𝑡𝑒𝑟𝑠𝑒𝑐𝑡𝑖𝑜𝑛\\alpha_{intersection}, we can calculate αm​i​nsubscript𝛼𝑚𝑖𝑛\\alpha_{min} as we did with raw stress in Equation 4. By computing ∂∂α​N​S​(α​X,D)𝛼𝑁𝑆𝛼𝑋𝐷\\frac{\\partial}{\\partial\\alpha}NS(\\alpha X,D) and setting it equal to zero, we find: The existence of a clear optimal scale for normalized stress enables us to find a single value for every drawing, shown in the following section.\n\nReference markers found: [39]\n\nContext windows for each [ref]:\n[ref]39: Raw Stress (RS):\nRaw stress (also known as unweighted stress) is the earliest defined stress metric, which appears in Shepard’s original MDS definition [ref]39.\nRaw stress is named as such as it contains no extra terms or normalizing factors, but is exactly the sum of squared differences of ideal and realized differences.\nIt has been widely used in\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 39,\n            \"titles\": [\n                \"Drawing Dynamic Graphs Without Timeslices\",\n                \"Graph drawing via gradient descent, (GD)2\",\n                \"Multicriteria scalable graph drawing via stochastic gradient descent, (SGD)2\",\n                \"Event-based dynamic graph drawing without the agonizing pain\",\n                \"Graph drawing: algorithms for the visualization of graphs\",\n                \"An experimental comparison of four graph drawing algorithms\",\n                \"Modern multidimensional scaling: Theory and applications\",\n                \"Stochastic Gradient Descent Works Really Well for Stress Minimization\",\n                \"D3 data-driven documents\",\n                \"Eigensolver methods for progressive multidimensional scaling of large data\",\n                \"An experimental study on distance-based graph drawing\",\n                \"Data visualization with multidimensional scaling\",\n                \"People prefer less stress and fewer crossings\",\n                \"Drawing graphs to convey proximity: An incremental arrangement method\",\n                \"The university of florida sparse matrix collection\",\n                \"Stress-Plus-X (SPX) Graph Layout\",\n                \"Evaluating graph layout algorithms: A systematic review of methods and best practices\",\n                \"Graph drawing algorithms\",\n                \"Shape-based quality metrics for large graph visualization\",\n                \"Graphviz—open source graph drawing tools\",\n                \"Toward a quantitative survey of dimension reduction techniques\",\n                \"A fast adaptive layout algorithm for undirected graphs (extended abstract and system demonstration)\",\n                \"Coast: A convex optimization approach to stress-based embedding\",\n                \"A maxent-stress model for graph layout\",\n                \"Graph drawing by stress majorization\",\n                \"CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs\",\n                \"Multi-level graph drawing using infomap clustering\",\n                \"Improving force-directed graph drawings by making compromises between aesthetics\",\n                \"ForceAtlas2, a continuous graph layout algorithm for handy network visualization designed for the Gephi software\",\n                \"Efficient algorithms for shortest paths in sparse networks\",\n                \"An algorithm for drawing general undirected graphs\",\n                \"An empirical evaluation of force-directed graph layout\",\n                \"Drawing graphs by eigenvectors: theory and practice\",\n                \"Graph layouts by t-SNE\",\n                \"Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis\",\n                \"Balancing between the Local and Global Structures (LGS) in Graph Embedding\",\n                \"A sparse stress model\",\n                \"A new approach to all-pairs shortest paths on real-weighted graphs\",\n                \"Metrics for graph drawing aesthetics\",\n                \"The analysis of proximities: multidimensional scaling with an unknown distance function. i.\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]39 based on the context and research area of Size Should not Matter: Scale-invariant Stress Metrics. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}