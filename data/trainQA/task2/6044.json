{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Disagreement amongst counterfactual explanations: How transparency can be deceptive\n\nPaper Abstract: \n\nPaper Section Text:\nPost-hoc explanations are techniques to explain AI decisions made after the model has been trained. The most popular methods can be divided into two groups: feature-based techniques (also called attribution methods) and example-based (also called instance-based) techniques  [ref]30. The first group contains methods like local interpretable model-agnostic explanations (LIME), Shapley additive explanations (SHAP), and other feature importance techniques. The second group, example-based post-hoc explanation methods, contains Anchors and counterfactuals. We will briefly explain the methods used in our experiments: SHAP, Anchors, and counterfactual explanations. Figure¬†1 provides a figurative example of the different XAI methods to explain why a person (the instance) is predicted not to get their loan approved. For a more detailed description and examples, we refer to [ref]30 and the works mentioned below. SHAP  and LIME  are similar in the sense that the impact of a certain feature is measured related to the predictive outcome. The basic idea of LIME is to sample instances in the neighborhood of the instance that is given to the ML model and then train an interpretable model like linear regression or decision tree to explain this neighborhood. The interpretable model can consequently be used to explain the prediction that is made by the actual, black-box model. For tabular data, which are used in the experiments of this work, the issue is to define the neighborhood of an instance. If LIME would only sample closely around the given instance, chances are high all predictions will be exactly the same and LIME cannot comprehend how predictions change. Therefore, samples are taken broadly, e.g., by using a normal distribution. A major disadvantage of LIME is that the explanations differ depending on the samples used, which makes the explanations unstable and manipulable. Therefore, we do not include LIME in our comparisons. A better feature-based technique can be found in SHAP, which combines the locality of LIME with the concept of Shapley values from coalitional or cooperative game theory. The contribution of each feature (player) to the prediction that is made by the model (outcome of the game) for a given instance is calculated. Moreover, the contribution of cooperation between players (multiple features) is examined. The average marginal contribution of a feature value across all cooperations is called the shapley value111A common misinterpretation of the Shapley value is that it amounts to the difference in prediction after removing the feature from the model training.. Because there are 2ksuperscript2ùëò2^{k} possible cooperations, for which models need to be trained, calculating all the Shapley values is computationally expensive. Therefore, by using the LIME-inspired sampling, the SHAP algorithm decreases the computation time. Anchors or scoped rules  are high-precision easy-to-understand if-then rules. They portray feature conditions together with a predictive outcome. The rules are called Anchors because any changes to other features than the ones mentioned, will not result in another prediction. In contrast to e.g., LIME, Anchors will provide a region of instances to describe the model‚Äôs behavior. They are consequently less instance-specific. For example, imagine if a person applies for a loan at a bank. This person is 50 years old, has a monthly income of $2000, his gender is male and he currently has $5000 in debt. A model has predicted that the loan application should be declined. The corresponding Anchor could then be: if the monthly income is lower than $5000 and the age is higher than 35, then predict that the loan application would be accepted. Counterfactual explanations describe a combination of feature changes that would alter the predicted class . In other words, they determine what features should change in order to change the prediction and are consequently sometimes called what-if statements. As mentioned in Section¬†1, this type of explanation is especially human-friendly because they are contrastive and selective . Counterfactual explanations are somewhat the opposite of Anchors. To revisit the same example: the person asking for a loan wants to know why he will not get one. A counterfactual explanation could then be: if your monthly income rises to $5000, you will get a loan. Besides its human-friendly nature, another advantage of the counterfactual method is that it does not need access to the data or the model itself. Only the model‚Äôs prediction function is required to generate the explanations.\nBecause of their many benefits, many different counterfactual methods came into existence. [ref]13 and  give an overview of counterfactual explanation techniques, however, to date, the state-of-the-art further unfolded with e.g., the introduction of NICE, a counterfactual generation algorithm which simultaneously achieves 100% coverage, model-agnosticism and fast counterfactual generation for different types of classification models. Feature importance estimates can be biased because it is dependent on the executed sampling [ref]11, while example-based methods like counterfactuals have no additional assumptions or mysteries at the back end of the method. They just tell the user how to change the prediction. On the downside, it is arguable if one instance can fully capture the complexity of a model. Furthermore, the literature recently sees a sprawl of different counterfactual methods, which possibly leads to different respective explanations. The question rises whether these counterfactual methods are still useful for stakeholders and which specific post-hoc explanation method to choose. To answer these questions, the field of XAI evaluation has tried to evaluate the quality of post-hoc explanation methods.\n\nReference markers found: [30]\n\nContext windows for each [ref]:\n[ref]30: model has been trained. The most popular methods can be divided into two groups: feature-based techniques (also called attribution methods) and example-based (also called instance-based) techniques  [ref]30. The first group contains methods like local interpretable model-agnostic explanations (LIME), Shapley additive explanations (SHAP), and other feature importance techniques. The second group,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 30,\n            \"titles\": [\n                \"Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations\",\n                \"Sanity checks for saliency maps\",\n                \"Fairwashing: the risk of rationalization\",\n                \"Data Representing Ground-Truth Explanations to Evaluate XAI Methods\",\n                \"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI\",\n                \"Post-Hoc Explanations Fail to Achieve their Purpose in Adversarial Contexts\",\n                \"Nice: an algorithm for nearest instance counterfactual explanations\",\n                \"Multi-objective counterfactual explanations\",\n                \"Towards a rigorous science of interpretable machine learning\",\n                \"UCI Machine Learning Repository\",\n                \"Explainable AI (XAI): Core ideas, techniques, and solutions\",\n                \"Explaining data-driven decisions made by AI systems: the counterfactual approach\",\n                \"European Union regulations on algorithmic decision-making and a ‚Äúright to explanation‚Äù\",\n                \"Counterfactual explanations and how to find them: literature review and benchmarking\",\n                \"Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post Hoc Explanations\",\n                \"Mitigating the Rashomon Effect in Counterfactual Explanation: A Game-theoretic Approach\",\n                \"An Initial Study of Machine Learning Underspecification Using Feature Attribution Explainable AI Algorithms: A COVID-19 Virus Transmission Case Study\",\n                \"Using rule extraction to improve the comprehensibility of predictive models\",\n                \"Model-agnostic counterfactual explanations for consequential decisions\",\n                \"Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI)\",\n                \"The Disagreement Problem in Explainable Machine Learning: A Practitioner‚Äôs Perspective\",\n                \"‚ÄúHow do I fool you?‚Äù: Manipulating User Trust via Misleading Black Box Explanations\",\n                \"Comparison-based inverse classification for interpretability in machine learning\",\n                \"Explainable ai: A review of machine learning interpretability methods\",\n                \"Synthetic Benchmarks for Scientific Research in Explainable Machine Learning\",\n                \"A Unified Approach to Interpreting Model Predictions\",\n                \"Data Science Ethics: Concepts, Techniques, and Cautionary Tales\",\n                \"Explaining Data-Driven Document Classifications\",\n                \"The magical number seven, plus or minus two: Some limits on our capacity for processing information.\",\n                \"Explanation in Artificial Intelligence: Insights from the Social Sciences\",\n                \"A guide for making black box models explainable\",\n                \"Order in the Court: Explainable AI Methods Prone to Disagreement\",\n                \"Artificial Intelligence as a Disruptive Technology‚ÄîA Systematic Literature Review\",\n                \"On Counterfactual Explanations under Predictive Multiplicity\",\n                \"‚ÄúWhy Should I Trust You?‚Äù Explaining the Predictions of Any Classifier\",\n                \"Anchors: High-precision model-agnostic explanations\",\n                \"Better metrics for evaluating explainable artificial intelligence\",\n                \"Why Don‚Äôt XAI Techniques Agree? Characterizing the Disagreements Between Post-hoc Explanations of Defect Predictions\",\n                \"GeCo: Quality Counterfactual Explanations in Real Time\",\n                \"Reckoning with the Disagreement Problem: Explanation Consensus as a Training Objective\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]30 based on the context and research area of Disagreement amongst counterfactual explanations: How transparency can be deceptive. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}