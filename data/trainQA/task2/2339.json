{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Entropy, Information, and the Updating of Probabilities\n\nPaper Abstract: \n\nPaper Section Text:\nEntropic methods of inference are of general applicability but there exist\nspecial situations ‚Äî such as, for example, those involving large numbers of\nindependent subsystems ‚Äî where inferences can be made by purely\nprobabilistic methods without ever invoking the concept of entropy. In such\ncases one can check (see e.g. in ) that the two methods of calculation are consistent with each other. It\nis significant, however, that alternative entropies such as those proposed in\n do not pass this\ntest , which rules them out as\ntools for updating. Some probability distributions obtained by maximizing the\nalternative entropies have, however, turned out to be physically relevant. It\nis therefore noteworthy that those successful distributions can also be\nderived through more standard application of MaxEnt or ME as advocated in this\nreview [ref]8. In other words, what is being\nruled out are not the distributions themselves, but the alternative entropies\nfrom which they were inferred. Choosing the prior density q‚Äã(x)ùëûùë•q(x) can be tricky. Sometimes symmetry\nconsiderations can be useful but otherwise there is no fixed set of rules to\ntranslate information into a probability distribution except, of course, for\nBayes‚Äô rule and the ME method themselves. What if the prior q‚Äã(x)ùëûùë•q(x) vanishes for some values of xùë•x? S‚Äã[p,q]ùëÜùëùùëûS[p,q] can be\ninfinitely negative when q‚Äã(x)ùëûùë•q(x) vanishes within some region ùíüùíü\\mathcal{D}.\nThis means that the ME method confers an infinite preference on those\ndistributions p‚Äã(x)ùëùùë•p(x) that vanish whenever q‚Äã(x)ùëûùë•q(x) does. One must emphasize that\nthis is as it should be. A similar situation¬†also arises in the context of\nBayes‚Äô theorem where assigning a vanishing prior represents a tremendously\nserious commitment because no amount of data to the contrary would allow us to\nrevise it. In both ME and Bayes updating we should recognize the implications\nof assigning a vanishing prior. Assigning a very low but non-zero prior\nrepresents a safer and possibly less prejudiced representation of one‚Äôs prior beliefs. The ME method allows one to process information in the form of constraints.\nWhen we are confronted with several constraints we must be particularly\ncautious. Should they be processed simultaneously or sequentially? And, if the\nlatter, in what order? The answer depends on the problem at hand . We refer to constraints as commuting when it makes no difference\nwhether they are handled simultaneously or sequentially. The most common\nexample is that of Bayesian updating on the basis of data collected in several\nindependent experiments. In this case the order in which the observed data\nx‚Ä≤={x1‚Ä≤,x2‚Ä≤,‚Ä¶}superscriptùë•‚Ä≤superscriptsubscriptùë•1‚Ä≤superscriptsubscriptùë•2‚Ä≤‚Ä¶x^{\\prime}=\\{x_{1}^{\\prime},x_{2}^{\\prime},\\ldots\\} is processed does not\nmatter for the purpose of inferring Œ∏ùúÉ\\theta. In general, however, constraints\nneed not commute and when this is the case the order in which they are\nprocessed is critical. To decide whether constraints are to be handled sequentially or simultaneously\none must be clear about how the ME method handles constraints. The ME\nmachinery interprets a constraint in a very mechanical way: all distributions\nsatisfying the constraint are in principle allowed while all distributions\nviolating it are ruled out. Therefore, sequential updating is appropriate when\nold constraints become obsolete and are superseded by new information while\nsimultaneous updating is appropriate when old constraints remain valid. The\ntwo cases refer to different states of information and therefore it is to be\nexpected that they will result in different inferences. These comments are\nmeant to underscore the importance of understanding what information is and\nhow it is processed by the ME method; failure to do so will lead to errors\nthat do not reflect a shortcoming of the ME method but rather a misapplication\nof it. Entropy is a tool for reasoning and ‚Äî as with all tools for reasoning or\notherwise ‚Äî it can be misused leading to unsatisfactory results\n. Should that happen, the inevitable questions are ‚Äòwhat\nwent wrong?‚Äô and ‚Äòhow do we fix it?‚Äô It helps to first ask what components of\nthe analysis can be trusted so that the possible mistakes can be looked for\nelsewhere. The answers proposed by the ME method are radically conservative:\nproblems always arise through a wrong choices of variables, or priors, or\nconstraints. Indeed, one should not blame the entropic method for not having\ndiscovered and taken into account relevant information that was not explicitly\nintroduced into the analysis. Indeed, just as one would be very reticent about\nquestioning the basic rules of arithmetic, or the basic rules of calculus, one\nshould not question the basic sum and product rules of the probability\ncalculus and, taking this one step farther, one should not question the\napplicability of entropy as the updating tool. The adoption of this\nconservative approach leads us to reject alternative entropies and quantum\nprobabilities. Fortunately, those constructs are not actually needed ‚Äî as\nmentioned above, those Tsallis distributions that have turned out be useful\ncan be derived with the standard entropic methods [ref]8 and quantum mechanics can be handled within standard\nprobability theory without invoking exotic probabilities [ref]36. I would like to acknowledge many valuable discussions on probability and\nentropy with N. Caticha, A. Giffin, K. Knuth, R. Preuss, C. Rodr√≠guez, J.\nSkilling, and K. Vanslette.\n\nReference markers found: [8]\n\nContext windows for each [ref]:\n[ref]8: turned out to be physically relevant. It\nis therefore noteworthy that those successful distributions can also be\nderived through more standard application of MaxEnt or ME as advocated in this\nreview [ref]8. In other words, what is being\nruled out are not the distributions themselves, but the alternative entropies\nfrom which they were inferred. Choosing the prior density q‚Äã(x)ùëûùë•q(x) can be\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 8,\n            \"titles\": [\n                \"Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±\",\n                \"Relative Entropy and Inductive Inference\",\n                \"Updating Probabilities\",\n                \"Information and Entropy\",\n                \"Towards an Informational Pragmatic Realism\",\n                \"Entropic Updating of Probabilities and Density Matrices\",\n                \"Lectures on Probability, Entropy, and Statistical Physics\",\n                \"Entropic Inference and the Foundations of Physics\",\n                \"Entropic Physics: Probability, Entropy, and the Foundations of Physics\",\n                \"Papers on Probability, Statistics and Statistical Physics\",\n                \"Can the Maximum Entropy Principle be explained as a consistency requirement?\",\n                \"On measures of entropy and information\",\n                \"On Measures of Information and their Characterizations\",\n                \"Possible Generalization of Boltzmann-Gibbs Statistics\",\n                \"Axiomatic derivation of the Principle of Maximum Entropy and the Principle of Minimum Cross-Entropy\",\n                \"The Axioms of Maximum Entropy\",\n                \"Classic Maximum Entropy\",\n                \"On the axiomatic approach to the maximum entropy principle of inference\",\n                \"Conceptual Inadequacy of the Shore and Johnson Axioms for Wide Classes of Complex systems\",\n                \"Maximum Entropy Principle in Statistical Inference: Case for Non-Shannonian Entropies\",\n                \"Nonadditive Entropies Yield Probability Distributions with Biases not Warranted by the Data\",\n                \"Reply to Tsallis‚Äô ‚ÄúConceptual inadequacy of the Shore and Johnson axioms for wide classes of complex systems\",\n                \"Bayesian Conditionalization and the Principle of Minimum Relative Information\",\n                \"Entropic dynamics and the quantum measurement problem\",\n                \"Quantum measurement and weak values in entropic quantum dynamics\",\n                \"Elements of Information Theory\",\n                \"Foundations of Info-Metrics: Modeling, Inference, and Imperfect Information\",\n                \"Modeling by shortest data description\",\n                \"Information is Physical\",\n                \"The thermodynamics of computation‚ÄîA review\",\n                \"Notes on Landauer‚Äôs principle, reversible computation, and Maxwell‚Äôs demon\",\n                \"Waiting for Landauer\",\n                \"The End of the Thermodynamics of Computation:¬†A No-Go Result\",\n                \"Entropic dynamics on Gibbs statistical manifolds\",\n                \"The Entropic Dynamics approach to Quantum Mechanics\",\n                \"Lattice duality: The origin of probability and entropy\",\n                \"Foundations of Inference\",\n                \"Updating Probabilities with Data and Moments\",\n                \"Differential-Geometrical Methods in Statistics\",\n                \"Maximum entropy and Bayesian data analysis: entropic prior distributions\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]8 based on the context and research area of Entropy, Information, and the Updating of Probabilities. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}