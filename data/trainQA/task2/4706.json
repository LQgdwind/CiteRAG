{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: First-order Policy Optimization for Robust Markov Decision Process This research was partially supported by NSF DMS-2134037 and AFOSR FA9550-22-1-0447.\n\nPaper Abstract: \n\nPaper Section Text:\nTo proceed, we introduce a characterization that upper bounds the difference of robust values for two policies, and serves as a keystone in the ensuing algorithmic developments. For any pair of policies Ï€,Ï€â€²ğœ‹superscriptğœ‹â€²\\pi,\\pi^{\\prime}, we have Let Î¾uÏ€â€²â€‹(s)â‰”{(st,at)}tâ‰¥0â‰”subscriptsuperscriptğœ‰superscriptğœ‹â€²ğ‘¢ğ‘ subscriptsubscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡ğ‘¡0\\xi^{\\pi^{\\prime}}_{u}(s)\\coloneqq\\left\\{(s_{t},a_{t})\\right\\}_{t\\geq 0} denote the trajectory generated from by Ï€â€²superscriptğœ‹â€²\\pi^{\\prime} within MDP â„³uÏ€â€²subscriptâ„³subscriptğ‘¢superscriptğœ‹â€²\\mathcal{M}_{u_{\\pi^{\\prime}}}, with initial state set as sğ‘ s.\nThat is, atâˆ¼Ï€â€²(â‹…|st)a_{t}\\sim\\pi^{\\prime}(\\cdot|s_{t}), st+1âˆ¼â„™uÏ€â€²(â‹…|st,at)s_{t+1}\\sim\\mathbb{P}_{u_{\\pi}^{\\prime}}(\\cdot|s_{t},a_{t}) for all tâ‰¥0ğ‘¡0t\\geq 0 and s0=ssubscriptğ‘ 0ğ‘ s_{0}=s.\nWe know that where (a)ğ‘(a) follows from (2.3); (b)ğ‘(b) follows from moving VrÏ€â€‹(s0)subscriptsuperscriptğ‘‰ğœ‹ğ‘Ÿsubscriptğ‘ 0V^{\\pi}_{r}(s_{0}) outside the summation;\n(c)ğ‘(c) follows from definition that s0=ssubscriptğ‘ 0ğ‘ s_{0}=s; (e)ğ‘’(e) follows from the definition of dsÏ€,uÏ€subscriptsuperscriptğ‘‘ğœ‹subscriptğ‘¢ğœ‹ğ‘ d^{\\pi,u_{\\pi}}_{s} in (1.7), and the relation between VrÏ€subscriptsuperscriptğ‘‰ğœ‹ğ‘ŸV^{\\pi}_{r} and QrÏ€subscriptsuperscriptğ‘„ğœ‹ğ‘ŸQ^{\\pi}_{r} in (2.6).\nIt remains to show that (d)ğ‘‘(d) holds. For any tâ‰¥0ğ‘¡0t\\geq 0, we have where (aâ€²)superscriptğ‘â€²(a^{\\prime}) follows from the definition of Î¾uÏ€â€²â€‹(s)subscriptsuperscriptğœ‰superscriptğœ‹â€²ğ‘¢ğ‘ \\xi^{\\pi^{\\prime}}_{u}(s), and (bâ€²)superscriptğ‘â€²(b^{\\prime}) follows from the property of QrÏ€subscriptsuperscriptğ‘„ğœ‹ğ‘ŸQ^{\\pi}_{r} (2.5).\nThus relation (d)ğ‘‘(d) follows immediately from the prior observation and the linearity of expectation.\nâˆ By applying Lemma 2.6, we know that for any policy Ï€âˆˆÎ ğœ‹Î \\pi\\in\\Pi and an optimal robust policy Ï€âˆ—superscriptğœ‹\\pi^{*}, we have where â„±s:Î Ã—Î â†’â„|ğ’®|â€‹|ğ’œ|:subscriptâ„±ğ‘ â†’Î Î superscriptâ„ğ’®ğ’œ\\mathcal{F}_{s}:\\Pi\\times\\Pi\\to\\mathbb{R}^{\\lvert{\\mathcal{S}}\\rvert\\lvert\\mathcal{A}\\rvert} is defined by Given the optimality of Ï€âˆ—superscriptğœ‹\\pi^{*}, we then know that\nâŸ¨â„±â€‹(Ï€,Ï€âˆ—),Ï€âˆ’Ï€âˆ—âŸ©â‰¥0â„±ğœ‹superscriptğœ‹ğœ‹superscriptğœ‹0\\langle\\mathcal{F}(\\pi,\\pi^{*}),\\pi-\\pi^{*}\\rangle\\geq 0\nfor all Ï€âˆˆÎ ğœ‹Î \\pi\\in\\Pi.\nNow for any Ï€âˆˆÎ ğœ‹Î \\pi\\in\\Pi, let Î±âˆˆ(0,1)ğ›¼01\\alpha\\in(0,1), we define Ï€Î±=Î±â€‹Ï€+(1âˆ’Î±)â€‹Ï€âˆ—subscriptğœ‹ğ›¼ğ›¼ğœ‹1ğ›¼superscriptğœ‹\\pi_{\\alpha}=\\alpha\\pi+(1-\\alpha)\\pi^{*}.\nSubstituting Ï€Î±subscriptğœ‹ğ›¼\\pi_{\\alpha} into the prior relation, we obtain\nâŸ¨â„±sâ€‹(Ï€Î±,Ï€âˆ—),Ï€Î±âˆ’Ï€âˆ—âŸ©=Î±â€‹âŸ¨â„±sâ€‹(Ï€Î±,Ï€âˆ—),Ï€âˆ’Ï€âˆ—âŸ©â‰¥0.subscriptâ„±ğ‘ subscriptğœ‹ğ›¼superscriptğœ‹subscriptğœ‹ğ›¼superscriptğœ‹ğ›¼subscriptâ„±ğ‘ subscriptğœ‹ğ›¼superscriptğœ‹ğœ‹superscriptğœ‹0\\langle\\mathcal{F}_{s}(\\pi_{\\alpha},\\pi^{*}),\\pi_{\\alpha}-\\pi^{*}\\rangle=\\alpha\\langle\\mathcal{F}_{s}(\\pi_{\\alpha},\\pi^{*}),\\pi-\\pi^{*}\\rangle\\geq 0.\nConsequently, for any Î±âˆˆ(0,1)ğ›¼01\\alpha\\in(0,1).\nIt is straightforward to verify that the visitation measure dsÏ€,usuperscriptsubscriptğ‘‘ğ‘ ğœ‹ğ‘¢d_{s}^{\\pi,u} is a continuous function of (Ï€,u)ğœ‹ğ‘¢(\\pi,u) (see proof of Lemma 2.1).\nOn the other hand, note that given (2.2), the worst-case uncertainty uÏ€(â‹…|s,a)âˆˆâˆ‚Ïƒğ’°s,a(VrÏ€)u_{\\pi}(\\cdot|s,a)\\in\\partial\\sigma_{\\mathcal{U}_{s,a}}(V^{\\pi}_{r}), where ÏƒXâ€‹(â‹…)subscriptğœğ‘‹â‹…\\sigma_{X}(\\cdot) denotes the support function of set Xğ‘‹X, and âˆ‚fğ‘“\\partial f denotes the subdifferential of function fğ‘“f.\nFor simplicity, let us assume that âˆ‚Ïƒğ’°s,aâ€‹(VrÏ€)subscriptğœsubscriptğ’°ğ‘ ğ‘subscriptsuperscriptğ‘‰ğœ‹ğ‘Ÿ\\partial\\sigma_{\\mathcal{U}_{s,a}}(V^{\\pi}_{r}) is always a singleton for any Ï€ğœ‹\\pi (See Lemma B.4 for example). Now let Î±â†“0â†“ğ›¼0\\alpha\\downarrow 0, we have Ï€Î±â†’Ï€âˆ—â†’subscriptğœ‹ğ›¼superscriptğœ‹\\pi_{\\alpha}\\to\\pi^{*}.\nSince VrÏ€subscriptsuperscriptğ‘‰ğœ‹ğ‘ŸV^{\\pi}_{r} is Lipschitz continuous in Ï€ğœ‹\\pi (see Lemma B.6 for an elementary proof), then VrÏ€Î±â†’VrÏ€âˆ—â†’subscriptsuperscriptğ‘‰subscriptğœ‹ğ›¼ğ‘Ÿsubscriptsuperscriptğ‘‰superscriptğœ‹ğ‘ŸV^{\\pi_{\\alpha}}_{r}\\to V^{\\pi^{*}}_{r}.\nThus given that Ïƒğ’°s,asubscriptğœsubscriptğ’°ğ‘ ğ‘\\sigma_{\\mathcal{U}_{s,a}} is a closed and proper convex function, and the fact that the subdifferential map of a closed convex function is closed (Theorem 24.4, [ref]35), we know that any\nlimit point of uÏ€Î±(â‹…|s,a)u_{\\pi_{\\alpha}}(\\cdot|s,a) is also a subgradient of Ïƒğ’°s,aâ€‹(VrÏ€âˆ—)subscriptğœsubscriptğ’°ğ‘ ğ‘subscriptsuperscriptğ‘‰superscriptğœ‹ğ‘Ÿ\\sigma_{\\mathcal{U}_{s,a}}(V^{\\pi^{*}}_{r}), and hence the limit point is indeed unique and the worst-case uncertainty for Ï€âˆ—superscriptğœ‹\\pi^{*}.\nDenoting this limit point as uÏ€âˆ—subscriptğ‘¢superscriptğœ‹u_{\\pi^{*}}, then by taking Î±â†“0â†“ğ›¼0\\alpha\\downarrow 0 in (2.15), we obtain In view of the discussions in the last two paragraphs, (2.16) suggests us to find the optimal robust policy via solving the following variational inequality (VI), Interested readers might find that VI (2.18) has a close analogy for solving non-robust MDPs, constructed in .\nSpecifically, for a non-robust, discounted finite MDP, the optimal policy satisfies the following VI, where Î½âˆ—superscriptğœˆ\\nu^{*} denotes the stationary state distribution induced by the optimal policy Ï€âˆ—superscriptğœ‹\\pi^{*}.\nOne can clearly see that the only difference between the non-robust VI (2.19) and the robust VI (2.18) is the additional role of worst-case environment for the latter.\nTo solve the non-robust version of VI constructed therein,  exploits the key fact that the constructed VI satisfies the so-called generalized monotonicity: for any policy Ï€âˆ‰Î âˆ—ğœ‹superscriptÎ \\pi\\notin\\Pi^{*}. However, we next show that unlike the performance difference lemma for standard MDPs  , for which\n(2.13) holds with equality,\nthe inequality in Lemma 2.6 seems unavoidable in the robust setting.\nConsequently, the VI for the robust MDP (2.18) no longer satisfies the generalized monotonicity as defined in (2.20). There exists a robust MDP instance â„³ğ’°subscriptâ„³ğ’°\\mathcal{M}_{\\mathcal{U}} such that holds for at least one state sâˆˆğ’®ğ‘ ğ’®s\\in{\\mathcal{S}}.\nIn particular, Ï€âˆ—superscriptğœ‹\\pi^{*} corresponds to a solution of (1.4), and Ï€ğœ‹\\pi is a strictly suboptimal policy.\nMoreover, denoting Î½âˆ—â‰”Î½Ï€âˆ—,uÏ€âˆ—â‰”superscriptğœˆsuperscriptğœˆsuperscriptğœ‹subscriptğ‘¢superscriptğœ‹\\nu^{*}\\coloneqq\\nu^{\\pi^{*},u_{\\pi^{*}}} as the stationary state distribution of the optimal policy Ï€âˆ—superscriptğœ‹\\pi^{*} within its worst-case MDP â„³uÏ€âˆ—subscriptâ„³subscriptğ‘¢superscriptğœ‹\\mathcal{M}_{u_{\\pi^{*}}}, we have Consider the MDP â„³â„³\\mathcal{M} that contains three states ğ’®={Sa,Sb,Sc}ğ’®subscriptğ‘†ğ‘subscriptğ‘†ğ‘subscriptğ‘†ğ‘{\\mathcal{S}}=\\left\\{S_{a},S_{b},S_{c}\\right\\}, and each state is associated with two actions ğ’œ={L,R}ğ’œğ¿ğ‘…\\mathcal{A}=\\left\\{L,R\\right\\}.\nThe transition of â„³â„³\\mathcal{M}, denoted by â„™â„™\\mathbb{P}, is fully deterministic, and is illustrated in Figure 2. Each edge starts from the current state, and ends at the next state, with its arc (a,c)ğ‘ğ‘(a,c) consisting of the action ağ‘a, and the cost cğ‘c associated with the action ağ‘a. We assume such a cost occurs immediately after the action is made, and is independent of the next state.\nIn addition, we assume C>1ğ¶1C>1. Let us consider the scenario where the uncertain environment can arbitrarily manipulate the transition probabilities at state Scsubscriptğ‘†ğ‘S_{c} except there is no returning to itself, and has no manipulation strength at other states.\nThat is, for any aâˆˆğ’œğ‘ğ’œa\\in\\mathcal{A}, we have ğ’«Sc,a=Î”{Sa,Sb}subscriptğ’«subscriptğ‘†ğ‘ğ‘subscriptÎ”subscriptğ‘†ğ‘subscriptğ‘†ğ‘\\mathcal{P}_{S_{c},a}=\\Delta_{\\left\\{S_{a},S_{b}\\right\\}}.\nOn the other hand, for any sâ‰ Scğ‘ subscriptğ‘†ğ‘s\\neq S_{c}, and for any aâˆˆğ’œğ‘ğ’œa\\in\\mathcal{A}, we have ğ’°s,a=ğŸsubscriptğ’°ğ‘ ğ‘0\\mathcal{U}_{s,a}=\\mathbf{0}. We consider a pair of policies Ï€,Ï€âˆ—ğœ‹superscriptğœ‹\\pi,\\pi^{*}, defined by Since C>1ğ¶1C>1, and the fact that Ï€â€‹(L|Sa)=1ğœ‹conditionalğ¿subscriptğ‘†ğ‘1\\pi(L|S_{a})=1, it should be clear that for policy Ï€ğœ‹\\pi, the worst case transition â„™uÏ€subscriptâ„™subscriptğ‘¢ğœ‹\\mathbb{P}_{u_{\\pi}} should satisfy Consequently, the robust value of Ï€ğœ‹\\pi is given by On the other hand, for policy Ï€âˆ—superscriptğœ‹\\pi^{*}, since Ï€âˆ—â€‹(R|Sa)=1superscriptğœ‹conditionalğ‘…subscriptğ‘†ğ‘1\\pi^{*}(R|S_{a})=1 and 1>0101>0, the worst case transition â„™uÏ€âˆ—subscriptâ„™subscriptğ‘¢superscriptğœ‹\\mathbb{P}_{u_{\\pi^{*}}} should satisfy It should also be clear that Ï€âˆ—superscriptğœ‹\\pi^{*} is an optimal robust policy.\nFrom the previous observations, simple calculation yields Now let Î¾uÏ€âˆ—â€‹(Sc)subscriptsuperscriptğœ‰superscriptğœ‹ğ‘¢subscriptğ‘†ğ‘\\xi^{\\pi^{*}}_{u}(S_{c}) denote the trajectory generated by Ï€âˆ—superscriptğœ‹\\pi^{*} within â„³Ï€âˆ—subscriptâ„³superscriptğœ‹\\mathcal{M}_{\\pi^{*}}, starting from state Scsubscriptğ‘†ğ‘S_{c}.\nThen we have where the strict inequality follows from observation (2.23) and (2.24).\nThus by repeating the same argument in the proof of Lemma 2.6, we know that inequality (d)ğ‘‘(d) in (2.14) holds with strict inequality for s=Scğ‘ subscriptğ‘†ğ‘s=S_{c}.\nSince (d)ğ‘‘(d) is the only inequality in the proof of Lemma 2.6, we conclude that We proceed to show that ğ”¼sâ€²âˆ¼dScÏ€âˆ—,uÏ€âˆ—â€‹âŸ¨QrÏ€,Ï€âˆ—âˆ’Ï€âŸ©sâ€²=0subscriptğ”¼similar-tosuperscriptğ‘ â€²subscriptsuperscriptğ‘‘superscriptğœ‹subscriptğ‘¢superscriptğœ‹subscriptğ‘†ğ‘subscriptsubscriptsuperscriptğ‘„ğœ‹ğ‘Ÿsuperscriptğœ‹ğœ‹superscriptğ‘ â€²0\\mathbb{E}_{s^{\\prime}\\sim d^{\\pi^{*},u_{\\pi^{*}}}_{S_{c}}}\\langle Q^{\\pi}_{r},\\pi^{*}-\\pi\\rangle_{s^{\\prime}}=0 and hence establishing (2.21).\nTo see this, note that\nQrÏ€â€‹(Sc,â‹…)=Î³â€‹VrÏ€â€‹(Sa)subscriptsuperscriptğ‘„ğœ‹ğ‘Ÿsubscriptğ‘†ğ‘â‹…ğ›¾subscriptsuperscriptğ‘‰ğœ‹ğ‘Ÿsubscriptğ‘†ğ‘Q^{\\pi}_{r}(S_{c},\\cdot)=\\gamma V^{\\pi}_{r}(S_{a}), and hence\nâŸ¨QrÏ€,Ï€âˆ—âˆ’Ï€âŸ©Sc=0subscriptsubscriptsuperscriptğ‘„ğœ‹ğ‘Ÿsuperscriptğœ‹ğœ‹subscriptğ‘†ğ‘0\\langle Q^{\\pi}_{r},\\pi^{*}-\\pi\\rangle_{S_{c}}=0.\nIn addition, we also have QrÏ€â€‹(Sb,â‹…)=1+Î³â€‹VrÏ€â€‹(Sc)subscriptsuperscriptğ‘„ğœ‹ğ‘Ÿsubscriptğ‘†ğ‘â‹…1ğ›¾subscriptsuperscriptğ‘‰ğœ‹ğ‘Ÿsubscriptğ‘†ğ‘Q^{\\pi}_{r}(S_{b},\\cdot)=1+\\gamma V^{\\pi}_{r}(S_{c}), and hence\nâŸ¨QrÏ€,Ï€âˆ—âˆ’Ï€âŸ©Sb=0subscriptsubscriptsuperscriptğ‘„ğœ‹ğ‘Ÿsuperscriptğœ‹ğœ‹subscriptğ‘†ğ‘0\\langle Q^{\\pi}_{r},\\pi^{*}-\\pi\\rangle_{S_{b}}=0.\nFinally, note that dScÏ€âˆ—,uÏ€âˆ—â€‹(Sa)=0superscriptsubscriptğ‘‘subscriptğ‘†ğ‘superscriptğœ‹subscriptğ‘¢superscriptğœ‹subscriptğ‘†ğ‘0d_{S_{c}}^{\\pi^{*},u_{\\pi^{*}}}(S_{a})=0, and hence ğ”¼sâ€²âˆ¼dScÏ€âˆ—,uÏ€âˆ—â€‹âŸ¨QrÏ€,Ï€âˆ—âˆ’Ï€âŸ©sâ€²=0subscriptğ”¼similar-tosuperscriptğ‘ â€²subscriptsuperscriptğ‘‘superscriptğœ‹subscriptğ‘¢superscriptğœ‹subscriptğ‘†ğ‘subscriptsubscriptsuperscriptğ‘„ğœ‹ğ‘Ÿsuperscriptğœ‹ğœ‹superscriptğ‘ â€²0\\mathbb{E}_{s^{\\prime}\\sim d^{\\pi^{*},u_{\\pi^{*}}}_{S_{c}}}\\langle Q^{\\pi}_{r},\\pi^{*}-\\pi\\rangle_{s^{\\prime}}=0. To show (2.22), it suffices to note that Î½âˆ—=Unifâ€‹({Sb,Sc})superscriptğœˆUnifsubscriptğ‘†ğ‘subscriptğ‘†ğ‘\\nu^{*}=\\mathrm{Unif}(\\left\\{S_{b},S_{c}\\right\\}) is the stationary state distribution of Ï€âˆ—superscriptğœ‹\\pi^{*} within MDP â„³uÏ€âˆ—subscriptâ„³subscriptğ‘¢superscriptğœ‹\\mathcal{M}_{u_{\\pi^{*}}}.\nWith the same arguments as in the last paragraph we can show that ğ”¼sâ€²âˆ¼dSbÏ€âˆ—,uÏ€âˆ—â€‹âŸ¨QrÏ€,Ï€âˆ—âˆ’Ï€âŸ©sâ€²=0subscriptğ”¼similar-tosuperscriptğ‘ â€²subscriptsuperscriptğ‘‘superscriptğœ‹subscriptğ‘¢superscriptğœ‹subscriptğ‘†ğ‘subscriptsubscriptsuperscriptğ‘„ğœ‹ğ‘Ÿsuperscriptğœ‹ğœ‹superscriptğ‘ â€²0\\mathbb{E}_{s^{\\prime}\\sim d^{\\pi^{*},u_{\\pi^{*}}}_{S_{b}}}\\langle Q^{\\pi}_{r},\\pi^{*}-\\pi\\rangle_{s^{\\prime}}=0.\nHence we obtain where the strict inequality follows from (2.25).\nThe claim then follows from the definition of ğ’¢ssubscriptğ’¢ğ‘ \\mathcal{G}_{s} in (2.17).\nâˆ The construction of strict inequality in Proposition 2.3 suggests that (2.13) in Lemma 2.6 fails to characterize even the simplest change of robust values when switching the policy.\nIn particular, as illustrated in (2.21), improvement of value when switching from Ï€ğœ‹\\pi to the optimal Ï€âˆ—superscriptğœ‹\\pi^{*} seems not being captured by the aggregated inner product defined in the second term of (2.21) at all.\nA closer look into the constructed example shows that the state Sasubscriptğ‘†ğ‘S_{a} is the only state where the inner product is nonzero (positive), but when changing to the optimal policy, the state Sasubscriptğ‘†ğ‘S_{a} is never visited by the optimal policy via ds~Ï€âˆ—,uÏ€âˆ—superscriptsubscriptğ‘‘~ğ‘ superscriptğœ‹subscriptğ‘¢superscriptğœ‹d_{\\widetilde{s}}^{\\pi^{*},u_{\\pi^{*}}}, for any s~â‰ Sa~ğ‘ subscriptğ‘†ğ‘\\widetilde{s}\\neq S_{a}. It seems to suggest that the fundamental difficulty causing the insufficiency of Lemma 2.6 is due to the fact that the very state (i.e., state Sasubscriptğ‘†ğ‘S_{a}) that leads to the improvement of policy is never visited by the improved policy Ï€âˆ—superscriptğœ‹\\pi^{*} within its worst-case environment â„³uÏ€âˆ—subscriptâ„³superscriptğ‘¢superscriptğœ‹\\mathcal{M}_{u^{\\pi^{*}}}. The following lemma aims to address the previously observed difficulty. For any policy Ï€ğœ‹\\pi, let uÏ€subscriptğ‘¢ğœ‹u_{\\pi} denote its worst-case uncertainty defined in (2.2), then we have We have where (a)ğ‘(a) follows from (2.7),\n(c)ğ‘(c) follows from the (2.3),\n(d)ğ‘‘(d) follows from the definition of VrÏ€âˆ—=maxuâˆˆğ’°â¡VuÏ€âˆ—subscriptsuperscriptğ‘‰superscriptğœ‹ğ‘Ÿsubscriptğ‘¢ğ’°subscriptsuperscriptğ‘‰superscriptğœ‹ğ‘¢V^{\\pi^{*}}_{r}=\\max_{u\\in\\mathcal{U}}V^{\\pi^{*}}_{u}.\nWe now proceed to establish (b)ğ‘(b). The proof of (b)ğ‘(b) follows similar lines as in the proof of Lemma 2.6.\nLet Î¾uÏ€Ï€âˆ—â€‹(s)â‰”{(st,at)}tâ‰¥0â‰”subscriptsuperscriptğœ‰superscriptğœ‹subscriptğ‘¢ğœ‹ğ‘ subscriptsubscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡ğ‘¡0\\xi^{\\pi^{*}}_{u_{\\pi}}(s)\\coloneqq\\left\\{(s_{t},a_{t})\\right\\}_{t\\geq 0} denote the trajectories generated by policy Ï€âˆ—superscriptğœ‹\\pi^{*}, within MDP â„³uÏ€subscriptâ„³subscriptğ‘¢ğœ‹\\mathcal{M}_{u_{\\pi}}, with initial state set as sğ‘ s.\nThat is, atâˆ¼Ï€âˆ—(â‹…|st)a_{t}\\sim\\pi^{*}(\\cdot|s_{t}), st+1âˆ¼â„™uÏ€(â‹…|st,at)s_{t+1}\\sim\\mathbb{P}_{u_{\\pi}}(\\cdot|s_{t},a_{t}) for all tâ‰¥0ğ‘¡0t\\geq 0 and s0=ssubscriptğ‘ 0ğ‘ s_{0}=s.\nWe then have where (e)ğ‘’(e) follows from (1.6).\nâˆ By comparing Proposition 2.3 and Lemma 2.7, it should be clear that the key to inequality (2.26) is the combination of policy and the environment when choosing the state-visitation measure.\nInstead of choosing the worst-case environment uÏ€âˆ—subscriptğ‘¢superscriptğœ‹u_{\\pi^{*}} for Ï€âˆ—superscriptğœ‹\\pi^{*} as in Proposition 2.3 (Lemma 2.6), we choose the worst-case environment uÏ€subscriptğ‘¢ğœ‹u_{\\pi} of the current policy Ï€ğœ‹\\pi.\nIn particular, returning back to the constructed example in Proposition 2.3, we see that the state (i.e., state Sasubscriptğ‘†ğ‘S_{a}) that leads to the improvement of policy can be now visited by the improved policy Ï€âˆ—superscriptğœ‹\\pi^{*}, if the environment is still fixed as the worst-case environment â„³uÏ€subscriptâ„³subscriptğ‘¢ğœ‹\\mathcal{M}_{u_{\\pi}} for the original policy Ï€ğœ‹\\pi. Before we end our discussion in this section, it is worth mentioning some similarities that robust state-action value function QrÏ€subscriptsuperscriptğ‘„ğœ‹ğ‘ŸQ^{\\pi}_{r} shares with the (sub)gradient in the convex optimization literature, in the sense that with proper aggregation over states:\n(1) it provides an upper bound on the local value changes, as shown in Lemma 2.6, much similar to using gradient-based local quadratic approximation for smooth objectives  (with the quadratic term being identically 00);\n(2) its inner product with the direction to the optimal policy is lower bounded by the optimality gap, as shown in Lemma 2.7, similar to the (sub)-gradient for convex objectives  .\nThese observations thus suggest using QrÏ€subscriptsuperscriptğ‘„ğœ‹ğ‘ŸQ^{\\pi}_{r} as the first-order information to update the policy.\nNevertheless, it should be noted that the objective (1.5) is neither convex nor smooth [ref]1 .\nIn the following section, we develop the robust policy mirror descent method that formalizes this intuition, and establish its computational efficiency in finding an optimal policy.\n\nReference markers found: [35]\n\nContext windows for each [ref]:\n[ref]35: that Ïƒğ’°s,asubscriptğœsubscriptğ’°ğ‘ ğ‘\\sigma_{\\mathcal{U}_{s,a}} is a closed and proper convex function, and the fact that the subdifferential map of a closed convex function is closed (Theorem 24.4, [ref]35), we know that any\nlimit point of uÏ€Î±(â‹…|s,a)u_{\\pi_{\\alpha}}(\\cdot|s,a) is also a subgradient of\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 35,\n            \"titles\": [\n                \"Reinforcement Learning under Model Mismatch\",\n                \"On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift\",\n                \"Robust Reinforcement Learning using Least Squares Policy Iteration with Provable Performance Guarantees\",\n                \"Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization\",\n                \"A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants\",\n                \"The theory of max-min and its application to weapons allocation problems\",\n                \"Twice regularized MDPs and the equivalence between robustness and regularization\",\n                \"A first-order approach to accelerated value iteration\",\n                \"Robust markov decision processes: Beyond rectangularity\",\n                \"Scalable First-Order Methods for Robust MDPs\",\n                \"Partial policy iteration for l1-robust markov decision processes\",\n                \"Robust estimation of a location parameter\",\n                \"Robust dynamic programming\",\n                \"Approximately optimal approximate reinforcement learning\",\n                \"A natural policy gradient\",\n                \"Robust modified policy iteration\",\n                \"On the linear convergence of natural policy gradient algorithm\",\n                \"Risk-Averse Learning by Temporal Difference Methods\",\n                \"On frÃ©chet subdifferentials\",\n                \"Efficient Policy Iteration for Robust Markov Decision Processes via Regularization\",\n                \"First-order and stochastic optimization methods for machine learning\",\n                \"Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes\",\n                \"Markov chains and mixing times, volume 107\",\n                \"Policy mirror descent inherently explores action space\",\n                \"Implicit bias of gradient descent based adversarial training on separable data\",\n                \"Homotopic policy mirror descent: Policy convergence, implicit regularization, and improved sample complexity\",\n                \"Distributionally robust qğ‘q-learning\",\n                \"Distributionally Robust Offline Reinforcement Learning with Linear Function Approximation\",\n                \"Overcoming Exploration in Reinforcement Learning with Demonstrations\",\n                \"Introductory lectures on convex optimization: A basic course, volumeÂ 87\",\n                \"Robust control of markov decision processes with uncertain transition matrices\",\n                \"Sample complexity of robust reinforcement learning with a generative model\",\n                \"Curiosity-driven Exploration by Self-supervised Prediction\",\n                \"Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703\",\n                \"Markov decision processes: discrete stochastic dynamic programming\",\n                \"Convex analysis, volumeÂ 18\",\n                \"Risk-averse dynamic programming for markov decision processes\",\n                \"Adaptive Trust Region Policy Optimization: Global Convergence and Faster Rates for Regularized MDPs\",\n                \"Deep Reinforcement Learning with Robust and Smooth Policy\",\n                \"Lectures on Geometric Measure Theory, volumeÂ 3\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]35 based on the context and research area of First-order Policy Optimization for Robust Markov Decision Process This research was partially supported by NSF DMS-2134037 and AFOSR FA9550-22-1-0447.. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}