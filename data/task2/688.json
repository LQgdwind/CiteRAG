{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Orientation-Preserving Vectorized Distance Between Curves\n\nPaper Abstract: \n\nPaper Section Text:\nThis paper on the other hand starts with the goal of embedding ordered/oriented curve (and shape) data to a Euclidean vector space, where inner products are natural, fast nearest neighbor search is easily available, and it can directly be dropped into any machine learning or other data analysis frameworks. This builds on recent work with a similar goals for halfspaces, curves, and other shapesÂ [ref]44 . But that work did not encode orientation.\nThis orientation-preserving aspect of these distances is clearly important for some applications; it is needed to say distinguish someone going to work versus returning from work. Why might FrÃ©chet be better for data analysis than discrete FrÃ©chet or DTW or the many other distances? One can potentially point to long segments and no need to discretize, or (quasi-)metric properties. Regardless, an equalizer is in determining how well a distance models data is the prediction error for classification tasks; such tasks demonstrate how well the distances encode what truely matters on real tasks. The previous vectorized representations matched or outperformed a dozen other measuresÂ [ref]44. In this paper, we show an oriented distance performs similarly on general tasks, but when orientation is essential, does significantly better than non-orientation preserving measures.\nMoreover, by extending properties from similar, but non-orientable vectorized distancesÂ [ref]44 , our proposed distance inherits metric properties, can handle long segments, and also captures curve orientation. More specifically, our approach assumes all objects are in a bounded domain Î©Î©\\Omega a subset of â„dsuperscriptâ„ğ‘‘\\mathbb{R}^{d} (typically â„2superscriptâ„2\\mathbb{R}^{2}). This domain contains a set of landmark points Qğ‘„Q, which might constitute a continuous uniform measure over Î©Î©\\Omega, or a finite sample approximation of that distribution. With respect to an object Î³ğ›¾\\gamma, each landmark qiâˆˆQsubscriptğ‘ğ‘–ğ‘„q_{i}\\in Q generates a value vqiâ€‹(Î³)subscriptğ‘£subscriptğ‘ğ‘–ğ›¾v_{q_{i}}(\\gamma). Each of these values vqiâ€‹(Î³)subscriptğ‘£subscriptğ‘ğ‘–ğ›¾v_{q_{i}}(\\gamma) can correspond with the iğ‘–ith coordinate in a vector vQâ€‹(Î³)subscriptğ‘£ğ‘„ğ›¾v_{Q}(\\gamma), which is finite (with |Q|=nğ‘„ğ‘›|Q|=n-dimensions) if Qğ‘„Q is finite. Then the distance between two objects Î³ğ›¾\\gamma and Î³â€²superscriptğ›¾â€²\\gamma^{\\prime} is the square-root of the average squared distance of these values â€“ or the Euclidean distance of the finite vectors The innovation of this paper is in the definition of the value vqiâ€‹(â‹…)subscriptğ‘£subscriptğ‘ğ‘–â‹…v_{q_{i}}(\\cdot) and the implications around that choice. In particular in previous works by [ref]44 , in this framework, this had been (mostly) set as the unsigned minDist function: vqğ—†ğ–£â€‹(Î³)=minpâˆˆÎ³â¡â€–pâˆ’qâ€–subscriptsuperscriptğ‘£ğ—†ğ–£ğ‘ğ›¾subscriptğ‘ğ›¾normğ‘ğ‘v^{\\mathsf{mD}}_{q}(\\gamma)=\\min_{p\\in\\gamma}\\|p-q\\|. In this paper we alter this definition to not only capture the distance to the shape Î³ğ›¾\\gamma, but in allowing negative values to also capture the orientation of it. This new definition leads to many interesting structural properties about shapes. These include: We show that ğšFsubscriptğšğ¹\\mathtt{d}_{F} is stable up to FrÃ©chet perturbations of the curves. In particular, for two curves Î³,Î³â€²ğ›¾superscriptğ›¾â€²\\gamma,\\gamma^{\\prime} which are closed, then\nğšQâ€‹(Î³,Î³â€²)<ğšFâ€‹(Î³,Î³â€²)subscriptğšğ‘„ğ›¾superscriptğ›¾â€²subscriptğšğ¹ğ›¾superscriptğ›¾â€²\\mathtt{d}_{Q}(\\gamma,\\gamma^{\\prime})<\\mathtt{d}_{F}(\\gamma,\\gamma^{\\prime}),\nwhere ğšFsubscriptğšğ¹\\mathtt{d}_{F} is the FrÃ©chet distance.\nMoreover, for an lâˆsuperscriptğ‘™l^{\\infty} variant of the distance ğšQâˆsuperscriptsubscriptğšğ‘„\\mathtt{d}_{Q}^{\\infty} (see Definition 1.1),\nand the curves are closed, we obtain ğšFâ€‹(Î³,Î³â€²)=ğšQâˆâ€‹(Î³,Î³â€²)subscriptğšğ¹ğ›¾superscriptğ›¾â€²superscriptsubscriptğšğ‘„ğ›¾superscriptğ›¾â€²\\mathtt{d}_{F}(\\gamma,\\gamma^{\\prime})=\\mathtt{d}_{Q}^{\\infty}(\\gamma,\\gamma^{\\prime}).\nThus ğšQsubscriptğšğ‘„\\mathtt{d}_{Q} captures orientation.\nIn contrast for a class of curves we show ğšQmD,âˆ{\\mathtt{d}^{\\textsf{mD}}_{Q}}^{,\\infty} can equal Hausdorff distance, so this older version explicitly does not capture orientation. We introduce a new stability notion called the signed local feature size for oriented curves and other shapes Î³ğ›¾\\gamma. While it does not rely on the new signed vqiâ€‹(Î³):â„dâ†’â„:subscriptğ‘£subscriptğ‘ğ‘–ğ›¾â†’superscriptâ„ğ‘‘â„v_{q_{i}}(\\gamma):\\mathbb{R}^{d}\\to\\mathbb{R} distance function, it captures a scale under which vqiâ€‹(Î³)subscriptğ‘£subscriptğ‘ğ‘–ğ›¾v_{q_{i}}(\\gamma) is stable.\nUnlike its unsigned counterpart (local feature size, which plays a prominent role in shape reconstructionÂ    and computational topologyÂ   ), the signed local feature size is infinite for closed simple curves. This in turn implies ğšQsubscriptğšğ‘„\\mathtt{d}_{Q} is stable with respect to Qğ‘„Q for all closed simple curves, where as notions which depend on (unsigned) local feature size (e.g., medial axis) are not. For curves with boundary, the signed local feature size requires a more intricate treatment.\nWe show that when the signed local feature size Î´ğ›¿\\delta is positive but finite (0<Î´<âˆ0ğ›¿0<\\delta<\\infty) then we can set a scale parameter Ïƒğœ\\sigma in the definition of vqsubscriptğ‘£ğ‘v_{q} (denoted vqÏƒsuperscriptsubscriptğ‘£ğ‘ğœv_{q}^{\\sigma}) and in ğšQsubscriptğšğ‘„\\mathtt{d}_{Q} (denoted ğšQÏƒsuperscriptsubscriptğšğ‘„ğœ\\mathtt{d}_{Q}^{\\sigma}) so when Ïƒ<Î´/(4(1+lnâ¡(2/Îµ))\\sigma<\\delta/(4(1+\\sqrt{\\ln(2/\\varepsilon)}), then the signed distance function viÏƒsuperscriptsubscriptğ‘£ğ‘–ğœv_{i}^{\\sigma} is stable up to value Îµğœ€\\varepsilon. Altogether, these results build and analyze a new vectorized, and sketchable distance between curves (or other geometric objects) which captures orientation like FrÃ©chet (or dynamic time warping, and other popular measures), but avoids all of the complications when actually used. As we demonstrate, fast nearest neighbor search, machine learning, clustering, etc are all now very easy. By a curve we mean the image of a continuous non-constant mapping Î³: â†’â„2:ğ›¾â†’01superscriptâ„2\\gamma: \\to\\mathbb{R}^{2}; we simply use Î³ğ›¾\\gamma to refer to these curves. Two curves are equivalent if they can be reparameterized by an increasing monotone function to be identical. Hence there are two equivalence classes corresponding to the same trace of a curve, these two classes correspond to the direction of the curve.\nA curve Î³ğ›¾\\gamma is closed if Î³â€‹(0)=Î³â€‹(1)ğ›¾0ğ›¾1\\gamma(0)=\\gamma(1).\nIt is simple if the mapping does not cross itself, i.e. Î³ğ›¾\\gamma is one-to-one function. Let Î“Î“\\Gamma be the class of all simple curves Î³ğ›¾\\gamma in â„2superscriptâ„2\\mathbb{R}^{2} with the property that at almost every point pğ‘p on Î³ğ›¾\\gamma, considering the direction of the curve, there is a unique normal vector npsubscriptğ‘›ğ‘n_{p} at pğ‘p, i.e. there is a tangent line almost everywhere on Î³ğ›¾\\gamma. Such points are called regular points of Î³ğ›¾\\gamma and the set of regular points of Î³ğ›¾\\gamma is denoted by reg(Î³)regğ›¾\\mathop{\\rm reg}(\\gamma). Points of Î³âˆ–reg(Î³)ğ›¾regğ›¾\\gamma\\setminus\\mathop{\\rm reg}(\\gamma) are called critical points of Î³ğ›¾\\gamma. The terminology â€œalmost every pointâ€ means that the Lebesgue measure of those tâˆˆ ğ‘¡01t\\in  such that Î³â€‹(t)ğ›¾ğ‘¡\\gamma(t) is a critical point is zero. We also assume that at critical points, which are not endpoints of a non-closed curve, Î³ğ›¾\\gamma is left and right differentiable but left and right derivatives are possibly not identical, that is, left and right tangent lines exist. Finally, we assume that non-closed curves in Î“Î“\\Gamma have left/right tangent line at endpoints. These assumptions will guarantee the existence of a unique normal vector at critical points that are defined in Section 1.1.\n\nReference markers found: [44]\n\nContext windows for each [ref]:\n[ref]44: easily available, and it can directly be dropped into any machine learning or other data analysis frameworks. This builds on recent work with a similar goals for halfspaces, curves, and other shapesÂ [ref]44 . But that work did not encode orientation.\nThis orientation-preserving aspect of these distances is clearly important for some applications; it is needed to say distinguish someone going to\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 44,\n            \"titles\": [\n                \"Sketched MinDist\",\n                \"Computing the discrete frechet distance in subquadratic time\",\n                \"Generalized self- approaching curves\",\n                \"Computing the frÃ©chet distance between two polygonal curves\",\n                \"Comparison of distance measures for planar curves\",\n                \"Surface reconstruction by voronoi filtering\",\n                \"The power crust\",\n                \"The power crust, unions of balls, and the medial axis transform\",\n                \"graph\",\n                \"Edit distance cannot be computed in strongly subquadratic time (unless seth is false)\",\n                \"A fast implementation of near neighbors queries for FrÃ©chet distance (GIS Cup)\",\n                \"Why walking the dog takes time: FrÃ©chet distance has no strongly subquadratic algorithms unless SETH fails\",\n                \"Quadratic Conditional Lower Bounds for String Problems and Dynamic Time Warping\",\n                \"Walking the Dog Fast in Practice: Algorithm Engineering of the FrÃ©chet Distance\",\n                \"Four soviets walk the dog: Improved bounds for computing the frÃ©chet distance\",\n                \"Efficient trajectory queries under the frÃ©chet distance (gis cup)\",\n                \"klcluster: Center-based clustering of trajectories\",\n                \"Geometric inference\",\n                \"Geometric inference for probability measures\",\n                \"Robust topolical inference: Distance-to-a-measure and kernel distance\",\n                \"The â€œÎ»ğœ†\\\\lambda-medial axisâ€\",\n                \"Robust and fast similarity search for moving object trajectories\",\n                \"Delaunay mesh generation\",\n                \"GPS Trajectories Data Set, February 2016.\",\n                \"Fast frechet queries\",\n                \"Approximating the frÃ©chet distance for realistic curves in near linear time\",\n                \"Clustering time series under the Frechet distance\",\n                \"Sublinear data structures for short FrÃ©chet queries\",\n                \"Locality-sensitive hashing of curves\",\n                \"A filter-and-refinement-algorithm for range queries based on the frÃ©chet distance (gis cup)\",\n                \"Computing discrete Frechet distance\",\n                \"Approximate Nearest Neighbor for Curves: Simple, Efficient, and Deterministic\",\n                \"Fast FrÃ©chet Distance Between Curves with Long Edges\",\n                \"Algebraic topology\",\n                \"On approximate nearest neighbors in non-euclidean spaces\",\n                \"Imagenet classification with deep convolutional neural networks\",\n                \"Medial axis transformation of a planar shape\",\n                \"Faster retrieval with a two-pass dynamic-time-warping lower bound\",\n                \"Marching cubes: A high resolution 3d surface construction algorithm\",\n                \"Objectively identifying landmark use and predicting flight trajectories of the homing pigeon using gaussian processes\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]44 based on the context and research area of Orientation-Preserving Vectorized Distance Between Curves. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}