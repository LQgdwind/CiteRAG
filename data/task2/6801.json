{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities\n\nPaper Abstract: \n\nPaper Section Text:\nIIMs or, as they are often termed, glass-box or white-box models due to their relative decision-making transparency are among the simplest methods to achieve explainable prediction in clinical risk modelling. Their transparency is usually a consequence of the internal structure of the model itself being open to analysis and the relationship between input and output can be clearly represented either through equations or visualisations. Another key attribute for identifying IIMs is their lack of needing an external model or method to render the prediction model explainable . Entire groups of these methods have been proposed for various applications throughout time with some, as in the case of linear regression, preceding the development of machine learning itself. Discretisation methods reduce a problem and its data into a set of discrete subsets thereby transforming the continuous solution space into one defined at discrete points. Decision trees and their ensembles resort to this strategy when learning by evaluating each feature value and selecting the cut-off to maximize the class separation as measured through entropy or Gini impurity (classification) or minimize the variance of the outcome (regression). Thus, each importance can be interpreted as share of the overall model importance . With its simple structure, it arguably provides more intuitive interpretability than sparse linear models whose outcomes are products of several layers of possibly non-linear functions in a multi-dimensional space. A limitation is that in cases of linear relationships between input and output, it approximates the behaviour through a series of discrete step functions which might not be of high fidelity. Another example of discretisation methods are rule-based or expert-based systems, one of the earliest examples of domain adaptation of ’machine learning’ to healthcare practice [ref]49. These are methods that, similarly to decision trees, break down the outcome modelling into a series of discrete steps or rather decisions of an IF-THEN nature whose cut-off criteria are not determined by optimising a certain metric but from domain expertise or, in the case of Bayesian rule lists, from data mining the features for patterns . As long as the decision rules are understandable, these models are radically interpretable, simple, and often times more compact than their decision tree cousins. Through domain expertise, they also apply a certain type of feature selection by a priori defining what features are of importance from expert knowledge and only including those in the decision rule structure whereas linear models would assing weights to all features and learn a less sparse model. Similarly to decision trees, they are not adapted to linear modelling problems while, additionally, only being able to solve classification problems with discrete features. As compared to the previous methodologies, k-Nearest Neighbors provides an example- or instance-based interpretability whereas the earlier models account for global feature importance. Since it uses no modular learning of any global structure to make predictions there are also no global weights or decisions computer to interpret in the first place. The algorithm, in the case of classification, classifies the instance depending on the most common class in the neighbourhood of the data point in the feature space. Neighbourhood then depends on the number of neighbours and the distance metric one uses to identify the nearest k neighbours. To explain a specific prediction, one relies on the nearest neighbours used and their common feature patterns. Some work has used linear models to automate finding these patterns once clusters are formed [ref]51. This implies, however, that in cases of higher dimensionality, seeing those patterns becomes intractable and, consequently, uninterpretable. Static or tabular data are not the only type of data encountered in clinical machine learning applications. Often, images or timeseries require a different modus operandi, and deep learning, a machine learning paradigm relying on neural networks with a large number of layers (convolutional, recurrent, or otherwise), has been a great fit for these data formats and corresponding problems. Traditional machine learning models require feature engineering to create the input vectors from data, especially when they are images, whereas deep learning, through representation learning, allows the method to learn from raw data directly the optimal representation of internal features for learning . Causality and fuzzy logic can help make combined systems with deep learning inherently interpretable. Traditional machine learning methods have seen their own implementations in the fuzzy logic realm but the focus here is on a more interesting approach of transforming inherently uninerpretable deep learning otherwise by fuzzy rules. For decision tree, support vector machine, and nearest neighbour fuzzy implementations, consult the following literature:     . The assumption in fuzzy logic is that instead of assuming input or output is deterministic, i.e. fixed value, one assigns uncertainty to both ends of the learning system. An example would be the decisions that a human makes while driving a car as provided by . The decision-making of “if the distance to the car ahead is less than 2.5 m and the road is 10% slippery then reduce car speed by 25%” is approximated with numerical uncertainty reflecting the imprecise language of If the distance to the car ahead is low and the road is slightly slippery Then slow down. The numerical meanings of “low”, “close”, and “slow down” can vary and are, thus, fuzzy. Breaking down the inference problem into fuzzy logic if-then steps aids interpretability and can be combined with high-performing opaque deep learning models through, for example, a fusion layer at the end of a neural network, one part of which is deep representational learning and the other having fuzzy rules . The weights in the fuzzy part of the networks are themselves fuzzy and can aid learning the joint probability distribution which, consequently, improves prediction performance. Another approach is to switch between neural and fuzzy layers or add the fuzzy logic system as a module with autoencoder structures through pre-trained layer replacement, often referred to as a deep type-2 fuzzy logic system (D2FLS)  . Whatever the strategy, using fuzzy if-then rules holds great promise in overcoming the black-box nature of deep learning while keeping the superior predictive performance. There are limitations, however, as to how well suited these methods are for interpretability itself. Since they were built for uncertainty integration, they have not been yet optimised for explainability applications especially in higher dimensions . Future research can help address these concerns by implementing these methods in different case scenarios from static data processing to image classification and prediction while addressing the higher dimensionality through possible grouping of features. The topic of using logic-based rules to interpret complex models without external add-ons necessarily also includes work on boolean rules generation with column generation methods (BRCGs). These methods attempt to reduce binary classification problems into a set of compact Boolean clauses similar to the ones seen earlier in rule-based models and fuzzy logic but with a set structure of more compact and easier to explain rules [ref]63. Each conjunction in these unordered rule sets is considered an individual rule and a positive prediction occurs when at least one of the rules is satisfied. These rules can be mined from the data (with decision trees for example) after which they can be selected through a myriad of methods including integer programming albeit which often comes with a limited search space  . Rule selection through column generation searches over an exponential number of all possible clauses optimised in a greedy manner with the combined reduced cost objective. This approach also allows a quantitative way of measuring intepretability by just looking at the size of the rules set itself. In cases of higher dimensionality, this approach becomes less tractable and is addressed by formulating an approximate Pricing Problem by randomly selecting a limited number of features and samples . The classification accuracy and computational costs for these binary classifiers are, however, a large limitation to their widespread acceptance as general IIM frameworks. Recent work has attempted to use optimised column generation to aid with the computational constraints and has included fairness into the objective evaluation through equality of opportunity and equalized odds constraints . Besides relying on fusion layers and rule-based methods to turn opaque models inherently interpretable by some type of combination, another example that should be investigated more is incorporating transfer learning into explainability. The concept of transfer learning rests on pre-training or learning weights for one task and ’transferring’ that knowledge and the learned weights to another problem, sometimes with a limited amount of re-learning and often to a different domain . By using transfer learning to preserve the high predictive performance of black-box models and transferring them to interpretable simpler models a more balanced trade-off between deep learning and transparency can be obtained. An abstract representation of this process can be seen in Figure 5. The limitation, however, is on how transferrable the features learned from the complex model are to the simpler model . Assuming sufficiently, through this approach, one can possibly simultaneously use a simpler and more reliable model that clinicians would be familiar with while preserving higher performance, apply a model trained on a large amount of data to a smaller dataset thereby avoiding overfitting, and reducing computational costs by simply ’cut-and-pasting’ a pre-trained model to the problem at hand  . All of these can help achieve more explainable systems that are not just interpretable, but easier to check for fairness and bias, especially considering the smaller and more accessible datasets that can now be used for deployment. Methods like ProfWeight transfer the learning process from a pre-trained deep neural network with a high predictive capability to a simpler interpretable model or a very shallow network of low complexity and a priori low test accuracy. The method relies on flattened intermediate representations used to generate confidence scores of linear probes for weighting of samples during training of the simpler models. This approach has not yet been sufficiently evaluated in real-world applications such as clinical risk prediction but could hold great promise in achieving a stable balance between higher test accuracies and white-box models . Possible extensions could include new and more challenging data domains and applications, experimenting with a combination of different models of varying complexity, and optimising for inference time and reducing costs since the initial weights still need to be computed from a complex deep learning model. Inherently interpretable models also have their limitations when their explanations are often burdened by possible and unidentified confounders. Recent work on human agency with explainable AI has found that the added user reliance on using an IIM with less features and cleared decision-making is not significant. Additionally, researchers found that using an IIM limited users’ \"abilities to detect when the model had made a sizable mistake, seemingly due to information overload caused by the amount of detail in front of them\" . A key factor in usability then is the number of features included in the model itself. The tendency for outliers to break the explainability framework is to be addressed through collaborative explainability mechanisms between human users and automated systems by inviting users to give their own predictions before seeing the model. In this way, users might be more likely to notice any outlier values. Thus, while users might be more likely to rely on simpler and clearer models, the benefits to prediction accuracy are negligible, meaning that using IIMs with clearer features of a smaller number has first and foremost a psychological effect in building trust even if not a practical one. Recent advances, however, have been in the field of explainability for models and methods that do not fit the IIM criteria. Large, complex, and often opaque models that deal with learning patterns from text, images, genomics, and timeseries measurements present unique challenges when applied to clinical risk score prediction. Often times, separate explainability methodologies of similar complexity as the models they purport to explain have to be developed and tested. That will be the focus of the next section.\n\nReference markers found: [49]\n\nContext windows for each [ref]:\n[ref]49: be of high fidelity. Another example of discretisation methods are rule-based or expert-based systems, one of the earliest examples of domain adaptation of ’machine learning’ to healthcare practice [ref]49. These are methods that, similarly to decision trees, break down the outcome modelling into a series of discrete steps or rather decisions of an IF-THEN nature whose cut-off criteria are not\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 49,\n            \"titles\": [\n                \"Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model\",\n                \"Where does alphago go: From church-turing thesis to alphago thesis and beyond\",\n                \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\",\n                \"Gpt-3: What’s it good for?\",\n                \"Highly accurate protein structure prediction with alphafold\",\n                \"International evaluation of an ai system for breast cancer screening\",\n                \"Combining the strengths of radiologists and ai for breast cancer screening: a retrospective analysis\",\n                \"Artificial Intelligence in Drug Discovery: Applications and Techniques\",\n                \"Ai-driven quantification, staging and outcome prediction of covid-19 pneumonia\",\n                \"Critical appraisal of artificial intelligence-based prediction models for cardiovascular disease\",\n                \"User trust and understanding of explainable ai: exploring algorithm visualisations and user biases\",\n                \"Human-centered explainability for life sciences, healthcare, and medical informatics\",\n                \"The false hope of current approaches to explainable artificial intelligence in health care\",\n                \"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI\",\n                \"Explainable agents for less bias in human-agent decision making\",\n                \"European Union regulations on algorithmic decision-making and a “right to explanation”\",\n                \"The eu general data protection regulation (gdpr)\",\n                \"“meaningful information” and the right to explanation\",\n                \"A right to reasonable inferences: re-thinking data protection law in the age of big data and ai\",\n                \"Application of explainable artificial intelligence for healthcare: A systematic review of the last decade (2011–2022)\",\n                \"A systematic review of human–computer interaction and explainable artificial intelligence in healthcare with artificial intelligence techniques\",\n                \"Explainable, trustworthy, and ethical machine learning for healthcare: A survey\",\n                \"Explainable artificial intelligence methods in combating pandemics: A systematic review\",\n                \"Methods for interpreting and understanding deep neural networks\",\n                \"Explanation in Artificial Intelligence: Insights from the Social Sciences\",\n                \"Ai: To interpret or to explain?\",\n                \"Clinical prediction models: diagnosis versus prognosis\",\n                \"Clinical text data in machine learning: systematic review\",\n                \"Deep ehr: a survey of recent advances in deep learning techniques for electronic health record (ehr) analysis\",\n                \"Explaining Explanations: An Overview of Interpretability of Machine Learning\",\n                \"Interpretability in healthcare: A comparative study of local machine learning interpretability techniques\",\n                \"Explainable ai: A review of machine learning interpretability methods\",\n                \"Rising to the challenge of bias in health care ai\",\n                \"Detection and evaluation of machine learning bias\",\n                \"A Survey on Bias and Fairness in Machine Learning\",\n                \"Fairsight: Visual analytics for fairness in decision making\",\n                \"Themis-ml: A fairness-aware machine learning interface for end-to-end discrimination discovery and mitigation\",\n                \"Counterfactual Fairness\",\n                \"Opportunities and challenges in explainable artificial intelligence (xai): A survey\",\n                \"Stakeholders in Explainable AI\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]49 based on the context and research area of Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}