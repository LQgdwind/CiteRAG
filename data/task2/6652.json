{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: General regularization in covariate shift adaptation\n\nPaper Abstract: \n\nPaper Section Text:\nIn the literature, various RKHS-based approaches are available for a Radon - NikodÃ½m derivative estimation. Here we may refer to  and to references therein. Conceptually, under the assumption that Î²âˆˆâ„‹Kğ›½subscriptâ„‹ğ¾\\beta\\in{\\mathcal{H}_{K}}, several of the above approaches can be derived from a regularization of an integral equation, which can be written in our terms as and is ill-pose similar to (LABEL:A.eq2.1.7). Here ğŸ1\\mathbf{1} is the constant function that takes the value 111 everywhere, and almost without loss of generality, we assume that ğŸâˆˆâ„‹K1subscriptâ„‹ğ¾\\mathbf{1}\\in{\\mathcal{H}_{K}}, because otherwise the kernel K1â€‹(x,xâ€²)=1+Kâ€‹(x,xâ€²)subscriptğ¾1ğ‘¥superscriptğ‘¥â€²1ğ¾ğ‘¥superscriptğ‘¥â€²K_{1}(x,x^{\\prime})=1+K(x,x^{\\prime}) will, for example, be used to generate a suitable RKHS containing all constant functions. Just as the equation (LABEL:A.eq2.1.7) is inaccessible, so is the equation (LABEL:A.eq3.1.1). But in contrast to (LABEL:A.eq2.1.7), the reduction of (LABEL:A.eq3.1.1) to a finite-dimensional problem does not require any labels, such as ğ²,ğ²{\\bf y}, that were necessary for dealing with the normal equation SSXâˆ—â€‹Bâ€‹SSXâ€‹f=Sğ±âˆ—â€‹Bâ€‹ğ²superscriptsubscriptğ‘†subscriptğ‘†ğ‘‹ğµsubscriptğ‘†subscriptğ‘†ğ‘‹ğ‘“subscriptsuperscriptğ‘†ğ±ğµğ²S_{S_{X}}^{*}BS_{S_{X}}f=S^{*}_{{{\\bf x}}}B{\\bf y}. Since in practice, the amount of unlabeled inputs is usually much greater than that of labeled ones, we assume that the sizes Mğ‘€M and Nğ‘N of i.i.d. samples (x1â€²,x2â€²,â€¦,xMâ€²)superscriptsubscriptğ‘¥1â€²superscriptsubscriptğ‘¥2â€²â€¦superscriptsubscriptğ‘¥ğ‘€â€²(x_{1}^{\\prime},x_{2}^{\\prime},\\ldots,x_{M}^{\\prime}) and (x1,x2,â€¦,xN)subscriptğ‘¥1subscriptğ‘¥2â€¦subscriptğ‘¥ğ‘(x_{1},x_{2},\\ldots,x_{N}) drawn respectively from ÏTsubscriptğœŒğ‘‡\\rho_{T} and ÏSsubscriptğœŒğ‘†\\rho_{S} are much larger than mğ‘šm and nğ‘›n appearing in Theorem LABEL:A.thm2.1. Then we consider two sample operators and the finite-dimensional problem which is an empirical version of the equation (LABEL:A.eq3.1.1), where, similar to the above notations the operators SN,Sâˆ—:RNâ†’â„‹K,:superscriptsubscriptğ‘†ğ‘ğ‘†â†’superscriptğ‘…ğ‘subscriptâ„‹ğ¾S_{N,S}^{*}:\\mathbbm R^{N}\\rightarrow{\\mathcal{H}_{K}}, SM,Tâˆ—:RMâ†’â„‹K:superscriptsubscriptğ‘†ğ‘€ğ‘‡â†’superscriptğ‘…ğ‘€subscriptâ„‹ğ¾S_{M,T}^{*}:\\mathbbm R^{M}\\rightarrow{\\mathcal{H}_{K}} are given as A regularization of equations (LABEL:A.eq3.1.1), (LABEL:A.eq3.1.3) may serve as a starting point for several approaches of estimating the Radon - NikodÃ½m derivative Î²ğ›½\\beta. For example, as it has been observed in , the known kernel mean matching (KMM) method  can be viewed as the regularization of (LABEL:A.eq3.1.1), (LABEL:A.eq3.1.3) by the method of quasi (least-squares) solutions, originally proposed by Valentin Ivanov (1963) and also known as Ivanov regularization (see, e.g.,  and 8 for its use in the context of learning). In KMM\nan Ivanov-type regularization is applied to the empirical version (LABEL:A.eq3.1.3) and leads to a quadratic problem. At the same time, the kernelized unconstrained least-squares importance fitting (KuLSIF) proposed in  allows an analytic-form solution and can be reduced to solving a linear problem with respect to corresponding variables. From Theorem 1 of  it follows that in KuLSIF the approximation Î²~~ğ›½\\tilde{\\beta} of the Radon - NikodÃ½m derivative Î²=dâ€‹ÏTdâ€‹ÏSğ›½ğ‘‘subscriptğœŒğ‘‡ğ‘‘subscriptğœŒğ‘†\\beta=\\frac{d\\rho_{T}}{d\\rho_{S}} is in fact constructed by application of the Tikhonov regularization scheme to the empirical version (LABEL:A.eq3.1.3) of the equation (LABEL:A.eq3.1.1), that is in KuLSIF we have where gÎ»â€‹(t)=(Î»+t)âˆ’1.subscriptğ‘”ğœ†ğ‘¡superscriptğœ†ğ‘¡1g_{\\lambda}(t)=(\\lambda+t)^{-1}. Though there are several studies devoted to KMM and KuLSIF, to the best of our knowledge there has been no study of pointwise approximation error Î²â€‹(x)âˆ’Î²~â€‹(x)ğ›½ğ‘¥~ğ›½ğ‘¥\\beta(x)-\\tilde{\\beta}(x), which is of interest in analysis of regularized domain adaptation methods, such as IWRLS. For example, in  and  (see Type I setting there) the statistical consistency and accuracy of KuLSIF have been analysed in the space L2,ÏSsubscriptğ¿2subscriptğœŒğ‘†L_{2,\\rho_{S}}, where pointwise evaluations are undefined. We can also mention the study , where KuLSIF represented as (LABEL:A.eq3.1.4) with gÎ»â€‹(t)=(Î»+t)âˆ’1subscriptğ‘”ğœ†ğ‘¡superscriptğœ†ğ‘¡1g_{\\lambda}(t)=(\\lambda+t)^{-1} was discussed in a RKHS, but only convergence of Î²~~ğ›½\\tilde{\\beta} to Î²ğ›½\\beta was proved, without quantifying its rate. At the same time, using the concept of source conditions naturally appearing because of equation (LABEL:A.eq3.1.1), we can obtain the following statement thm3.1\nAssume that Î²=dâ€‹ÏTdâ€‹ÏSğ›½ğ‘‘subscriptğœŒğ‘‡ğ‘‘subscriptğœŒğ‘†\\beta=\\frac{d\\rho_{T}}{d\\rho_{S}} meets source condition Î²=Ï•â€‹(JSâˆ—â€‹JS)â€‹Î½Î²ğ›½italic-Ï•superscriptsubscriptğ½ğ‘†subscriptğ½ğ‘†subscriptğœˆğ›½\\beta=\\phi(J_{S}^{*}J_{S})\\nu_{\\beta}, where Ï•âˆˆâ„±â€‹(0,c),italic-Ï•â„±0ğ‘\\phi\\in\\mathcal{F}(0,c), and cğ‘c is large enough. Consider the approximant Î²M,NÎ»superscriptsubscriptğ›½ğ‘€ğ‘ğœ†\\beta_{M,N}^{\\lambda} given by (LABEL:A.eq3.1.4), where the regularization scheme indexed by gÎ»â€‹(t)subscriptğ‘”ğœ†ğ‘¡g_{\\lambda}(t) has the qualification Î½ğœˆ\\nu that covers the index function Ï•â€‹(t)italic-Ï•ğ‘¡\\phi(t). Let Î»=Î»M,N=Î¸Ï•âˆ’1â€‹(Mâˆ’12+Nâˆ’12),ğœ†subscriptğœ†ğ‘€ğ‘superscriptsubscriptğœƒitalic-Ï•1superscriptğ‘€12superscriptğ‘12\\lambda=\\lambda_{M,N}=\\theta_{\\phi}^{-1}(M^{-\\frac{1}{2}}+N^{-\\frac{1}{2}}), where Î¸Ï•â€‹(t)=Ï•â€‹(t)â€‹t.subscriptğœƒitalic-Ï•ğ‘¡italic-Ï•ğ‘¡ğ‘¡\\theta_{\\phi}(t)=\\phi(t)t. Then for sufficiently large Mğ‘€M an Nğ‘N with probability at least 1âˆ’Î´1ğ›¿1-\\delta it holds rem_new2\nAs we already mentioned, for gÎ»â€‹(t)=(Î»+t)âˆ’1subscriptğ‘”ğœ†ğ‘¡superscriptğœ†ğ‘¡1g_{\\lambda}(t)=(\\lambda+t)^{-1} the convergence in probability of the approximation (LABEL:A.eq3.1.4) to Î²ğ›½\\beta in RKHS has been proven in . Such convergence has been established in  on the base of an error estimation (see Supplementary material A in ), which in our terms can be written as follows: where Î²Î»=(Î»â€‹I+JSâˆ—â€‹JS)âˆ’1â€‹JTâˆ—â€‹JTâ€‹ğŸ,superscriptğ›½ğœ†superscriptğœ†ğ¼superscriptsubscriptğ½ğ‘†subscriptğ½ğ‘†1superscriptsubscriptğ½ğ‘‡subscriptğ½ğ‘‡1\\beta^{\\lambda}=(\\lambda I+J_{S}^{*}J_{S})^{-1}J_{T}^{*}J_{T}\\mathbf{1}, and 0<a<12,0<b<12.formulae-sequence0ğ‘120ğ‘120<a<\\frac{1}{2},0<b<\\frac{1}{2}.\nNote that in the regularization theory, the quantities âˆ¥Î²âˆ’Î²Î»âˆ¥delimited-âˆ¥âˆ¥ğ›½superscriptğ›½ğœ†\\left\\lVert\\beta-\\beta^{\\lambda}\\right\\rVert are sometimes called the profile functions, and Corollary 2 of  estimates them in terms of the source condition Î²=Ï•â€‹(JSâˆ—â€‹JS)â€‹Î½Î²ğ›½italic-Ï•superscriptsubscriptğ½ğ‘†subscriptğ½ğ‘†subscriptğœˆğ›½\\beta=\\phi(J_{S}^{*}J_{S})\\nu_{\\beta} as follows Furthermore, the bound presented in Theorem LABEL:thm3.1 can be expressed in the following form Comparing (LABEL:new_eq2) - (LABEL:new_eq1) and keeping in mind that Mâˆ’12+Nâˆ’12Î»superscriptğ‘€12superscriptğ‘12ğœ†\\frac{M^{-\\frac{1}{2}}+N^{-\\frac{1}{2}}}{\\lambda} is smaller in the sense of the order than (Nâˆ’aÎ»2+Mâˆ’bÎ»)superscriptğ‘ğ‘superscriptğœ†2superscriptğ‘€ğ‘ğœ†\\left(\\frac{N^{-a}}{\\lambda^{2}}+\\frac{M^{-b}}{\\lambda}\\right), one can conclude that the error bound (LABEL:new_eq1) obtained by our argument generalized and refines the results of . rem_new3\nRecall that for our purpose we restrict ourselves to the estimation of accuracy of (LABEL:A.eq3.1.4) in RKHS. At the same time, there are studies, where the accuracy of (LABEL:A.eq3.1.4) has been analysed in the space L2,ÏSsubscriptğ¿2subscriptğœŒğ‘†L_{2,\\rho_{S}}. For example, in  L2,ÏSsubscriptğ¿2subscriptğœŒğ‘†L_{2,\\rho_{S}} - convergence rate of KuLSIF estimator, which corresponds to (LABEL:A.eq3.1.4) with gÎ»â€‹(t)=(Î»+t)âˆ’1,subscriptğ‘”ğœ†ğ‘¡superscriptğœ†ğ‘¡1g_{\\lambda}(t)=(\\lambda+t)^{-1}, has been established in terms of the order Î³ğ›¾\\gamma of the so-called bracketing entropy of the underlying space â„‹Ksubscriptâ„‹ğ¾\\mathcal{H}_{K}. For a given space â„‹Ksubscriptâ„‹ğ¾\\mathcal{H}_{K}, the Î³ğ›¾\\gamma- value is fixed within the interval (0,2)02(0,2), i.e., 0<Î³<2,0ğ›¾20<\\gamma<2, and Theorem 2 of  tells that if Î²âˆˆâ„‹K,ğ›½subscriptâ„‹ğ¾\\beta\\in\\mathcal{H}_{K}, then for arbitrary Ïµitalic-Ïµ\\epsilon satisfying 1âˆ’2(2+Î³)<Ïµ<1,122ğ›¾italic-Ïµ11-\\frac{2}{(2+\\gamma)}<\\epsilon<1, and Î»M,N,Ïµâˆ’1=Oâ€‹((Nâˆ§M)1âˆ’Ïµ)superscriptsubscriptğœ†ğ‘€ğ‘italic-Ïµ1ğ‘‚superscriptğ‘ğ‘€1italic-Ïµ\\lambda_{M,N,\\epsilon}^{-1}=O\\left((N\\wedge M)^{1-\\epsilon}\\right) with high probability it holds where Nâˆ§M=minâ¡{N,M}ğ‘ğ‘€ğ‘ğ‘€N\\wedge M=\\min\\{N,M\\}. Note that the rate established by (LABEL:new_eq4) cannot be better than Oâ€‹((Nâˆ§M)âˆ’12+Î³)ğ‘‚superscriptğ‘ğ‘€12ğ›¾O\\left((N\\wedge M)^{-\\frac{1}{2+\\gamma}}\\right), and it does not take into account additional smoothness that any particular element Î²âˆˆâ„‹Kğ›½subscriptâ„‹ğ¾\\beta\\in\\mathcal{H}_{K} has in the underlying space â„‹Ksubscriptâ„‹ğ¾\\mathcal{H}_{K}. Such additional smoothness can be caught in the form of a source condition because as we already know from , there is always an index function Ï•italic-Ï•\\phi such that Î²=Ï•â€‹(JSâˆ—â€‹JS)â€‹Î½Î².ğ›½italic-Ï•superscriptsubscriptğ½ğ‘†subscriptğ½ğ‘†subscriptğœˆğ›½\\beta=\\phi(J_{S}^{*}J_{S})\\nu_{\\beta}. Then assuming that Ï•â€‹(t)â€‹titalic-Ï•ğ‘¡ğ‘¡\\phi(t)\\sqrt{t} is covered by the qualification of the regularization used in (LABEL:A.eq3.1.4), and applying almost the same argument as in Theorem LABEL:thm3.1 we can obtain the bound To simplify a comparison of (LABEL:new_eq5) with the best possible rate Oâ€‹((Nâˆ§M)âˆ’12+Î³)ğ‘‚superscriptğ‘ğ‘€12ğ›¾O\\left((N\\wedge M)^{-\\frac{1}{2+\\gamma}}\\right) of (LABEL:new_eq4) we consider the same index functions Ï•â€‹(t)=trâˆ’12italic-Ï•ğ‘¡superscriptğ‘¡ğ‘Ÿ12\\phi(t)=t^{r-\\frac{1}{2}} as in Section LABEL:sec2.3. Then (LABEL:new_eq5) gives the rate of order Oâ€‹((Mâˆ’12+Nâˆ’12)2â€‹r2â€‹r+1)ğ‘‚superscriptsuperscriptğ‘€12superscriptğ‘122ğ‘Ÿ2ğ‘Ÿ1O\\left((M^{-\\frac{1}{2}}+N^{-\\frac{1}{2}})^{\\frac{2r}{2r+1}}\\right), and for r>1Î³ğ‘Ÿ1ğ›¾r>\\frac{1}{\\gamma} this rate is better than the one given by (LABEL:new_eq4), because This is one more example of how the study generalizes, specifies and refines previously known results. In, we highlight that the convergence of algorithms for Radon-NikodÃ½m differentiation is impacted by both, the smoothness of the function being approximated and the capacity of the approximating space. However, the result in Theorem LABEL:thm3.1 only takes into consideration the smoothness of the approximated derivative Î²ğ›½\\beta. Therefore, we follow  and employ the concept of the so-called regularized Christoffel function that allows direct incorporation of the regularization parameter Î»ğœ†\\lambda into the definition of a capacity measure.\nConsider the function Note that in  the reciprocal of CÎ»â€‹(x)subscriptğ¶ğœ†ğ‘¥C_{\\lambda}(x), i.e. 1CÎ»â€‹(x)1subscriptğ¶ğœ†ğ‘¥\\frac{1}{C_{\\lambda}(x)}, was called the regularized Christoffel function, but for the sake of simplicity, we will keep the same name also for (LABEL:def:Chris_func). Note also that in the context of supervised learning, where usually only one probability measure, say pğ‘p, is involved, the expected value of CÎ»â€‹(x)subscriptğ¶ğœ†ğ‘¥C_{\\lambda}(x), called the effective dimension, has been proven to be useful . This function is also frequently used as a capacity measure of â„‹Ksubscriptâ„‹ğ¾{\\mathcal{H}_{K}}. At the same time, if more than one measure appears in the supervised learning context, as is, for example, the case in the analysis of NystrÃ¶m subsampling  , then the Cğ¶C-norm of the regularized Christoffel function is used in parallel with the effective dimension ğ’©â€‹(Î»)ğ’©ğœ†\\mathcal{N}(\\lambda). This gives a hint that ğ’©âˆâ€‹(Î»)subscriptğ’©ğœ†\\mathcal{N}_{\\infty}(\\lambda) could also be a suitable capacity measure for analysing the accuracy of Radon-NikodÃ½m numerical differentiation, since there more than one measure is also involved. To estimate the regularized Christoffel functions we slightly generalize a source condition for kernel sections Kâ€‹(â‹…,x)ğ¾â‹…ğ‘¥K(\\cdot,x) that has been used in various contexts in  and [ref]6. assum:source_cond_kernel\n(Source condition for kernel) There is an operator concave index function Î¾:[0,âˆ¥JSâˆ—â€‹JSâˆ¥]â†’[0,âˆ):ğœ‰â†’0delimited-âˆ¥âˆ¥subscriptsuperscriptğ½ğ‘†subscriptğ½ğ‘†0\\xi:[0,\\left\\lVert J^{*}_{S}J_{S}\\right\\rVert]\\to[0,\\infty) such that Î¾2superscriptğœ‰2\\xi^{2} is covered by the qualification Î½=1ğœˆ1\\nu=1, and for all xâˆˆğ—ğ‘¥ğ—x\\in\\mathbf{X}, where cğ‘c does not depend on xğ‘¥x. We mention the following consequence of Assumption LABEL:assum:source_cond_kernel. lem:N_infty_bound\nUnder Assumption LABEL:assum:source_cond_kernel, By taking into account the smoothness properties of the Radon-NikodÃ½m derivative Î²ğ›½\\beta and the capacity of â„‹Ksubscriptâ„‹ğ¾{\\mathcal{H}_{K}} expressed in terms of the regularized Christoffel functions, in, we establish a novel bound that relates Î²M,NÎ»superscriptsubscriptğ›½ğ‘€ğ‘ğœ†\\beta_{M,N}^{\\lambda} to Î²ğ›½\\beta in RKHS. thm4.5\nLet Kğ¾K satisfies Assumption LABEL:assum:source_cond_kernel, and Î»â‰¥Î»âˆ—ğœ†superscriptğœ†\\lambda\\geq\\lambda^{*} with Î»âˆ—subscriptğœ†\\lambda_{*} satisfying ğ’©â€‹(Î»âˆ—)/Î»âˆ—=Nğ’©subscriptğœ†subscriptğœ†ğ‘\\mathcal{N}(\\lambda_{*})/\\lambda_{*}=N. If Î²=dâ€‹ÏTdâ€‹ÏSğ›½ğ‘‘subscriptğœŒğ‘‡ğ‘‘subscriptğœŒğ‘†\\beta=\\frac{d\\rho_{T}}{d\\rho_{S}} meets source condition (LABEL:A.eq2.1.8) Î²=Ï•â€‹(JSâˆ—â€‹JS)â€‹Î½Î²ğ›½italic-Ï•superscriptsubscriptğ½ğ‘†subscriptğ½ğ‘†subscriptğœˆğ›½\\beta=\\phi(J_{S}^{*}J_{S})\\nu_{\\beta}, where Ï•=Ï‘â€‹Ïˆâˆˆâ„±â€‹(0,c)italic-Ï•italic-Ï‘ğœ“â„±0ğ‘\\phi=\\vartheta\\psi\\in\\mathcal{F}(0,c) with large enough cğ‘c, and the qualification gÎ»subscriptğ‘”ğœ†g_{\\lambda} covers Ï‘â€‹(t)â€‹t32italic-Ï‘ğ‘¡superscriptğ‘¡32\\vartheta(t)t^{\\frac{3}{2}}, then with probability at least 1âˆ’Î´1ğ›¿1-\\delta, it holds Consider Î¸Ï•,Î¾â€‹(t)=Ï•â€‹(t)â€‹tÎ¾â€‹(t)subscriptğœƒitalic-Ï•ğœ‰ğ‘¡italic-Ï•ğ‘¡ğ‘¡ğœ‰ğ‘¡\\theta_{\\phi,\\xi}(t)=\\frac{\\phi(t)t}{\\xi(t)} and Î»=Î»M,N=Î¸Ï•,Î¾âˆ’1â€‹(Mâˆ’12+Nâˆ’12)ğœ†subscriptğœ†ğ‘€ğ‘superscriptsubscriptğœƒitalic-Ï•ğœ‰1superscriptğ‘€12superscriptğ‘12\\lambda=\\lambda_{M,N}=\\theta_{\\phi,\\xi}^{-1}(M^{-\\frac{1}{2}}+N^{-\\frac{1}{2}}), then rem_sec4\nIn order to compare the bounds presented in Theorem LABEL:thm3.1 and Theorem LABEL:thm4.5, let us consider the case when Î²ğ›½\\beta meets the source condition (LABEL:A.eq2.1.8) with Ï•â€‹(t)=tÎ·italic-Ï•ğ‘¡superscriptğ‘¡ğœ‚\\phi(t)=t^{\\eta}.\nIn this case the bound (LABEL:1st_bound_beta) can be reduced to It is noteworthy that the error bound established in Theorem LABEL:thm3.1 does not take into consideration the capacity of â„‹Ksubscriptâ„‹ğ¾{\\mathcal{H}_{K}}. Such an additional factor can be accounted for in terms of Assumption LABEL:assum:source_cond_kernel. Assume that Kğ¾K satisfies Assumption 3.2 with Î¾â€‹(t)=tÏ‚,0<Ï‚â‰¤12formulae-sequenceğœ‰ğ‘¡superscriptğ‘¡ğœ0ğœ12\\xi(t)=t^{\\varsigma},0<\\varsigma\\leq\\frac{1}{2}, then for Î»=Î»M,N=Î¸Ï•,Î¾âˆ’1â€‹(Mâˆ’12+Nâˆ’12)ğœ†subscriptğœ†ğ‘€ğ‘superscriptsubscriptğœƒitalic-Ï•ğœ‰1superscriptğ‘€12superscriptğ‘12\\lambda=\\lambda_{M,N}=\\theta_{\\phi,\\xi}^{-1}(M^{-\\frac{1}{2}}+N^{-\\frac{1}{2}}), the bound in Theorem LABEL:thm4.5 gives that is better than the order of accuracy given by (LABEL:rate_in_acha). Then one can conclude that the bound in Theorem LABEL:thm4.5 obtained by our argument generalizes, specifies, and refines the results in Theorem LABEL:thm3.1.\n\nReference markers found: [6]\n\nContext windows for each [ref]:\n[ref]6: is also involved. To estimate the regularized Christoffel functions we slightly generalize a source condition for kernel sections Kâ€‹(â‹…,x)ğ¾â‹…ğ‘¥K(\\cdot,x) that has been used in various contexts in  and [ref]6. assum:source_cond_kernel\n(Source condition for kernel) There is an operator concave index function Î¾:[0,âˆ¥JSâˆ—â€‹JSâˆ¥]â†’[0,âˆ):ğœ‰â†’0delimited-âˆ¥âˆ¥subscriptsuperscriptğ½ğ‘†subscriptğ½ğ‘†0\\xi:[0,\\left\\lVert\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 6,\n            \"titles\": [\n                \"Addressing Parameter Choice Issues in Unsupervised Domain Adaptation by Aggregation\",\n                \"On regularization algorithms in learning theory\",\n                \"Optimal rates for the regularized least-squares algorithm\",\n                \"On the mathematical foundations of learning\",\n                \"Discretization error analysis for Tikhonov regularization in learning theory\",\n                \"Learning from examples as an inverse problem\",\n                \"Learning sets with separating kernels\",\n                \"Regularization networks and support vector machines\",\n                \"On a regularization of unsupervised domain adaptation in rkhs\",\n                \"Statistical outlier detection using direct density ratio estimation\",\n                \"Correcting sample selection bias by unlabeled data\",\n                \"A least-squares approach to direct importance estimation\",\n                \"fğ‘“f-divergence estimation and two-sample homogeneity test under semiparametric density-ratio models\",\n                \"Statistical analysis of kernel-based least-squares density-ratio estimation\",\n                \"Learning from positive and unlabeled data with a selection bias\",\n                \"Test of homogeneity in semiparametric two-sample density ratio models\",\n                \"Analysis of regularized NystrÃ¶m subsampling for regression functions of low smoothness\",\n                \"Balancing principle in supervised learning for a general regularization scheme\",\n                \"Regularization Theory for Ill-posed Problems - Selected Topics\",\n                \"How general are general source conditions?\",\n                \"Domain adaptation for segmentation of critical structures for prostate cancer therapy\",\n                \"Learning in Implicit Generative Models\",\n                \"On regularized Radon-Nikodym differentiation\",\n                \"Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization\",\n                \"Estimating divergence functionals and the likelihood ratio by convex risk minimization\",\n                \"Domain-invariant partial-least-squares regression\",\n                \"Tikhonov, Ivanov and Morozov regularization for support vector machine learning\",\n                \"Ivanov-regularised least-squares estimators over large rkhss and their interpolation spaces\",\n                \"Relating Leverage Scores and Density using Regularized Christoffel Functions\",\n                \"Inverse density as an inverse problem: The Fredholm equation approach\",\n                \"Spectral algorithms for supervised learning\",\n                \"Less is More: \\\\Nystrom Computational Regularization\",\n                \"Kernel conditional density operators\",\n                \"Improving predictive inference under covariate shift by weighting the log-likelihood function\",\n                \"Learning theory estimates via integral operators and their approximations\",\n                \"Relative novelty detection\",\n                \"Input-dependent estimation of generalization error under covariate shift\",\n                \"Density Ratio Estimation in Machine Learning\",\n                \"The Nature of Statistical Learning Theory\",\n                \"Generated Reference Title 39 for [ref]6\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]6 based on the context and research area of General regularization in covariate shift adaptation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}