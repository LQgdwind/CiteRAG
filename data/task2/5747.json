{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Logic of Differentiable Logics: Towards a Uniform Semantics of DL\n\nPaper Abstract: \n\nPaper Section Text:\nLogics for reasoning with uncertainty and probability have long been known and used in programming: e.g. fuzzy logicÂ , probabilistic logicÂ  and variants of thereof in logic programming domainÂ    . Recently, rising awareness of the problems related to machine learning verification opened a novel application area for those ideas.\nDifferentiable Logics (DLs) is a family of methods that applies key insights from fuzzy logic and probability theory to enhance this domain with property-driven learningÂ 6. As a motivating example, consider the problem of verification of neural networks. A neural network is a function\nf:â„nâ†’â„m:ğ‘“â†’superscriptâ„ğ‘›superscriptâ„ğ‘šf:{\\mathbb{R}}^{n}\\rightarrow{\\mathbb{R}}^{m} parametrised by a set of weights ğ°ğ°\\mathbf{w}. A training datasetÂ ğ’³ğ’³\\mathcal{X} is a set of pairsÂ (ğ±,ğ²)ğ±ğ²(\\mathbf{x},\\mathbf{y}) consisting of an input ğ±âˆˆâ„nğ±superscriptâ„ğ‘›\\mathbf{x}\\in{\\mathbb{R}}^{n} and the desired output ğ²âˆˆâ„mğ²superscriptâ„ğ‘š\\mathbf{y}\\in{\\mathbb{R}}^{m}. It is assumed that the outputs ğ²ğ²\\mathbf{y} are generated from ğ±ğ±\\mathbf{x} by some functionÂ â„‹:â„nâ†’â„m:â„‹â†’superscriptâ„ğ‘›superscriptâ„ğ‘š\\mathcal{H}:{\\mathbb{R}}^{n}\\rightarrow{\\mathbb{R}}^{m} and that ğ±ğ±\\mathbf{x} is drawn from some probability distribution over â„nsuperscriptâ„ğ‘›{\\mathbb{R}}^{n}.\nThe goal of training is to use the datasetÂ ğ’³ğ’³\\mathcal{X} to find weightsÂ ğ°ğ°\\mathbf{w} such thatÂ fğ‘“f approximatesÂ â„‹â„‹\\mathcal{H} well over input regions with high probability density.\nThe standard approach is to use a loss function â„’â„’\\mathcal{L}, that given a pair (ğ±,ğ²)ğ±ğ²(\\mathbf{x},\\mathbf{y}) calculates how much fâ€‹(ğ±)ğ‘“ğ±f(\\mathbf{x}) differs from the desired output ğ²ğ²\\mathbf{y}.\nGradients of â„’â„’\\mathcal{L} with respect to the networkâ€™s weights can then be used to update the weights during training. However in addition to the dataset ğ’³ğ’³\\mathcal{X}, in certain problem domains we may have additional information about â„‹â„‹\\mathcal{H} in the form of a mathematical property Ï•italic-Ï•\\phi that we know â„‹â„‹\\mathcal{H} must satisfy.\nGiven that â„‹â„‹\\mathcal{H} satisfies property Ï•italic-Ï•\\phi, we would like to ensure that fğ‘“f also satisfies Ï•italic-Ï•\\phi.\nA common example of such a property is that â„‹â„‹\\mathcal{H} should be robust, i.e.Â small perturbations to the input only result in small perturbations to the output. For example, in image classification tasks changing a single pixel should not significantly alter what the image is classified asÂ  . Given constants Ïµ,Î´âˆˆâ„italic-Ïµğ›¿â„\\epsilon,\\delta\\in{\\mathbb{R}}, a function fğ‘“f is Ïµitalic-Ïµ\\epsilon-Î´ğ›¿\\delta-robust around a point ğ±^âˆˆâ„n^ğ±superscriptâ„ğ‘›\\hat{\\mathbf{x}}\\in{\\mathbb{R}}^{n} if: Casadio etÂ al.  formulate several possible notions of robustness. Throughout this paper, we will use the term â€œrobustness propertyâ€ to refer to the above definition.\nThe problem of verifying the robustness of neural networks has received significant attention from the verification communityÂ  [ref]44, and it is known to be difficult both theoreticallyÂ  and in practiceÂ .\nHowever, even leaving aside the challenges of undecidability of non-linear real arithmeticÂ , and scalabilityÂ [ref]44, the biggest obstacle to verification is that the majority of neural networks do not succeed in learning Ï•italic-Ï•\\phi from the training dataset ğ’³ğ’³\\mathcal{X}Â 6 . The concept of a differentiable logic (DL) was introduced to address this challenge by verification-motivated training. This idea is sometimes referred to as continuous verificationÂ  , referring to the loop between training and verification.\nThe key idea in differentiable logic is to use Ï•italic-Ï•\\phi to generate an alternative logical loss function â„’Ï•subscriptâ„’italic-Ï•\\mathcal{L}_{\\phi}, that calculates a penalty depending on how much fğ‘“f deviates from satisfying Ï•italic-Ï•\\phi. When combined with the standard data-driven loss function â„’â„’\\mathcal{L}, the network is trained to both fit the data and satisfy Ï•italic-Ï•\\phi. A DL therefore has two components: a suitable language for expressing the properties and an interpretation function that can translate expressions in the language into a suitable loss function. Although the idea sounds simple, developing good principles of DL design has been surprisingly challenging. The machine-learning community has proposed several DLs such as DL2 for supervised learningÂ , and a signal temporal logic based-DL for reinforcement learning (STL)Â . However, both approaches had shortcomings from the perspective of formal logic:\nthe former fell short of introducing quantifiers as part of the language (and thus modelled robustness semi-formally in all experiments),\nand the latter only covered propositional fragment (which is not sufficient to reasoning about robustness). Solutions were offered from the perspective of formal logic. In Â 8 9 it was shown that one can use various propositional fuzzy logics to create loss functions. However, these fuzzy DLs did not stretch to cover the features of the DLs that came from machine learning community, and in particular could not stretch to express the above robustness property (DefinitionÂ 0.1.1), which needs not just quantification over infinite domains (quantifiers for finite domains were given inÂ 8), but also a formalism to express properties concerning vectors and functions over vectors. The first problem is thus: There does not exist a DL that formally covers a sufficient fragment of first-order logic to express key properties in machine learning verification, such as robustness. The second problem has to do with formalisation of different DLs.\nIn many of the existing DL approaches syntax, semantics and pragmatics are not well-separated, which inhibits their formal analysis. We have already given an example of DL2 treating quantifiers empirically outside of the language.\nBut the problem runs deeper than modelling quantifiers. To illustrate, let us take a fragment of syntax on which all DLs are supposed to agree. It will give us a toy propositional language They assume that each propositional variable pğ‘p is interpreted in a domain ğ’ŸâŠ†â„ğ’Ÿâ„{\\mathcal{D}}\\subseteq{\\mathbb{R}}. The domains vary vastly across the DLs (from fuzzy set    to (âˆ,âˆ)(\\infty,\\infty) in STL) and the choice of a domain can have important ramifications for both the syntax and the semantics. For example DL2â€™s domain [0,âˆ]0[0,\\infty] severely restricts the translation of negation compared to other DLs. Conjunction is interpreted by ++ in DL2Â , by mâ€‹iâ€‹nğ‘šğ‘–ğ‘›min, Ã—\\times and other more complex operations in different fuzzy DLs in van Krieken etÂ al. 9.\nIn STLÂ , in order to make loss functions satisfy a shadow-lifting property,\nthe authors propose a complex formula computing conjunction, that includes natural exponents alongside other operations.\nBut this forces them to redefine the syntax for conjunction and thus obtain a different language where â‹€Msubscriptğ‘€\\bigwedge_{M} denotes a (non-associative!) conjunction for Mğ‘€M elements. As consequence of the above two problems, the third problem is lack of unified, general syntax and semantics able to express multiple different DLs and modular on the choice of DL, that would make it possible to choose one best suited for concrete task or design new DLs in an easy way. In this paper, we propose a solution to all of these problems. The solution comes in a form of a meta-DL, which we call a logic of differentiable logics (LDL). On the syntactic side (SectionÂ 0.3),\nit is a typed first-order language with negation and universal and existential quantification that can express properties of functions and vectors. On the semantic side (SectionÂ 0.4), interpretation is defined to be parametric on the choice of the interpretation domain ğ’Ÿğ’Ÿ{\\mathcal{D}} or a particular choice of logical connectives.\nThis parametric nature of interpretation simplifies both the theoretical study and implementations that compare different DLs. Moreover, the language has an implicit formal treatment of neural networks via a special kind of context â€“ a solution that we found necessary in achieving a sufficient level of generality in the semantics. For the first time the semantics formally introduces the notion of a probability distribution that corresponds to the data from which the data is assumed to be drawn. We demonstrate the power of this approach by using LDL to\nprove soundness of various DLs in SectionsÂ 0.5.1, 0.5.1 and systematically compare their geometric properties in SectionÂ 0.5.2. In SectionÂ 0.5.3 we use LDL to provide a uniform empirical comparison of performance of all mentioned DLs\non improving robustness.\n\nReference markers found: [44]\n\nContext windows for each [ref]:\n[ref]44: use the term â€œrobustness propertyâ€ to refer to the above definition.\nThe problem of verifying the robustness of neural networks has received significant attention from the verification communityÂ  [ref]44, and it is known to be difficult both theoreticallyÂ  and in practiceÂ .\nHowever, even leaving aside the challenges of undecidability of non-linear real arithmeticÂ , and scalabilityÂ [ref]44,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 44,\n            \"titles\": [\n                \"A Semantic Loss Function for Deep Learning with Symbolic Knowledge\",\n                \"Propositional Logics for the Lawvere Quantale\",\n                \"Logic tensor networks\",\n                \"The Second International Verification of Neural Networks Competition (VNN-COMP 2021): Summary and Results\",\n                \"Lambda calculus notation with nameless dummies, a tool for automatic formula manipulation, with application to the Church-Rosser theorem\",\n                \"Neural Network Robustness as a Verification Property: A Principled Case Study\",\n                \"Handbook of mathematical fuzzy logic (in 2 volumes), volume 37, 38 of studies in logic, mathematical logic and foundations\",\n                \"Vehicle: A high-level language for embedding logical specifications in neural networks\",\n                \"Compiling higher-order specifications to smt solvers: How to deal with rejection constructively\",\n                \"Probabilistic (logic) programming concepts\",\n                \"Problog: A probabilistic prolog and its application in link discovery\",\n                \"The mnist database of handwritten digit images for machine learning research\",\n                \"Neuro-Symbolic Constraint Programming for Structured Prediction\",\n                \"DL2: Training and querying neural networks with logic\",\n                \"Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning\",\n                \"Investigations into logical deduction\",\n                \"Deep Learning with Logical Constraints\",\n                \"Deep Learning\",\n                \"Distilling the Knowledge in a Neural Network\",\n                \"MultiplexNet: Towards Fully Satisfied Logical Constraints in Neural Networks\",\n                \"Harnessing Deep Neural Networks with Logic Rules\",\n                \"Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks\",\n                \"Triangular norms. position paper ii: general constructions and parameterized families\",\n                \"Neural networks, secure by construction: An exploration of refinement types\",\n                \"Vehicle\",\n                \"Continuous verification of machine learning: a declarative programming approach\",\n                \"Smoothness Analysis for Probabilistic Programs with Application to Optimised Variational Inference\",\n                \"Kripke semantics for higher-order type theory applied to constraint logic programming languages\",\n                \"Probabilistic logic programming\",\n                \"Cross-Entropy Loss and Low-Rank Features Have Responsibility for Adversarial Examples\",\n                \"An investigation for loss functions widely used in machine learning\",\n                \"Probabilistic logic\",\n                \"Foundations of probabilistic logic programming: Languages, semantics, inference and learning\",\n                \"An introduction to probability and statistical inference\",\n                \"An abstract domain for certifying neural networks\",\n                \"Lectures on the Curry-Howard isomorphism\",\n                \"Intriguing properties of neural networks\",\n                \"Analyzing Differentiable Fuzzy Implications\",\n                \"Analyzing Differentiable Fuzzy Logic Operators\",\n                \"On robustness metrics for learning stl tasks\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]44 based on the context and research area of Logic of Differentiable Logics: Towards a Uniform Semantics of DL. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}