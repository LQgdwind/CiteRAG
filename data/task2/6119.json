{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Sparse Stream Semantic Registers: A Lightweight ISA Extension Accelerating General Sparse Linear Algebra\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, we compare our approach to numerous state-of-the-art CPU, GPU, and accelerator solutions, covering both hardware and software proposals. We summarize our findings in two tables. Table II collects the evaluation platform, matrix format, and highest fraction of peak compute achieved on all discussed FP64 sM×dV software implementations. Table III compares the features, flexibility, and architectural cost of all discussed hardware designs. Scalar processor architectures: A processor with SSSRs can be considered an extended and specialized decoupled access/execute (DAE) architecture .\nDAE architectures combine two decoupled processors, one handling memory accesses and another execution, which communicate through register-mapped FIFOs.\nSSSRs provide a lightweight access processor design with multiple performance enhancements.\nThey leverage multi-element streams as abstractions, which improves decoupling, minimizes offloads from the main instruction stream, and attains near-ideal compute and memory throughputs.\nTheir job types define a compact, orthogonal instruction set that minimizes access code complexity and enables lightweight implementation.\nUnlike the original DAE concept, SSSRs support multiple concurrent streams on different registers.\nOur low-cost sparsity extensions maintain high throughput on indirect and joint streams without the hardware and bookkeeping overheads of a more conventional access processor architecture, enabling efficient general sparse compute without major execute processor modifications.\n\n Vector processor architectures: Indirection in ISSRs is similar to scatter-gather in vector architectures, which finds recent adoption in superscalar out-of-order processors. Arm’s Scalable Vector Extensions  and Intel’s Knights Landing (KNL)  add support for SIMD scatter-gather. However, the short lengths and alignment requirements of SIMD vectors limit scatter-gather benefits on sparse workloads compared to ISSRs.\nThe Unlimited Vector Extension (UVE)  relaxes vector length constraints and introduces vector-register-bound indirect streams similar to ISSRs. However, it requires vectorizable workloads and cannot operate efficiently at scalar granularity.\nGong et al.  extend a vector multiply-add unit to skip ineffectual and coalesce useful computations. While this is orthogonal to SSSRs, it is only effective on low, regular sparsity in uncompressed data. Software optimizations for scalar and vector processors: Numerous software solutions aim to accelerate sparse LA on scalar and vector processors.\nThe CVR matrix format  aims to improve sM×dV SIMD lane and cache efficiency on x86 CPUs. Still, it utilizes at most 0.69 %/times0.69dividepercentabsent0.69\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} of peak FP64 compute on Xeon Phi 7250.\nZhang et al.  optimize sM×dV for Intel’s AVX-512 extensions, proposing a format based on sliced ELLPACK (SELL) that enables peak FP64 compute utilizations of up to 1.5 %/times1.5dividepercentabsent1.5\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} on Xeon Phi 7230.\nRegu2D  leverages an adaptive 2D partitioning scheme and arranges rows as similar-sized groups with zero-padding as necessary to fill SIMD lanes, improving their platform’s peak FP64 utilization to 3.1 %/times3.1dividepercentabsent3.1\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/}.\nAlappat et al.  model the performance of sM×dV on Fujitsu’s A64FX using the SELL-C-\\textsigma matrix format and report up to 130.9 Gflop/stimes130.9dividegigaflopsecond130.9\\text{\\,}\\mathrm{Gflop}\\text{/}\\mathrm{s} or 4.7 %/times4.7dividepercentabsent4.7\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} of peak FP64 compute. Nevertheless, this is 9.9×\\times less than what we achieve on a Snitch cluster with SSSRs.\nZ. Xie et al.  present a CSR-interfaced sM×sM library for multi- and manycores selecting the best internal format and algorithm with a deep learning model based on nonzero patterns. Streaming ISA extensions: General-purpose processor extensions increasingly leverage streams as abstractions to accelerate compute.\nProdigy  uses compile-time-inferred metadata to prefetch nested indirect streams. However, it does not eliminate load-store or bookkeeping instructions.\nSpZip  reads and writes compressed irregular data using decoupled streaming units. This is similar to SSSRs reading and writing fiber-based formats, but SpZip does not support intersection or union, and a fetcher-compressor pair incurs 116 kGE/times116dividekiloGEabsent116\\text{\\,}\\mathrm{kGE}\\text{/} or 3.9×\\times more area than an SSSR streamer.\nZ. Wang et al.  propose a decoupled register-mapped extension accelerating affine and indirect memory streams similar to ISSRs. However, streams need to stepped explicitly, severely limiting possible speedups on in-order cores, and intersection and union are not supported.\nSparseCore  accelerates intersection, union, and subtraction of indexed streams and provides dedicated functional units. However, indirection is not accelerated, making the handling of one-sided sparsity inherently inefficient. Additionally, SparseCore incurs 0.183 mm2times0.183superscriptmm20.183\\text{\\,}\\mathrm{m}\\mathrm{m}^{2} per stream unit in Open-Cell 15 nm/times15dividenanometerabsent15\\text{\\,}\\mathrm{nm}\\text{/}, which is comparable to two entire Snitch CCs with SSSRs. L/M/H = Low/Medium/High. * Qualitative, based on operand granularity, use constraints, overheads. † Qualitative, based on efficiently handled range, structure. ‡ Based on authors’ data, different cores used. ** gem5 model only, no RTL. ¶ Arch. state only at 6T=1.5GE per bit. § Only node given; we assume GlobalFoundries’ 22nm and 12nm, resp. GPU software: Moving from general-purpose processors to more specialized architectures, GPUs have been the subject of intensive investigations aimed at improving their efficiency on sparse data computations.\nOne-sided sparsity on GPUs is often accelerated in software through specialized algorithms and tensor formats.\nMerrill et al.  improve the performance consistency of CSR sM×dV on GPUs using a merge-based decomposition, reaching up to 2.0 %/times2.0dividepercentabsent2.0\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} of peak FP64 compute on a K40 GPU.\nNvidia provides optimized sparse LA kernels for their GPUs through cuSPARSE . We evaluate their CSR sM×dV kernels from CUDA Toolkit 10.0 on a Jetson AGX Xavier (FP32 only) and a GTX 1080 Ti GPU (FP32 and FP64), profiling 100 invocations with nvprof on the same matrices as for SSSRs. For FP32, both GPUs exhibit high peak streaming multiprocessor (SM) occupancies of 96 %/times96dividepercentabsent96\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} and 87 %/times87dividepercentabsent87\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/}, but low peak floating-point utilizations of and 2.1 %/times2.1dividepercentabsent2.1\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} and 0.75 %/times0.75dividepercentabsent0.75\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} on active SMs, suggesting low thread parallelism among warps. FP64 on the GTX 1080 Ti shows similar SM occupancies, but a notably higher peak floating-point utilization of 17 %/times17dividepercentabsent17\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/}, likely due to the 32×\\times fewer FP64 cores per SM. Tsai et al.  create optimized sM×dV library kernels for both Nvidia and AMD GPUs and various formats. Their CSR kernels regularly outperform vendor code and reach up to 1.6 %/times1.6dividepercentabsent1.6\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} and 3.2 %/times3.2dividepercentabsent3.2\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} of peak FP64 utilization on Nvidia V100 and AMD Radeon VII GPUs, respectively.\nTileSpMV  uses warp-optimized kernels on sparse tiles in a given or adaptively selected format. It achieves peak FP64 utilizations of 2.9 %/times2.9dividepercentabsent2.9\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} on A100 and 27 %/times27dividepercentabsent27\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/} on Titan RTX, which again has 32×\\times fewer FP64 than FP32 cores per SM.\nStill, this is 1.7×\\times lower than in a Snitch cluster with SSSRs.\n\nShi et al.  accelerate sM×dM by encoding matrices as arrays of coordinate-encoded tiles, improving access contiguity and data reuse. However, the average FP32 speedups of 1.7×\\times over cuSPARSE on SuiteSparse matrices are insufficient to reach FP utilizations comparable to those of Snitch with SSSRs.\nKarimi et al. [ref]49 design a set of memory-aware matrix formats accelerating sM×dV by improving cache usage and thread-level parallelism. While improving performance, their best format achieves median FP32 speedups of only 1.8×\\times (K40) and 3.5×\\times (V100) over cuSPARSE CSR on Suite- Sparse matrices, still not enough to match our utilizations. Some sparse-sparse software approaches for GPUs also exist: Zachariadis et al. [ref]50 accelerate sM×sV using a tensor-core-based outer-product approach with dense 16×\\times16 tiles, accepting low flexibility and losses of unnecessary computations within blocks.\nLi et al.  optimize general sparse matrix-vector multiply by dynamically choosing one of eight sM×dV/sM×sV approaches based on tensor properties using a ML model. This solution inherently cannot exceed the peak performance of the existing approaches chosen from, but can be adapted to improve overall performance on other systems including ones with SSSRs. GPU hardware: Extensions to GPU hardware accelerating sparsity have also been proposed.\nNvidia’s A100 architecture  introduces support for one-sided structured sparsity by efficiently handling up to two zeros in every block of four values. SSSRs efficiently handle a much wider range of sparsities (≫much-greater-than\\gg\\;50 %/times50dividepercentabsent50\\text{\\,}\\mathrm{\\char 37\\relax}\\text{/}) without any structural requirements and enable random accesses into a full TCDM.\nZhu et al.  present an algorithm and co-optimized tensor core modifications for efficient sparse-dense neural network inference.\nY. Wang et al.  extend V100 tensor cores to efficiently handle sparse-sparse matrix multiply and convolution with an outer product approach. However, the bitmap encoding used hinders efficient performance and memory scaling beyond low sparsities, and the dense output allows for the overall approach to be replicated with scattering ISSRs. Hardware accelerators: Sparse accelerators usually target a single application domain like deep learning (DL)  or specific LA operations and dataflows.\nMost use custom data formats and precisions tailored to their use case and are incapable of general-purpose computation. Notably, a general-purpose processor with SSSRs can implement many of the employed specialized computation schemes.\nMatRaptor  and OuterSPACE  accelerate sM×sM with row-wise and outer product dataflows, respectively; SSSRs can efficiently handle both dataflows.\nSadi et al.  accelerate large-scale sM×dV by column-slicing matrices and then accumulating the sparse results, improving vector locality and reuse; iterating over slices could be done on a host core while SSSRs handle multiplication and result accumulation.\nHan et al.’s EIE  minimize DRAM access energy in DL inference by keeping full, weight-shared models in on-chip SRAM; SSSRs also enable high utilization on sparse data with codebooked values, but are not limited to one application, schedule, or dataflow.\nExTensor  hierarchically intersects CSF tensors to avoid redundant computation and transfers; a processor with SSSRs can replicate this by handling the upper axes in software while the streamer intersects the performance-critical primary axes.\nDadu et al.  present a compute fabric accelerating both stream joins and streaming indirection. However, the loosely-coupled spatial compute approach incurs notable costs in area and programmability.\nSISA [ref]47 introduces a set-centric ISA to accelerate graph mining algorithms. In addition to set intersection and union, it also accelerates set difference, membership and size queries, and element insertion or removal. It supports both indexed and bitmap sets and a binary-search-based ”galloping” mode for index joins between sets of divergent sizes. While SISA is highly specialized for graph mining and does not target one-sided sparsity or ”low-complexity” algorithms, many of its additional capabilities could be added to SSSRs. However, SISA’s architectural cost is discussed only in loose analogy to other designs, making such tradeoffs hard to assess.\n\nReference markers found: [49]\n\nContext windows for each [ref]:\n[ref]49: data reuse. However, the average FP32 speedups of 1.7×\\times over cuSPARSE on SuiteSparse matrices are insufficient to reach FP utilizations comparable to those of Snitch with SSSRs.\nKarimi et al. [ref]49 design a set of memory-aware matrix formats accelerating sM×dV by improving cache usage and thread-level parallelism. While improving performance, their best format achieves median FP32\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 49,\n            \"titles\": [\n                \"Accelerating Sparse Matrix-Matrix Multiplication with GPU Tensor Cores\",\n                \"Indirection Stream Semantic Register Architecture for Efficient Sparse-Dense Linear Algebra\",\n                \"A survey of sparse representation: algorithms and applications\",\n                \"The university of florida sparse matrix collection\",\n                \"Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks\",\n                \"The pagerank citation ranking : Bringing order to the web\",\n                \"Sparsecore: stream isa and processor specialization for sparse computation\",\n                \"Performance modeling of streaming kernels and sparse matrix-vector multiplication on a64fx\",\n                \"Prodigy: Improving the memory latency of data-indirect irregular workloads using hardware-software co-design\",\n                \"Stream-based memory access specialization for general purpose processors\",\n                \"Dual-side sparse tensor core\",\n                \"Sparten: A sparse tensor accelerator for convolutional neural networks\",\n                \"Extensor: An accelerator for sparse tensor algebra\",\n                \"Towards general purpose acceleration by exploiting common data-dependence forms\",\n                \"Stream semantic registers: A lightweight risc-v isa extension achieving full compute utilization in single-issue cores\",\n                \"Data oblivious algorithms for multicores\",\n                \"Snitch: A tiny Pseudo Dual-Issue Processor for Area and Energy Efficient Execution of Floating-Point Intensive Workloads\",\n                \"NVIDIA A100 Tensor Core GPU Architecture.\",\n                \"Yale Sparse Matrix Package\",\n                \"Sparse Matrix Test Problems\",\n                \"Tensor-matrix products with a compressed sparse tensor\",\n                \"Improving performance of sparse matrix-vector multiplication\",\n                \"Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights\",\n                \"An overview of quantization in jpeg 2000\",\n                \"Compiling stencils in high performance fortran\",\n                \"Sparse matrix transpose unit\",\n                \"Radix sort for vector multiprocessors\",\n                \"Dramsys: A flexible dram subsystem design space exploration framework\",\n                \"Decoupled access/execute computer architectures\",\n                \"The ARM Scalable Vector Extension\",\n                \"Knights Landing: Second-Generation Intel Xeon Phi Product\",\n                \"Unlimited vector extension with data streaming support\",\n                \"Save: Sparsity-aware vector engine for accelerating dnn training and inference on cpus\",\n                \"Cvr: efficient vectorization of spmv on x86 processors\",\n                \"Vectorized parallel sparse matrix-vector multiplication in petsc using avx-512\",\n                \"Regu2d: Accelerating vectorization of spmv on intel processors through 2d-partitioning and regular arrangement\",\n                \"A pattern-based spgemm library for multi-core and many-core architectures\",\n                \"Sparse linear algebra on amd and nvidia gpus – the race is on\",\n                \"Merge-based parallel sparse matrix-vector multiplication\",\n                \"Tilespmv: A tiled algorithm for sparse matrix-vector multiplication on gpus\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]49 based on the context and research area of Sparse Stream Semantic Registers: A Lightweight ISA Extension Accelerating General Sparse Linear Algebra. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}