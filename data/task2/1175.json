{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\n\nPaper Abstract: \n\nPaper Section Text:\nVideo big data are facing the challenge of sparsity and redundancy, i.e., hours of videos with less meaningful information, which creates many issues for viewing, mining, browsing, and storing videos. It has motivated researchers to find ways to shorten hours of videos and led to the field of video summarization. Video summarization is a process of generating a shorter video of the original one without spoiling the capability to comprehend the meaning of the whole video . The video summarization can be classified as Static Video Abstracts, Dynamic Video Skimming, and Video Synopsis. These approaches include a video table of contents, a storyboard, and a pictorial video summary. For example, Xie and Wu  propose an algorithm to generate a video summary for broadcasting news videos automatically. An affinity propagation-based clustering algorithm is used to group the extracted keyframes into clusters, aiming to keep the relevant keyframes that distinguish one scene from the others and remove redundant keyframes. J. Wu et al.  were motivated by the notion from high-density peaks search clustering algorithm. They proposed a clustering algorithm by incorporating significant properties of video to gather similar frames into clusters. Finally, all clusters’ centers were presented as a static video summary. Bhaumik et al.  proposed a summarization technique where they detect keyframes from each shot that eliminates redundancy at the intra-shot and inter-shot levels. For frames redundancy elimination, SURF and GIST feature descriptors were extracted for computing the similarity between the frames. The quality of the summaries obtained by using SURF and GIST descriptors are also compared in terms of precision and recall. Similarly, Zhang et al.  propose a subset selection technique that leverages supervision in the form of human-created summaries to perform automatic keyframe-based video summarization. They were motivated by the intuition that similar videos share similar summary structures. The fundamental notion is to nonparametrically transfer summary structures from annotated videos to unseen test videos. Concretely, for each fresh video, they first compute the frame-level similarity between annotated and test videos. Then the summary structures are encoded in the annotated videos with kernel matrices made of binarized pairwise similarity among their frames. Those structures are then combined into a kernel matrix that encodes the summary structure for the test video. Finally, the summary is decoded by feeding the kernel matrix to a probabilistic model called the determinantal point process to extract a globally optimal subset of frames. M. Gygli et al.  used a supervised approach to learn the importance of the global characteristics in summary by extracting deep features of video frames. J. Mohan et al.  proposed a technique that utilizes sparse autoencoders. Motion vectors have been used for the elimination of redundant frames, and then high-level features are extracted from frames using sparse autoencoders. These high-level feature vectors are then clustered using the K-means algorithm. The frames closest to the centroid of each cluster are selected as keyframes of the input video. Ji, Zhong, et al.  employed tag information, i.e., titles and descriptions, as the side information for the generation of summarization. A sparse auto-encoder was used as the primary model to generate the final summary, where the input and output were multiple videos and keyframes set, respectively. They fused the visual and tag information to guide visual features, which constrained the sparse auto-encoder to select the candidate keyframes. A summary video is formed from the video segments of the original video to remove redundancy or to summarize based on object action or events. As an example of object-based skimming, Peker et al.  proposed a video skimming algorithm while utilizing face detection on broadcast video programs. In the algorithm, the attention was given to faces, as they establish the focus of most consumer video programs. Ngo et al.  represent a video as a complete undirected graph and exploit the normalized cut algorithm to form optimal graph clusters. One-shot was taken from each cluster of visually alike shots to remove duplicate shots. Xiao et al.  mine frequent patterns from a video. A video shot importance evaluation model is utilized to choose useful video shots to create a video summary. For personal videos, Gao et al.  developed a video summarization technique, which encompasses a two-level redundancy detection procedure. First, they terminated redundant video content with the hierarchical agglomerative cluster method at the shot level. Then parts of shots were selected, based on the ranking of the scenes and keyframes, to generate an initial video summary. Finally, to terminate the redundant information, a repetitive frame segment detection step was utilized. They verified the proposed technique through a prototype while using TV datasets (movies and cartoons videos) and reported the performance in terms of compression ratio (81%) and recall (87.4%). In [ref]270, for user-generated video summarization, both the representativeness and the quality of the selected segments from an original video were considered. They stated that user-generated videos contain semantic and emotional content, and its preservation is vital. They have designed a scheme to pick representative segments that include consistent semantics and emotions for the whole video. To ensure the quality of the summary, they computed quality measures, i.e., motion and lighting conditions, and integrate them with the semantic and emotional clues for segment selection. Ji, Zhong, et al.  addresses the issue of supervised video summarization by formulating it as a sequence-to-sequence learning problem, where the input and output is a sequence of original video frames and a keyshot sequence, respectively. The notion is to learn a deep summarization network with attention mechanism to mimic the way of selecting the human keyshots. The proposed framework was called attentive encoder-decoder networks for video summarization. They utilized the BiLSTM encoder for encoding the contextual information among the input video frames. For the decoder, two attention-based LSTM networks, are explored by using additive and multiplicative objective functions, respectively. The results demonstrate the superiority of the proposed framework against the state-of-the-art approaches, with remarkable improvements. J. Wu et al.  were motivated by the fact that multi-video summarization is significant for video browsing and proposed a technique where multi-video summarization was formulated as a graph problem. They also introduced a dynamic graph convolutional network to measure the importance and relevance of each video shot locally as well as globally. They adopted two approaches to address the inherent class imbalance issue of video summarization. Additionally, a diversity regularization to encourage the model to generate a diverse summary was introduced. The results demonstrate the effectiveness of our proposed model in generating a representative summary for multiple videos with encouraging diversity. Z. Sheng-hua et al.  proposed a deep learning-based dynamic video summarization model. First, they addressed the issue of the imbalanced class distribution in video summarization. The over-sampling algorithm is used to balance the class distribution on training data. They proposed two-stream deep architecture with cost-sensitive learning to handle the class imbalance problem in feature learning. RGB images are utilized to represent the appearance of video frames in the spatial stream. Likely, multi-frame motion vectors with deep learning framework are introduced to represent and extract temporal information of the input video. Moreover, they stated that the proposed method highlights the video content with the active level of arousal in effective computing tasks and can automatically preserve the connection between consecutive frames. In this approach, activities from the stated time interval are collected and moved in time to form a smaller video synopsis showing maximum activity, as shown in Figure 13. The notion of video synopsis was pioneered by  in 2006 and proposed a two-phase approach, i.e., online and offline. The former phase includes the queuing of the generated activities. The later phase started after selecting a time interval of video synopsis with tube readjustment, background formation, and object stitching. A global energy function was defined and encompassing activity, temporal consistency, and collision cost. Then the simulated annealing method was applied for energy minimization. The video synopsis domain was further researched in single and multi-view scenarios. Some recent examples of a single-view are   . He et al.   brought advancement in activity collision analysis by describing collision statuses between activities such as collision-free, colliding in the same direction and opposite directions. They also offered a graph-based optimization technique by considering these collision states to improve the activity density and put activity collisions at the center of their optimization strategy. Baskurt and Samet  concentrated on rising robustness of object detection by suggesting an adaptive background generation method. In another study, Baskurt and Samet  planned the object tracking method specified for video synopsis requirements. Their approach focused on long term tracking to represent each target with just one activity in video synopsis. Single view scalable approaches for video synopsis were projected by Lin et al.  while utilizing distributed computing technology. Their proposed video synopsis approach encompasses steps like object detection, tracking, classification, and optimization, which were performed in a distributed environment. Ahmed, S. A.  proposed a query-based method to generate a synopsis of long videos. Objects were tracked and utilized deep learning for objects classification (e.g., car, bike, etc.). Through unsupervised clustering, they identified regions in the surveillance scene. The source and the destination represented spatiotemporal object trajectories. Finally, user queries were allowed to generate video synopsis by smoothly blending the appropriate tubes over the background frame through energy minimization. Further, the examples of multi-view video analytics are that of  . Zhu et al.  proposed a framework to generate a unified synopsis of multi-view videos. The synopsis is visualized by mapping multiple views to a common ground plane. Multiple camera activities were allied via trajectory matching in overlapping camera views. The process of producing a synopsis requires a balance among minimizing the synopsis length, maximizing the information coverage, and reducing the collision among objects’ track that are presented concurrently. Likewise, Mahapatra et al.  proposed a multi-camera approach for an overlapping camera network and modeled the synopsis generation as a scheduling problem. They utilized three distinct methods, i.e., table-driven approach, contradictory binary graph coloring approach, and simulated annealing. Action recognition modules were integrated to recognize significant actions, i.e., walking, running, bending, jumping, hand-shaking, and one or both hands waving. The inclusion of such essential actions can help in the synopsis length reduction while preserving the value. The synopsis length was further reduced while utilizing a fuzzy inference system that computes the visibility score for each object tracking. They stated that through the contradictory binary graph coloring approach, they achieved a maximum reduction in synopsis length. Zhang, Z. et al. , tried to address the issue of video synopsis by joint object-shifting and camera view-switching to show multiple synopsis results more compactly and understandably. The input videos were synchronized and grouped the same object in different videos together. Then they shifted the grouped objects with respect to the time axis to gain multiple synopsis videos. They constructed a simultaneous object-shifting and view-switching optimization framework to achieve encouraging synopsis results. To address unified optimization, they further presented an optimization strategy composed of graph cuts and dynamic programming.\n\nReference markers found: [270]\n\nContext windows for each [ref]:\n[ref]270: They verified the proposed technique through a prototype while using TV datasets (movies and cartoons videos) and reported the performance in terms of compression ratio (81%) and recall (87.4%). In [ref]270, for user-generated video summarization, both the representativeness and the quality of the selected segments from an original video were considered. They stated that user-generated videos\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 270,\n            \"titles\": [\n                \"Video Summarization with Attention-Based Encoder-Decoder Networks\",\n                \"Video analytics market\",\n                \"Facebook\",\n                \"Youtube statistics\",\n                \"Netflix\",\n                \"Multimedia big data analytics: A survey\",\n                \"Dynamic threshold for resource tracking in observed scenes\",\n                \"The growth in connected iot devices\",\n                \"Surveillance video: The biggest big data\",\n                \"The nist definition of cloud computing\",\n                \"Harness the power of big data The IBM big data platform\",\n                \"Amazon web services\",\n                \"Overview of microsoft azure services\",\n                \"Oracle: Big data for the enterprise\",\n                \"Big data and the next wave of infras-tress\",\n                \"big data: big gaps of knowledge in the field of internet science\",\n                \"The rise of “big data” on cloud computing: Review and open research issues\",\n                \"Big data analytics: a literature review paper\",\n                \"Yesterday, today and tomorrow of big data\",\n                \"Advancing big data for humanitarian needs\",\n                \"Big Data: Using SMART big data, analytics and metrics to make better decisions and improve performance\",\n                \"Artificial intelligence and big data\",\n                \"Principles of big data: preparing, sharing, and analyzing complex information\",\n                \"Big data: A revolution that will transform how we live, work, and think\",\n                \"Why only one of the 5 vs of big data really matters\",\n                \"In search of clusters\",\n                \"High performance cluster computing: Architectures and systems (volume 1)\",\n                \"Cloud computing\",\n                \"Cloud monitoring: A survey\",\n                \"Big Data: Principles and best practices of scalable realtime data systems\",\n                \"Intelligent video systems and analytics: A survey\",\n                \"Video analytics for visual surveillance and applications: An overview and survey\",\n                \"A survey on visual content-based video indexing and retrieval\",\n                \"Content based video retrieval systems\",\n                \"[survey paper] a review of video retrieval based on image and video semantic understanding\",\n                \"Hierarchical and networked vehicle surveillance in its: a survey\",\n                \"Abnormal behavior recognition for intelligent video surveillance systems: A review\",\n                \"Big data: survey, technologies, opportunities, and challenges\",\n                \"Big data analytics: a survey\",\n                \"Big data and cloud computing: current state and future opportunities\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]270 based on the context and research area of Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}