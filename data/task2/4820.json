{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Decoupled Context Processing for Context Augmented Language Modeling\n\nPaper Abstract: \n\nPaper Section Text:\nLarge language models, typically in the form of big neural networks, are trained with a huge amount of training data rich in unstructured knowledge.\nResearchers have found that after model training, the neural networks often end up storing a surprisingly large amount of memorized information within its weights   which are then leveraged as a knowledge base.\nMultiple hypotheses have been developed on how components such as fully-connected layers  and attention layers  may be responsible for such memorization behavior.\nWhile the capability of storing world knowledge is desirable, memorization also contributes to huge model sizes and the lack of explicit control over knowledge base, such as performing selection or updates. An alternative strategy is to enable language models to incorporate world knowledge in the form of retrieved context from external sources, instead of having to memorize them.\nMultiple works have proposed architectures that support external retrieval, usually composed of a context retriever that searches a large external key value store and a method of integrating retrieved information. There are various ways to construct the key value store. The keys are primarily used for similarity matching, and they can be sparse vectors such as BM25 , or dense embeddings extracted from part of the model 1 5 , or from pretrained embedders  8, or embedders trained for specific downstream tasks  0  [ref]35 4 . The values also have various different forms. For example, TOME  stores a dense embedding about the contextual information of an entity mention, while Realm , RAG 6, FID , Retro , MARGE 5, DenSPI [ref]35 and DensePhrases 4 store the raw text as the value. Works such as k𝑘kNN-LM 1 and Spalm 4 store one token as a value. Finally the key value store is searched over using vector similarity techniques, typically with some off-the-shelf nearest neighbor search implementations such as FAISS , ScaNN , HNSW 7 or SPTAG . There are also many different ways to integrate the retrieved context. A popular approach is to concatenate the retrieval results with the original input and jointly process them. It has been adopted by works such as Realm , RAG 6, and FiD . Other works utilize some forms of cross attention for the context integration, such as the Chunked-Cross-Attention with input conditioning in Retro , k𝑘kNN Attention in Memorizing Transformer , Memory Attention in TOME  and Cross-Attention in MARGE 5. For token level integration, k𝑘kNN-LM 1 uses simple linear interpolation while Spalm 4 uses a learned gate based on the last layer embedding. There are also works that directly utilize the retrieval results without any integration, such as DenSPI [ref]35 and DensePhrases 4. Most of the works use retrieval as a way to augment tasks such as language modeling or question answering, with the exception of MARGE 5 where retrieval is treated as an autoencoder bottleneck for multilingual pretraining, and is not strictly necessary for inference. We compare representative previous works and contrast with our proposal in Table 1.\n\nReference markers found: [35]\n\nContext windows for each [ref]:\n[ref]35: and they can be sparse vectors such as BM25 , or dense embeddings extracted from part of the model 1 5 , or from pretrained embedders  8, or embedders trained for specific downstream tasks  0  [ref]35 4 . The values also have various different forms. For example, TOME  stores a dense embedding about the contextual information of an entity mention, while Realm , RAG 6, FID , Retro ,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 35,\n            \"titles\": [\n                \"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost\",\n                \"Ieee standard for floating-point arithmetic.\",\n                \"A Closer Look at Memorization in Deep Networks\",\n                \"Natural language processing with Python: analyzing text with the natural language toolkit.\",\n                \"Improving language models by retrieving from trillions of tokens\",\n                \"Attention Approximates Sparse Distributed Memory\",\n                \"Language Models are Few-Shot Learners\",\n                \"Extracting Training Data from Large Language Models\",\n                \"Universal Sentence Encoder\",\n                \"SPTAG: A library for fast approximate nearest neighbor search,\",\n                \"PaLM: Scaling Language Modeling with Pathways\",\n                \"Mention Memory: incorporating textual knowledge into Transformers through entity mention attention\",\n                \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n                \"Transformer feed-forward layers are key-value memories.\",\n                \"Accelerating Large-Scale Inference with Anisotropic Vector Quantization\",\n                \"REALM: Retrieval-Augmented Language Model Pre-Training\",\n                \"Efficient natural language response suggestion for smart reply.\",\n                \"Distilling knowledge from reader to retriever for question answering.\",\n                \"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\",\n                \"Billion-scale similarity search with GPUs\",\n                \"Dense Passage Retrieval for Open-Domain Question Answering\",\n                \"Generalization through Memorization: Nearest Neighbor Language Models\",\n                \"Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.\",\n                \"Natural questions: a benchmark for question answering research.\",\n                \"Learning Dense Representations of Phrases at Scale\",\n                \"Pre-training via Paraphrasing\",\n                \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n                \"Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.\",\n                \"Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models.\",\n                \"Carbon emissions and large neural network training.\",\n                \"Scaling Language Models: Methods, Analysis & Insights from Training Gopher\",\n                \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\",\n                \"Squad: 100,000+ questions for machine comprehension of text.\",\n                \"The probabilistic relevance framework: BM25 and beyond.\",\n                \"Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index\",\n                \"End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\",\n                \"Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.\",\n                \"Attention Is All You Need\",\n                \"Memorizing Transformers\",\n                \"Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]35 based on the context and research area of Decoupled Context Processing for Context Augmented Language Modeling. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}