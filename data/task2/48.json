{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: One Explanation Does Not Fit All\n\nPaper Abstract: \n\nPaper Section Text:\nDuring the development stage and early trials of Glass-Box we identified a collection of desiderata and properties that should be considered when building such systems. Some of these attributes are inspired by relevant literature¬† [ref]26 [ref]20 , while others come from our experience gained in the process of building the system, presenting it to various audiences, discussing its properties at different events and collecting feedback about interacting with it. While this and the following sections focus on desiderata for interactive and customisable explanations, we provide an in-depth discussion on this topic for generic explainability systems in our work on ‚ÄúExplainability Fact Sheets‚Äù¬†. The relevant subset of these desiderata are summarised in Table¬†1 as well as collected and discussed below. Section¬†3.3, on the other hand, examines the properties of interactive explainability systems. Given the complex nature of such systems, it would be expected that some of these objectives might be at odds with each other, their definition may be ‚Äúfuzzy‚Äù, they might be difficult to operationalise, their ‚Äúcorrect‚Äù application might depend on the use case, etc. Furthermore, striking the right balance between these desiderata can be challenging. Nevertheless, we argue that considering them while designing interactive explainers will improve the overall quality of the system, help the designers and users understand their strengths and limitations, and make the interaction feel more natural to humans. Furthermore, some of these desired properties can be achieved (and ‚Äúoptimised‚Äù for the explainees) by simply allowing user interaction, thereby alleviating the need of explicitly building them into the system. For example, interactive personalisation of the explanations (on-line, with user input) can mean that it does not have to be solved fully algorithmically off-line. The main advantage of Glass-Box interactiveness is the explainee‚Äôs ability to transfer knowledge onto the system ‚Äì in this particular case various preferences with respect to the desired explanation ‚Äì thereby personalising the resulting explanation¬†[36, property U10, see Table¬†1]. In our experience, personalisation can come in many different shapes and forms, some of which are discussed below. By interacting with the system the explainee should be able to adjust the breadth and scope of an explanation¬†[36, property F4]. Given the complexity of the underlying predictive model, the explainee may start by asking for an explanation of a single data point (black-box prediction) and continue the interrogation by generalising it to an explanation of a sub-space of the data space (a cohort explanation) with the final stage entailing the explanation of the entire black-box model. Such a shift in explainee‚Äôs interest may require the explainability method to adapt and respond by changing the target of the explanation¬†[36, property F3]. The user may request an explanation of a single data point or a summary of the whole data set (training, test, validation, etc.), an explanation of a predictive model (or its subspace) or any number of its predictions. Furthermore, interactive personalisation of an explanation can increase the overall versatility of such systems as customised explanations may serve different purposes and have different functions¬†[36, property O7]. An appropriately phrased explanation may be used as an evidence that the system is fair ‚Äì either with respect to a group or an individual depending on the scope and breadth of the explanation ‚Äì or that it is accountable, which again can be investigated with a varied scope, for example, a ‚ÄúWhat if?‚Äù question uncovering that two seemingly indistinguishable data points yield significantly different class assignment, aka adversarial examples¬†. Importantly, if the explainer is flexible enough and the interaction allows such customisation, however the explanations were designed to serve only one purpose, e.g., transparency, the explainee should be explicitly warned of such limitations to avoid any unintended consequences. For example, the explanations may be counterfactually actionable but they are not causal as they were not derived from a causal model¬†[36, property O8]. Some of the aforementioned principles can be observed in how Glass-Box operates. The contrastive statements about the underlying black-box model can be used to assess its transparency (their main purpose), fairness (disparate treatment via contrastive statements conditioned on protected attributes) and accountability (e.g., answers to ‚ÄúWhat if?‚Äù questions that indicate an unexpected non-monotonic behaviour). The contrastive statements are personalised via user-specified constrains of the conditional part (foil) of the counterfactual explanation and by default are with respect to a single prediction. Cohort-based insights can be retrieved by asking ‚ÄúWhat if?‚Äù questions with regard to counterfactual explanations generated by Glass-Box ‚Äì Section¬†4 discusses how the scope and the target of our explanations can be broadened to global explanations of the black-box model. Given the wide range of possible explanations and their uses some systems may produce contradictory or competing explanations. Glass-Box is less prone to such issues as the employed explainer is ante-hoc¬†[36, property F7], i.e., predictions and explanations are derived from the same ML model, hence they are always truthful with respect to the predictive model. This means that contradictory explanations are indicative of flaws in the underlying ML model, hence can be very helpful in improving its accountability. In day-to-day human interactions we are able to communicate effectively and efficiently because we share common background knowledge about the world that surrounds us ‚Äì a mental model of how to interact with the world and other people¬†. Often, human-machine interactions lack this implicit link making the whole process feel unnatural and frustrating. Therefore, the creators of interactive explainability techniques should strive to make their systems coherent with the explainee‚Äôs mental model to mitigate this phenomenon as much as possible¬†[36, property U7]. While this objective may not be achievable in general, modelling a part of the user‚Äôs mental model, however small, can make a significant difference. The two main approaches to extracting an explainee‚Äôs mental model are interactive querying of the explainee in an iterative dialogue (on-line), or embedding the user‚Äôs characteristics and preferences in the data or in the parameters of the explainer (off-line), both of which are discussed in Section¬†2. For explainability systems this task is possible to some extent as their operation and purpose are limited in scope in contrast to more difficult tasks like developing a generic virtual personal assistant. Designers of such systems should also be aware that many interactions are underlined by implicit assumptions that are embedded in the explainee‚Äôs mental model and perceived as mundane, hence not voiced, for example, the context of a follow-up question. However, for human-machine interactions the context and its dynamic changes can be more subtle, which may cause the coherence of the internal state of an explainer and the explainee‚Äôs mental model to diverge¬†[36, property U3]. This issue can be partially mitigated by explicitly grounding explanations in a context at certain stages, for example, whenever the context shifts, which will help the users to adapt by updating their mental model and assumptions. Contextfullness will also help the explainee better understand the limitations of the system, e.g., whether an explanation produced for a single prediction can (or must not) be generalised to other (similar) instances: ‚Äúthis explanation can be generalised to other data points that have all of the feature values the same but feature x5subscriptùë•5x_{5}, which can span the 0.4‚â§x5<1.70.4subscriptùë•51.70.4\\leq x_{5}<1.7 range.‚Äù Regardless of the system‚Äôs interactivity, the explanations should be parsimonious ‚Äì as short as possible but not shorter than necessary ‚Äì to convey the required information without overwhelming the explainee¬†[36, property U11]. Maintaining a mental model of the user can help to achieve this objective as the system can provide the explainee only with novel explanations ‚Äì accounting for factors that the user is not familiar with ‚Äì therefore reducing the amount of information carried by the explanation¬†[36, property U8]. Another two user-centred aspects of an explanation are its complexity and granularity¬†[36, property U9]. The complexity of explanations should be adjusted according to the depth of the technical knowledge expected of the intended audience, and the level of detail chosen appropriately for their intended use. This can either be achieved by design (i.e., incorporated into the explainability technique), be part of the system configuration and parametrisation steps (off-line) or adjusted interactively by the user as part of the explanatory dialogue (on-line). Another aspect of an explanation, which is often expected by humans¬†[ref]26, is the chronology of factors presented therein: the explainee expects to hear more recent events first¬†[36, property U6]. While this property is data set-specific, the explainee should be given the opportunity to trace the explanation back in time, which can easily be achieved via interaction. Glass-Box attempts to approximate its users‚Äô mental models by mapping their interests and interaction context (inferred from posed questions) to data features that are used to compose counterfactual explanations. Memorising previous interactions, their sequence and the frequency of features mentioned by the user help to achieve this goal and avoid repeating the same answers ‚Äì once all of the explanations satisfying given constraints were presented, the system explicitly states this fact. Contextfullness of explanations is based on user interactions and is implicitly preserved for follow-up queries in case of actions that do not alter the context and are initiated by the user ‚Äì e.g., interrogative dialogue. Whenever the context shifts ‚Äì e.g., a new personalised explanation is requested by the user or an interaction is initiated by Glass-Box ‚Äì it is explicitly communicated to the user. Contrastive explanations are inherently succinct, but a lack of parsimony could be observed for some of Glass-Box explanations, which resulted in a long ‚Äúmonologue‚Äù delivered by the system. In most of the cases this was caused by the system ‚Äúdeciding‚Äù to repeat the personalisation conditions provided by the user to ensure their coherence with the explainee‚Äôs mental model. Glass-Box is capable of producing novel explanations by using features that have not been acknowledged by the user during the interaction. Interestingly, there is a trade-off between novelty of explanations and their coherence with the user‚Äôs mental model, which we have not explored when presenting our system but which should be navigated carefully to avoid jeopardising explainee‚Äôs trust. Glass-Box was built to explain predictions of the underlying ML model and did not account for possible generalisation of its explanations to other data points (the users were informed about it prior to interacting with the device). However, the explainees can ask ‚ÄúWhat if?‚Äù questions with respect to the counterfactual explanations, e.g., using slight variations of the explained data point, to explicitly check whether their intuition about the broader scope of an explanation holds up. Finally, chronology was not required of Glass-Box explanations as the data set used to train the underlying predictive model does not have any time-annotated features.\n\nReference markers found: [26]\n\nContext windows for each [ref]:\n[ref]26: early trials of Glass-Box we identified a collection of desiderata and properties that should be considered when building such systems. Some of these attributes are inspired by relevant literature¬† [ref]26 [ref]20 , while others come from our experience gained in the process of building the system, presenting it to various audiences, discussing its properties at different events and collecting\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 26,\n            \"titles\": [\n                \"Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences\",\n                \"Natural Language Interaction with Explainable AI Models\",\n                \"Power to the people: The role of humans in interactive machine learning\",\n                \"Formalizing explanatory dialogues\",\n                \"Justification narratives for individual classifications\",\n                \"Wizard of Oz studies ‚Äî why and how\",\n                \"Assumption-based argumentation\",\n                \"Bringing transparency design into practice\",\n                \"Greedy function approximation: a gradient boosting machine\",\n                \"Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation\",\n                \"Explaining and Harnessing Adversarial Examples\",\n                \"DARPA‚Äôs Explainable Artificial Intelligence Program\",\n                \"Towards a Generic Framework for Black-box Explanation Methods\",\n                \"Interactive optimization for steering machine classification\",\n                \"iBCM: Interactive Bayesian case model empowering humans via intuitive interaction\",\n                \"Using visual analytics to interpret predictive machine learning models\",\n                \"Interacting with predictions: Visual inspection of black-box machine learning models\",\n                \"Explanatory debugging: Supporting end-user debugging of machine-learned programs\",\n                \"Tell me more?: the effects of mental model soundness on personalizing an intelligent agent\",\n                \"Too much, too little, or just right? Ways explanations impact end users‚Äô mental models\",\n                \"Principles of explanatory debugging to personalize interactive machine learning\",\n                \"Counterfactual Fairness\",\n                \"Faithful and customizable explanations of black box models\",\n                \"The Doctor Just Won‚Äôt Accept That!\",\n                \"A Unified Approach to Interpreting Model Predictions\",\n                \"A Grounded Interaction Protocol for Explainable Artificial Intelligence\",\n                \"Explanation in Artificial Intelligence: Insights from the Social Sciences\",\n                \"Scikit-learn: Machine Learning in Python\",\n                \"FACE: Feasible and Actionable Counterfactual Explanations\",\n                \"‚ÄúWhy Should I Trust You?‚Äù Explaining the Predictions of Any Classifier\",\n                \"Anchors: High-precision model-agnostic explanations\",\n                \"Perturbation-Based Explanations of Prediction Models\",\n                \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\",\n                \"Personalized Explanation for Machine Learning: a Conceptualization\",\n                \"Counterfactual Explanations of Machine Learning Predictions: Opportunities and Challenges for AI Safety\",\n                \"Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches\",\n                \"Glass-Box: Explaining AI Decisions With Counterfactual Statements Through Conversation With a Voice-enabled Virtual Assistant\",\n                \"bLIMEy: Surrogate Prediction Explanations Beyond LIME\",\n                \"Contrastive Explanations with Local Foil Trees\",\n                \"Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GPDR\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]26 based on the context and research area of One Explanation Does Not Fit All. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}