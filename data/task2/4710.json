{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Optimization of FPGA-based CNN Accelerators Using Metaheuristics\n\nPaper Abstract: \n\nPaper Section Text:\nConvolutional neural network (CNN) is a powerful method used for processing data with predefined grid-like topology, such as 111-D time series in speech recognition , 222-D image retrieval in face detection and recognition [ref]2 , and in intelligent transportation systems   , to name a few. The CNN proved its effectiveness when it was used to win the ImageNet challenge, an annual competition for visual object recognition, in 201220122012, by dropping the classification error record from 262626% to 151515%, which was a significant improvement at the time . Since then, more studies and applications have emerged to enhance CNN application in different fields   0. In general, CNNs consist of an input layer, an output layer, and multiple intermediate hidden layers. The key operations involved in the construction of CNN layers include convolution (CONV), pooling (POOL), and inner product. The CONV layer extracts the unique features from the input image. Specifically, the first CONV layer in the architecture captures low-level features, such as edges, colors, gradients, orientations, etc., while subsequent CONV layers adapt to extract higher-level features, resulting in a wholesome understanding of the processed image. On the other hand, the POOL layer reduces the dimensionality of the processed data, extracts rotational and positional invariant features, and suppresses noise by applying the m​a​x𝑚𝑎𝑥max, a​v​g𝑎𝑣𝑔avg, or m​i​n𝑚𝑖𝑛min functions. Last, but not least, the inner product layer, also known as the dense or the fully connected (FC) layer, is a layer whose neurons are connected to all the neurons of its preceding layer, hence the name. It is worth noting that the last FC layer in the architecture is also referred to as the classification layer because it is responsible for decision-making, such as the class score. Here, we must emphasize that there are other layers that can also be used in constructing CNNs such as normalization (Norm) and rectified linear unit (ReLU). The Norm layer is used to smooth the data, whereas the ReLU activation layer helps in learning and modeling complex data. CNN layers are represented by a set of matrices that reflect the main elements and the relationship between them according to the network structure. Specifically, each layer receives the output matrix of the previous layer as input, which is a set of 222-D arrays named feature maps (FMs). Then, it performs its operation to produce the FMs for the subsequent layer. Figure 1 shows a well-known CNN architecture called AlexNet . It consists of 555 CONV layers each of which is followed by a ReLU activation function, interspersed by 222 Norm layers, 333 MAX-POOL layers, and concluded by 333 FC layers. In CNNs, as the name implies, CONV layers are the most critical ones, and their operations constitute over 909090% of the total computation time 1. The CONV layer receives a set of N𝑁N FMs from the previous layer. Then, it convolves the input FMs (IF) with a set of M𝑀M small kernels, also known as filters, using a shifting window that slides over the IF with a stride of size S𝑆S. The kernel is a matrix of numbers called weight parameters whose values are learned during the training phase by the backpropagation algorithm. Each kernel is used to compute one specific output FM, that is, the m​-th𝑚-th{m\\textnormal{-th}} kernel produces the m𝑚m-th output FM as demonstrated in Figure 2. Thus, the number of output FMs (OF) equals the number of CONV layer kernels. Additionally, each kernel has one bias term (B). Each bias is added to every element in its corresponding output FM to produce the final OF, which form the IF for the next layer. The number of CONV kernels, their size, and the number of channels are defined by the CNN designers depending on the type of convolution. Specifically, standard convolution and point-wise convolution employ M𝑀M kernels of size K×K𝐾𝐾{K\\times K} and 1×111{1\\times 1}, respectively, each of which contains N𝑁N channels 2. Conversely, depth-wise convolution uses N𝑁N single-channel kernels of size K×K𝐾𝐾{K\\times K}, where each kernel is applied to one input FM to produce the corresponding output FM, i.e., M=N𝑀𝑁{M=N}. To determine the value of the neuron at position (r,c)𝑟𝑐(r,c) of m𝑚m-th output FM, the following computation is performed Here, 𝐁​[m]𝐁delimited-[]𝑚\\mathbf{B}\\!\\left[m\\right] represents the bias parameter of the m​-th𝑚-th{m\\textnormal{-th}} kernel, m∈[M]𝑚delimited-[]𝑀{m\\in[M]}, 𝐖​[m]​[n]​[i]​[j]𝐖delimited-[]𝑚delimited-[]𝑛delimited-[]𝑖delimited-[]𝑗{\\mathbf{W}\\!\\left[m\\right]\\!\\left[n\\right]\\!\\left[i\\right]\\!\\left[j\\right]} denotes the weight parameter at position (i,j)𝑖𝑗(i,j) in the n𝑛n-th channel of the m𝑚m-th kernel, n∈[N]𝑛delimited-[]𝑁{n\\in[N]}, i𝑖i and j∈[K]𝑗delimited-[]𝐾{j\\in[K]}, 𝐈𝐅​[n]​[x]​[y]𝐈𝐅delimited-[]𝑛delimited-[]𝑥delimited-[]𝑦{\\mathbf{IF}\\!\\left[n\\right]\\!\\left[x\\right]\\!\\left[y\\right]} indicates the neuron at position (x,y)𝑥𝑦(x,y) of the n𝑛n-th input FM, and S𝑆S is the stride size. By applying Equation (1) to compute all neurons of OF, we can see that the main structure of CONV layer consists of 666 nested loops as shown in Algorithm 1. One can also note that these loops contain a massive number of the multiply-accumulate (MAC) operations. Precisely, each layer performs M×R×C×N×K2𝑀𝑅𝐶𝑁superscript𝐾2{M\\times R\\times C\\times N\\times K^{2}} MAC operation. Overall, AlexNet requires about 1.461.46{1.46} Giga operations (GOPs) to process a single RGB image of size 224×224224224{224\\times 224} pixels. The intensive computational requirements of CNNs limit their real-time applications on general central processing units (CPUs). On the other hand, the computational efficiency of graphics processing units (GPUs) and field-programmable gate arrays (FPGAs) make them excellent platforms for accelerating CNNs. However, the critical need for lower power consumption in today’s applications, due to CNN deployment on battery-powered devices such as drones and Internet of things (IoT) 3 4, makes FPGAs more suitable for CNN acceleration . This is because of the high power efficiency, also known as performance per watt, that FPGAs offer. With regards to model parameters, each standard CONV layer learns M×N×K2𝑀𝑁superscript𝐾2{M\\times N\\times K^{2}} weight parameters. For example, AlexNet’s first CONV layer receives 333 input FMs, uses 11×11111111\\times 11 kernels, and produces 969696 output FMs, resulting in 34,8483484834,848 parameters. Overall, AlexNet has over 606060 million parameters which need about 240240240 MB of memory space. The memory requirements for FMs and model parameters exceed what commercially available FPGAs can provide in on-chip memory. Thus, they must be stored in off-chip memory and transferred to on-chip memory during computation on need basis. The significant amount of storage and external memory bandwidth required become a performance bottleneck. Therefore, many attempts have been made to maximize the throughput of CNN applications by designing a hardware accelerator known as a convolutional layer processor (CLP) 5 6 7 8 9. The CLP optimizes the implementation of CONV layers by applying loop transformations to Algorithm 1. Specifically, the loop unrolling technique is used to maximize the parallelism of CONV computations by replicating the hardware resources of MAC unit. Furthermore, loop tilling and local memory promotion techniques are adopted to reduce off-chip memory accesses and maximize data sharing and reuse. Here, we must emphasize that the CLP design is parametrized by the number of input/output FMs to/from CONV layers as well as their dimensions. However, different CONV layers, even those in the same architecture, vary considerably in their configurations (N𝑁N, M𝑀M, R𝑅R, C𝐶C, K𝐾K, and S𝑆S). To overcome this problem, three main design schemes have been proposed in the literature. In the first approach, such as in 5, a CLP is modeled for each CONV layer. In this way, a CNN of L𝐿L CONV layers is accelerated using L𝐿L CLPs that process L𝐿L independent images in a pipelined fashion. Even though this approach optimizes the computations of each CONV layer, it suffers from non-negligible latency and bandwidth overheads. This is due to the need for orchestrating the off-chip memory accesses for a large number of CLPs. Additionally, dividing the limited on-chip memory among many CLPs reduces the overall data locality. Last, but not least, implementing a dedicated controller for each CLP leaves them with insufficient resources for computation. The second approach, on the other hand, designs a single, unified CLP based on the optimal parameters that achieve the lowest overall latency 6 7. The globally optimized CLP is then used to iteratively process CONV layers, one layer at a time. However, a CLP that provides the best performance across all layers is not necessarily optimal in utilizing its hardware resources. This is due to the radically varying CONV configurations. Considering the three CONV layers shown in Figure 3 as an example, one can note that the Single-CLP accelerator in Figure 3a is underutilized while processing L​1𝐿1L1 and the portions of L​3𝐿3L3. To alleviate this issue, the third approach implements multiple CLPs and optimally distributes the hardware resources among them 8 9. In Figure 3b, the same hardware resources used to implement the Single-CLP accelerator in Figure 3a are partitioned into two CLPs (C​L​P​1𝐶𝐿𝑃1CLP1 and C​L​P​2𝐶𝐿𝑃2CLP2). Thereafter, by mapping L​1𝐿1L1 and L​3𝐿3L3 to C​L​P​1𝐶𝐿𝑃1CLP1 and L​2𝐿2L2 to C​L​P​2𝐶𝐿𝑃2CLP2, the Multi-CLP design achieves Δ​tΔ𝑡\\Delta t reduction in the overall execution time compared to the Single-CLP design. The problem now is, how to determine the appropriate number of CLPs to use, on what basis to assign a CONV layer to a CLP, how many hardware resources each CLP can utilize, and how to determine the appropriate tiling factors to assign to each CONV layer? This intractable design space makes finding near-optimal configurations by exhaustive search algorithms almost impossible. Thus, intelligent techniques are needed to efficiently explore the design space for the optimal configurations of Multi-CLP design that improve the throughput of CNN applications. Metaheuristics algorithms have proved their capabilities for finding optimal solutions for several NP-hard problems, with very efficient performance . However, very few studies exist in the current literature that have proposed and exploited the use of metaheuristics algorithms in optimizing CNNs. Specifically, the standard genetic algorithm was employed in 7 to explore the design space for an optimal Single-CLP accelerator. On the other hand, the authors in [ref]21 adopted the simulated annealing algorithm to improve the training of the LeNet-5 architecture. In this paper, we present a systematic methodology for the optimization of the throughput of an FPGA-based accelerator with Multi-CLP implementation. Specifically, we present an analytical and empirical design scheme for accurate modeling of cost, in terms of hardware resources used, and performance for a Multi-CLP design given its parameters. Then, we employ simulated annealing and tabu search metaheuristic algorithms to search for the optimal Multi-CLP design, constrained by the given CNN architecture and target FPGA specifications. As a case study, we implemented CNN accelerators for four well-known benchmark architectures, namely, AlexNet , SqueezeNet 1.11.11.1 , VGGNet , and GoogLeNet  on VC707 and VC709 FPGA boards and compared them with previous approaches. Our implementation achieves a performance of 113.92113.92113.92 Giga floating-point operations (GFLOPs) under 100100100 MHz working frequency. The key contributions of this work are summarized as follows: Designing a CNN accelerator with loop tiling, loop unrolling, and loop pipelining techniques. Modeling CNN implementation and identifying the critical optimization variables for multiple CLPs design on FPGA platforms. Clarifying the complexity of finding the optimal configurations for multiple CLPs accelerator, i.e., defining the problem statement and the resultant intractable design space. Introducing a metric to quantify the dimensional mismatches between CLP dimensions and CONV layer dimensions, and use it to improve CNN throughput. Presenting computational performance and resource usage models for cost estimation of a candidate multiple CLPs design. Proposing a metaheuristic-based optimization framework that employs the estimation models to efficiently explore the design space using simulated annealing and tabu search algorithms, and find the optimal multiple CLPs design for CNNs on FPGAs based on performance and resources constraints. Validating the solutions obtained and comparing them with related works. The rest of the paper is organized as follows. Section II reviews previous work related to accelerating CNNs using FPGAs. In Section III, we discuss the optimization techniques applied to achieve efficient implementation of CNNs on FPGA platforms. Furthermore, we present an analytical and empirical model to accurately predict the performance and hardware resources required for a given design variant. Section IV presents the methodology used for exploring the design space to achieve an efficient implementation of Multi-CLP hardware accelerator for CONV layers. The results of our experiments are presented and discussed in Section V, and the paper is concluded in Section VI.\n\nReference markers found: [2]\n\nContext windows for each [ref]:\n[ref]2: (CNN) is a powerful method used for processing data with predefined grid-like topology, such as 111-D time series in speech recognition , 222-D image retrieval in face detection and recognition [ref]2 , and in intelligent transportation systems   , to name a few. The CNN proved its effectiveness when it was used to win the ImageNet challenge, an annual competition for visual object\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 2,\n            \"titles\": [\n                \"Multi-view Face Detection Using Deep Convolutional Neural Networks\",\n                \"Mandarin speech recognition using convolution neural network with augmented tone features\",\n                \"A convolutional neural network approach for face verification\",\n                \"CNN Based Vehicle Counting with Virtual Coil in Traffic Surveillance Video\",\n                \"A pedestrian and vehicle rapid identification model based on convolutional neural network\",\n                \"Malaysia traffic sign recognition with convolutional neural network\",\n                \"Imagenet classification with deep convolutional neural networks\",\n                \"FPGA-Based Accelerators of Deep Learning Networks for Learning and Classification: A Review\",\n                \"Computer vision algorithms and hardware implementations: A survey\",\n                \"A Survey on Efficient Convolutional Neural Networks and Hardware Acceleration\",\n                \"Minimizing computation in convolutional neural networks\",\n                \"Mobilenets: Efficient convolutional neural networks for mobile vision applications\",\n                \"The Smart Image Recognition Mechanism for Crop Harvesting System in Intelligent Agriculture\",\n                \"IoT-based agriculture: Deep learning in detecting apple fruit diseases\",\n                \"A high performance FPGA-based accelerator for large-scale convolutional neural networks\",\n                \"Optimizing FPGA-based accelerator design for deep convolutional neural networks\",\n                \"Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks\",\n                \"Maximizing CNN Accelerator Efficiency Through Resource Partitioning\",\n                \"FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks\",\n                \"Meta-heuristics: an overview\",\n                \"Simulated annealing algorithm for deep learning\",\n                \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\",\n                \"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\n                \"Going deeper with convolutions\",\n                \"FxP-QNet: A Post-Training Quantizer for the Design of Mixed Low-Precision DNNs with Dynamic Fixed-Point Representation\",\n                \"FPGA-Based Convolutional Neural Network Accelerator with Resource-Optimized Approximate Multiply - Accumulate Unit\",\n                \"Polyhedral-based data reuse optimization for configurable computing\",\n                \"Roofline: an insightful visual performance model for multicore architectures\",\n                \"Vivado Design Suite Product Guide: Floating-Point Operator v7.1 [Online]\",\n                \"User Guide: 7 Series FPGAs Memory Resources [Online]\",\n                \"Iterative Computer Algorithms with Applications in Engineering: Solving Combinatorial Optimization Problems\",\n                \"VLSI physical design automation: theory and practice\",\n                \"Optimization by simulated annealing\",\n                \"Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm\",\n                \"Equation of state calculations by fast computing machines\",\n                \"Evolutionary algorithms, simulated annealing and tabu search: a comparative study\",\n                \"Tabu search—part I\",\n                \"Tabu search—part II\",\n                \"A user’s guide to tabu search\",\n                \"Tabu search\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]2 based on the context and research area of Optimization of FPGA-based CNN Accelerators Using Metaheuristics. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}