{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Joint Policy Search for Multi-agent Collaboration with Imperfect Information\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, we apply the online version of JPS (Sec. 5.2) to the bidding phase of Contract Bridge (a 4-player game, 2 in each team), to improve collaboration between teammates from a strong baseline model. Note that we insert JPS in the general self-play framework to improve collaboration between teammates and thus from JPS’s point of view, it is still a fully collaborative IG with fixed opponents. Unlike  that only models 2-player collaborative bidding, our baseline and final model are for full Bridge Bidding. Note that since Bridge is not a pure collaborative games and we apply an online version of JPS, the guarantees of Theorem. 2 won’t hold. A Crash Course of Bridge Bidding. The bidding phase of Contract Bridge is like Mini-Bridge (Def. 3) but with a much larger state space (each player now holds a hand with 13 cards from 4 suits). Unlike Mini-Bridge, a player has both her teammate and competitors, making it more than a full-collaborative IG. Therefore, multiple trade-offs needs to be considered. Human handcrafted conventions to signal private hands, called bidding systems. For example, opening bid 2♡♡\\heartsuit used to signal a very strong hand with hearts\nhistorically, but now signals a weak hand with long hearts. Its current usage blocks opponents from getting their best contract, which happens more frequently than its previous usage (to build a strong heart contract). Please see Appendix A for more details. Evaluation Metric. We adopt duplicate bridge tournament format: each board (hands of all 4 players) is played twice, where a specific team sits North-South in one game (called open table), and East-West in another (called close table). The final reward is the difference of the results of two tables. This reduces the impact of card dealing randomness and can better evaluate the strength of an agent. We use IMPs (International Matching Point) per board, or IMPs/b, to measure the strength difference between two Bridge bidding agents. See Appendix A for detailed definition. Intuitively, IMPs/b is the normalized score difference between open and close table in duplicate Bridge, ranging from −2424-24 to +2424+24. In Compute Bridge, a margin of +0.1 IMPs/b is considered significant [ref]32. In a Bridge tournament, a forfeit in a game counts as −33-3 IMPs/b. The difference between a top professional team and an advanced amateur team is about 1.5 IMPs/b. Reward. We focus on the bidding part of the bridge game and replace the playing phase with Double Dummy Solver (DDS) , which computes the maximum tricks each team can get in playing, if all actions are optimal given full information. While this is not how humans plays and in some situations the maximum tricks can only be achieved with full-information, DDS is shown to be a good approximate to human expert plays [ref]32. Therefore, after bidding we skip the playing phase and directly compute IMPs/b from the two tables, each evaluated by DDS, as the only sparse reward. Note that Commercial software like Wbridge5, however, are not optimized to play under the DDS setting, and we acknowledge that the comparison with Wbridge5 is slightly unfair. We leave end-to-end evaluation including the playing phase as future work. Dataset. We generate a training set of 2.5 million hands, drawn from uniform distribution on permutations of 52 cards. We pre-compute their DDS results. The evaluation dataset contains 50k such hands. Both datasets will be open sourced for the community and future work. Baselines. We use baseline16 , baseline19 [ref]32 and baseline  as our baselines, all are neural network based methods. See Appendix B for details of each baseline.\n\nReference markers found: [32]\n\nContext windows for each [ref]:\n[ref]32: IMPs/b is the normalized score difference between open and close table in duplicate Bridge, ranging from −2424-24 to +2424+24. In Compute Bridge, a margin of +0.1 IMPs/b is considered significant [ref]32. In a Bridge tournament, a forfeit in a game counts as −33-3 IMPs/b. The difference between a top professional team and an advanced amateur team is about 1.5 IMPs/b. Reward. We focus on the\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 32,\n            \"titles\": [\n                \"Finding Friend and Foe in Multi-Agent Games\",\n                \"Emergent tool use from multi-agent autocurricula\",\n                \"The hanabi challenge: A new frontier for ai research\",\n                \"Combining Deep Reinforcement Learning and Search for Imperfect-Information Games\",\n                \"Safe and nested subgame solving for imperfect-information games\",\n                \"Superhuman ai for heads-up no-limit poker: Libratus beats top professionals\",\n                \"Superhuman ai for multiplayer poker\",\n                \"Time and space: Why imperfect information games are hard\",\n                \"Solving imperfect information games using decomposition\",\n                \"Deep blue\",\n                \"On the np-completeness of finding an optimal strategy in games with common payoffs\",\n                \"Awesome: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents\",\n                \"Wbridge5\",\n                \"Efficient selectivity and backup operators in monte-carlo tree search\",\n                \"Learning to communicate with deep multi-agent reinforcement learning\",\n                \"Learning with Opponent-Learning Awareness\",\n                \"Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning\",\n                \"Gib: Steps toward an expert-level bridge-playing program\",\n                \"Simple is better: Training an end-to-end contract bridge bidding agent without human knowledge\",\n                \"Double dummy solver\",\n                \"Deep reinforcement learning from self-play in imperfect-information games\",\n                \"Depth-first iterative-deepening: An optimal admissible tree search\",\n                \"Jack bridge\",\n                \"Monte carlo sampling for regret minimization in extensive games\",\n                \"Improving Policies via Search in Cooperative Partially Observable Games\",\n                \"Suphx: Mastering mahjong with deep reinforcement learning\",\n                \"Planning in the presence of cost functions controlled by an adversary\",\n                \"Asynchronous methods for deep reinforcement learning\",\n                \"Playing atari with deep reinforcement learning\",\n                \"Deepstack: Expert-level artificial intelligence in heads-up no-limit poker\",\n                \"Dota 2 with Large Scale Deep Reinforcement Learning\",\n                \"Anytime point-based approximations for large pomdps\",\n                \"Competitive Bridge Bidding with Deep Neural Networks\",\n                \"Mastering the game of go with deep neural networks and tree search\",\n                \"A general reinforcement learning algorithm that masters chess, shogi, and go through self-play\",\n                \"Mastering the game of go without human knowledge\",\n                \"Extending q-learning to general adaptive multi-agent systems\",\n                \"ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games\",\n                \"ELF OpenGo: An Analysis and Open Reimplementation of AlphaZero\",\n                \"Better Computer Go Player with Neural Network and Long-term Prediction\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]32 based on the context and research area of Joint Policy Search for Multi-agent Collaboration with Imperfect Information. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}