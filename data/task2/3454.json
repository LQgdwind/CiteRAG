{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Supported Policy Optimization for Offline Reinforcement Learning\n\nPaper Abstract: \n\nPaper Section Text:\nNext, we evaluate our approach on the D4RL benchmark [ref]6 in comparison to state-of-the-art methods. We focus on Gym-MuJoCo locomotion domains and much more challenging AntMaze domains, which consists of sparse-reward tasks and requires ‚Äústitching‚Äù fragments of suboptimal trajectories traveling undirectedly in order to find a path from the start to the goal of the maze. ¬±plus-or-minus\\pm4.3 ¬±plus-or-minus\\pm7.1 ¬±plus-or-minus\\pm0.5 Baselines. We select the classic BC  and state-of-the-art offline RL methods as baselines. For methods based on dynamic programming, we compare to AWAC , Onestep RL [ref]2, TD3+BC [ref]8, CQL , and IQL . We also include the sequence-modeling method Decision Transformer . Hyperparameter tuning. The weight ŒªùúÜ\\lambda in Eq. (9) is essential for policy constraint. Following prior works [ref]2  , we allow access to the online environment to tune a small set of the hyperparameter ŒªùúÜ\\lambda (<10absent10<10 choices) , which is a reasonable setup for practical applications. See Appendix C.1 for additional discussion and details. Gym-MuJoCo domains. Results for the Gym-MuJoCo domains are shown in Table 2. As we show, SPOT substantially outperforms state-of-the-art methods, especially in suboptimal ‚Äúmedium‚Äù and ‚Äúmedium-replay‚Äù datasets with a large margin, which further demonstrates the advantages of direct constraint on behavior density proposed by SPOT. AntMaze domains. Results for the AntMaze domains are shown in Table 3. Note that D4RL recently releases a bug-fixed ‚Äú-v2‚Äù version of AntMaze datasets, and thus we select competitive baselines and rerun their author-provided implementations for comparison. We also include results for BCQ and BEAR trained on ‚Äú-v0‚Äù datasets, directly from [ref]6. It is not suitable to compare them with reproduced baselines but we include them in order to highlight that previous policy constraint methods struggle to succeed in training on challenging AntMaze domains. ¬±plus-or-minus\\pm4.1 ¬±plus-or-minus\\pm34.0 ¬±plus-or-minus\\pm16.7 ¬±plus-or-minus\\pm5.7 ¬±plus-or-minus\\pm4.2 ¬±plus-or-minus\\pm11.4 ¬±plus-or-minus\\pm7.3 ¬±plus-or-minus\\pm7.9 ¬±plus-or-minus\\pm6.7 ¬±plus-or-minus\\pm5.1 ¬±plus-or-minus\\pm0.0 ¬±plus-or-minus\\pm0.0 ¬±plus-or-minus\\pm21.5 ¬±plus-or-minus\\pm1.6 ¬±plus-or-minus\\pm4.6 ¬±plus-or-minus\\pm0.0 ¬±plus-or-minus\\pm0.4 ¬±plus-or-minus\\pm27.7 ¬±plus-or-minus\\pm24.0 ¬±plus-or-minus\\pm7.0 ¬±plus-or-minus\\pm0.0 ¬±plus-or-minus\\pm0.0 ¬±plus-or-minus\\pm3.8 ¬±plus-or-minus\\pm17.1 ¬±plus-or-minus\\pm8.3 ¬±plus-or-minus\\pm0.0 ¬±plus-or-minus\\pm0.0 ¬±plus-or-minus\\pm6.7 ¬±plus-or-minus\\pm4.1 ¬±plus-or-minus\\pm13.7 ¬±plus-or-minus\\pm15.5 ¬±plus-or-minus\\pm41.7 ¬±plus-or-minus\\pm84.3 ¬±plus-or-minus\\pm59.2 ¬±plus-or-minus\\pm39.7 As shown in Table 3, SPOT performs slightly worse than IQL but outperforms remaining modern offline RL baselines, including the pessimistic value method CQL and the sequence modeling method Decision Transformer. Note that IQL ingeniously designs multi-step dynamic programming and policy extraction steps to apply an implicit constraint for offline RL, but when online fine-tuned after offline RL initialization, IQL is inferior to SPOT. SPOT‚Äôs pluggable design can take full advantage of existing online RL algorithms (see Section 5.3 and Table 4). To the best of our knowledge, our algorithm is the first to train successfully in challenging AntMaze domains with pluggable modification on top of off-policy RL methods for offline RL. Ablation. In Figure 2, we perform an ablation study over the components in our method. First, we replace the VAE-based density estimator with a behavioral-cloned Gaussian policy model  . As expected, it degrades the performance due to the lack of flexibility to model complex distribution, especially on Gym-MuJoCo medium-expert datasets and AntMaze datasets. Then, we evaluate SPOT without Q normalization and find that the removal provides some benefits as well as damages on different datasets, with only an insignificant impact on total performance. Nevertheless, we include it as a default option following TD3+BC [ref]8. Lastly, we investigate how the base off-policy algorithm matters. Since SAC  has similar off-policy performance with TD3, we attempt to adopt SAC as our base method. As Xu et al. 0 argue that the entropy term will do harm to the offline setting, we also implement a variant of SAC without maximum entropy regularization. Surprisingly, both variants are vulnerable to pathological extrapolation error on AntMaze domains and provide poor performance (see Figure 2). We argue that several key native designs of TD3 are beneficial to offline RL. (1).¬†For the case of learning a stochastic policy without entropy regularization, we find that the learned policy quickly degenerates into a deterministic one (with near-zero standard deviation). A concern with deterministic policies is that they are prone to overfit overestimated actions and propagate the estimation error through Bellman backups, which is even serious in offline RL. Hopefully, TD3 introduces Target Policy Smoothing   into Eq.¬†(1), which adds random noise to target actions and can alleviate the effect of error propagation. (2).¬†For the case with entropy regularization, Bellman backup of a stochastic policy resembles Target Policy Smoothing, then the primary distinction may come from the actor learning objective. While TD3 produces a deterministic action minimizing Eq.¬†(9), stochastic policies are more likely to produce out-of-distribution actions with erroneous estimated values, so the policy gradient may become biased as well as with high variance. On much easier Gym-MuJoCo domains, SAC-style SPOT variants are comparable to the TD3 variant on most of the datasets, but we remark that TD3 indeed extends the limit of policy constraint methods on some tasks, such as hopper-medium. Our analysis suggests that TD3 may be preferable for offline RL as the base off-policy method with native designs (such as ‚Äústochastic‚Äù critic training and deterministic actor training) addressing function approximation error not only in the online setting but also in the offline setting [ref]8.\n\nReference markers found: [6]\n\nContext windows for each [ref]:\n[ref]6: Next, we evaluate our approach on the D4RL benchmark [ref]6 in comparison to state-of-the-art methods. We focus on Gym-MuJoCo locomotion domains and much more challenging AntMaze domains, which consists of sparse-reward tasks and requires ‚Äústitching‚Äù\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 6,\n            \"titles\": [\n                \"Benchmarks for Deep Off-Policy Evaluation\",\n                \"Pattern Recognition and Machine Learning\",\n                \"Offline RL Without Off-Policy Evaluation\",\n                \"Importance Weighted Autoencoders\",\n                \"Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling\",\n                \"Decision Transformer: Reinforcement Learning via Sequence Modeling\",\n                \"D4RL: Datasets for Deep Data-Driven Reinforcement Learning\",\n                \"A Minimalist Approach to Offline Reinforcement Learning\",\n                \"Addressing Function Approximation Error in Actor-Critic Methods\",\n                \"Off-Policy Deep Reinforcement Learning without Exploration\",\n                \"MADE: Masked Autoencoder for Distribution Estimation\",\n                \"EMaQ: Expected-max q-learning operator for simple yet effective offline and online rl\",\n                \"Soft Actor-Critic Algorithms and Applications\",\n                \"Denoising Diffusion Probabilistic Models\",\n                \"Offline reinforcement learning as one big sequence modeling problem\",\n                \"Way off-policy batch deep reinforcement learning of implicit human preferences in dialog\",\n                \"MIMIC-III, a freely accessible critical care database\",\n                \"Approximately optimal approximate reinforcement learning\",\n                \"QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation\",\n                \"Adam: A method for stochastic optimization\",\n                \"Auto-encoding variational bayes\",\n                \"An Introduction to Variational Autoencoders\",\n                \"Active Offline Policy Selection\",\n                \"Offline Reinforcement Learning with Fisher Divergence Critic Regularization\",\n                \"Offline Reinforcement Learning with Implicit Q-Learning\",\n                \"Stabilizing off-policy q-learning via bootstrapping error reduction\",\n                \"A Workflow for Offline Model-Free Robotic Reinforcement Learning\",\n                \"Conservative Q-Learning for Offline Reinforcement Learning\",\n                \"Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters\",\n                \"Batch reinforcement learning\",\n                \"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\",\n                \"Provably Good Batch Reinforcement Learning Without Great Exploration\",\n                \"1 year, 1000 km: The oxford robotcar dataset\",\n                \"Human-level control through deep reinforcement learning\",\n                \"AWAC: Accelerating online reinforcement learning with offline datasets\",\n                \"Hyperparameter Selection for Offline Reinforcement Learning\",\n                \"Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning\",\n                \"Alvinn: An autonomous land vehicle in a neural network\",\n                \"Stochastic backpropagation and approximate inference in deep generative models\",\n                \"Trust Region Policy Optimization\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]6 based on the context and research area of Supported Policy Optimization for Offline Reinforcement Learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}