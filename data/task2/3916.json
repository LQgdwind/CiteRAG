{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Trustworthy Graph Neural Networks: Aspects, Methods and Trends\n\nPaper Abstract: \n\nPaper Section Text:\nFrom the view of deep learning, GNNs provide a generalised way to explore graph data. Unlike common image data, graph data is derived from non-Euclidean space  and has certain unique characteristics (e.g., discreteness, irregularity, non-IID nodes), that distinguish trustworthiness practices in GNNs from those in general AI.\nTo better illustrate our trustworthy GNN framework, we here briefly explain each core aspect of trustworthiness, describe the metrics used for evaluation, and pinpoint the differences between research into trustworthy GNNs and general trustworthy AI (also summarised in Table II). Robustness.\nRobustness refers to the ability of GNNs to remain stable under perturbations, especially those that are created by attackers.\nFor example, a credit rating GNN in a financial system [ref]51 should remain secure despite a perturbed transaction graph in which attackers attempt to forge connections between users with high credit scores.\n\nMetrics.\nGenerally, the robustness of GNNs can be measured with reference to the model accuracy [ref]52, attack success rate of certain attack algorithms , mis-classification rate , structural similarity score , and attack budget .\n\nResearch Differences.\nSince graph structure is highly discrete, the notion of perturbations on graph samples (e.g., adding/deleting edges) is fundamentally different from those on ordinary samples in Euclidean space (e.g., images) . Explainability.222We will expand on the difference between interpretability and explainability in Section IV-A.\nSimilar to general deep learning models, most GNNs are regarded as black-box models due to the absence of explainability.\nExplainability enables the predictions of GNNs to be traceable, which in turn enables humans to understand the behaviours of GNNs and discover knowledge.\nFor instance, GNN explanations (e.g., subgraphs) can reveal which components in molecule graphs support the final biochemical functionality predictions of GNNs .\n\nMetrics.\nExplanation accuracy , explanation faithfulness , explanation stability  and explanation sparsity  are commonly used metrics for evaluating GNN explanations. In specific applications, some metrics based on domain knowledge (e.g., correlated separability  in computational pathology) are also used to measure explanations.\n\nResearch Differences.\nUnlike explanations for general AI (e.g., saliency maps , sub-images ), GNN explanations (e.g., nodes, edges, subgraphs)  inherit the irregularity and discreteness of graph data, which go beyond the data characteristics in general AI (e.g., regularity of image data). Thus, the generation of GNN explanations requires specially designed methods . Privacy.\nPrivacy indicates how private data within GNNs can be protected to prevent it from being leaked.\nSpecifically, the privacy of graph data and model parameters, which are regarded as confidential information belonging to their owners, should be guaranteed, and any exposure to unauthorised parties should be prevented  .\n\nMetrics.\nGenerally, a private GNN requires that no leakage of private data (e.g., nodes, edges, the graphs themselves, GNN model parameters, and hyper-parameters for GNN training) occurs in its systems . Privacy can also be measured based on the ability of GNNs to defend against privacy attacks and reduce their attack success rates .\n\nResearch Differences. Graph data, as an important component of GNNs, imposes additional privacy requirements. In addition to protecting the sensitive information of the entities (i.e., nodes), the relationships between them (i.e., edges) also need to be protected . Fairness.\nDiscrepancies in predictions made with regard to marginalised groups indicate that the behaviour of some GNNs is unfair.\nRecent studies have shown that GNNs can discriminate against samples based on protected and sensitive attributes like gender and nationality (e.g., discrimination in credit risk prediction ).\nFairness requires GNNs to accommodate differences between people and provide the same quality of service to users from diverse backgrounds. Metrics.\nCurrently, group fairness (e.g., dyadic fairness , demographic parity and equalized odds ), individual fairness (e.g., similarity-based fairness [ref]33, ranking-based fairness ), and counterfactual fairness  are commonly used metrics for evaluating the fairness of GNNs. Research Differences.\nUnlike the data distribution in general AI fairness, the existence of edges between nodes breaks the assumption that samples are independent and identically distributed. This implies that specific fairness definitions (e.g., involving a similarity function defined on edges) are necessary when studying fairness in GNNs for node-focused computational tasks (see Section II); examples include node classification [ref]33 and link prediction . Accountability.\nAccountability is another aspect of trustworthy GNNs that considers the ability to determine whether a system is performing in accordance with procedural and substantive standards .\nAccountable GNNs contain a set of specific assessments and metrics to justify and identify the responsibilities associated with individual roles or development steps in GNNs.\n\nMetrics.\nThe accountability of GNNs can be measured by whether a standard evaluation process exists  and how well the specifications are established for each step or role of the GNN life cycle .\n\nResearch Differences.\nDue to the unique characteristics of graph data, the evaluation standards for GNNs differ from those of  other deep learning methods.\nFor example, graph datasets with different graph-based metrics (e.g., node degrees) are used to assess how GNN architectures perform differently on various datasets .\nTherefore, researchers need to design new procedures and techniques specifically for building an accountable GNN. In this table,   indicates “Not Covered”,   indicates “Partially Covered”,   indicates “Fully Covered”. Environmental Well-being.\nWell-being evaluates if GNNs are aligned to people’s expectations regarding social good.333Well-being is a generalised term, which encompasses both environmental well-being and various other forms of well-being (ranging from  generating long-term value for shareholders to ending poverty ). In contrast to other forms of well-being, environmental well-being has mainly been explored from the technology perspective. Thus, in this survey, discussions of well-being will focus on introducing existing advancements in environmental well-being.\nThere is currently an urgent need to deploy GNNs on edge devices for real-time inference systems with limited processing power and memory resources (e.g., point cloud segmentation in autonomous vehicles ).\nAs a representative sub-aspect of well-being, environmental well-being focuses on measuring the efficiency of GNNs.\n\nMetrics.\nGenerally, resource/efficiency-related metrics such as inference time , memory footprint , or nodes-per-joule  are employed to evaluate efforts on environmental well-being of GNNs.\n\nResearch Differences.\nThe efficiency bottlenecks of general AI are caused by the scale  and energy-intensive architecture  of models. Due to the powerful presentation capacity and characteristics of graphs, the graph scale and data irregularity are the primary sources of the efficiency bottleneck in GNNs, whose operations rely on input graph structures . Others. While we focus primarily on the above six aspects, which represent the mainstream studies, our proposed trustworthy GNN framework is an open framework that can readily incorporate other trust-oriented characteristics derived from principles outlined in trustworthiness guidelines  .\nFor example, trustworthy systems should be able to obtain knowledge from limited training data and achieve convincing competence on unseen data .\nThis requires GNNs to make compelling predictions on unseen real-world data, especially from domains or distributions that are distant from the training data  .\nFor these trust-oriented characteristics, definition-related metrics are employed to evaluate the extent to which GNNs conform to them.\nFor example, one study on extrapolation  evaluates GNNs by calculating their prediction accuracy on test data, which lies outside the distribution of training data.\n\nReference markers found: [51]\n\nContext windows for each [ref]:\n[ref]51: Robustness.\nRobustness refers to the ability of GNNs to remain stable under perturbations, especially those that are created by attackers.\nFor example, a credit rating GNN in a financial system [ref]51 should remain secure despite a perturbed transaction graph in which attackers attempt to forge connections between users with high credit scores.\n\nMetrics.\nGenerally, the robustness of GNNs\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 51,\n            \"titles\": [\n                \"Adversarial Attack on Graph Structured Data\",\n                \"Wild and interesting facebook statistics and facts (2021)\",\n                \"A Comprehensive Survey on Graph Neural Networks\",\n                \"Graph neural networks: Architectures, stability, and transferability\",\n                \"Aligraph: A comprehensive graph neural network platform\",\n                \"PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest\",\n                \"Learning to Simulate Complex Physics with Graph Networks\",\n                \"Study of mass composition of cosmic rays with icetop and icecube\",\n                \"Improving fake news detection with domain-adversarial and graph-attention neural network\",\n                \"Deep learning identifies synergistic drug combinations for treating COVID-19\",\n                \"Trustworthy AI: A Computational Perspective\",\n                \"Trustworthy AI: From Principles to Practices\",\n                \"Trustworthy machine learning\",\n                \"Toward trustworthy AI development: Mechanisms for supporting verifiable claims\",\n                \"Governance principles for the new generation artificial intelligence–developing responsible artificial intelligence\",\n                \"The montreal declaration of responsible ai\",\n                \"How Powerful are Graph Neural Networks?\",\n                \"Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View\",\n                \"Training Graph Neural Networks with 1000 Layers\",\n                \"Graph Self-Supervised Learning: A Survey\",\n                \"BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection\",\n                \"Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information\",\n                \"GNNExplainer: Generating Explanations for Graph Neural Networks\",\n                \"Definition of trustworthy\",\n                \"Trust as a social reality\",\n                \"gazelle: A Low Latency Framework for Secure Neural Network Inference\",\n                \"Communication-efficient learning of deep networks from decentralized data\",\n                \"Deep Learning with Differential Privacy\",\n                \"Adversarial attacks and defenses on graphs\",\n                \"Explainability in graph neural networks: A taxonomic survey\",\n                \"Towards a rigorous theoretical analysis and evaluation of GNN explanations\",\n                \"K. Balasubramanian\",\n                \"“On dyadic fairness: Exploring and mitigating bias in graph connections\",\n                \"“Inform: Individual fairness on graph mining\",\n                \"A pipeline for fair comparison of graph neural networks in node classification tasks\",\n                \"“Hardware acceleration of graph neural networks\",\n                \"BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant Weight Matrices\",\n                \"Explaining and Harnessing Adversarial Examples\",\n                \"“Why Should I Trust You?” Explaining the Predictions of Any Classifier\",\n                \"“Deep inside convolutional networks: Visualising image classification models and saliency maps\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]51 based on the context and research area of Trustworthy Graph Neural Networks: Aspects, Methods and Trends. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}