{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Complexity of a Class of First-Order Objective-Function-Free Optimization Algorithms\n\nPaper Abstract: \n\nPaper Section Text:\nWe first consider a choice of scaling factors directly derived from the\ndefinition of the Adagrad algorithm .\nFor given\nς∈(0,1]𝜍01\\varsigma\\in(0,1], ϑ∈(0,1]italic-ϑ01\\vartheta\\in(0,1], θ>0𝜃0\\theta>0 and μ∈(0,1)𝜇01\\mu\\in(0,1), define,\nfor all i∈{1,…,n}𝑖1…𝑛i\\in\\{1,\\ldots,n\\} and for all k≥0𝑘0k\\geq 0, The Adagrad scaling factors are recovered by μ=12𝜇12\\mu={\\scriptstyle\\frac{1}{2}}, θ=1𝜃1\\theta=1 and ϑ=1italic-ϑ1\\vartheta=1, and\nASTR1 with (3.1) and Bk=0subscript𝐵𝑘0B_{k}=0 is then the standard (deterministic) Adagrad\nmethod. The formulation (3.1) allows a parametric analysis of methods “in\nthe neighbourhood” of Adagrad, using not only first-order but also second-order information.\nThe ϑitalic-ϑ\\vartheta parameter is introduced for flexibility, in\nparticular allowing non-monotone scaling factors(3)(3)(3)Typical\nvalues are ςi=1100subscript𝜍𝑖1100\\varsigma_{i}={\\scriptstyle\\frac{1}{100}} and ϑ=11000italic-ϑ11000\\vartheta={\\scriptstyle\\frac{1}{1000}}.\nThe additional scaling parameter(4)(4)(4)Which can be viewed as\na stepsize/learning rate parameter when Bk=0subscript𝐵𝑘0B_{k}=0. θ𝜃\\theta is introduced\nas a technique to improve the convergence rate of the resulting algorithm.\nSuch a scaling may be useful when the gradient is sparse\n or when designing a private Adagrad version\n[ref]1 in order to improve the complexity bound with respect\nto the problem’s parameters. The above parametrization\nwith θ=1𝜃1\\theta=1 has also been considered in [ref]11 in the\ncontext of a continuous Ordinary Differential Equations analysis and,\nwith θ𝜃\\theta depending on problem’s constants, in \nwhere the sum on ℓℓ\\ell in (3.1) is terminated at ℓ=k−1ℓ𝑘1\\ell=k-1\nand μ𝜇\\mu is restricted to the interval [12,1)121[\\frac{1}{2},1) in the\nstochastic regime. In what follows, we consider the discrete\ndeterministic case for the interval (0,1)01(0,1). Before stating the global rate of convergence of the variant of ASTR1\nusing (3.1), we first prove a lemma, partly inspired by\n[ref]50 . \n\nLemma 3.1\n\nLet {ak}k≥0subscriptsubscript𝑎𝑘𝑘0\\{a_{k}\\}_{k\\geq 0} be a non-negative sequence,\nα>0𝛼0\\alpha>0, ξ>0𝜉0\\xi>0 and define, for each k≥0𝑘0k\\geq 0,\nbk=∑j=0kajsubscript𝑏𝑘superscriptsubscript𝑗0𝑘subscript𝑎𝑗b_{k}=\\sum_{j=0}^{k}a_{j}. Then if α≠1𝛼1\\alpha\\neq 1,\n\n\n\n∑j=0kaj(ξ+bj)α≤1(1−α)​((ξ+bk)1−α−ξ1−α).superscriptsubscript𝑗0𝑘subscript𝑎𝑗superscript𝜉subscript𝑏𝑗𝛼11𝛼superscript𝜉subscript𝑏𝑘1𝛼superscript𝜉1𝛼\\sum_{j=0}^{k}\\frac{a_{j}}{(\\xi+b_{j})^{\\alpha}}\\leq\\frac{1}{(1-\\alpha)}((\\xi+b_{k})^{1-\\alpha}-\\xi^{1-\\alpha}).\n\n(3.2)\n\nOtherwise (i.e. if α=1𝛼1\\alpha=1),\n\n\n\n∑j=0kaj(ξ+bj)≤log⁡(ξ+bkξ).superscriptsubscript𝑗0𝑘subscript𝑎𝑗𝜉subscript𝑏𝑗𝜉subscript𝑏𝑘𝜉\\sum_{j=0}^{k}\\frac{a_{j}}{(\\xi+b_{j})}\\leq\\log\\left(\\frac{\\xi+b_{k}}{\\xi}\\right).\n\n(3.3)\n\n\n  Proof.   \nConsider first the case where α≠1𝛼1\\alpha\\neq 1 and note that\n1(1−α)​x1−α11𝛼superscript𝑥1𝛼\\frac{1}{(1-\\alpha)}x^{1-\\alpha} is then a non-decreasing and concave\nfunction on (0,+∞)0(0,+\\infty). Setting b−1=0subscript𝑏10b_{-1}=0 and using these properties, we\nobtain that, for j≥0𝑗0j\\geq 0, We then obtain (3.2) by summing this inequality for j∈{0,…,k}𝑗0…𝑘j\\in\\{0,\\ldots,k\\}. Suppose now that α=1𝛼1\\alpha=1, we then use the concavity and non-decreasing\ncharacter of the logarithm to derive that The inequality (3.3) then again follows by summing for j∈{0,…,k}𝑗0…𝑘j\\in\\{0,\\ldots,k\\}.\n  □□\\Box From (3.2), we also obtain that, for α<1𝛼1\\alpha<1, while, for α>1𝛼1\\alpha>1, Note that both the numerator and the denominator of the right-hand side of\n(3.2) tend to zero when α𝛼\\alpha tends to one. Applying\nl’Hospital rule, we then see that this right-hand side tends to the right-hand\nside of (3.3) and the bounds on ∑j=0kaj/(ξ+bj)αsuperscriptsubscript𝑗0𝑘subscript𝑎𝑗superscript𝜉subscript𝑏𝑗𝛼\\sum_{j=0}^{k}a_{j}/(\\xi+b_{j})^{\\alpha} are therefore continuous at α=1𝛼1\\alpha=1. Lemma 3.1 is crucial in the proof of our main complexity\nresult, which we now state. \n\nTheorem 3.2\n\nSuppose that AS.1–AS.3 hold and that the\nASTR1 algorithm is applied to problem (2.1) with its\nscaling given by (3.1).\nIf we define\n\n\n\nΓ0=deff​(x0)−flow,superscriptdefsubscriptΓ0𝑓subscript𝑥0subscript𝑓low\\Gamma_{0}\\stackrel{{\\scriptstyle\\rm def}}{{=}}f(x_{0})-f_{\\rm low},\n\n\nthen,\n\n(i) \n\nif 0<μ<120𝜇120<\\mu<{\\scriptstyle\\frac{1}{2}},\n\n\n\naveragej∈{0,…,k}‖gj‖2≤κ1k+1,subscriptaverage𝑗0…𝑘superscriptnormsubscript𝑔𝑗2subscript𝜅1𝑘1\\operatorname*{average}_{j\\in\\{0,\\ldots,k\\}}\\|g_{j}\\|^{2}\\leq\\frac{\\kappa_{1}}{k+1},\n\n(3.6)\n\nwith\n\n\n\nκ1=max⁡{ς,[22​μ​ϑ​(1−2​μ)​θ2​Γ0n​(κB+L)]11−2​μ,[4​n​κBBL(1−2​μ)​τ​θ​ςμ​ϑ32]1μ};subscript𝜅1𝜍superscriptdelimited-[]superscript22𝜇italic-ϑ12𝜇superscript𝜃2subscriptΓ0𝑛subscript𝜅B𝐿112𝜇superscriptdelimited-[]4𝑛subscript𝜅BBL12𝜇𝜏𝜃superscript𝜍𝜇superscriptitalic-ϑ321𝜇\\kappa_{1}=\\max\\left\\{\\varsigma,\\left[\\frac{2^{2\\mu}\\vartheta(1-2\\mu)\\theta^{2}\\Gamma_{0}}{n(\\kappa_{\\mbox{\\tiny B}}+L)}\\right]^{{\\scriptstyle\\frac{1}{1-2\\mu}}},\\left[\\frac{4\\,n\\,\\kappa_{\\mbox{\\tiny BBL}}}{(1-2\\mu)\\tau\\theta\\varsigma^{\\mu}\\vartheta^{\\scriptstyle\\frac{3}{2}}}\\right]^{\\scriptstyle\\frac{1}{\\mu}}\\right\\};\n\n(3.7)\n\n\n(ii) \n\nif μ=12𝜇12\\mu={\\scriptstyle\\frac{1}{2}},\n\n\n\naveragej∈{0,…,k}‖gj‖2≤κ2k+1,subscriptaverage𝑗0…𝑘superscriptnormsubscript𝑔𝑗2subscript𝜅2𝑘1\\operatorname*{average}_{j\\in\\{0,\\ldots,k\\}}\\|g_{j}\\|^{2}\\leq\\frac{\\kappa_{2}}{k+1},\n\n(3.8)\n\nwith\n\n\n\nκ2=max⁡{ς,12​e2​Γ0​ϑ​θ2n​(κB+L),12​ς​(8​n​κB​(κB+L)τ​ϑ32​θ)2​|W−1​(−τ​ς​θ​ϑ328​n​κB​(κB+L))|2},subscript𝜅2𝜍12superscript𝑒2subscriptΓ0italic-ϑsuperscript𝜃2𝑛subscript𝜅𝐵𝐿12𝜍superscript8𝑛subscript𝜅Bsubscript𝜅B𝐿𝜏superscriptitalic-ϑ32𝜃2superscriptsubscript𝑊1𝜏𝜍𝜃superscriptitalic-ϑ328𝑛subscript𝜅Bsubscript𝜅B𝐿2\\kappa_{2}=\\max\\left\\{\\varsigma,\\frac{\\displaystyle 1}{\\displaystyle 2}e^{\\frac{2\\Gamma_{0}\\vartheta\\theta^{2}}{n(\\kappa_{B}+L)}},\\frac{1}{2\\varsigma}\\left(\\frac{8n\\kappa_{\\mbox{\\tiny B}}(\\kappa_{\\mbox{\\tiny B}}+L)}{\\tau\\vartheta^{\\scriptstyle\\frac{3}{2}}\\theta}\\right)^{2}\\,\\left|W_{-1}\\left(-\\frac{\\tau\\varsigma\\theta\\vartheta^{\\scriptstyle\\frac{3}{2}}}{8n\\kappa_{\\mbox{\\tiny B}}(\\kappa_{\\mbox{\\tiny B}}+L)}\\right)\\right|^{2}\\right\\},\n\n(3.9)\n\nwhere W−1subscript𝑊1W_{-1} is the second branch of the Lambert function ;\n\n(iii) \n\nif 12<μ<112𝜇1{\\scriptstyle\\frac{1}{2}}<\\mu<1,\n\n\n\naveragej∈{0,…,k}‖gj‖2≤κ3k+1subscriptaverage𝑗0…𝑘superscriptnormsubscript𝑔𝑗2subscript𝜅3𝑘1\\operatorname*{average}_{j\\in\\{0,\\ldots,k\\}}\\|g_{j}\\|^{2}\\leq\\frac{\\kappa_{3}}{k+1}\n\n(3.10)\n\nwith\n\n\n\nκ3=def=max{ς,[21+μ​κBτ​ςμ​ϑ(Γ0θ+n​(κB+L)​ς1−2​μ2​ϑ​θ​(2​μ−1))]11−μ}\\kappa_{3}\\stackrel{{\\scriptstyle\\rm def}}{{=}}=\\max\\left\\{\\varsigma,\\left[\\frac{2^{1+\\mu}\\kappa_{\\mbox{\\tiny B}}}{\\tau\\varsigma^{\\mu}\\sqrt{\\vartheta}}\\left(\\!\\Gamma_{0}\\theta+\\frac{n(\\kappa_{\\mbox{\\tiny B}}+L)\\varsigma^{1-2\\mu}}{2\\vartheta\\theta(2\\mu-1)}\\right)\\right]^{{\\scriptstyle\\frac{1}{1-\\mu}}}\\right\\}\n\n(3.11)\n\n\n\n\n  Proof.   \nWe see from (3.1) that wi,ksubscript𝑤𝑖𝑘w_{i,k} verifies AS.4. We may\nthus use Lemma 2.1. Moreover, (3.1) also implies that for all j≥0𝑗0j\\geq 0 and all i∈{1,…,n}𝑖1…𝑛i\\in\\{1,\\ldots,n\\}.\nWe now deduce from\n(2.2) and (2.12) that, for k≥0𝑘0k\\geq 0, For each i∈{1,…,n}𝑖1…𝑛i\\in\\{1,\\ldots,n\\}, we then apply Lemma 3.1\nwith aℓ=gi,ℓ2subscript𝑎ℓsuperscriptsubscript𝑔𝑖ℓ2a_{\\ell}=g_{i,\\ell}^{2}, ξ=ς𝜉𝜍\\xi=\\varsigma and α=2​μ<1𝛼2𝜇1\\alpha=2\\mu<1, and obtain\nfrom (2.2) and (3.1) that, Now and thus substituting this bound in (3.13) and using AS.3 gives that Suppose now that implying Then, using (3.16) and (3.12), Solving this inequality for ∑j=0k‖gj‖2superscriptsubscript𝑗0𝑘superscriptnormsubscript𝑔𝑗2\\sum_{j=0}^{k}\\|g_{j}\\|^{2} gives that and therefore Alternatively, if (3.17) fails, then Combining (3.18) and (3.19)\ngives (3.6). Let us now consider the case where μ=12𝜇12\\mu={\\scriptstyle\\frac{1}{2}}.\nFor each i∈{1,…,n}𝑖1…𝑛i\\in\\{1,\\ldots,n\\}, we apply Lemma 3.1\nwith ak=gi,k2subscript𝑎𝑘superscriptsubscript𝑔𝑖𝑘2a_{k}=g_{i,k}^{2}, ξ=ς𝜉𝜍\\xi=\\varsigma and α=2​μ=1𝛼2𝜇1\\alpha=2\\mu=1 and obtain that, and substituting this bound in (3.13) then gives that Suppose now that implying that Using (3.12) for μ=12𝜇12\\mu={\\scriptstyle\\frac{1}{2}}, we obtain then that that is Now define and observe that that γ2>3​γ1subscript𝛾23subscript𝛾1\\gamma_{2}>3\\gamma_{1} because τ​ς​ϑ32≤1𝜏𝜍superscriptitalic-ϑ321\\tau\\sqrt{\\varsigma}\\vartheta^{\\scriptstyle\\frac{3}{2}}\\leq 1\nand κB≥1subscript𝜅B1\\kappa_{\\mbox{\\tiny B}}\\geq 1.\nThe inequality (3.21) can then be rewritten as Let us denote by ψ​(u)=defγ1​u−γ2​log⁡(u)superscriptdef𝜓𝑢subscript𝛾1𝑢subscript𝛾2𝑢\\psi(u)\\stackrel{{\\scriptstyle\\rm def}}{{=}}\\gamma_{1}u-\\gamma_{2}\\log(u). Since\nγ2>3​γ1subscript𝛾23subscript𝛾1\\gamma_{2}>3\\gamma_{1}, the equation ψ​(u)=0𝜓𝑢0\\psi(u)=0 admits two roots u1≤u2subscript𝑢1subscript𝑢2u_{1}\\leq u_{2} and (3.23) holds for u∈[u1,u2]𝑢subscript𝑢1subscript𝑢2u\\in[u_{1},u_{2}].\nThe definition of u2subscript𝑢2u_{2} then gives that which is Setting z=−γ1γ2​u2𝑧subscript𝛾1subscript𝛾2subscript𝑢2z=-\\frac{\\gamma_{1}}{\\gamma_{2}}u_{2}, we obtain that Thus z=W−1​(−γ1γ2)<0𝑧subscript𝑊1subscript𝛾1subscript𝛾20z=W_{-1}(-\\frac{\\gamma_{1}}{\\gamma_{2}})<0, where W−1subscript𝑊1W_{-1} is the second\nbranch of the Lambert function defined over [−1e,0)1𝑒0[-\\frac{1}{e},0).\nAs −γ1γ2≥−13subscript𝛾1subscript𝛾213-\\frac{\\gamma_{1}}{\\gamma_{2}}\\geq-\\frac{1}{3}, z𝑧z is well defined and thus As a consequence, we deduce from (3.23) and (3.22) that and If (3.20) does not hold, we have that Combining (3.24) and (3.25)\ngives (3.8). Finally, suppose that 12<μ<112𝜇1{\\scriptstyle\\frac{1}{2}}<\\mu<1.\nOnce more, we apply Lemma 3.1 for each i∈{1,…,n}𝑖1…𝑛i\\in\\{1,\\ldots,n\\}\nwith aℓ=gi,ℓ2subscript𝑎ℓsuperscriptsubscript𝑔𝑖ℓ2a_{\\ell}=g_{i,\\ell}^{2}, ξ=ς𝜉𝜍\\xi=\\varsigma and α=2​μ>1𝛼2𝜇1\\alpha=2\\mu>1 and obtain that Substituting the bound (3.26) in (3.13) and using\n(3.12) and AS.3 gives that If we now suppose that then If (3.27) does not hold, we derive that Thus, (3.28) and(3.29)\nfinally imply (3.10).\n  □□\\Box These result suggest additional remarks. That the bounds given are not continuous as a function μ𝜇\\mu\nat μ=12𝜇12\\mu={\\scriptstyle\\frac{1}{2}} is a result of our bounding process within the\nproof of Theorem 3.2 (for instance in the last inequality of\n(3.14)). Continuous bounds have been proved\n(see) if one is ready to\nassume that the objective functions’ gradients remain uniformly\nbounded. If the algorithm is terminated as soon as ‖gk‖≤ϵnormsubscript𝑔𝑘italic-ϵ\\|g_{k}\\|\\leq\\epsilon (which\nis customary for deterministic algorithms searching for first-order points),\nit must stop at the latest at iteration where κ⋆=κ1subscript𝜅⋆subscript𝜅1\\kappa_{\\star}=\\kappa_{1} for μ∈(0,12)𝜇012\\mu\\in(0,{\\scriptstyle\\frac{1}{2}}),\nκ⋆=κ2subscript𝜅⋆subscript𝜅2\\kappa_{\\star}=\\kappa_{2} for μ=12𝜇12\\mu={\\scriptstyle\\frac{1}{2}} and κ⋆=κ3subscript𝜅⋆subscript𝜅3\\kappa_{\\star}=\\kappa_{3} for\nμ∈(12,1)𝜇121\\mu\\in({\\scriptstyle\\frac{1}{2}},1). It is truly remarkable that there exist first-order\nOFFO methods whose global complexity order is identical to that of\nstandard first-order methods using function evaluations (see\n   or\n[10, Chapter 2]), despite the fact that the latter\nexploit significantly more information. As mentionned in the\nintroduction, this complexity rate was also derived for the\nanalysis of Adagrad-Norm in [ref]50. It is possible to give a weaker but more explicit bound on κ2subscript𝜅2\\kappa_{2} by finding an upper\nbound on the value of the involved Lambert function. This can be obtained by\nusing [12, Theorem 1] which states that, for x>0𝑥0x>0, Remembering that, for γ1subscript𝛾1\\gamma_{1} and γ2subscript𝛾2\\gamma_{2} given by (3.22),\nlog⁡(γ2γ1)≥log⁡(3)>1subscript𝛾2subscript𝛾131\\log\\left(\\frac{\\gamma_{2}}{\\gamma_{1}}\\right)\\geq\\log(3)>1\nand taking x=log⁡(γ2γ1)−1>0𝑥subscript𝛾2subscript𝛾110x=\\log\\left(\\frac{\\gamma_{2}}{\\gamma_{1}}\\right)-1>0\nin (3.31) then gives that It is also possible to extend the definition of skLsuperscriptsubscript𝑠𝑘𝐿s_{k}^{L} in (2.6)\nby premultiplying it by a stepsize αk∈[αmin,1]subscript𝛼𝑘subscript𝛼1\\alpha_{k}\\in[\\alpha_{\\min},1] for some\nαmin∈(0,1]subscript𝛼01\\alpha_{\\min}\\in(0,1]. Our results again remain valid (with modified\nconstants). Covering a deterministic momentum-less Adam would require extending the\nresults to allow for (3.1) to be replaced by for some β2<1subscript𝛽21\\beta_{2}<1. This can be done by following the argument of Theorem 2\nin . However, as in this reference, the final bound\non the squared gradient norms does not tend to zero when k𝑘k grows(5)(5)(5)A\nconstant term in −log⁡(β2)subscript𝛽2-\\log(\\beta_{2}) refuses to vanish., illustrating the\n(known) lack of convergence of Adam. We therefore do not investigate this\noption in detail. Focusing on the choice of the parameter μ𝜇\\mu independently of ϑitalic-ϑ\\vartheta and\nθ𝜃\\theta, we verify that the choice μ=12𝜇12\\mu=\\frac{1}{2} is best, yielding an\nupper complexity bound for the deterministic Adagrad algorithm and recovering\na similar result obtained in [ref]50 for the Adagrad-Norm algorithm. The choice μ=1𝜇1\\mu=1 is not covered by our theory but has been considered in\n, where a specific choice of the constant θ𝜃\\theta with μ=1𝜇1\\mu=1\nis used to derive a first order method with optimal regret for strongly convex\nproblems. Unfortunately, the proposed SC(Strong Convex)-Adagrad algorithm requires\nthe knowledge of the problem’s Lipschitz constant and noise characteristics. We now discuss the dependence of the bounds given by Theorem 3.2 as\na function of the problem’s constants L𝐿L, n𝑛n and Γ0subscriptΓ0\\Gamma_{0} and recall that\nif n𝑛n is known a priori, this is generally not the case for the Lispchitz\nconstant L𝐿L, while the gap Γ0subscriptΓ0\\Gamma_{0} may be available for some classes of problems\n(such as nonlinear regressions where Γ0≤f​(x0)subscriptΓ0𝑓subscript𝑥0\\Gamma_{0}\\leq f(x_{0})). We are mostly interested\nin the explicit dependence of the bounds on n𝑛n, taking into account that the unknown\nL𝐿L often varies little with dimension –such as in problems arising from\ndiscretizations– but admittedly ignoring the fact it may also depend on n𝑛n, sometimes\nseverely [10, p. 14]. As we are allowed to do so, we would like\nto choose the scaling parameter θ𝜃\\theta in order to offset this dependence as much as possible.\nIn view of (3.7), (3.9) and (3.11), and noting that\n(3.31) indicates that the |W−1|2superscriptsubscript𝑊12|W_{-1}|^{2} term in (3.9) can be\nsummarized as 𝒪(log(nL)2){\\cal O}\\left(\\log(nL)^{2}\\right), we attempt to balance the impact\nof the various terms involving both θ𝜃\\theta and n𝑛n and, in the absence of additional\ninformation on the problem, select We may then compare the bounds obtained by reinjecting this value in (3.9)\n(μ=12𝜇12\\mu={\\scriptstyle\\frac{1}{2}}) with the results for deterministic Adagrad-Norm [ref]50 and\nAdagrad . This comparison is summarized in the following table. Are the (good) upper bound given Theorem 3.2 sharp? \n\nTheorem 3.3\n\nThe bounds (3.6), (3.8) and\n(3.10) are essentially sharp in that, for each\nμ∈(0,1)𝜇01\\mu\\in(0,1) and each η∈(0,1]𝜂01\\eta\\in(0,1], there exists a univariate function fμ,ηsubscript𝑓𝜇𝜂f_{\\mu,\\eta}\nsatisfying AS.1-AS.3 such that, when applied to minimize fμ,ηsubscript𝑓𝜇𝜂f_{\\mu,\\eta} from the\norigin, the ASTR1 algorithm with (3.1), Bk=0subscript𝐵𝑘0B_{k}=0 and ϑ=θ=1italic-ϑ𝜃1\\vartheta=\\theta=1 produces a sequence of\ngradient norms given by ‖gk‖=1k12+ηnormsubscript𝑔𝑘1superscript𝑘12𝜂\\|g_{k}\\|=\\frac{1}{k^{{\\scriptstyle\\frac{1}{2}}+\\eta}}.\n\n  Proof.   Following ideas of [10, Theorem 2.2.3], we first\nconstruct a sequence of iterates {xk}subscript𝑥𝑘\\{x_{k}\\} for which\nfμ,η​(xk)=fksubscript𝑓𝜇𝜂subscript𝑥𝑘subscript𝑓𝑘f_{\\mu,\\eta}(x_{k})=f_{k} and ∇x1fμ,η​(xk)=gksuperscriptsubscript∇𝑥1subscript𝑓𝜇𝜂subscript𝑥𝑘subscript𝑔𝑘\\nabla_{x}^{1}f_{\\mu,\\eta}(x_{k})=g_{k}\nfor associated sequences of function and gradient values\n{fk}subscript𝑓𝑘\\{f_{k}\\} and {gk}subscript𝑔𝑘\\{g_{k}\\}, and then apply Hermite interpolation\nto exhibit the function fμ,ηsubscript𝑓𝜇𝜂f_{\\mu,\\eta} itself. We start by defining yielding that (remember that g02=4superscriptsubscript𝑔024g_{0}^{2}=4).\nWe then define Bk=def0superscriptdefsubscript𝐵𝑘0B_{k}\\stackrel{{\\scriptstyle\\rm def}}{{=}}0 for all k≥0𝑘0k\\geq 0, and where ζ​(⋅)𝜁⋅\\zeta(\\cdot) is the Riemann zeta function.\nObserve that the sequence {fk}subscript𝑓𝑘\\{f_{k}\\} is decreasing and that, for all k≥0𝑘0k\\geq 0, where we used (3.38) and (3.36). Hence\n(3.38) implies that Also note that, using (3.38), while, using (3.35), Moreover, using the fact that 1/x12+η1superscript𝑥12𝜂1/x^{{\\scriptstyle\\frac{1}{2}}+\\eta} is a convex function of x𝑥x over\n[1,+∞)1[1,+\\infty), and that from (3.35)\nsk≥1k12+η​(ς+4+k)μsubscript𝑠𝑘1superscript𝑘12𝜂superscript𝜍4𝑘𝜇s_{k}\\geq\\frac{1}{k^{{\\scriptstyle\\frac{1}{2}}+\\eta}\\left(\\varsigma+4+k\\right)^{\\mu}}, we\nderive that, for k>0𝑘0k>0, These last bounds and (3.39) allow us to use standard Hermite\ninterpolation on the data given by {fk}subscript𝑓𝑘\\{f_{k}\\} and {gk}subscript𝑔𝑘\\{g_{k}\\}: see, for instance,\nTheorem A.9.1 in  with p=1𝑝1p=1 and (the second term in the max bounding |fk|subscript𝑓𝑘|f_{k}| because of\n(3.39) and the third bounding |gk|subscript𝑔𝑘|g_{k}| because of (3.34)).\nWe then deduce that there exists a continuously differentiable function fμ,ηsubscript𝑓𝜇𝜂f_{\\mu,\\eta}\nfrom IR to IR with Lipschitz continuous gradient (i.e. satisfying\nAS.1 and AS.2) such that, for k≥0𝑘0k\\geq 0, Moreover, the range of fμ,ηsubscript𝑓𝜇𝜂f_{\\mu,\\eta} and ∇x1fμ,ηsuperscriptsubscript∇𝑥1subscript𝑓𝜇𝜂\\nabla_{x}^{1}f_{\\mu,\\eta} are constant independent\nof η𝜂\\eta, hence guaranteeing AS.3 and AS.3.\nThe definitions (3.34), (3.35), (3.37) and (3.38)\nimply that the sequences {xk}subscript𝑥𝑘\\{x_{k}\\}, {fk}subscript𝑓𝑘\\{f_{k}\\} and {gk}subscript𝑔𝑘\\{g_{k}\\} can be seen as generated by the\nASTR1 algorithm (with (3.1), Bk=0subscript𝐵𝑘0B_{k}=0 and\nϑ=θ=1italic-ϑ𝜃1\\vartheta=\\theta=1) applied to fμ,ηsubscript𝑓𝜇𝜂f_{\\mu,\\eta}, starting from x0=0subscript𝑥00x_{0}=0\nand the desired conclusion follows.\n  □□\\Box The bounds (3.6), (3.8) and\n(3.10) are therefore essentially sharp (in the sense of\n) for the ASTR1 algorithm with (3.1) and ϑ=θ=1italic-ϑ𝜃1\\vartheta=\\theta=1, which\nis to say that the lower complexity bound for the algorithm is arbitrarily close\nto its upper bound.\nInterestingly, the argument in the\nproof of the above theorem fails for η=0𝜂0\\eta=0, as this choice yields that Since tends to infinity as k𝑘k grows, this indicates (in view (3.38)) that\nAS.3 cannot hold. Also note that (3.34) implies that the\ngradients remain uniformly bounded.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: is introduced\nas a technique to improve the convergence rate of the resulting algorithm.\nSuch a scaling may be useful when the gradient is sparse\n or when designing a private Adagrad version\n[ref]1 in order to improve the complexity bound with respect\nto the problem’s parameters. The above parametrization\nwith θ=1𝜃1\\theta=1 has also been considered in [ref]11 in the\ncontext of a\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance\",\n                \"Private Adaptive Gradient Methods for Convex Optimization\",\n                \"Two-point step size gradient method\",\n                \"First-order Methods in Optimization\",\n                \"The Impact of Noise on Evaluation Complexity: The Deterministic Trust-Region Case\",\n                \"Nonlinear Programming\",\n                \"Convergence rate analysis of a stochastic trust region method via supermartingales\",\n                \"On the complexity of steepest descent, Newton’s and regularized Newton’s methods for nonconvex unconstrained optimization\",\n                \"Worst-case evaluation complexity and optimality of second-order methods for nonconvex smooth optimization\",\n                \"Evaluation complexity of algorithms for nonconvex optimization\",\n                \"Generalized AdaGrad (G-AdaGrad) and Adam: A State-Space Perspective\",\n                \"Bounds on the Lambert function and their application to the outage analysis of user cooperation\",\n                \"Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks\",\n                \"Trust-Region Methods\",\n                \"On the Lambert W function\",\n                \"Concise complexity analyses for trust region methods\",\n                \"A Simple Convergence Proof of Adam and Adagrad\",\n                \"Optimality measures for performance profiles\",\n                \"Adaptive subgradient methods for online learning and stochastic optimization\",\n                \"Estimation, optimization, and parallelism when data is sparse\",\n                \"A stochastic trust region algorithm based on careful step normalization\",\n                \"A new trust region algorithm with trust region radius converging to zero\",\n                \"Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD\",\n                \"The power of adaptivity in SGD: Self-tuning step sizes with unbounded gradients and affine variance\",\n                \"Stochastic first- and zeroth-order methods for nonconvex stochastic programming\",\n                \"Solving the trust-region subproblem using the Lanczos method\",\n                \"CUTEst: a constrained and unconstrained testing environment with safe threads for mathematical optimization\",\n                \"An adaptive trust-region method without function evaluation\",\n                \"First-order objective-function-free optimization algorithms and their complexity\",\n                \"Parametric complexity analysis for a class of first-order Adagrad-like algorithms\",\n                \"Convergence properties of an objective-function-free optimization regularization algorithm, including an 𝒪​(ϵ−3/2)𝒪superscriptitalic-ϵ32\\\\mathcal{O}(\\\\epsilon^{-3/2}) complexity bound\",\n                \"Recursive trust-region methods for multiscale nonlinear optimization\",\n                \"OPM, a collection of optimization problems in Matlab\",\n                \"Online Adaptive Methods, Universality and Acceleration\",\n                \"Adam: A method for stochastic optimization\",\n                \"On the convergence of stochastic gradient descent with adaptive stepsizes\",\n                \"On the limited memory BFGS method for large scale optimization\",\n                \"Adaptive bound optimization for online convex optimization\",\n                \"Algorithms for bound constrained quadratic programming problems\",\n                \"On the solution of large quadratic programming problems with bound constraints\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of Complexity of a Class of First-Order Objective-Function-Free Optimization Algorithms. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}