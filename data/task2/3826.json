{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: 1 Introduction\n\nPaper Abstract: \n\nPaper Section Text:\nLet us formally introduce the tallying bandit setting. An online learning game is an (m,g,h)ğ‘šğ‘”â„(m,g,h)-tallying bandit if the adversary is mğ‘šm-memory bounded, gğ‘”g-restricted and hâ„h-tallying, and if after playing action atsubscriptğ‘ğ‘¡a_{t} the player observes a random variable h~atâ€‹(yt)âˆˆ [ref]1subscript~â„subscriptğ‘ğ‘¡subscriptğ‘¦ğ‘¡01\\widetilde{h}_{a_{t}}(y_{t})\\in [ref]1 satisfying where yt=âˆ‘tâ€²=maxâ¡{1,tâˆ’m+1}tğ•€â€‹(atâ€²=at)subscriptğ‘¦ğ‘¡superscriptsubscriptsuperscriptğ‘¡â€²1ğ‘¡ğ‘š1ğ‘¡ğ•€subscriptğ‘superscriptğ‘¡â€²subscriptğ‘ğ‘¡y_{t}=\\sum_{t^{\\prime}=\\max\\{1,t-m+1\\}}^{t}\\mathbb{I}(a_{t^{\\prime}}=a_{t}). We assume the cardinality Kğ¾K of the action set ğ’³ğ’³\\mathcal{X} is finite, and also that mğ‘šm is known (although we discuss how to relax this in SectionÂ 6). With this definition of the setting in hand, we can restate our goal of efficiently minimizing the CPR. Concretely, we desire an algorithm, which when given an (m,g,h)ğ‘šğ‘”â„(m,g,h)-tallying bandit problem, has a CPR bound that is polynomial in K,mğ¾ğ‘šK,m and is sublinear in Tğ‘‡T. The tallying bandit strictly generalizes the well studied stochastic multi armed bandit (sMAB)Â  , simply via a choice of m=1ğ‘š1m=1. Hence, we generalize the study of sMAB to m>1ğ‘š1m>1. We remark that tallying bandit is a special case of the rested bandit, a general framework for nonstationary MAB where the reward of an arm evolves when it is pulled, and we defer detailed discussion of this to SectionÂ 5. Recall that in sMAB, the optimal policy plays the same optimal arm at each timestep. Hence, in sMAB obtaining zero constant action traditional regret is equivalent to obtaining zero constant action policy regret and also zero CPR. Given that the tallying bandit is highly structured and generalizes sMAB via m=1ğ‘š1m=1, it is natural to question whether minimizing constant action policy (or traditional) regret implies minimizing CPR. The following counterexample answers this question in the negative when m>1ğ‘š1m>1. More generally, this counterexample shows that even when the adversary is restricted (as in tallying bandits), minimizing the constant action policy (or traditional) regret can lead to solutions whose cumulative loss is Î©~â€‹(T)~Î©ğ‘‡\\widetilde{\\Omega}(T) larger than the minimum achievable total loss. Let ğ’³={x1,x2}ğ’³subscriptğ‘¥1subscriptğ‘¥2\\mathcal{X}=\\{x_{1},x_{2}\\} and m=2ğ‘š2m=2. Define hx1â€‹(1)=hx2â€‹(1)=0subscriptâ„subscriptğ‘¥11subscriptâ„subscriptğ‘¥210h_{x_{1}}(1)=h_{x_{2}}(1)=0 and hx1â€‹(2)=hx2â€‹(2)=1subscriptâ„subscriptğ‘¥12subscriptâ„subscriptğ‘¥221h_{x_{1}}(2)=h_{x_{2}}(2)=1. A policy that fixes either of the two actions, and then plays this action at every timestep, incurs cumulative loss Tâˆ’1ğ‘‡1T-1 but has zero constant action policy regret and zero constant action traditional regret. Meanwhile, the complete policy regret of this policy is Tâˆ’1ğ‘‡1T-1, since the optimal policy that alternates playing actions x1subscriptğ‘¥1x_{1} and x2subscriptğ‘¥2x_{2} incurs zero cumulative loss. Thus far, our motivation for the tallying bandit setting has been primarily theoretical, to resolve the gap in our understanding of when we can efficiently minimize CPR. Nevertheless, in similar vein to Heidari et al.Â , Lindner et al.Â 3 and Awasthi et al.Â , we believe that the tallying bandit is a simple approximation for various practical settings. For instance, in recommender systems the reward associated with an action is rarely static, because the stimulus of recommended content influences user preferencesÂ  . Moreover, literature on psychology and cognition suggests that humans often forget prior stimuli and do not always encode them in permanent memoryÂ 9 . Thus, one way to model a userâ€™s preferences is via an (unknown) function of the number of times a content genre has been recommended in a recent time interval, motivating both the hâ„h-tallying structure as well as bounded mğ‘šm. Nevertheless, the tallying bandit is just one plausible model, and we suggest possible extensions in SectionÂ 6. Let us now discuss potential avenues for efficiently minimizing CPR in tallying bandit problems. One approach is to observe that any tallying bandit problem can be cast as a reinforcement learning (RL) problem, where each state corresponds to a sequence of actions taken in the last mğ‘šm timesteps. However, methods for solving such RL problems typically scale with the cardinality of the state spaceÂ [ref]8 [ref]9 6, and such approaches would suffer Î©~â€‹(Km)~Î©superscriptğ¾ğ‘š\\widetilde{\\Omega}(K^{m}) CPR. Hence it is necessary to leverage the additional properties of tallying bandit problems. To gain intuition, let us consider the simplified setting of deterministic bandit feedback, where for each t,a1:tâˆˆğ’³tğ‘¡subscriptğ‘:1ğ‘¡superscriptğ’³ğ‘¡t,a_{1:t}\\in\\mathcal{X}^{t} the player observes ftâ€‹(a1:t)subscriptğ‘“ğ‘¡subscriptğ‘:1ğ‘¡f_{t}(a_{1:t}) with no noise. Since the loss functions ftsubscriptğ‘“ğ‘¡f_{t} are fully defined by the functions {hx}xâˆˆğ’³subscriptsubscriptâ„ğ‘¥ğ‘¥ğ’³\\{h_{x}\\}_{x\\in\\mathcal{X}}, it is natural to consider the following algorithm, which we denote ğ”¸â€‹ğ•ƒâ€‹ğ”¾detğ”¸ğ•ƒsubscriptğ”¾det\\mathbb{ALG}_{\\text{det}}. First, the algorithm queries hxâ€‹(y)subscriptâ„ğ‘¥ğ‘¦h_{x}(y) at each (x,y)âˆˆğ’³Ã—{1,2â€‹â€¦â€‹m}ğ‘¥ğ‘¦ğ’³12â€¦ğ‘š(x,y)\\in\\mathcal{X}\\times\\{1,2\\dots m\\}. This yields full information about the loss functions ftsubscriptğ‘“ğ‘¡f_{t}. Then, the algorithm plans offline an optimal sequence of actions for the remaining timesteps. ğ”¸â€‹ğ•ƒâ€‹ğ”¾detğ”¸ğ•ƒsubscriptğ”¾det\\mathbb{ALG}_{\\text{det}} is formally specified as AlgorithmÂ 2 in AppendixÂ C, and the following result shows that its CPR is minimax optimal (upto constant factors). Consider any (m,g,h)ğ‘šğ‘”â„(m,g,h)-tallying bandit problem with deterministic bandit feedback. Then the complete policy regret of AlgorithmÂ 2 (ğ”¸â€‹ğ•ƒâ€‹ğ”¾detğ”¸ğ•ƒsubscriptğ”¾det\\mathbb{ALG}_{\\text{det}}) is almost surely upper bounded as â„›Tcpâ‰¤(m+1)â€‹Ksubscriptsuperscriptâ„›cpğ‘‡ğ‘š1ğ¾\\mathcal{R}^{\\text{cp}}_{T}\\leq(m+1)K. Moreover, there exists an (m,g,h)ğ‘šğ‘”â„(m,g,h)-tallying bandit problem, such that the (expected) complete policy regret of any (possibly randomized) algorithm on this problem with deterministic feedback is lower bounded as ğ”¼â€‹[â„›Tcp]â‰¥mâ€‹K/128ğ”¼delimited-[]subscriptsuperscriptâ„›cpğ‘‡ğ‘šğ¾128\\mathbb{E}\\left[\\mathcal{R}^{\\text{cp}}_{T}\\right]\\geq mK/128. The proof of PropositionÂ 1 is included in AppendixÂ C. Due to the optimality of ğ”¸â€‹ğ•ƒâ€‹ğ”¾detğ”¸ğ•ƒsubscriptğ”¾det\\mathbb{ALG}_{\\text{det}} given deterministic feedback, it is reasonable to extend it to handle stochastic feedback. One natural way to do so is via an â€œexplore then exploitâ€ modification, which has been studied even for sMABÂ . Consider the following â€œexplore then exploitâ€ algorithm, which we denote ğ”¸â€‹ğ•ƒâ€‹ğ”¾stochğ”¸ğ•ƒsubscriptğ”¾stoch\\mathbb{ALG}_{\\text{stoch}}. It queries repeatedly to receive stochastic realizations of hxâ€‹(y)subscriptâ„ğ‘¥ğ‘¦h_{x}(y) at each (x,y)âˆˆğ’³Ã—{1,2â€‹â€¦â€‹m}ğ‘¥ğ‘¦ğ’³12â€¦ğ‘š(x,y)\\in\\mathcal{X}\\times\\{1,2\\dots m\\}, and constructs tight confidence intervals for these hxâ€‹(y)subscriptâ„ğ‘¥ğ‘¦h_{x}(y). It then uses these estimated values of hxâ€‹(y)subscriptâ„ğ‘¥ğ‘¦h_{x}(y) to plan offline an optimal sequence of actions for the remaining timesteps. ğ”¸â€‹ğ•ƒâ€‹ğ”¾stochğ”¸ğ•ƒsubscriptğ”¾stoch\\mathbb{ALG}_{\\text{stoch}} is provably efficient, in the sense that its CPR is sublinear in Tğ‘‡T and polynomial in m,Kğ‘šğ¾m,K. However, a standard argumentÂ  shows that the dependency on Tğ‘‡T for ğ”¸â€‹ğ•ƒâ€‹ğ”¾stochğ”¸ğ•ƒsubscriptğ”¾stoch\\mathbb{ALG}_{\\text{stoch}} scales as Î˜~â€‹(T2/3)~Î˜superscriptğ‘‡23\\widetilde{\\Theta}\\left(T^{2/3}\\right), and it is unclear whether this dependency is optimal for tallying bandits. In the forthcoming section, we show that this dependency on Tğ‘‡T can be significantly improved.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: bandit if the adversary is mğ‘šm-memory bounded, gğ‘”g-restricted and hâ„h-tallying, and if after playing action atsubscriptğ‘ğ‘¡a_{t} the player observes a random variable h~atâ€‹(yt)âˆˆ [ref]1subscript~â„subscriptğ‘ğ‘¡subscriptğ‘¦ğ‘¡01\\widetilde{h}_{a_{t}}(y_{t})\\in [ref]1 satisfying where\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"Congested Bandits: Optimal Routing via Short-term Resets\",\n                \"Best arm identification in multi-armed bandits\",\n                \"Finite-time analysis of the multiarmed bandit problem\",\n                \"The nonstochastic multiarmed bandit problem\",\n                \"Policy Regret in Repeated Games\",\n                \"Online bandit learning against an adaptive adversary: From regret to policy regret\",\n                \"Competing in the dark: An efficient algorithm for bandit linear optimization\",\n                \"Minimax pac bounds on the sample complexity of reinforcement learning with a generative model\",\n                \"Minimax Regret Bounds for Reinforcement Learning\",\n                \"Multi-armed Bandit Problem with Known Trend\",\n                \"Stochastic multi-armed-bandit problem with non-stationary rewards\",\n                \"Blocking Bandits\",\n                \"Online learning with switching costs and other adaptive adversaries\",\n                \"Stochastic bandits with delay-dependent payoffs\",\n                \"Discrepancy-based algorithms for non-stationary rested bandits\",\n                \"Is seeing believing? how recommender system interfaces affect usersâ€™ opinions\",\n                \"A neurocognitive model of advertisement content and brand name recall\",\n                \"Bandits with switching costs: T2/3superscriptğ‘‡23T^{2/3} regret\",\n                \"Is a good representation sufficient for sample efficient reinforcement learning?\",\n                \"PAC bounds for multi-armed bandit and markov decision processes\",\n                \"Online convex optimization in the bandit setting: Gradient descent without a gradient\",\n                \"On upper-confidence bound policies for switching bandit problems\",\n                \"Online submodular minimization\",\n                \"Tight policy regret bounds for improving and decaying bandits\",\n                \"Probability inequalities for sums of bounded random variables\",\n                \"Is Q-learning provably efficient?\",\n                \"Recharging bandits\",\n                \"Exploration in metric state spaces\",\n                \"Human memory: structures and processes\",\n                \"A sparse sampling algorithm for near-optimal planning in large markov decision processes\",\n                \"Break your bandit routine with LSD rewards: a last switch dependent analysis of satiation and seasonality\",\n                \"Rotting Bandits\",\n                \"Addressing the Long-term Impact of ML Decisions via Policy Regret\",\n                \"Asymptotically efficient adaptive allocation rules\",\n                \"On sequential strategies for loss functions with memory\",\n                \"Sample efficient reinforcement learning in continuous state spaces: A perspective beyond linearity\",\n                \"Competing with automata-based expert sequences\",\n                \"Recovering Bandits\",\n                \"Reinforcement Learning: An Introduction\",\n                \"Deconvolving Feedback Loops in Recommender Systems\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of 1 Introduction. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}