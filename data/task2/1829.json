{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning\n\nPaper Abstract: \n\nPaper Section Text:\nWhen machine learning models are designed to optimize only one performance metric, such as profit or accuracy, they can have inadvertent but detrimental consequences . When these models produce discriminatory results based on sensitive traits such as gender, we consider them to be ’biased’ or ’unfair.’\nExamples of gender-based unfairness in real applications are abundant. For instance, the Gender Shades project, a black-box algorithmic audit of three commercial facial analysis models using their Application Programming Interfaces (APIs), found that the classifiers performed better on male faces than female faces and lighter faces than darker faces. All of the classifiers in the study performed worst on Black, female faces. The Microsoft and IBM facial classifiers performed best on white, male faces . Several months after the study, the three audited companies updated their facial recognition APIs, decreasing accuracy disparities between lighter and darker skinned and male and female individuals [ref]12. The Gender Shades project thus illustrates the importance of algorithmic fairness research for changing the industry. Machine translation has also been shown to be gender biased. For example, Prates et al.  created simple sentences in 12 gender neutral languages of the form ”He/she is a teacher” (where ”teacher” is an occupation selected from a list of jobs) and then translated them into English using Google Translate. They found that Google Translate was biased against women when picking which gender pronoun to use, choosing the pronoun ”he” over ”she” for occupations in STEM, legal, corporate, and several other fields. Moreover, the study found that Google Translate amplified gender disparities, yielding male pronouns instead of female ones even more than reflected in the training data. After the paper’s placement on arXiv.org and its increasing coverage in the media, Google began offering both masculine and feminine translations, once again indicating the importance of research on gender bias in machine learning. In natural language processing (NLP), gender biases have been found in examples of word embedding, a method that vectorizes natural language to portray semantic similarities by vector proximity. Even embeddings trained rigorously on large datasets like Google News have been shown to be inherently sexist. For example, when Bolukbasi et al  trained a model to solve analogy puzzles, the model returned that ”man is to computer programmer as woman is to homemaker.” Similarly stereotypical outputs of the model included relating man to ”surgeon” and woman to ”nurse,” man to ”shopkeeper” and woman to ”housewife,” and man to ”brilliant” and woman to ”lovely.” In the realm of search engines, Datta et al.  found that Google displayed fewer high-paying jobs to women compared to men. In healthcare, AI algorithms have been deployed to predict personalized preventative and therapeutic care based on genetic and environmental factors. Many of these models discriminate against women by misdiagnosing them or ignoring relevant links between sex and health differences . This bias in model prediction can lead to sub-optimal (and sometimes even fatal) outcomes. Thus, across applications training models on biased data can reinforce gender discrimination, ultimately leading to detrimental societal results.\n\nReference markers found: [12]\n\nContext windows for each [ref]:\n[ref]12: Several months after the study, the three audited companies updated their facial recognition APIs, decreasing accuracy disparities between lighter and darker skinned and male and female individuals [ref]12. The Gender Shades project thus illustrates the importance of algorithmic fairness research for changing the industry. Machine translation has also been shown to be gender biased. For\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 12,\n            \"titles\": [\n                \"Assessing Gender Bias in Machine Translation – A Case Study with Google Translate\",\n                \"Optimal taxation and insurance using machine learning — sufficient statistics and beyond\",\n                \"Crowdsourcing city government: Using tournaments to improve inspection accuracy\",\n                \"Data-Driven Decisions for Reducing Readmissions for Heart Failure: General Methodology and Case Study\",\n                \"Criminal Justice Forecasts of Risk: A Machine Learning Approach\",\n                \"Systematic bias and nontransparency in us social security administration forecasts\",\n                \"The challenges and limits of big data algorithms in technocratic governance\",\n                \"Learning Optimal Fair Policies\",\n                \"Certifying and removing disparate impact\",\n                \"Mitigating Unwanted Biases with Adversarial Learning\",\n                \"Balancing Competing Objectives with Noisy Data Score-Based Classifiers for Welfare-Aware Machine Learning\",\n                \"Gender shades: Intersectional accuracy disparities in commercial gender classification\",\n                \"Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products\",\n                \"Man is to computer programmer as woman is to homemaker? debiasing word embeddings\",\n                \"Automated experiments on ad privacy settings: A tale of opacity, choice, and discrimination\",\n                \"Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare\",\n                \"Fair Housing Act\",\n                \"Equal Credit Opportunity Act\",\n                \"Fairness definitions explained\",\n                \"On Formalizing Fairness in Prediction with Machine Learning\",\n                \"A Survey on Bias and Fairness in Machine Learning\",\n                \"On the Apparent Conflict Between Individual and Group Fairness\",\n                \"Counterfactual Fairness\",\n                \"Equality of Opportunity in Supervised Learning\",\n                \"AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias\",\n                \"How machine learning advances will improve the fairness of algorithms\",\n                \"UCI machine learning repository\",\n                \"A survey of bias in machine learning through the prism of statistical parity for the adult data set\",\n                \"Fairness-Aware Classifier with Prejudice Remover Regularizer\",\n                \"Fair inference on outcomes\",\n                \"A causal framework for discovering and removing direct and indirect discrimination\",\n                \"Optimized pre-processing for discrimination prevention\",\n                \"Fairness Constraints: Mechanisms for Fair Classification\",\n                \"Fairness-aware Classification: Criterion, Convexity, and Bounds\",\n                \"On Fairness and Calibration\",\n                \"Algorithmic decision making and the cost of fairness\",\n                \"Machine bias: There’s software used across the country to predict future criminals. and it’s biased against blacks.\",\n                \"Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction\",\n                \"Nonconvex optimization for regression with fairness constraints\",\n                \"Debiasing Community Detection: The Importance of Lowly-Connected Nodes\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]12 based on the context and research area of End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}