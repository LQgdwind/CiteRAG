{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Meta-Sift : How to Sift Out a Clean Subset in the Presence of Data Poisoning?\n\nPaper Abstract: \n\nPaper Section Text:\nConstructing high-performance machine learning (ML) systems requires large and diverse data. The data-hungry nature will inevitably force individuals and organizations to leverage data from external sources, the beginning of which is already evident. For instance, CLIP , the state-of-the-art image representation, is learned from 400 million image-text pairs collected from the Internet. Various data marketplaces and crowd-sourcing platforms also emerge to enable data exchange at scale. While incorporating external data sources into training has clear benefits, it exposes ML systems to security threats on account of data poisoning attacks, in which attackers modify training data to degrade model performance or control model prediction. In fact, data poisoning has been remarked as the top security concern regarding ML systems in the industry [ref]2. In this paper, the term “data poisoning” will be used in a broad sense, referring to attacks that involve training data manipulation. In particular, it includes both the attacks that interfere only with training data and backdoor attacks that embed a backdoor trigger during the training time and further inject the trigger into test-time inputs to control their corresponding predictions.\nWithin the scope of this paper, we divide existing data poisoning attacks into three categories based on the attribute being manipulated: Label-only attacks that only alter labels, such as targeted [ref]3 and Random Label-Flipping attacks [ref]4 aimed at degrading model utility; Feature-only attacks that only manipulate features without changing the labels, such as feature collision attacks  and clean-label backdoor attacks  ; Label-Feature attacks that change both feature and label, such as standard backdoor attacks   0. Intensive efforts have been invested in mitigating data poisoning. The types of defenses in the prior work range from identifying poisoned samples in a training set 1 (Poison Detection) to detecting whether a model has been trained on a poisoned dataset 2 (Trojan-Net Detection) to removing backdoors from a poisoned model 3 4 (Backdoor Removal) to redesigning training algorithms to prevent poisoning from taking effect [ref]4 (Robust Training) Most existing defenses assume that the defender can access a set of clean data (referred to as the base set hereafter). Despite the prevalence of the assumption in existing literature, focused discussion about its validity is lacking. If the defender were capable of collecting a set of clean samples from trusted sources of data, then this assumption could be met easily. However, it has become increasingly common to learn solely from untrusted data sources, such as training with the data scraped from the Internet or purchased from specific vendors. In that case, the defender needs to identify a clean subset within the poisoned dataset to form the base set. Many important questions remain unclear:\n\n\nHow does the defense performance change if the identification is imperfect and some poisoned data are mixed into the base set? Are there any existing automated methods that can reliably identify a clean base set in the presence of various types of poisoning attacks? Can human inspection fulfill the need? If not, how can we reliably identify enough clean samples to support those defenses?\n Takeaway #1: Defense performance is sensitive to the purity of the base set. We start by examining the sensitivity of defense performance to the ratio of poisoned points in the base set. We study five representative defense techniques that rely on access to a base set. The techniques considered either achieve state-of-the-art performance or are popular baselines. We find that their performance degrades significantly (e.g., attack success rate exceeding 80%) with less than 1% of poisoned points in the base set. Surprisingly, even a single poisoned point is sufficient to nullify the effect of a state-of-the-art poisoned data detector. These findings suggest that the ability to sift out a base set with high precision is critical to successfully applying these defenses. Takeaway #2: Both existing automated methods and human inspection fail to identify a clean subset with high enough precision. We investigate how precise existing automated methods and human inspection can be in identifying clean data in the presence of data poisoning and the result is illustrated in Figure 1.\nThe precision of both humans and existing automated methods varies a lot across different attack categories. Humans are proficient at identifying poisoned samples that involve label changes, including Label-only attacks and Label-Feature attacks, and outperform existing automated methods by a large margin. However, humans still miss many poisons and cannot realize a 100% success rate in sifting out a clean base set.\nNotably, for these two attack categories, several automated methods even underperform the random baseline.\n red On the other hand, for Feature-only attacks, human inspection results in a precision close to the random baseline.\nAs these attacks inject small perturbations only to the features while not changing the overall semantics,\nhuman experts perform worse than most automated methods.\nThis finding is in direct contrast to the traditional wisdom that treats human supervision as the final backstop of data poisoning. Besides being time-consuming and cost-intensive, human inspection becomes less trustworthy in identifying poisoned data given the fast-growing research on stealthy attacks.\nOverall, both existing automated methods and human inspection cannot reach the level of precision required to enable successful defense. Takeaway #3: Meta-Sift — a scalable and effective automated method to sift out a clean base set. We propose Meta-Sift to sift a clean subset from the poisoned set. Our approach is based on a novel insight that\ndata manipulation techniques exploited by existing poisoning attacks inevitably result in a distributional shift from the clean data. Hence, training on the clean portion of the contaminated dataset and testing the trained model on the other corrupted portion will lead to a high prediction loss. We formulate a bilevel optimization problem to split the contaminated dataset in a way that training on one split and testing on the other leads to the highest prediction loss. However, this splitting problem is hard to solve exactly as it has a combinatorial search space and at the same time, contains two nested optimization problems. To address the computational challenge, we first relax it into a continuous splitting problem, where we associate each sample with a continuous weight indicating their likelihood of belonging to one of the splits and then optimize the weights via gradient-based methods. Secondly, we adapt the online algorithm that was originally designed for training sample reweighting 5 to efficiently solve the continuous relaxation of the bilevel problem. Furthermore, we adopt the idea of “ensembling” to improve the precision of selection. In particular, we propose to apply random perturbations to each point, run the online algorithm on each perturbed version to obtain a weight, and aggregate the weights for final clean data selection.\nOur evaluation shows that Meta-Sift can robustly sift out a clean base set with 100% precision under a wide range of poisoning attacks. The selected base set is large enough to give rise to successful defense when plugged into the existing defense techniques. It is worth noting that Meta-Sift significantly outperforms the existing automated methods (illustrated in Figure 1) while being orders of magnitude faster (Table 5, 6, 15, 16). Our contributions can be summarized as follows: We identify an overlooked problem of the accessibility of a clean base set in the presence of data poisoning. We systematically evaluate the performance of existing automated methods and human inspection in distinguishing between poisoned and clean samples; We propose a novel splitting-based idea to sift out a clean subset from a poisoned dataset and formalize it into a bilevel optimization problem. We propose Meta-Sift, comprising an efficient algorithm to solve the bilevel problem as well as a series of techniques to enhance sifting precision. We extensively evaluate Meta-Sift  and compare with existing automated methods on four benchmark datasets under twelve different data poisoning attack settings. Our method significantly outperforms existing methods in both sifting precision and efficiency. At the same time, plugging our sifted samples into existing defenses achieves comparable or even better performance than plugging in randomly selected clean samples. We open-source the project to promote research on this topic and facilitate the successful application of existing defenses in settings without a clean base set\n111https://github.com/ruoxi-jia-group/Meta-Sift.\n\nReference markers found: [2]\n\nContext windows for each [ref]:\n[ref]2: attackers modify training data to degrade model performance or control model prediction. In fact, data poisoning has been remarked as the top security concern regarding ML systems in the industry [ref]2. In this paper, the term “data poisoning” will be used in a broad sense, referring to attacks that involve training data manipulation. In particular, it includes both the attacks that\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 2,\n            \"titles\": [\n                \"Data Poisoning Attacks Against Federated Learning Systems\",\n                \"Learning Transferable Visual Models From Natural Language Supervision\",\n                \"Adversarial machine learning-industry perspectives\",\n                \"Learning to Reweight Examples for Robust Deep Learning\",\n                \"Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks\",\n                \"Label-Consistent Backdoor Attacks\",\n                \"Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information\",\n                \"Badnets: Evaluating backdooring attacks on deep neural networks\",\n                \"Targeted backdoor attacks on deep learning systems using data poisoning\",\n                \"Wanet-imperceptible warping-based backdoor attack\",\n                \"Rethinking the Backdoor Attacks’ Triggers: A Frequency Perspective\",\n                \"Detecting AI Trojans Using Meta Neural Analysis\",\n                \"Neural cleanse: Identifying and mitigating backdoor attacks in neural networks\",\n                \"Adversarial unlearning of backdoors via implicit hypergradient\",\n                \"Faster Meta Update Strategy for Noise-Robust Deep Learning\",\n                \"TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems\",\n                \"Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks\",\n                \"Revealing perceptible backdoors in dnns, without the training set, via the maximum achievable misclassification fraction statistic\",\n                \"Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting\",\n                \"Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios\",\n                \"Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases\",\n                \"Strip: A defence against trojan attacks on deep neural networks\",\n                \"Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks.\",\n                \"Model inversion attacks that exploit confidence information and basic countermeasures\",\n                \"Spectral Signatures in Backdoor Attacks\",\n                \"Anti-Backdoor Learning: Training Clean Models on Poisoned Data\",\n                \"Understanding Black-box Predictions via Influence Functions\",\n                \"Right for better reasons: Training differentiable models by constraining their influence functions\",\n                \"Resolving training biases via influence-based data relabeling\",\n                \"Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses\",\n                \"Poisoning and backdooring contrastive learning\",\n                \"ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms\",\n                \"Robust learning: Information theory and algorithms\",\n                \"Deep Learning using Rectified Linear Units (ReLU)\",\n                \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification\",\n                \"Categorical reparameterization with gumbel-softmax\",\n                \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\n                \"Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond\",\n                \"On the Iteration Complexity of Hypergradient Computation\",\n                \"Rethinking the Trigger of Backdoor Attack\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]2 based on the context and research area of Meta-Sift : How to Sift Out a Clean Subset in the Presence of Data Poisoning?. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}