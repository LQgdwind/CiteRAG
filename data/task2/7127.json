{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Mitigating Procrastination in Spatial Crowdsourcing Via Efficient Scheduling Algorithm\n\nPaper Abstract: \n\nPaper Section Text:\nThe following analysis are motivated by  and . The probability of allocation of all higher cost jobs (suppose JùêΩJ) among nùëõn jobs (where |J|<nùêΩùëõ|J|<n) in a single schedule is 1lJ1superscriptùëôùêΩ\\frac{1}{l^{J}}, where lùëôl is the number of schedules. Let us fix any arbitrary schedule jùëój among lùëôl schedules. Now, we define the event Ai‚Äãjsubscriptùê¥ùëñùëóA_{ij} (where i={1,‚ãØ,J}ùëñ1‚ãØùêΩi=\\{1,\\cdots,J\\}) as any i‚ààJùëñùêΩi\\in J, is scheduled in an arbitrary schedule jùëój. Given the event Ai‚Äãjsubscriptùê¥ùëñùëóA_{ij}, we are interested in finding P‚Äãr‚Äã{A1‚Äãj‚à©A2‚Äãj‚Äã‚ãØ‚à©AJ‚Äãj}ùëÉùëüsubscriptùê¥1ùëósubscriptùê¥2ùëó‚ãØsubscriptùê¥ùêΩùëóPr\\{A_{1j}\\cap A_{2j}\\cdots\\cap A_{Jj}\\}. If this probability is calculated then we will find that for all i‚ààJùëñùêΩi\\in J, event Ai‚Äãjsubscriptùê¥ùëñùëóA_{ij} will occur. So, we can write Now, according to our method, any higher cost jobs (JùêΩJ) may be allocated into same schedule. But allocation of one higher cost job into one schedule has no impact on the other job which is yet to be allocated into the same schedule. Consider the probability P‚Äãr‚Äã{A2‚Äãj|A1‚Äãj}ùëÉùëüconditional-setsubscriptùê¥2ùëósubscriptùê¥1ùëóPr\\{A_{2j}|A_{1j}\\}, where event A1‚Äãjsubscriptùê¥1ùëóA_{1j} which has already occurred, i.e.,formulae-sequenceùëñùëíi.e., the (1‚Äãj)t‚Äãhsuperscript1ùëóùë°‚Ñé(1j)^{th} job is already been allocated into a schedule, has no impact on the event A2‚Äãjsubscriptùê¥2ùëóA_{2j}, i.e.,formulae-sequenceùëñùëíi.e., event A2‚Äãjsubscriptùê¥2ùëóA_{2j} is independent of event A1‚Äãjsubscriptùê¥1ùëóA_{1j}. So we can write,\nP‚Äãr‚Äã{A2‚Äãj|A1‚Äãj}=P‚Äãr‚Äã{A2‚Äãj}ùëÉùëüconditional-setsubscriptùê¥2ùëósubscriptùê¥1ùëóùëÉùëüsubscriptùê¥2ùëóPr\\{A_{2j}|A_{1j}\\}=Pr\\{A_{2j}\\}. Similarly for probability P‚Äãr‚Äã{A3‚Äãj|A1‚Äãj‚à©A2‚Äãj}ùëÉùëüconditional-setsubscriptùê¥3ùëósubscriptùê¥1ùëósubscriptùê¥2ùëóPr\\{A_{3j}|A_{1j}\\cap A_{2j}\\} where two events A1‚Äãj‚Äãa‚Äãn‚Äãd‚ÄãA2‚Äãjsubscriptùê¥1ùëóùëéùëõùëësubscriptùê¥2ùëóA_{1j}\\ and\\ A_{2j} which has already occurred have no impact on the event A3‚Äãjsubscriptùê¥3ùëóA_{3j}. So, it can be written as, P‚Äãr‚Äã{A3‚Äãj|A1‚Äãj‚à©A2‚Äãj}=P‚Äãr‚Äã{A3‚Äãj}ùëÉùëüconditional-setsubscriptùê¥3ùëósubscriptùê¥1ùëósubscriptùê¥2ùëóùëÉùëüsubscriptùê¥3ùëóPr\\{A_{3j}|A_{1j}\\cap A_{2j}\\}=Pr\\{A_{3j}\\}. In similar manner we can also write P‚Äãr‚Äã{AJ‚Äãj|A1‚Äãj‚à©A2‚Äãj‚à©‚ãØ‚à©A(J‚àí1)‚Äãj}=P‚Äãr‚Äã{AJ‚Äãj}ùëÉùëüconditional-setsubscriptùê¥ùêΩùëósubscriptùê¥1ùëósubscriptùê¥2ùëó‚ãØsubscriptùê¥ùêΩ1ùëóùëÉùëüsubscriptùê¥ùêΩùëóPr\\{A_{Jj}|A_{1j}\\cap A_{2j}\\cap\\cdots\\cap A_{(J-1)j}\\}=Pr\\{A_{Jj}\\}.\nAccordingly, we can rewrite Eq.¬†2 for the event Ai‚Äãjsubscriptùê¥ùëñùëóA_{ij} that job iùëñi is allocated into same schedule jùëój as follows: P‚Äãr‚Äã{A1‚Äãj‚à©A2‚Äãj‚à©‚ãØ‚à©AJ‚Äãj}=P‚Äãr‚Äã{A1‚Äãj}‚ãÖP‚Äãr‚Äã{A2‚Äãj}‚Äã‚ãØ‚ÄãP‚Äãr‚Äã{AJ‚Äãj}ùëÉùëüsubscriptùê¥1ùëósubscriptùê¥2ùëó‚ãØsubscriptùê¥ùêΩùëó‚ãÖùëÉùëüsubscriptùê¥1ùëóùëÉùëüsubscriptùê¥2ùëó‚ãØùëÉùëüsubscriptùê¥ùêΩùëóPr\\{A_{1j}\\cap A_{2j}\\cap\\cdots\\cap A_{Jj}\\}=Pr\\{A_{1j}\\}\\cdot Pr\\{A_{2j}\\}\\cdots Pr\\{A_{Jj}\\}. The probability of it‚Äãhsuperscriptùëñùë°‚Ñéi^{th} higher cost job to be allocated into jt‚Äãhsuperscriptùëóùë°‚Ñéj^{th} schedule is: P‚Äãr‚Äã{Ai‚Äãj}=1lùëÉùëüsubscriptùê¥ùëñùëó1ùëôPr\\{A_{ij}\\}=\\frac{1}{l} So, for two higher cost jobs P‚Äãr‚Äã{Ai‚Äãj‚à©Ak‚Äãj}=1l‚ãÖ1l,w‚Äãh‚Äãe‚Äãr‚Äãe,i<kformulae-sequenceùëÉùëüsubscriptùê¥ùëñùëósubscriptùê¥ùëòùëó‚ãÖ1ùëô1ùëôùë§‚ÑéùëíùëüùëíùëñùëòPr\\{A_{ij}\\cap A_{kj}\\}=\\frac{1}{l}\\cdot\\frac{1}{l},\\ where,i<k Similarly, for JùêΩJ higher cost jobs we have, P‚Äãr‚Äã{A1‚Äãj‚à©A2‚Äãj‚à©‚ãØ‚à©AJ‚Äãj}=1l‚ãÖ1l‚Äã‚ãØ‚Äã1l=(1l)J=1lJùëÉùëüsubscriptùê¥1ùëósubscriptùê¥2ùëó‚ãØsubscriptùê¥ùêΩùëó‚ãÖ1ùëô1ùëô‚ãØ1ùëôsuperscript1ùëôùêΩ1superscriptùëôùêΩPr\\{A_{1j}\\cap A_{2j}\\cap\\cdots\\cap A_{Jj}\\}=\\frac{1}{l}\\cdot\\frac{1}{l}\\cdots\\frac{1}{l}=\\left(\\frac{1}{l}\\right)^{J}=\\frac{1}{l^{J}}. When |Œì|Œì|\\Gamma| is large, we can say that the probability of allocating all higher cost jobs in a single schedule is much less. For example, consider 50 jobs out of 200 jobs are of higher cost, and there are 6 schedules. So the probability of allocating higher cost jobs into any one schedule out of 6 schedules is 1650=1.2371931‚Äãe‚àí391superscript6501.2371931ùëí39\\frac{1}{6^{50}}=1.2371931e-39 which is very much less. It means that over repeated random allocation of jobs, the chances of accumulating higher cost jobs into any one schedule is almost 0. The expected number of jobs to be assigned in a given schedule is |Œì||Œî|ŒìŒî\\frac{|\\Gamma|}{|\\Delta|} considering an arbitrary iteration. We have here, |Œì|Œì|\\Gamma| number of trials of assigning jobs into the given schedule (as |Œì|Œì|\\Gamma| is the total number of jobs available in our setting). Among those trials, there are successes (the sampled job is landing in the given schedule) such that each success occurs with a probability of ‚Ñô‚Ñô\\mathbb{P} and some failure with a probability of ‚Ñö=‚Ñô‚àí1‚Ñö‚Ñô1\\mathbb{Q}=\\mathbb{P}-1. Every trial is independent and has the probability 1|Œî|1Œî\\frac{1}{|\\Delta|}, as |Œî|Œî|\\Delta| is the number of schedules available. Now, we take ‚Ñù‚Ñù\\mathbb{R} as a random variable to denote the successful assignment of jobs into the given schedule among |Œì|Œì|\\Gamma| trials range within [1,‚ãØ,|Œì|]1‚ãØŒì[1,\\cdots,|\\Gamma|]. So the value of ‚Ñù‚Ñù\\mathbb{R} can be represented as ‚Ñù=i‚Ñùùëñ\\mathbb{R}=i, where i‚àà[0,‚ãØ,|Œì|]ùëñ0‚ãØŒìi\\in[0,\\cdots,|\\Gamma|]. So, the probability of ‚Ñù‚Ñù\\mathbb{R} for i number of successes can be written as, P‚Äãr‚Äã{‚Ñù=i}=(|Œì|i)‚Äã‚Ñôi‚Äã‚Ñö|Œì|‚àíiùëÉùëü‚ÑùùëñbinomialŒìùëñsuperscript‚Ñôùëñsuperscript‚ÑöŒìùëñPr\\{\\mathbb{R}=i\\}=\\binom{|\\Gamma|}{i}\\mathbb{P}^{i}\\mathbb{Q}^{|\\Gamma|-i}. This is true as there are (|Œì|i)binomialŒìùëñ\\binom{|\\Gamma|}{i} ways to choose iùëñi number of successes among |Œì|Œì|\\Gamma| trials and any one such successes occurs with the probability ‚Ñôi‚Äã‚Ñö(|Œì|‚àíi)superscript‚Ñôùëñsuperscript‚ÑöŒìùëñ\\mathbb{P}^{i}\\mathbb{Q}^{(|\\Gamma|-i)} (as in that elementary event, iùëñi successes will be there along with (|Œì|‚àíi)Œìùëñ(|\\Gamma|-i) number of failures). Also, we can write; Now, the expected number of successes, i.e.,formulae-sequenceùëñùëíi.e., expected number of jobs in every schedule can be calculated as:\nobservation\nConsider the synthetic dataset where |Œì|=200Œì200|\\Gamma|=200, we have |Œì||Œî|=2004=50ŒìŒî200450\\frac{|\\Gamma|}{|\\Delta|}=\\frac{200}{4}=50, and this value is the number of jobs allocated in each schedule. In Section¬†7.1.2, we have furnished the comparison between OffPSP and PPSJBP against the parameter number of jobs (Fig.¬†7a) where we can see that schedules are assigned with number of jobs as:\n|Œ¥1|:51,|Œ¥2|:48,|Œ¥3|:53,|Œ¥4|:48:subscriptùõø151subscriptùõø2:48subscriptùõø3:53subscriptùõø4:48|\\delta_{1}|:51,|\\delta_{2}|:48,|\\delta_{3}|:53,|\\delta_{4}|:48 which is almost balanced allocation of jobs in each of Œ¥i‚ààŒîsubscriptùõøùëñŒî\\delta_{i}\\in\\Delta.\nThe similar observations we have for our real life dataset (KDD Cup 2015 and Bus Driver Scheduling) in the simulation (given in Fig.¬†7b and Fig.¬†7c). E‚Äã[‚Ñù]=‚àëi=0|Œì|i‚ãÖP‚Äãr‚Äã{‚Ñù=i}=‚àëi=1|Œì|i‚Äã(|Œì|i)‚Äã(1|Œî|)i‚Äã(1‚àí1|Œî|)|Œì|‚àíi=‚àëi=1|Œì|i‚Äã(|Œì|!i!‚Äã(|Œì|‚àíi)!)‚Äã(1|Œî|)i‚Äã(1‚àí1|Œî|)|Œì|‚àíi=‚àëi=1|Œì|i‚Äã(|Œì|‚Äã(|Œì|‚àí1)!i‚Äã(i‚àí1)!‚Äã(|Œì|‚àíi)!)‚Äã(1|Œî|)i‚Äã(1‚àí1|Œî|)|Œì|‚àíi=‚àëi=1|Œì|(|Œì|‚Äã(|Œì|‚àí1)!(i‚àí1)!‚Äã(|Œì|‚àíi)!)‚Äã(1|Œî|)i‚Äã(1‚àí1|Œî|)|Œì|‚àíi=|Œì||Œî|‚Äã‚àëi=1|Œì|(|Œì|‚àí1i‚àí1)‚Äã(1|Œî|)i‚àí1‚Äã(1‚àí1|Œî|)|Œì|‚àíi[a‚Äãs‚Äã(|Œì|‚àí1‚àíi+1)!=(|Œì|‚àíi)!]=|Œì||Œî|‚Äã‚àëi=0|Œì|‚àí1(|Œì|‚àí1i)‚Äã(1|Œî|)i‚Äã(1‚àí1|Œî|)(|Œì|‚àí1)‚àíi=|Œì||Œî|[b‚Äãy‚ÄãEq.¬†3]formulae-sequenceùê∏delimited-[]‚Ñùsuperscriptsubscriptùëñ0Œì‚ãÖùëñùëÉùëü‚Ñùùëñsuperscriptsubscriptùëñ1ŒìùëñbinomialŒìùëñsuperscript1Œîùëñsuperscript11ŒîŒìùëñsuperscriptsubscriptùëñ1ŒìùëñŒìùëñŒìùëñsuperscript1Œîùëñsuperscript11ŒîŒìùëñsuperscriptsubscriptùëñ1ŒìùëñŒìŒì1ùëñùëñ1Œìùëñsuperscript1Œîùëñsuperscript11ŒîŒìùëñsuperscriptsubscriptùëñ1ŒìŒìŒì1ùëñ1Œìùëñsuperscript1Œîùëñsuperscript11ŒîŒìùëñŒìŒîsuperscriptsubscriptùëñ1ŒìbinomialŒì1ùëñ1superscript1Œîùëñ1superscript11ŒîŒìùëñdelimited-[]ùëéùë†Œì1ùëñ1ŒìùëñŒìŒîsuperscriptsubscriptùëñ0Œì1binomialŒì1ùëñsuperscript1Œîùëñsuperscript11ŒîŒì1ùëñŒìŒîdelimited-[]ùëèùë¶Eq.¬†3\\begin{split}E[\\mathbb{R}]&=\\sum\\limits_{i=0}^{|\\Gamma|}i\\cdot Pr\\{\\mathbb{R}=i\\}\\\\\n&=\\sum\\limits_{i=1}^{|\\Gamma|}i\\binom{|\\Gamma|}{i}\\left(\\frac{1}{|\\Delta|}\\right)^{i}\\left(1-\\frac{1}{|\\Delta|}\\right)^{|\\Gamma|-i}\\\\\n&=\\sum\\limits_{i=1}^{|\\Gamma|}i\\left(\\frac{|\\Gamma|!}{i!(|\\Gamma|-i)!}\\right)\\left(\\frac{1}{|\\Delta|}\\right)^{i}\\left(1-\\frac{1}{|\\Delta|}\\right)^{|\\Gamma|-i}\\\\\n&=\\sum\\limits_{i=1}^{|\\Gamma|}i\\left(\\frac{|\\Gamma|(|\\Gamma|-1)!}{i(i-1)!(|\\Gamma|-i)!}\\right)\\left(\\frac{1}{|\\Delta|}\\right)^{i}\\left(1-\\frac{1}{|\\Delta|}\\right)^{|\\Gamma|-i}\\\\\n&=\\sum\\limits_{i=1}^{|\\Gamma|}\\left(\\frac{|\\Gamma|(|\\Gamma|-1)!}{(i-1)!(|\\Gamma|-i)!}\\right)\\left(\\frac{1}{|\\Delta|}\\right)^{i}\\left(1-\\frac{1}{|\\Delta|}\\right)^{|\\Gamma|-i}\\\\\n&=\\frac{|\\Gamma|}{|\\Delta|}\\sum\\limits_{i=1}^{|\\Gamma|}\\binom{|\\Gamma|-1}{i-1}\\left(\\frac{1}{|\\Delta|}\\right)^{i-1}\\left(1-\\frac{1}{|\\Delta|}\\right)^{|\\Gamma|-i}\\\\\n&\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ [as\\ (|\\Gamma|-1-i+1)!=(|\\Gamma|-i)!]\\\\\n&=\\frac{|\\Gamma|}{|\\Delta|}\\sum\\limits_{i=0}^{|\\Gamma|-1}\\binom{|\\Gamma|-1}{i}\\left(\\frac{1}{|\\Delta|}\\right)^{i}\\left(1-\\frac{1}{|\\Delta|}\\right)^{(|\\Gamma|-1)-i}\\\\\n&=\\frac{|\\Gamma|}{|\\Delta|}\\ \\ \\ \\ \\ \\ \\ \\ [by\\ \\lx@cref{creftype~refnum}{eqn8}]\\\\\n\\end{split} From this proof, it is evident that our proposed PPSJBP distributes the jobs in each schedule in a balanced way. observation\nConsider the synthetic dataset where |Œì|=200Œì200|\\Gamma|=200, we have |Œì||Œî|=2004=50ŒìŒî200450\\frac{|\\Gamma|}{|\\Delta|}=\\frac{200}{4}=50, and this value is the number of jobs allocated in each schedule. In Section¬†7.1.2, we have furnished the comparison between OffPSP and PPSJBP against the parameter number of jobs (Fig.¬†7a) where we can see that schedules are assigned with number of jobs as:\n|Œ¥1|:51,|Œ¥2|:48,|Œ¥3|:53,|Œ¥4|:48:subscriptùõø151subscriptùõø2:48subscriptùõø3:53subscriptùõø4:48|\\delta_{1}|:51,|\\delta_{2}|:48,|\\delta_{3}|:53,|\\delta_{4}|:48 which is almost balanced allocation of jobs in each of Œ¥i‚ààŒîsubscriptùõøùëñŒî\\delta_{i}\\in\\Delta.\nThe similar observations we have for our real life dataset (KDD Cup 2015 and Bus Driver Scheduling) in the simulation (given in Fig.¬†7b and Fig.¬†7c). The expected number of jobs to be sampled so that every schedule is assigned with |‚Ñ§|‚â•1‚Ñ§1|\\mathbb{Z}|\\geq 1 jobs is approximately |Œî|‚Äãln‚Å°|Œî|ŒîŒî|\\Delta|\\ln|\\Delta|, where |‚Ñ§|‚Ñ§|\\mathbb{Z}| is the number of assigned jobs in a schedule. Let us consider the number of jobs that have to be sampled before every schedule gets a job is ùïÑ‚àósuperscriptùïÑ\\mathbb{M}^{*} and the allocation of a job into an empty schedule a ‚Äùsuccess.‚Äù In this proof, we will apply the concept of geometric distribution as we know that the number of jobs to be sampled before every schedule gets atleast one job follows geometric distribution. We shall divide the sampling of jobs into partitions. Each partition will then boil down to the geometric distribution. The it‚Äãhsuperscriptùëñùë°‚Ñéi^{th} success requires sampling of jobs after the (i‚àí1)s‚Äãtsuperscriptùëñ1ùë†ùë°(i-1)^{st} successes. In the first partition, since all the schedules are initially empty, a schedule will get a job with probability |Œî|Œî=1ŒîŒî1\\frac{|\\Delta|}{\\Delta}=1.\nWhen one schedule has got one job, sampling of jobs in second partition may allocated an arbitrary job either into the non-empty schedule or into any one of the empty schedules. As there are (|Œî|‚àí1)Œî1(|\\Delta|-1) empty schedules, while sampling in second partition, a job will be assigned in one of the (|Œî|‚àí1)Œî1(|\\Delta|-1) empty schedules with the probability (|Œî|‚àí1)|Œî|Œî1Œî\\frac{(|\\Delta|-1)}{|\\Delta|}.\nIn it‚Äãhsuperscriptùëñùë°‚Ñéi^{th} partition, there are (|Œî|‚àíi+1)Œîùëñ1(|\\Delta|-i+1) number of empty schedules as (i‚àí1)ùëñ1(i-1) schedules are allocated with atleast one job in (i‚àí1)ùëñ1(i-1) partitions. Hence, the probability of getting a job into an empty schedule in it‚Äãhsuperscriptùëñùë°‚Ñéi^{th} partition is (|Œî|‚àíi+1)|Œî|Œîùëñ1Œî\\frac{(|\\Delta|-i+1)}{|\\Delta|}.\nIn each partition the number of sampling of jobs can be represented with a random variable ùïÑi‚àósubscriptsuperscriptùïÑùëñ\\mathbb{M}^{*}_{i} where iùëñi lies within the range of {1,‚ãØ,|Œî|}1‚ãØŒî\\{1,\\cdots,|\\Delta|\\}. So, in order to get |Œî|Œî|\\Delta| successes we need ùïÑ‚àó=‚àëi|Œî|ùïÑi‚àósuperscriptùïÑsuperscriptsubscriptùëñŒîsubscriptsuperscriptùïÑùëñ\\mathbb{M}^{*}=\\sum\\limits_{i}^{|\\Delta|}\\mathbb{M}^{*}_{i} number of samplings. As in each partition, the random variable ùïÑi‚àósubscriptsuperscriptùïÑùëñ\\mathbb{M}^{*}_{i} follows geometric distribution and probability of success is P‚Äãr‚Äã{ùïÑi‚àó}=(|Œî|‚àíi+1)|Œî|ùëÉùëüsuperscriptsubscriptùïÑùëñŒîùëñ1ŒîPr\\{\\mathbb{M}_{i}^{*}\\}=\\frac{(|\\Delta|-i+1)}{|\\Delta|} So the expected number of samples in it‚Äãhsuperscriptùëñùë°‚Ñéi^{th} partition is thus E‚Äã[ùïÑi‚àó]=|Œî||Œî|‚àíi+1ùê∏delimited-[]subscriptsuperscriptùïÑùëñŒîŒîùëñ1E[\\mathbb{M}^{*}_{i}]=\\frac{|\\Delta|}{|\\Delta|-i+1} Now the expected number of sampling of jobs required in all partitions can be obtained as: The probability P‚Äãr‚Äã{‚Ñù‚â•1.5‚Äãùîº‚Äã[‚Ñù]}‚â§e‚àí|Œì|12‚Äã|Œî|ùëÉùëü‚Ñù1.5ùîºdelimited-[]‚ÑùsuperscriptùëíŒì12ŒîPr\\{\\mathbb{R}\\geq 1.5\\mathbb{E}[\\mathbb{R}]\\}\\leq e^{-\\frac{|\\Gamma|}{12|\\Delta|}}, for any schedule Œ¥i,subscriptùõøùëñ\\delta_{i}, where i‚àà[1,|Œî|]ùëñ1Œîi\\in[1,|\\Delta|]. We can find the probability P‚Äãr‚Äã{‚Ñù‚â•1.5‚Äãùîº‚Äã[‚Ñù]}ùëÉùëü‚Ñù1.5ùîºdelimited-[]‚ÑùPr\\{\\mathbb{R}\\geq 1.5\\mathbb{E}[\\mathbb{R}]\\} using Chernoff Bound as: P‚Äãr‚Äã(‚Ñù‚â•(1.5)‚Äãùîº‚Äã[‚Ñù])=P‚Äãr‚Äã(‚Ñù‚â•(1+12)‚Äãùîº‚Äã[‚Ñù])‚â§e‚àíùîº‚Äã[‚Ñù]‚Äã(12)23ùëÉùëü‚Ñù1.5ùîºdelimited-[]‚ÑùùëÉùëü‚Ñù112ùîºdelimited-[]‚Ñùsuperscriptùëíùîºdelimited-[]‚Ñùsuperscript1223Pr(\\mathbb{R}\\geq(1.5)\\mathbb{E}[\\mathbb{R}])=Pr(\\mathbb{R}\\geq(1+\\frac{1}{2})\\mathbb{E}[\\mathbb{R}])\\leq e^{\\frac{-\\mathbb{E}[\\mathbb{R}]{(\\frac{1}{2})}^{2}}{3}}. We know that ùîº‚Äã[‚Ñù]=|Œì||Œî|ùîºdelimited-[]‚ÑùŒìŒî\\mathbb{E}[\\mathbb{R}]=\\frac{|\\Gamma|}{|\\Delta|}. So, plugging this in the above equation we get: From the above discussion we can see that a little deviation of any schedule from balanced job assignment has the probability is upper bounded by e‚àí|Œì|12‚Äã|Œî|superscriptùëíŒì12Œîe^{-\\frac{|\\Gamma|}{12|\\Delta|}}. Now, as |Œì|Œì|\\Gamma| grows larger, P‚Äãr‚Äã(‚Ñù‚â•(1.5)‚Äãùîº‚Äã[‚Ñù])ùëÉùëü‚Ñù1.5ùîºdelimited-[]‚ÑùPr{(\\mathbb{R}\\geq(1.5)\\mathbb{E}[\\mathbb{R}])} will be decreasing exponentially. With |Œì|=200Œì200|\\Gamma|=200 and Œî=4Œî4\\Delta=4, the above said probability can be calculated as e‚àí20012√ó4=e‚àí4.17=0.0154superscriptùëí200124superscriptùëí4.170.0154e^{-\\frac{200}{12\\times 4}}=e^{-4.17}=0.0154, which is much less. In terms of our dataset, namely, Bus Driver Scheduling dataset and KDD Cup 2015 dataset, |Œì|=359Œì359|\\Gamma|=359 and |Œì|=3000Œì3000|\\Gamma|=3000 respectively. So the said probability will be much less than the case where |Œì|=200Œì200|\\Gamma|=200. The time complexity by PPSJBP is ùí™‚Äã(K√ó|Œì|)+ùí™‚Äã(|Œî|‚Äãlog‚Å°|Œî|)ùí™ùêæŒìùí™ŒîŒî\\mathcal{O}(K\\times|\\Gamma|)+\\mathcal{O}(|\\Delta|\\log|\\Delta|), where KùêæK is the number of repetition of sampling of jobs, |Œì|Œì|\\Gamma| is the total number of jobs and |Œî|Œî|\\Delta| is the number of schedules. Our algorithm PPSJBP consists of three subroutines - RRA, MBDF, and LCSF. In RRA |Œì|Œì|\\Gamma| number of jobs are allocated randomly in |Œî|Œî|\\Delta| schedules and this random allocation is repeated for KùêæK times. That means for each of the KùêæK iterations, |Œì|Œì|\\Gamma| of jobs have been allocated into |Œî|Œî|\\Delta| schedules. So the time complexity for RRA subroutine is ùí™‚Äã(K√ó|Œì|)=ùí™‚Äã(K‚Äã|Œì|)ùí™ùêæŒìùí™ùêæŒì\\mathcal{O}(K\\times|\\Gamma|)=\\mathcal{O}(K|\\Gamma|). MBDF subroutine finds the most balanced set of schedules by calculating the variance of each set of schedule out of KùêæK such sets. Variance calculation of each set takes ùí™‚Äã(|Œî|)ùí™Œî\\mathcal{O}(|\\Delta|) time. This is for one iteration. So, variance calculation for KùêæK such set of schedules take ùí™(K√ó|Œî|),i.e.,ùí™(K|Œî|)\\mathcal{O}(K\\times|\\Delta|),\\ i.e.,\\mathcal{O}(K|\\Delta|) time.\nNow, in the last subroutine (LCSF), the minimum variant set of schedules i.e., most balanced set of schedules are arranged in descending order according to their costs. To achieve that arrangement, merge sort is used. The input for the sorting is |Œî|Œî|\\Delta| number of costs (as each schedule will provide one cost). So the time complexity for LCSF subroutine is ùí™‚Äã(|Œî|‚Äãlog‚Å°|Œî|)ùí™ŒîŒî\\mathcal{O}(|\\Delta|\\log|\\Delta|).\nAccordingly, the total time complexity for the three subroutine can be calculated as ùí™‚Äã(K‚Äã|Œì|)+ùí™‚Äã(K‚Äã|Œî|)+ùí™‚Äã(|Œî|‚Äãlog‚Å°|Œî|)=ùí™‚Äã(K‚Äã|Œì|)+ùí™‚Äã(|Œî|‚Äãlog‚Å°|Œî|)ùí™ùêæŒìùí™ùêæŒîùí™ŒîŒîùí™ùêæŒìùí™ŒîŒî\\mathcal{O}(K|\\Gamma|)+\\mathcal{O}(K|\\Delta|)+\\mathcal{O}(|\\Delta|\\log|\\Delta|)=\\mathcal{O}(K|\\Gamma|)+\\mathcal{O}(|\\Delta|\\log|\\Delta|). If |Œî|<<|Œì|much-less-thanŒîŒì|\\Delta|<<|\\Gamma| (which is most realistic scenario), the time complexity of PPSJBP is ùí™‚Äã(K‚Äã|Œì|)ùí™ùêæŒì\\mathcal{O}(K|\\Gamma|). Now a connection will be established between finding our minimum variance with repetition and online hiring assistant problem for stating our next analytical result. In online hiring assistant problem, there are mùëöm number of candidates who appear for the interview process in an online fashion i.e.,formulae-sequenceùëñùëíi.e., we don‚Äôt know the list of the candidates apriori.\nIn this problem we don‚Äôt want to interview all the mùëöm number of candidates yet, we want to find the best candidate or a close to the best candidate (that is the target). The natural question that arises is that, how many samples we need to explore before we are sure that our target is achieved with a good probability. In literature [ref]55[ref]56, it is shown that we need to sample meùëöùëí\\frac{m}{e} number of candidates to make sure that our target is achieved with a probabilistic lower bound of 1e1ùëí\\frac{1}{e}.\nIn our problem, for each iteration, we calculate a variance. So, as a thought experiment, we can think the variances are appearing in an online fashion just like the candidates in the online hiring assistant problem. So the variances here are resembling the candidates and we need to find out the best (minimum) variance. So, the problem is boiling down to the fact that how many such variances to be sampled (to be calculated) before we achieve our target of finding the minimum variance. Keùêæùëí\\frac{K}{e} number of variances to be calculated (per iteration we calculate one variance) to make sure that the minimum variance will be achieved with a probabilistic lower bound 1e1ùëí\\frac{1}{e}. The proof follows from the connection we have established above, i.e.,formulae-sequenceùëñùëíi.e., we need meùëöùëí\\frac{m}{e} number of samples for the probability to be atleast 1e1ùëí\\frac{1}{e}. Following this connection, we can say that Keùêæùëí\\frac{K}{e} number of variances to be calculated for making sure that we get a probabilistic lower bound of 1e1ùëí\\frac{1}{e} (i.e,formulae-sequenceùëñùëíi.e, with probability atleast 1e1ùëí\\frac{1}{e}). Consider the value of K=4000ùêæ4000K=4000 repeated random allocation of jobs. According to Corollary 1, we have to iterate the random allocation of jobs into schedules Keùêæùëí\\frac{K}{e} times i.e.,formulae-sequenceùëñùëíi.e., 4000e‚âà14724000ùëí1472\\frac{4000}{e}\\approx 1472 times to get success with the probability of 1/e‚âà0.371ùëí0.371/e\\approx 0.37 i.e.,formulae-sequenceùëñùëíi.e., if we the repeat random allocation process for 1472 times, the chance to get best minimum variance is atleast 37%. Now, if we iterate our random allocation process for KùêæK times, the lower bound of the probability will also increase. So, we can say that with larger value of KùêæK, the lower bound of the probability of getting the best minimum variance is greater than 1e1ùëí\\frac{1}{e}.\n\nReference markers found: [55]\n\nContext windows for each [ref]:\n[ref]55: candidate (that is the target). The natural question that arises is that, how many samples we need to explore before we are sure that our target is achieved with a good probability. In literature [ref]55[ref]56, it is shown that we need to sample meùëöùëí\\frac{m}{e} number of candidates to make sure that our target is achieved with a probabilistic lower bound of 1e1ùëí\\frac{1}{e}.\nIn our problem,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 55,\n            \"titles\": [\n                \"Sample-driven optimal stopping: From the secretary problem to the i.i.d. prophet inequality\",\n                \"Ïù∏Í≥µÏßÄÎä• Í∏∞Î∞ò Í≥µÍ≤© Í∑∏ÎûòÌîÑ ÏÉùÏÑ±\",\n                \"The rise of crowdsourcing\",\n                \"Crowdsourcing from the true crowd: Device, vehicle, road-surface and driving independent road profiling from smartphone sensors\",\n                \"Crowdsourcing and open innovation: a systematic literature review, an integrated framework and a research agenda\",\n                \"Crowdsourcing ecosystem. the crowd pillars and their implementation process\",\n                \"A survey of crowdsourcing as a means of collaboration and the implications of crowdsourcing for interaction design\",\n                \"Crowdhelp: A crowdsourcing application for improving disaster management\",\n                \"Crowdsourcing for climate and atmospheric sciences: Current status and future potential\",\n                \"An exact single-agent task selection algorithm for the crowdsourced logistics\",\n                \"A Budget Feasible Peer Graded Mechanism For IoT-Based Crowdsourcing\",\n                \"Promoting users‚Äô participation in mobile crowdsourcing: A distributed truthful incentive mechanism (dtim) approach\",\n                \"Crowdsourcing to smartphones: Incentive mechanism design for mobile phone sensing\",\n                \"Architecture of mobile crowdsourcing systems\",\n                \"Quacentive: a quality-aware incentive mechanism in mobile crowdsourced sensing (mcs)\",\n                \"mcrowd: a platform for mobile crowdsourcing\",\n                \"txteagle: Mobile crowdsourcing\",\n                \"A review of mobile crowdsourcing architectures and challenges: Toward crowd-empowered internet-of-things\",\n                \"Spatial crowdsourcing: a survey\",\n                \"A survey of spatial crowdsourcing\",\n                \"Worker collaborative group estimation in spatial crowdsourcing\",\n                \"A spatial mobile crowdsourcing framework for event reporting\",\n                \"Spatialrecruiter: Maximizing sensing coverage in selecting workers for spatial crowdsourcing\",\n                \"Foodnet: Toward an optimized food delivery network based on spatial crowdsourcing\",\n                \"Task allocation in spatial crowdsourcing: Current state and future directions\",\n                \"A secure and privacy-preserving navigation scheme using spatial crowdsourcing in fog-based vanets\",\n                \"On reliable task assignment for spatial crowdsourcing\",\n                \"Toward privacy by design in spatial crowdsourcing in emergency and disaster response\",\n                \"Mobile crowdsourcing in smart cities: Technologies, applications, and future challenges\",\n                \"Toward privacy-preserving task assignment for fully distributed spatial crowdsourcing\",\n                \"Expertise-aware truth analysis and task allocation in mobile crowdsourcing\",\n                \"A budget and deadline aware task assignment scheme for crowdsourcing environment\",\n                \"Buda: Budget and deadline aware scheduling algorithm for task graphs in heterogeneous systems\",\n                \"Fairness of task allocation in crowdsourcing workflows\",\n                \"Maximizing the number of worker‚Äôs self-selected tasks in spatial crowdsourcing\",\n                \"An egalitarian approach of scheduling time restricted tasks in mobile crowdsourcing for double auction environment\",\n                \"Mixed priority queue scheduling based on spectral clustering in spatial crowdsourcing\",\n                \"CS269I: Incentives in Computer Science Lecture# 19: Time-Inconsistent Planning\",\n                \"Time-Inconsistent Planning: A Computational Problem in Behavioral Economics\",\n                \"Computational issues in time-inconsistent planning\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]55 based on the context and research area of Mitigating Procrastination in Spatial Crowdsourcing Via Efficient Scheduling Algorithm. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}