{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation\n\nPaper Abstract: \n\nPaper Section Text:\nDatasets¬†¬†¬†\nIn this paper, we adopt the test sets of IU X-ray  and MIMIC-CXR  for evaluation.\nAll protected health information (e.g., patient name and date of birth) was de-identified.\nIn particular, the IU X-ray  is a widely-used public benchmark dataset for medical report generation and contains 7,470 chest X-ray images associated with 3,955 fully de-identified medical reports.\nEach report is composed of impression, findings and indication sections, etc. [ref]22.\nFollowing , our method also focuses on the findings section as it is the most important component of reports.\nThen, following  [ref]22  [ref]6, we randomly select 70%-10%-20% image-report pairs of dataset to form the training-validation-testing sets.\nThe MIMIC-CXR  includes 377,110 chest x-ray images associated with 227,835 reports.\nThe dataset is officially split into 368,960 images (222,758 reports) for training, 2,991 images (1,808 reports) for validation and 5,159 images (3,269 reports) for testing.\nIt is worth noting that we focus on the unsupervised medical report generation, where the image-report pairs are not available, thus the image-report training pairs of both the IU X-ray and MIMIC-CXR datasets are discarded and are not used in our unsupervised training stage.\nOnly the training reports of MIMIC-CXR and IU X-ray, i.e., 222,758 + 2,770 = 225,528 reports, are used as the independent report corpus to train our unsupervised model.\nOnly under the (semi-)supervised training setting, we will adopt the image-report training pairs to train our approach. Metrics¬†¬†¬†\nTo fairly compare with existing models  [ref]6, we adopt the evaluation toolkit  to calculate the widely-used natural language generation metrics, i.e., BLEU , METEOR  and ROUGE-L ,\nwhich measure the match between the generated reports and ground truth reports, but are not specialized for the abnormalities in the reports.\nTherefore, to measure the accuracy of descriptions for clinical abnormalities,\nwe further report clinical efficacy metrics following the work of Chen et¬†al. [ref]6. The clinical efficacy metrics are calculated by comparing the generated reports with ground truth reports in 14 different categories related to thoracic diseases and support devices, producing the Precision, Recall and F1 scores. Settings¬†¬†¬†\nThe size dùëëd is set to 256.\nFor the attention mechanism in Eq.¬†(4) and Eq.¬†(5), we adopt the multi-head attention 7, the number of heads nùëõn in multi-head attention is set to 8. The intermediate dimension in ‚Ñ±‚Ñ±\\mathcal{F}, i.e., Eq.¬†(4), is set to 1024.\nBased on the average performance on the validation set, the NBsubscriptùëÅBN_{\\text{B}} in knowledge bank, i.e., Eq.¬†(6), is set to 10,000. The NKGsubscriptùëÅKGN_{\\text{KG}} in the knowledge graph is set to 200.\nIn our knowledge-driven encoder, the image embedding module adopts the ResNet-50  pre-trained on ImageNet  and fine-tuned on CheXpert dataset  to extract the image embeddings in the shape of 7 √ó\\times 7 √ó\\times 2048, which will be projected to d=256ùëë256d=256, acquiring I‚Ä≤‚àà‚Ñù49√ó256superscriptùêº‚Ä≤superscript‚Ñù49256I^{\\prime}\\in\\mathbb{R}^{49\\times 256}, i.e., NI=49subscriptùëÅùêº49N_{I}=49; The report embedding module is implemented by the Transformer 7 equipping with the self-attention mechanism provided in Lin et¬†al. ; NR=NI=49subscriptùëÅùëÖsubscriptùëÅùêº49N_{R}=N_{I}=49.\nIn both unsupervised and (semi-)supervised settings (see Section¬†3.3), the batch size is set to 16 and Adam optimizer  with a learning rate of 1e-4 is used for parameter optimization.\nWe also use momentum of 0.8 and weight decay of 0.999.\nAll re-implementations and our experiments were run on 8 V100 GPUs.\n\nReference markers found: [22]\n\nContext windows for each [ref]:\n[ref]22: report generation and contains 7,470 chest X-ray images associated with 3,955 fully de-identified medical reports.\nEach report is composed of impression, findings and indication sections, etc. [ref]22.\nFollowing , our method also focuses on the findings section as it is the most important component of reports.\nThen, following  [ref]22  [ref]6, we randomly select 70%-10%-20% image-report\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 22,\n            \"titles\": [\n                \"Auxiliary Signal-Guided Knowledge Encoder-Decoder for Medical Report Generation\",\n                \"Bottom-up and top-down attention for image captioning and VQA\",\n                \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n                \"METEOR: an automatic metric for MT evaluation with improved correlation with human judgments\",\n                \"Discrepancy and error in radiology: Concepts, causes and consequences\",\n                \"Microsoft COCO captions: Data collection and evaluation server\",\n                \"Generating Radiology Reports via Memory-driven Transformer\",\n                \"Meshed-Memory Transformer for Image Captioning\",\n                \"Difficulties in the interpretation of chest radiography\",\n                \"Preparing a collection of radiology examinations for distribution and retrieval\",\n                \"Imagenet: A large-scale hierarchical image database\",\n                \"Evidence-based guideline for the written radiology report: Methods, recommendations and implementation challenges\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Long short-term memory\",\n                \"Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison\",\n                \"On the Automatic Generation of Medical Imaging Reports\",\n                \"Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-Ray Reports\",\n                \"MIMIC-CXR: A large publicly available database of labeled chest radiographs\",\n                \"Adam: A method for stochastic optimization\",\n                \"A Hierarchical Approach for Generating Descriptive Image Paragraphs\",\n                \"Imagenet classification with deep convolutional neural networks\",\n                \"Ask me anything: Dynamic memory networks for natural language processing\",\n                \"Knowledge-driven encode, retrieve, paraphrase for medical image report generation\",\n                \"Gated Graph Sequence Neural Networks\",\n                \"Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation\",\n                \"ROUGE: A package for automatic evaluation of summaries\",\n                \"A Structured Self-attentive Sentence Embedding\",\n                \"simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions\",\n                \"Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations\",\n                \"Knowing when to look: Adaptive attention via a visual sentinel for image captioning\",\n                \"Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling\",\n                \"Key-value memory networks for directly reading documents\",\n                \"BLEU: a Method for automatic evaluation of machine translation\",\n                \"Self-critical Sequence Training for Image Captioning\",\n                \"Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning\",\n                \"End-to-end memory networks\",\n                \"Attention Is All You Need\",\n                \"Show and Tell: A Neural Image Caption Generator\",\n                \"Matching Networks for One Shot Learning\",\n                \"TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-rays\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]22 based on the context and research area of Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}