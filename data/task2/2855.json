{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Ego4D: Around the World in 3,000 Hours of Egocentric Video\n\nPaper Abstract: \n\nPaper Section Text:\nThis section details the Episodic Memory benchmark task definitions, annotations, baseline models, and results. As presented in the main paper, there are three kinds of Episodic Memory queriesâ€”visual, natural language, and momentsâ€”each of which requires localizing the response in the video. Their formal definitions are as follows. This task aims to query an egocentric video based on a static image crop of an object. Specifically, it asks the question â€˜Where was object X last seen in the video?â€™, where X is a single â€˜canonicalâ€™ image crop in which the object is clearly visible and human-identifiable. A potential use case for visual queries is where a user teaches the system a new object by showing a photo (â€œthese are my keysâ€) and then later queries for it among past video. By enabling visual queries, as opposed to categorical queries, this is a form of open-world object localization. We formulate the problem as follows. Given an egocentric video ğ’±ğ’±\\mathcal{V}, a query object oğ‘œo specified via a static visual crop vğ‘£v, and a query frame qğ‘q, the goal is to identify when the object oğ‘œo was last seen in the video before the query frame qğ‘q. The response is specified as a â€˜response trackâ€™ rğ‘Ÿr which is a temporally contiguous set of bounding boxes surrounding the object oğ‘œo in each frame: where sğ‘ s is the frame where the object oğ‘œo (at least partially) enters the camera-wearerâ€™s field of view, eğ‘’e is the frame where the object exits the camera-wearerâ€™s field of view, and risubscriptğ‘Ÿğ‘–r_{i} is a bounding box (x,y,w,h)ğ‘¥ğ‘¦ğ‘¤â„(x,y,w,h) in frame iğ‘–i. If the object appears multiple times in the video, the response only refers to the â€˜most recent occurrenceâ€™ of the object in the past, i.e., the response track which minimizes qâˆ’reğ‘subscriptğ‘Ÿğ‘’q-r_{e} with q>reğ‘subscriptğ‘Ÿğ‘’q>r_{e}. When a 3D scan of the environment associated with the video is available, the response additionally includes a 3D displacement vector Î”â€‹d=(Î”â€‹x,Î”â€‹y,Î”â€‹z)Î”ğ‘‘Î”ğ‘¥Î”ğ‘¦Î”ğ‘§\\Delta d=(\\Delta x,\\Delta y,\\Delta z) between the 3D location where the query was made (i.e., at query frame qğ‘q), and the 3D location in the environment where the object was last seen (i.e., at the end of the response track resubscriptğ‘Ÿğ‘’r_{e}). The motivation behind the NLQ task is to enable searching through an egocentric video\nusing a natural language query.\nThe system responds to a query by providing a temporal window localized in the video,\nfrom which the answer to the query can be deduced.\nThese queries can be related to objects, places, people, and activities that appeared in\nthe episodic memory of the user.\nNote that we only consider episodic queries, i.e., queries that can be answered/deduced\nfrom the egocentric videos, and not factual queries, i.e., queries that require an external knowledge base to answer. NLQ is a challenging multimodal task requiring visual and linguistic\nunderstanding and reasoning.\nConsider the query â€œWhat did I pick up before leaving the party?â€\nIn order to fulfill this request, the system needs to:\n(a) break down and understand the language query as a search for an object (what)\nwith which the user interacted (pick up) before an event\n(leaving the party),\n(b) go through the egocentric video and identify the desired event of\nâ€œleaving the partyâ€,\n(c) visually search for the object with which the user interacted prior to this event.\nThis example demonstrates the complexity of NLQ from both visual\n(recognizing events, objects, places, etc.) and\nlinguistic (breaking down reasoning, understanding relations, etc.) perspective.\nIn addition, the diverse set of queries within NLQ, while facilitating a flexible search and retrieval through an intuitive interface of language, also increases the complexity of\nthe task. Concretely, NLQ is formulated as follows:\nGiven an egocentric video ğ’±ğ’±\\mathcal{V} and a natural language query ğ’¬ğ’¬\\mathcal{Q},\nthe goal is again to identify a â€˜response trackâ€™ rğ‘Ÿr, such that the answer to ğ’¬ğ’¬\\mathcal{Q}\ncan be deduced from rğ‘Ÿr.\nThe response track should be a set of temporally contiguous frames within ğ’±ğ’±\\mathcal{V}.\nGiven the episodic nature of our task, rğ‘Ÿr should be sufficient to answer ğ’¬ğ’¬\\mathcal{Q},\nwithout the additional need for ğ’±ğ’±\\mathcal{V} or any external knowledge bases. This task aims to query an egocentric video based on a category of actions. Specifically, it poses the following request â€˜Retrieve all the moments that I do X in the video.â€™, where â€˜Xâ€™ comes from a pre-defined taxonomy of action categories, such as â€˜interact with someoneâ€™ or â€˜use phoneâ€™. Compared to the natural language queries, the moment queries focus on daily-life actions or activities. One moment query can correspond to multiple response instances (temporal windows) in the video. This task provides the user a fast and convenient way to retrieve multiple action moments at a time, where the user does not need to come up with a sentence to describe what he/she wants, but instead can directly choose among the pre-defined categories. The moment queries task is related to the task of temporal action detectionÂ [ref]141  [ref]237, which aims to identify and localize all instances of all action categories that take place in a video. Both tasks have a list of action categories pre-defined, and both aim to predict multiple action instances with their temporal boundaries. The difference is that 1) our moment queries task is a retrieval task where action categories are provided as queries, meaning it does not need to produce instances of categories that are not among the queries; and 2) our moments taxonomy is specific to first-person activity. We aim for moments that are activities at a medium level of granularityâ€”coarser than the actions in Forecasting, and finer than the â€œscenarioâ€ labels shown in FigureÂ 3 of the main paper. The MQ task is also related to temporal language grounding in videosÂ [ref]236, which aims to retrieve a segment from a video, as queried by a natural language sentence. Both tasks have a query and aim to predict corresponding temporal segments. The difference is that MQ uses pre-defined query categories rather than natural language sentences, and one query can correspond to multiple instances rather than a unique one.  We formulate the problem as follows. Given an egocentric video ğ’±ğ’±\\mathcal{V}, and a query action category cğ‘c, the goal is to retrieve all the instances of this action category in the video, assuming that the query is made at the end of the video. The response is a set of action instances of the category cğ‘c Î¦c={Ï•n=(tn,s,tn,e,sn)}n=1NsubscriptÎ¦ğ‘superscriptsubscriptsubscriptitalic-Ï•ğ‘›subscriptğ‘¡ğ‘›ğ‘ subscriptğ‘¡ğ‘›ğ‘’subscriptğ‘ ğ‘›ğ‘›1ğ‘\\Phi_{c}=\\left\\{\\phi_{n}=\\left({t}_{n,s},{t}_{n,e},{s}_{n}\\right)\\right\\}_{n=1}^{N}, where nğ‘›n is the number of instances for this category, tn,ssubscriptğ‘¡ğ‘›ğ‘ {t}_{n,s} and tn,esubscriptğ‘¡ğ‘›ğ‘’{t}_{n,e} are start time and end time of the ntâ€‹hsuperscriptğ‘›ğ‘¡â„n^{th} instance respectively, and snsubscriptğ‘ ğ‘›{s}_{n} is its prediction confidence. For all benchmarks we sample video clips to annotate based on criteria for geographic diversity and scenario diversity. For Episodic Memory we impose additional sampling criteria meant to highlight data most interesting for the task, as follows. Video clips to annotate for visual queries (VQ) are selected based on the frequency of object occurrences and amount of navigation in the video. To have interesting visual queries in a video, there must be several â€˜interestingâ€™ objects that can be queried about. An object is â€˜interestingâ€™ in the context of visual queries if there is a sufficiently high separation in space and time between any two occurrences of the object. This typically happens when the camera-wearer visits the location near the object briefly, and then navigates elsewhere before revisiting the object again. For example, consider a person who finishes cleaning a living room, visits the kitchen for some period of time before revisiting the living room again. Most objects in the living room are interesting to query about when the person is in the kitchen. To select videos based on these considerations, we use a two-step process. First, we filter out videos based on the associated â€˜scenarioâ€˜ labels (see FigureÂ 3) that provide high-level information about the content and activities in videos (e.g., cooking, cleaning, golfing, etc.). We manually preview randomly sampled videos from each scenario to identify interesting scenarios such as cooking, indoor navigation, farmer, cleaning, and grocery shopping. We then sort videos within each scenario based on a scoring function using the narrations for the video. Specifically, we extract the list of verbs in the narrations (along with their frequencies). We then measure the entropy of the distribution of manually curated navigation verbs (See Tab.Â 4). The video is more likely to allow challenging visual queries if its navigation entropy is higher. For videos with near-zero entropy, we observe that the camera-wearer is usually staying static in a single location without any movement. Finally, a limited number of 3D scans were available for the 3D localization task. Videos associated with these scans were prioritized, regardless of their navigation entropy, in support of the 3D response version of the VQ task. For NLQ we apply similar sampling criteria as above for VQ, but augment it to avoid repetitive actions (e.g., sewing while sitting on the couch). First, we manually select amenable scenarios (see FigureÂ 3). Among those, we prioritize clips with high entropy computed over navigational terms as above. Finally, we prioritize non-repetitive actions by computing the ratio of the number of unique verbs in a clipâ€™s narration vs.Â the total number of verbs in that same narrationâ€”higher is better. To select clips for moments queries, we compute\nthe overlap of verbs/nouns with the moments taxonomy. We calculate a similar entropy-based score and sort videos according to this score. In addition, we restrict videos to a fixed set of categories present in our taxonomy to avoid labeling videos that do not contain relevant activities. Next we describe the annotation procedures and outputs for Episodic Memory. For annotating visual queries, we first sample contiguous clips of varying lengths (5 mins, 8 mins, and 16 mins) from the set of interesting videos. The annotators are instructed to create and annotate 3 visual queries for each clip. A visual query consists of the query frame qğ‘q, the visual crop vğ‘£v of the query object oğ‘œo, the response track r={rs,rs+1,â‹¯,reâˆ’1,re}ğ‘Ÿsubscriptğ‘Ÿğ‘ subscriptğ‘Ÿğ‘ 1â‹¯subscriptğ‘Ÿğ‘’1subscriptğ‘Ÿğ‘’r=\\{r_{s},r_{s+1},\\cdots,r_{e-1},r_{e}\\}, and a textual name for the object (eg. cup, hammer, broomstick, etc). The annotators performed the following steps to annotate a given clip: Identify three interesting query objects in the clip. An object is interesting if it occurs in at least two different parts of the video. For a given object, enter a textual name. While our current task queries with the image crop, not the name, this annotation will allow future variants that do query for the object by name. Select one of the object occurrences in the video and mark a visual crop v=(xv,yv,wv,hv)ğ‘£subscriptğ‘¥ğ‘£subscriptğ‘¦ğ‘£subscriptğ‘¤ğ‘£subscriptâ„ğ‘£v=(x_{v},y_{v},w_{v},h_{v}). The visual crop must be a good representative view of the object, and it must have good lighting, large-enough size, and must not be blurred. Mark a different occurrence of the object as the response track r={rs,â‹¯,re}ğ‘Ÿsubscriptğ‘Ÿğ‘ â‹¯subscriptğ‘Ÿğ‘’r=\\{r_{s},\\cdots,r_{e}\\}. The response track starts from the frame when the object is first visible and ends when the object leaves the field-of-view. The response track must also be contiguous in time and the bounding boxes must accurately mark the position and size of the object. The query frame qğ‘q is sampled some time after the response track rğ‘Ÿr. The object oğ‘œo must not appear anywhere between the response track rğ‘Ÿr and the query frame qğ‘q, so that the ground truth is well-defined and unique for â€œwhen did I last seeâ€¦?â€. For each annotation, we apply automated and manual quality checks to ensure correctness. In case the quality falls below a certain threshold, the clip is reannotated. For visual queries associated with 3D scans, we also collect 3D annotations in the form of 3D bounding boxes capturing where the object was last seen. We then use those bounding boxes to establish the ground truth displacement vector from the query frame to the object, which is the target of the task. Each annotation aqsubscriptğ‘ğ‘a_{q} is collected in the scan coordinate system sğ‘ s: where qâˆˆ{1,â€¦,ğ’¬}ğ‘1â€¦ğ’¬q\\in\\{1,\\dots,\\mathcal{Q}\\}, ğ’¬ğ’¬\\mathcal{Q} the total number of queries, and where Tsâˆˆâ„4subscriptğ‘‡ğ‘ superscriptâ„4T_{s}\\in\\mathbb{R}^{4} is the transformation matrix of the bounding box. Rssubscriptğ‘…ğ‘ R_{s} and tssubscriptğ‘¡ğ‘ t_{s} are the corresponding rotation and translation for annotation aqsubscriptğ‘ğ‘a_{q}. The annotation procedure is defined as follows:\nA query consists of a video clip, a visual crop, and a response track. For each query, the goal is to retrieve in the scan the location of the object defined in the video. Once the location is found, we draw a 3D bounding box at this position with the appropriate scale and orientation. It is important to note that 3D scans and videos have been recorded at different times. Therefore, it is likely that an object at a certain location in the video will not be present at that same location in the 3D scan. In such cases, we ask the annotator to hallucinate a 3D bounding box in the 3D scan at the position of the target object defined in the video. In order to validate an annotation we collect two 3D bounding boxes per query from two different annotators. Leveraging the two boxes we compute the following validation metrics: where c1subscriptğ‘1c_{1} and c2subscriptğ‘2c_{2} are the centroids of the two boxes, mdâ€‹iâ€‹aâ€‹gsubscriptğ‘šğ‘‘ğ‘–ğ‘ğ‘”m_{diag} is the average diagonal length of the two boxes, Vgâ€‹lâ€‹oâ€‹bâ€‹aâ€‹lsubscriptğ‘‰ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™V_{global} is the volume of the 3D convex hull of the two boxes, and Vuâ€‹nâ€‹iâ€‹oâ€‹nsubscriptğ‘‰ğ‘¢ğ‘›ğ‘–ğ‘œğ‘›V_{union} is the volume of the union of the two boxes. These metrics measure the agreement level betwen the two annotators. When the two annotations are perfectly aligned, the metrics are equal to dnâ€‹oâ€‹râ€‹m=0subscriptğ‘‘ğ‘›ğ‘œğ‘Ÿğ‘š0d_{norm}=0 and Vnâ€‹oâ€‹râ€‹m=1.0subscriptğ‘‰ğ‘›ğ‘œğ‘Ÿğ‘š1.0V_{norm}=1.0. The assumption is that if the two annotators agree on the position, scale, and orientation of the bounding box then it is likely to be correct. If the two annotations are far from each other we will discard the query. There are a couple of reasons that can explain such case: (1) one annotator mislabeled the query, (2) the query is hard to annotate.\nSome queries require a significant amount of hallucination to retrieve the object location in the scan which clearly leads to subjective annotations.\nWe empirically defined two thresholds of 1.5 over dnâ€‹oâ€‹râ€‹msubscriptğ‘‘ğ‘›ğ‘œğ‘Ÿğ‘šd_{norm} and 15 over Vnâ€‹oâ€‹râ€‹msubscriptğ‘‰ğ‘›ğ‘œğ‘Ÿğ‘šV_{norm} to filter out poor annotations. Any query that has either one of the two metrics above the threshold of acceptance is rejected. To collect NLQ annotations, we sample contiguous clips of length 8 minutes and 20 minutes. The annotators are instructed to watch these clips and generate natural language queries, focused on retrieving information about objects, places, and people in the egocentric\nvideo clips.\nTo reduce the cognitive overload on the annotators, and focus their efforts on memory-relevant queries, we also provide a list of 131313\nquery templates (see TableÂ 5), corresponding to\nqueries a user might ask to augment their memory.\nNote that these templates are provided only to guide their choice of query, and\ndoes not limit the linguistic variability since the annotators are instructed\nto paraphrase the template without copying them as is. To elaborate, the annotators performed the following steps: Watch the entire video clip ğ’±ğ’±\\mathcal{V} in order to understand the high-level\ncontext (optionally in 2Ã—2\\times fast-forward), Pick a query template from the available list and paraphrase/reword the query to obtain ğ’¬ğ’¬\\mathcal{Q},\ne.g., template â€˜Where was object X before/after event Y?â€™ can be paraphrased\nas â€˜Where was the blue bucket prior to my dog exiting the living room?â€™ Find the temporal window where the response to the natural language query can be deduced visually, and annotate it as rğ‘Ÿr. During our data collection, we also requested the annotators to mark the slot values\nand corresponding verbs, for the selected language query templates.\nWhile we do not use this information for our task, it may be useful for other future research. The desiderata for the collected queries are as follows. They should:\n(a) reflect the underlying motivation of augmenting human memory,\n(b) be rich and diverse in terms of language and the objects, places, people, and\nevents, and,\n(c) be challenging enough for an intelligent system but not too complicated or\nconvoluted to reduce the naturalness of the queries.\nFor instance, though a query like â€˜What was playing on the television when I was\nfolding my seventh T-shirt after my dog exited the room?â€™ is challenging from a\nlearning perspective, it is not natural from an application standpoint.\nIn order to ensure the above qualities for NLQ, we enforce the following constraints: All paraphrased language queries must be in past tense, and must be posed as\nquestions asked at the end of the entire video clip.\nThis resembles the real-life scenario of querying about episodic memory (past) of\nthe user, and resolves ambiguity when there are multiple occurrences of an object to the\nthe last relevant one. To account for momentary shifts of view for the egocentric video, we allow\nsmall interruptions (<< 3 seconds) between the truly relevant frames for a given query.\nIn other words, frames where the object/person/place of interest goes out of view for less\nthan 3 seconds as a result of momentary gaze shift are still considered to be contiguous. For a given query, if there are multiple non-contiguous temporal windows (separated by\nmore than 3 seconds) as independently valid answers, we instruct the annotators to either\ndiscard the query and create a different one, or add more details to the wording to make it\nmore specific.\nSimilarly, queries that require multiple temporal windows (separated by more than 3 seconds)\nto deduce the answer are also disallowed. For example,\nâ€˜How many shirts did I pack in my suitcase?â€ is invalid if packing happens across\nmultiple temporal windows, separated by more than 3 seconds (e.g., the user pauses to make\ncoffee, and then returns to packing). We encourage diversity by instructing that the query responses not be concentrated at one\npart of the video clip, or around few objects/places/people.\nIn addition, we also disallow the query response window to be more than 50%percent5050\\% of the total\nclip length. Finally, queries that require reasoning and knowledge on top of visual evidence are\ninvalid.\nFor instance, â€˜What countryâ€˜s flag was hanging on the wall?â€ is invalid while\nâ€˜Where was the flag that was hanging on the wall?â€ is valid.\nSimilarly, queries that guess the motivation or intentions of the user or people\nin the video clip are also not allowed.\nAs an example, â€˜Why did the person at the door leave a package on the porch?â€™ is\ndisallowed while â€˜What did the person leave on the porch?â€™ is accepted. After the annotation process, we apply both automatic and manual quality checks, including the\ndiversity of language queries and temporal window locations, to score the annotations.\nIf the overall quality score is below a threshold, the clip is re-annotated. To annotate moments queries, we sample contiguous clips of 8 minutes from the set of interesting moments videos. The annotators are instructed to mark instances of activities with a temporal window and the activityâ€™s name from a fixed taxonomy of activities. We have each instance labeled by three independent annotators. By assuming each annotator is reliable, we take the union\nof moments across annotators to ensure completeness of annotations. The taxonomy was created semi-automatically from the narrations. Specifically, we use the summary narrations collected for five-minute clip segments, as they capture higher-level events and activities that are suitable for the moments retrieval task. This is in contrast to the verb-noun taxonomy that is sourced from individual narrations for each atomic action, which are used in the Forecasting and Hands and Objects benchmarks (see AppendicesÂ G andÂ J). The taxonomy was created as follows. First, each summary narration was encoded into a feature vector using a pre-trained BERTÂ  language model, and then concatenated with the word embeddings for the main verb and noun extracted from the summary. These summaries were then clustered into groups, and then labels were manually assigned to groups based on the coherent activities they described. Note that this process was done independently for a set of scenarios that we selected based on how frequently they occur in the dataset, the diversity of activities they represent, and how likely they contain high-level, event-like activities. For example videos that primary involve a single activity like â€œdrivingâ€ are not interesting categories in this context, whereas â€œhousehold cleaningâ€ contains several different activities that are shared across other indoor tasks, making it an appropriate scenario. In total, we select videos from 5 scenarios to create our moments taxonomy: Cooking, Cleaning, Shopping, Handyman, Farmer/Gardener.\nEach annotation is in the format of (start time, end time, label). We now overview the statistics of the annotations per query type. The VQ annotations consist of samples from a diverse set of scenarios and universities (see FigureÂ 20 andÂ 21). In total, 433433433 hours of videos are annotated with 22,6022260222,602 visual queries. These videos are sampled from 10 universities and consist of 54 scenarios. The statistics over the train/val/test splits are provided in TableÂ 6. We ensured that the splits contain a disjoint set of videos. To look for possible biases in the data, we plot the distribution over three measures. 1) Query to response separation is the temporal distance (in frames) between the query frame and the end of the response track. This measures how far back in time an algorithm needs to search in order to find the query object. \n2) Response track size measures the temporal length of the response track. \n3) Response bbox position is the spatial start and end (x,y)ğ‘¥ğ‘¦(x,y) coordinates for each bounding box in the response track. We normalize the coordinates by the image width and height to account for varying image sizes in the data. Each pixel within the bounding box contributes to an image heatmap that shows the frequency of each pixel belonging to a response track bounding box. The analyses are shown in FigureÂ 22. The query to response separation distances are fairly spread between 1 to 200 frames with a mode of âˆ¼30similar-toabsent30\\sim 30 frames (see FigureÂ 22, left). The response track sizes are well distributed between 1 to 40 frames with a mode of âˆ¼8similar-toabsent8\\sim 8 frames (see FigureÂ 22, center). The bounding boxes are near-uniformly distributed throughout the image, with very few bounding boxes annotated at the top 10%percent1010\\% of the image (see FigureÂ 22, right). Our analyses indicate that there may be a potential bias in the first two measures, while the bounding boxes positions are largely unbiased. For the 3D localization task, we annotate a subset of 1,043 visual queries with 3D annotations. These comprise of 13 video hours associated\nwith 4 scans from the University of Catania (UNICT). As outlined in TableÂ 7, the NLQ annotations are from 227227227 hours of\nvideo, with a total of 19.2â€‹K19.2ğ¾19.2K queries spanning the selected 131313 query templates.\nThe associated video clips come from 101010 different universities with a total of 343434 scenarios\n(with at least 1 hour of video annotated).\nSimilar to other tasks within the episodic memory, we ensure that the\ntrain/val/test splits (60%,20%,20%percent60percent20percent2060\\%,20\\%,20\\%) contain a disjoint set of video clips.\nWe further analyze the data through:\n(a) Distribution over template queries, shown in Figure 24. The challenging â€˜Where is object X before/after event Y?â€™\nis the most popular template with around 3â€‹K3ğ¾3K queries, with a reasonable distribution over other templates.\nOverall, the queries in NLQ have 8.3Â±2.1plus-or-minus8.32.18.3\\pm 2.1 words in them.\n(b) Distribution of the response window length is shown in Figure 25.\nTypically, the windows are 9.3Â±21.5plus-or-minus9.321.59.3\\pm 21.5 seconds long. Most response windows are quite short compared to the full video clip, making the task a challenging â€œneedle in the haystackâ€ search problem.\n(c) Distribution of query words is shown in Figure 19.\nThe branching off evidences the richness and diversity of the queries in NLQ. For MQ, similar to other tasks in episodic memory, we maintain a ratio of 6:2:2 among the train/val/test splits, which contains disjoint sets of video clips. To make sure there are enough samples in each category, we only keep categories that have at least 50 instances from the annotations and have instances in all train/val/test splits. Consequently, the MQ dataset has 110 categories, spans a total 326.4 hours of videos, 2,488 video clips and 22.2kğ‘˜k action instances. We summarize the statistics across the three splits in TableÂ 8. We further explore the data through the following aspects. (a) The distribution of action duration is shown in FigÂ 26. We can see that most moments have very short duration. The majority of moments last less than 1 minute, and 22.4% actions have duration less than 3 seconds. Note that there is also a peak (2.6% instances) at the largest duration bin, where the actions almost cover the whole video clip. The average duration each instance is 45.2 seconds. (b) The distribution of different categories is shown in FigÂ 18. We notice that this is a long-tailed distribution, some categories (e.g., â€˜use phoneâ€™, â€˜converse/interact with someoneâ€™) with over 1000 instances and some categories with less than 100 instances. Each category has 205 instances on average. (c) The distribution of instance numbers in a video clip is shown in FigÂ 27. The majority of video clips have 1-20 moment instances, whereas very few can have as many as over 80 instances. Next we detail the evaluation metrics for all three query types. We define the following localization metrics for the 2D localization task with top-1 retrieval.\nTemporal AP (tAP) measures how closely the temporal extent of the prediction matches with the ground-truth response track. It is calculated as the average-precision of the predicted response trackâ€™s temporal extent, and is based on the ActivityNet mAP metricÂ . We evaluate the tAP at 4 different tIoU thresholds {0.25, 0.50, 0.75, 0.95}, as well as their average value. \nSpatio-temporal AP (stAP) measures how closely the spatio-temporal extent of the prediction matches the ground-truth response track. It is calculated as the average-precision of the predicted spatial-tube, and is based on the video-AP metric fromÂ . We evaluate the stAP at 4 different stIoU thresholds {0.25, 0.50, 0.75, 0.95}, as well as their average value. \nSuccess (Succ) measures whether the prediction has any overlap with the ground truth at all. It is calculated as the percentage of samples where the predicted response track has atleast 0.050.050.05 spatio-temporal IoU with the ground truth. \nRecovery% (rec%) measures how much of the ground-truth response track is accurately recovered by the prediction. It is calculated as the % of frames in the response track where the predicted bounding box has at least 0.50.50.5 IoU with the ground truth. This is motivated by the tracking robustness metric from the VOT challengeÂ .\nSearch efficiency (sEff) measures the efficiency of the algorithm searching for the query object. It is calculated as where nğ‘›n is the number of video frames previewed by an algorithm to predict the response track, and Nğ‘N is the total number of frames in the video before the query was made (i.e., the search window). An algorithm that accesses every frame in the search window before localizing the query object gets 0.00.00.0 search efficiency. This â€œtimelinessâ€ metric is designed to encourage research on methods performing intelligent contextual-search. We evaluate performance on the 3D VQ localization task using the root mean square error (RMSE) and the angular error metrics: where tssubscriptğ‘¡ğ‘ t_{s} and t^ssubscript^ğ‘¡ğ‘ \\hat{t}_{s} are the ground-truth and predicted object position in the scan coordinate system. vQsubscriptğ‘£ğ‘„v_{Q} and v^Qsubscript^ğ‘£ğ‘„\\hat{v}_{Q} are the ground-truth and predicted 3D displacement vector in the query frame Qğ‘„Q coordinate system. We also define a success metric leveraging the two annotations per query: With câ€‹1ğ‘1c1 and câ€‹2ğ‘2c2 the centroids of the two bounding box annotations, cmsubscriptğ‘ğ‘šc_{m} the mid-centroid between câ€‹1ğ‘1c1 and câ€‹2ğ‘2c2 and Î´=expâˆ’mdâ€‹iâ€‹aâ€‹gğ›¿superscriptsubscriptğ‘šğ‘‘ğ‘–ğ‘ğ‘”\\delta=\\exp^{-m_{diag}}, with mdâ€‹iâ€‹aâ€‹gsubscriptğ‘šğ‘‘ğ‘–ğ‘ğ‘”m_{diag} the average diagonal length of the two boxes. Evaluation for NLQ is similar to existing video-language\ngrounding problems.\nFollowing prior work [ref]236, we use recall@k, IoU=m, where\nwe select k={1,5}ğ‘˜15k=\\{1,5\\} and m={0.3,0.5}ğ‘š0.30.5m=\\{0.3,0.5\\}.\nThis metric computes the percentage of times at least one of the top kğ‘˜k predicted candidates\nhave an intersection-over-union (IoU) of at least mğ‘šm.\nNote that we lean towards lower threshold values (mğ‘šm) as the average length of the\nwindow (âˆ¼similar-to\\sim10â€‹s10ğ‘ 10s) is much smaller than that of the video clip (500â€‹s500ğ‘ 500s), about\n2%percent22\\% of the clip length. Considering that the moment queries task is related to the tasks of temporal action detectionÂ [ref]141  [ref]237  and video groundingÂ [ref]236, we adapt their respective metrics to moment queries. Average Precision (AP) is a commonly adopted metric in temporal action detection. It measures how closely the temporal extent of the predictions matches the ground-truth action instances for each action categoryÂ [ref]141  [ref]237  in terms of both precision and recall. The temporal intersection over union (tIoU) between a prediction and a ground-truth action instance is used to measure their distance. If the tIoU is higher than a threshold, the prediction is considered as true positive; otherwise, false positive. In representative temporal action detection datasets, such as ActivityNetÂ , the mean AP (mAP) over all categories is computed given a tIoU threshold. Multiple tIoU thresholds are adopted, and the average mAP over all these tIoU thresholds is computed. For moment queries, we evaluate mAP at 5 different tIoU thresholds {0.1, 0.2, 0.3, 0.4, 0.5}, as well as their average value. Recall@kx, tIoU=m, is a metric adapted from the metric recall@kğ‘˜k, tIoU=mğ‘šm, used for NLQ. The metric recall@kğ‘˜k, tIoU=mğ‘šm measures the percentage of the query sentences that have at least one prediction with a tIoU larger than the threshold mğ‘šm in the top-kğ‘˜k results. In our moment queries case, since we might have more than one instance corresponding to a query moment category, we need to measure the percentage of all the correctly predicted instances that have at least one prediction with a tIoU larger than the threshold mğ‘šm in the top-kğ‘˜k results of this instance. Considering that predictions are usually made based on a category not a specific instance, we modify the metric to be the following recall@kâ€‹xğ‘˜ğ‘¥kx, tIoU=mğ‘šm, where xğ‘¥x stands for the number of instances for a query category in one video. This metric measures the percentage of all the correctly predicted instances that have at least one prediction with a tIoU larger than the threshold mğ‘šm in the top-kâ€‹xğ‘˜ğ‘¥kx results of the action category. This metric has a similar idea to the multi-label metric proposed in  when dealing with multiple instances for a query. We use k=1,2,3ğ‘˜123k=1,2,3 and m=0.3,0.5,0.7ğ‘š0.30.50.7m=0.3,0.5,0.7 in the metric. Compared to average precision, this metric only evaluates the recall for the query categories, and does not penalize for false positive predictions given a category that has no instances in the video. We developed baseline models for each task. We designed these models to address our tasks, using state-of-the-art components where relevant. They represent a starting point upon which future work can build. We treat visual queries with 2D localization (VQ2D) as a detection + tracking problem (see FigureÂ 28). At a high level, our approach consists of three steps. First, we perform frame-level detection over the input video where we detect the presence of the query object in each frame using an object detection model (FigureÂ 28 top). For each frame, we get the bounding box that is most similar to the visual crop and a score indicating its visual similarity. Second, we consider the sequence of per-frame similarity scores over the entire video and\nidentify the most recent peak in these scores (FigureÂ 28 bottom-left). Finally, we initialize a tracker at the video-frame corresponding to the peak detection, and track the query object\non both forward and backward directions to recover the complete response track (FigureÂ 28 bottom-right). We propose Siam-RCNN, a Faster-RCNNÂ  based approach to detect the query object\nin a given image. See FigureÂ 28 top.\nGiven a video frame at time tğ‘¡t, a pre-trained Region Proposal Network (RPN)Â  with a Feature Pyramid Network (FPN)Â  backbone is used\nto generate bounding box proposals {b1,â‹¯,bN}subscriptğ‘1â‹¯subscriptğ‘ğ‘\\{b_{1},\\cdots,b_{N}\\}. The RoI-Align operationÂ  is then used to extract visual features for each bounding box {â„±â€‹(b1),â‹¯,â„±â€‹(bN)}â„±subscriptğ‘1â‹¯â„±subscriptğ‘ğ‘\\{\\mathcal{F}(b_{1}),\\cdots,\\mathcal{F}(b_{N})\\}.\nWe use the same FPN backbone to extract features for the visual crop vğ‘£v.\nTo detect the presence of the query object in frame tğ‘¡t, each proposal feature â„±â€‹(bi)â„±subscriptğ‘ğ‘–\\mathcal{F}(b_{i}) is compared with the visual crop feature â„±â€‹(v)â„±ğ‘£\\mathcal{F}(v) using a Siamese head ğ’®ğ’®\\mathcal{S} that predicts a 0-1 similarity score The Siamese network projects each proposal / visual-crop feature to a\n1024-D feature vector using a convolutional projection module ğ’«ğ’«\\mathcal{P}, and predicts a 00 - 111 similarity score using a bilinear operation: where Ïƒğœ\\sigma is a sigmoid non-linearity. After computing the similarities to each bounding box proposal, the proposal btsubscriptğ‘ğ‘¡b_{t} with the highest similarity score stsubscriptğ‘ ğ‘¡s_{t} for frame tğ‘¡t can be obtained as follows: After repeating the above steps for all the video frames, we can obtain the final per-frame predictions as\n[(b1,s1),â‹¯,(bqâˆ’1,sqâˆ’1)]subscriptğ‘1subscriptğ‘ 1â‹¯subscriptğ‘ğ‘1subscriptğ‘ ğ‘1[(b_{1},s_{1}),\\cdots,(b_{q-1},s_{q-1})]. So far, we used Siam-RCNN to get the most similar proposals and their similarity\nscores for every frame in the video. Next, the goal is to temporally detect the â€˜most recent occurrenceâ€˜ of the object in the video (see FigureÂ 28 bottom-left).\nThis is a challenging problem since our goal is not to identify the best detection of the object, but instead the most recent one, even if the similarity is not as high.\nTo tackle this problem, we treat the per-frame similarity scores ğ’”={s1,â‹¯,sqâˆ’1}ğ’”subscriptğ‘ 1â‹¯subscriptğ‘ ğ‘1\\bm{s}=\\{s_{1},\\cdots,s_{q-1}\\} as a temporal signal, and use a signal peak detection\napproach to identify the salient peaks (a.k.a. local maxima) in ğ’”ğ’”\\bm{s}. To avoid spurious peaks, we first smooth ğ’”ğ’”\\bm{s} using a median filter with a window size of 555. Depending on the video, the algorithm may return multiple peaks spread throughout the video (see signal peaks in FigureÂ 28 bottom-right). Since our goal is to detect the most recent occurrence of the object,\nwe select the peak pğ‘p that is temporally nearest to the query frame. After temporal detection, we have identified a peak-frame pğ‘p in the video which is estimated to have the\nmost recent occurrence of the object. For this frame pğ‘p, we can obtain the highest-scoring bounding box bpsubscriptğ‘ğ‘b_{p} from the per-frame detections in step 1. Note that this only represents one frame where the object most recently occurred. However, the task objective is to obtain the response track, i.e., the contiguous set of all frames, starting from when the object first entered the field-of-view until the object exits the field-of-view. See FigureÂ 28 bottom-right.\nTo compute the rest of the response track, we use bpsubscriptğ‘ğ‘b_{p} as a starting point, and run a single-object tracker forward\nand backward until the tracking fails (i.e., the object exits the field-of-view). For both directions, we initialize the apperance model of the tracker using the proposal bpsubscriptğ‘ğ‘b_{p}. For the forward tracking, we run the tracker starting from frame p+1ğ‘1p+1 to qâˆ’1ğ‘1q-1 and obtain the tracked regions: ğ’ƒğ’‡=[bÂ¯p+1,â‹¯,bÂ¯e]subscriptğ’ƒğ’‡subscriptÂ¯ğ‘ğ‘1â‹¯subscriptÂ¯ğ‘ğ‘’\\bm{b_{f}}=[\\bar{b}_{p+1},\\cdots,\\bar{b}_{e}]. For the backward tracking, we run the tracking starting from frame pâˆ’1ğ‘1p-1 to 00 and obtain the tracked regions: ğ’ƒğ’ƒ=[bÂ¯s,â‹¯,bÂ¯pâˆ’1]subscriptğ’ƒğ’ƒsubscriptÂ¯ğ‘ğ‘ â‹¯subscriptÂ¯ğ‘ğ‘1\\bm{b_{b}}=[\\bar{b}_{s},\\cdots,\\bar{b}_{p-1}]. We then concatenate ğ’ƒğ’ƒsubscriptğ’ƒğ’ƒ\\bm{b_{b}}, bpsubscriptğ‘ğ‘b_{p}, and ğ’ƒğ’‡subscriptğ’ƒğ’‡\\bm{b_{f}} to obtain the complete response track prediction. We use the KYS trackerÂ , which was shown to achieve state-of-the-art results for single-object tracking. We now discuss the training procedure for the VQ2D baseline. Each datapoint for the VQ2D task (defined on Ego4D videos) consists of the following: video Vğ‘‰V, visual crop image vğ‘£v, query frame number qğ‘q, and response track boxes r={rs,rs+1,â‹¯,re}ğ‘Ÿsubscriptğ‘Ÿğ‘ subscriptğ‘Ÿğ‘ 1â‹¯subscriptğ‘Ÿğ‘’r=\\{r_{s},r_{s+1},\\cdots,r_{e}\\}, where sğ‘ s and eğ‘’e are the start and end frames of rğ‘Ÿr, and risubscriptğ‘Ÿğ‘–r_{i} is a bounding box defined on frame iğ‘–i of video Vğ‘‰V. As a high-level overview, we initialize and freeze the backbone â„±â„±\\mathcal{F} and RPN using weights from an MS-COCO pre-trained Mask-RCNN model. We use the VQ2D annotations to train the SiamHead (ğ’®ğ’®\\mathcal{S}). We initialize and freeze the KYS tracker using weights pre-trained on GOT-10kÂ , LaSOTÂ , and TrackingNetÂ  datasets. We next detail the training procedure for the SiamHead (ğ’®ğ’®\\mathcal{S}). We use a similarity retrieval approach were the model is trained to predict high visual similarity between the visual crop vğ‘£v and positives, and low visual similarity between vğ‘£v and negatives. The loss function for ğ’®ğ’®\\mathcal{S} is a binary cross entropy loss defined over each (v,Dp,Dn)ğ‘£subscriptğ·ğ‘subscriptğ·ğ‘›(v,D_{p},D_{n}) tuple (see Eqn.Â 16), where Dp={pi}i=1|Dp|subscriptğ·ğ‘superscriptsubscriptsubscriptğ‘ğ‘–ğ‘–1subscriptğ·ğ‘D_{p}=\\{p_{i}\\}_{i=1}^{|D_{p}|} are positive detections, Dn={nj}j=1|Dn|subscriptğ·ğ‘›superscriptsubscriptsubscriptğ‘›ğ‘—ğ‘—1subscriptğ·ğ‘›D_{n}=\\{n_{j}\\}_{j=1}^{|D_{n}|} are negative detections, and sx,v=ğ’®â€‹(â„±â€‹(x),â„±â€‹(v))subscriptğ‘ ğ‘¥ğ‘£ğ’®â„±ğ‘¥â„±ğ‘£s_{x,v}=\\mathcal{S}(\\mathcal{F}(x),\\mathcal{F}(v)): Both positives and negatives are defined based on proposals generated by the RPN. Given a visual crop vğ‘£v, a proposal pisubscriptğ‘ğ‘–p_{i} for iâˆˆ(s,e)ğ‘–ğ‘ ğ‘’i\\in(s,e) is a positive if the IoUâ€‹(pi,ri)â‰¥0.5IoUsubscriptğ‘ğ‘–subscriptğ‘Ÿğ‘–0.5\\textrm{IoU}(p_{i},r_{i})\\geq 0.5, where risubscriptğ‘Ÿğ‘–r_{i} is the response track box in frame iğ‘–i. We remove all risubscriptğ‘Ÿğ‘–r_{i} which are too small, or have significantly different aspect ratios from the largest box in rğ‘Ÿr since these typically correspond to obstructed views of the object.\nA proposal pjsubscriptğ‘ğ‘—p_{j} is a negative if it satisfies any of the following two conditions: jâˆˆ(s,e)ğ‘—ğ‘ ğ‘’j\\in(s,e) and IoUâ€‹(pj,rj)<0.5IoUsubscriptğ‘ğ‘—subscriptğ‘Ÿğ‘—0.5\\textrm{IoU}(p_{j},r_{j})<0.5 pjsubscriptğ‘ğ‘—p_{j} is sampled from another video. We also found it beneficial to use hard-negative mining, where we initially sample a large number of negatives and then select the top-K negatives with the highest loss value. \n We employ a few different augmentation strategies to artificially expand the dataset. First, we augment each data sample by replacing the visual crop vğ‘£v by a bounding box risubscriptğ‘Ÿğ‘–r_{i} from the response track. This works because the response track and the visual crop correspond to the same object. Next, we augment the visual crop vğ‘£v by applying random rotations between âˆ’120âˆ˜superscript120-120^{\\circ} to 120âˆ˜superscript120120^{\\circ}. This exploits the fact that objects can have significant viewpoint variations in egocentric videos (unlike internet photos). Finally, we apply a random brightness augmentation to the video frames and the visual crop to simulate differing lighting. We train the SiamHead ğ’®ğ’®\\mathcal{S} using the Detectron2 libraryÂ 5. We use the default configuration file and make the following changes for our experiments. For each experiment, we use 8 GPUs, 64 visual crops per batch, and train for 300,000300000300,000 iterations with an initial learning rate of 0.020.020.02 followed by a 0.1Ã—0.1\\times decay after 200,000200000200,000 iterations. We extract backbone features from the â€œp3â€ layer of FPN. Based on validation performance, we use 666 positives and 646464 negatives for each visual crop. Specifically, we sample 585858 negatives per video frame which results in 58Ã—64=37125864371258\\times 64=3712 negatives per batch. For each visual crop, we sample the 646464 hardest negatives out of 371237123712. In the SiamHead ğ’®ğ’®\\mathcal{S} architecture, the projection module ğ’«ğ’«\\mathcal{P} consists of four residual blocks followed by average pooling, and a 2-layer multi-layer perceptron (MLP) with a hidden size of 102410241024-D and ReLU activation. For signal peak detection, we utilize the find_peaks function from the scipy library121212Peak detection: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html with the following hyperparameters selected through validation: distance = 252525, width = 333, and prominence = 0.20.20.2. We evaluate the performance of multiple baselines on the VQ2D task in Tab.Â 9. The first column in the table shows the detection and tracking methods, and the second column shows the SiamHead projection architecture ğ’«ğ’«\\mathcal{P}. In addition to the KYS tracker, we also experiment with a simple particle filter tracker (denoted â€˜PFâ€™) to assess the impact of the tracking quality. As an ablation of SiamRCNN, we replace the 4 residual blocks in the SiamHead projection module with a simple 3-layer CNN which has lower capacity with no residual connections (indicated by â€˜Simpleâ€™). We make several observations. When we use a simple projection model with a particle filter tracker, we already observe a good validation performance of 32.4%percent32.432.4\\% success, and 0.140.140.14 tAP25. These can be attributed to using a strong proposal generator (RPN pre-trained on MS-COCO) and a learned siamese comparison model. Upon replacing the particle filter tracker with a SoTA KYS trackerÂ , while the validation success rate remains similar at 33.0%percent33.033.0\\%, we observe significant gains (absolute) in all other metrics: 2%percent22\\% tAP, 2%percent22\\% stAP25, and 14.3%percent14.314.3\\% recovery. This suggests that a good tracker is necessary to accurately capture the full response track after localizing a single frame within it. Finally, upon replacing the â€˜Simpleâ€™ siamese projection with 4 residual blocks, we observe a significant gains of 6.8%percent6.86.8\\% in success, 5%percent55\\% in tAP25, 4%percent44\\% in stAP25, and 5%percent55\\% in recovery %percent\\%. This suggests that using a higher capacity model for the SiamHead is helpful for improving the per-frame detection performance for the VQ2D task. We observe similar trends on the test set. Please see Fig.Â 29 for qualitative examples of the modelâ€™s predictions. In all cases from Tab.Â 9, the search efficiency is 0%percent00\\% since the detectors are used on every frame in the search window. In Fig.Â 30 we experiment with two simple techniques for improving the search efficiency. The first approach uniformly subsamples k%percentğ‘˜k\\% of the frames in the search window (denoted as â€˜SSâ€™). The second approach searches over only k%percentğ‘˜k\\% of the most recent frames in the search window, i.e., frames that are nearest to the query (denoted as â€˜Nâ€™). We consider 3 values of kğ‘˜k in both cases: 10%percent1010\\%, 25%percent2525\\%, and 50%percent5050\\%. Consider the results in Fig.Â 30. In both strategies, the search efficiency improves as we reduce kğ‘˜k. The performance drops drastically for the 1st strategy where we subsample the search window, while it remains relatively stable for the second strategy where we preview a fraction of frames closest to the query. For example, we can achieve a search efficiency of 48.0%percent48.048.0\\% with only a 6âˆ’16%6percent166-16\\% relative drop in performance with k=50%ğ‘˜percent50k=50\\% in the 2nd strategy. However, the performance drops significantly if we reduce kğ‘˜k further. For example, we observe a reduction of 38âˆ’60%38percent6038-60\\% for k=10%ğ‘˜percent10k=10\\% with the 2nd strategy. This suggests that more intelligent methods that perform contextual search are needed to improve the search efficiency for VQ2D while maintaining good performance. Next we describe the baseline for the visual query with 3D localization task.\nRecall the task definition: given a video, a query frame, and a visual crop of a target object, the goal is to output a 3D displacement vector from the camera center of the query frame to the center of the target object in 3D. The 3D position of the target object is defined at its most recent appearance in the video. Figure 31 shows a sample of the task. Our baseline strategy has three steps. We first estimate the camera poses of the video. Then we retrieve the most recent instance of the target object in the video. Lastly, we estimate the depth of the detected object and retrieve its 3D position from the query frame. The camera poses are estimated using a keypoint matching strategy along with a Perspective-n-Point (PnP) resolution approach. At a high level our approach consists of the following four steps. First we estimate the camera intrinsic parameters using Structure-from-Motion (SfM). Secondly, we extract and match keypoints from each frame in the video to keypoints extracted from the Matterport3D panoramas. Then, using the matched keypoints we set up and solve a PnP problem for each frame in the video to estimate the corresponding camera pose. Lastly, we refine the poses using temporal constraints. Step 1: Camera intrinsics estimation\nWe start by extracting a set of contiguous non-blurry frames from the video. In order to select non-blurry frames we compute the variance of the Laplacian on each image and select the ones with a value higher than a 100 threshold. We then select the largest contiguous set of non-blurry images. We cap the number of selected frames to 10 to limit the computational time of the SfM module. Once we have selected the images we run the automatic reconstruction module of COLMAP  to estimate the camera instrinsic parameters with a radial fisheye camera model. Step 2: Keypoint extraction and matching\nWe use SuperGlue  to extract and match keypoints.\nWe first extract keypoints from the scan panoramas {k{p,n},pâˆˆğ’«,nâˆˆğ’©}formulae-sequencesubscriptğ‘˜ğ‘ğ‘›ğ‘ğ’«ğ‘›ğ’©\\{k_{\\{p,n\\}},p\\in\\mathcal{P},n\\in\\mathcal{N}\\} where ğ’«ğ’«\\mathcal{P} is the number of panoramas and ğ’©ğ’©\\mathcal{N} is the number of keypoints. The scan panoramas are generated using the Matterport SDK.131313Matterport-SDK: https://matterport.github.io/showcase-sdk/sdk_intersection_inspector.html We render RGB and depth images at each scan position and sweep over pitch values âˆˆ[âˆ’30,30]absent3030\\in[-30,30] with a step size of 5 deg. and yaw values âˆˆ[âˆ’180,180]absent180180\\in[-180,180] with a step size of 15 deg. We generate on average 7K images per scan. Note that while we are not releasing the panoramas because of data anonymization concerns, we are providing the precomputed keypoints.\nSimilarily, we extract keypoints from the video frames {k{i,m},iâˆˆâ„,mâˆˆâ„³}formulae-sequencesubscriptğ‘˜ğ‘–ğ‘šğ‘–â„ğ‘šâ„³\\{k_{\\{i,m\\}},i\\in\\mathcal{I},m\\in\\mathcal{M}\\} where â„â„\\mathcal{I} is the number of images in the video and â„³â„³\\mathcal{M} is the number of keypoints.\nOnce the keypoints are extracted we loop through each frame iâˆˆâ„ğ‘–â„i\\in\\mathcal{I} in the video and match the extracted frame keypoints {k{i,m},mâˆˆâ„³}subscriptğ‘˜ğ‘–ğ‘šğ‘šâ„³\\{k_{\\{i,m\\}},m\\in\\mathcal{M}\\} to all the panoramas keypoints {k{p,n},pâˆˆğ’«,nâˆˆğ’©}formulae-sequencesubscriptğ‘˜ğ‘ğ‘›ğ‘ğ’«ğ‘›ğ’©\\{k_{\\{p,n\\}},p\\in\\mathcal{P},n\\in\\mathcal{N}\\}.\nWe use the pretained models available141414SuperGlue weights: https://github.com/magicleap/SuperGluePretrainedNetwork of SuperPoint  for keypoints and descriptors extraction and SuperGlue  for matching. Step 3: PnP resolution\nWe compute the camera pose for the video frames having at least 20 matched keypoints. We empirically find that a threshold of 20 provides a good tradeoff between the number of overall pose estimates and the quality of the estimations. The positions of the 3D keypoints are computed from a pinhole camera model of the Matterport camera using the rendered panorama depth, camera intrinsics, and camera pose. The positions of the 2D keypoints are directly extracted from the video frames pixels.\nWe then use the OpenCV library to solve the PnP setup and estimate the camera pose from the matched pairs of 3D and 2D points and using the estimated camera intrinsic parameters.\nUsing this method we can estimate the camera pose of roughly 2%percent22\\% of the total number of frames in the video. Next we incorporate temporal constraints to increase this number. Step 4: Temporal constraints and final pose estimation\nTo increase the number of estimates we refine the pose estimation pipeline by incorporating temporal constraints in an iterative procedure. We start by extracting and matching 2D keypoints from localized frames to non-localized ones in the video. This step is similar to the above Step 2; we use the same SuperGlue . Using the matched keypoints and current estimated poses we triangulate new 3D keypoints for the non-localized images.\nWe then solve a new PnP setup with the new keypoints. We apply this procedure iteratively until convergence.\nAfter refinement we achieve a performance of 15%percent1515\\% of pose estimates of the total number of frames accross all video clips. We qualitatively evaluate the camera pose estimation pipeline by rendering the views in the 3D scans. Recall that the scans and videos have been recorded at different times and thus the scenes can contain large differences. Figure 32 shows camera poses estimates where left is the frame from the video, middle is the view from the scan, and right is the superposition. We see that even with large scene differences between the scan and video (e.g., the wheel in the middle example) the algorithm is capable of producing good pose estimates. The remaining unlocalized frames are due to abrupt motion (lost track) and when the view is too close-up to the scene (not enough keypoints matched). We build our solution on top of the visual queries 2D localization baseline. The 2D localization baseline outputs a response track with 2D detections of the target object. Our baseline combines these 2D detections along with depth estimation and camera pose estimation to retrieve the 3D position of the object. We estimate the depth of the most recent frame of the response track for which we have a pose estimate. We use the DPT network  with pretrained weights on NYU_v2 . Figure 33 shows depth estimation results where left is the frame from the video, middle is the estimated depth, and right is the depth from the scan rendered at the estimated viewpoint (not available to the baseline model). Note that due to scene differences between the video and the scan, the two depths frames will differ in some region of the image.\nWe then compute the depth value of the target centroid as the median of a square region centered at the 2D detection. Given the estimated depth dğ‘‘d of the object centroid cğ‘c in frame fğ‘“f of the response track and the estimated camera instrisics Kğ¾K, we construct the 3D vector displacement v^fsubscript^ğ‘£ğ‘“\\hat{v}_{f} in the current frame fğ‘“f coordinate system using a pinhole camera model: where u,vğ‘¢ğ‘£u,v are the pixel indices of the centroid cğ‘c in frame fğ‘“f.\nWe then estimate the object centroid position t^ssubscript^ğ‘¡ğ‘ \\hat{t}_{s} in the scan coordinate system: where Pfssubscriptsuperscriptğ‘ƒğ‘ ğ‘“P^{s}_{f} is the camera pose for the frame fğ‘“f.\nWe further retrieve the displacement vector v^Qsubscript^ğ‘£ğ‘„\\hat{v}_{Q} in the query frame Qğ‘„Q coordinate system: where PQssubscriptsuperscriptğ‘ƒğ‘ ğ‘„P^{s}_{Q} is the camera pose of the query frame. We compare the performance of multiple baselines along with ablation studies. We present the results in Table 10. Numbers are computed on the validation set (164 queries) of the VQ3D task. We report the query ratio QwP, for which we have camera pose estimates for the response track and query frame. Additionally, we report the success rate Succâˆ—superscriptğ‘c^{*} which is the success metric computed only for queries with associated pose estimates. Overall, we notice a low QwP ratio leading to a low success rate. These low metrics are due to a small number of camera pose estimates (15% overall). Nonetheless, we observe that the best VQ2D baseline method combined with the pretrained DPT  depth estimator yields the best performances in terms of Lâ€‹2ğ¿2L2 and success. These numbers tell that there are opportunities for enhancement in designing better camera pose estimators. Additionally, we perform ablation studies using the ground-truth response tracks and different depth estimators (random, from the scan, using DPT). For the random experiment we uniformly sample a depth value between 0.1 and 10 meters. From the ablation experiments we note that rendering the depth from the scan at the estimated viewpoint increases the performances compared to using DPT (lines 2 and 3). This suggests that there is also room for improvement in designing better depth estimators. Since the natural language queries can be seen as a language-grounding problem in a video,\nwe adopt two prior methods in order to implement the baselines for this task. (a) 2D Temporal Adjacent Networks (2D-TAN)[ref]236:\nWe apply 2D-TAN with a sliding window method to implement the natural language query baseline. The goal of 2D-TAN is to answer where the semantically corresponding video moment is, given a language query in an untrimmed video. The language query stems from one of the 13 template questions. The core idea of 2D-TAN is to consider adjacent moment candidates as the temporal context on a two-dimensional temporal map and retrieve the most relevant moment from the candidates. More concretely, 2D-TAN takes each moment candidate as one element in the 2D temporal map such that the adjacent moment candidates on the map can have much-overlapped content or share the same start or end time slot. It applies a convolutional neural network on the 2D map to predict the Intersection over Union of each moment candidate and the ground-truth moment. Please see [ref]236 for more details. Since the 2D-TAN enumerates all the possible combinations of start-end pairs, the Oâ€‹(N2)ğ‘‚superscriptğ‘2O(N^{2}) space complexity of the 2D map leads to a heavy model, especially when we require a precise moment boundary. To make 2D-TAN more appropriate to our problem, we further use a sliding window method on top of 2D-TAN. We break down the clip into a number of overlapping windows, where a window presents a small portion of the clip. The windows are taken as the input of the 2D-TAN model in both training and testing phases. During the training of the 2D-TAN model, we use Ego4Dâ€™s provided pre-extracted features for both the video clip and language query. The clip feature is from a SlowFast  network pretrained on Kinetics 400 dataset, and the language feature is a based on the BERT model . The window duration is 40s, and stride is 20s in the sliding window method. Notably, we only use windows that contain or are next to a ground-truth moment in training, but we use all the windows in testing. We keep all the other hyper-parameters in 2D-TAN the same as its default except for tIoU threshold and learning rate. We decreased the tIoU threshold from 0.5 to 0.3 to enable more positive samples during training and empirically set the learning rate to 0.001.\nWe train the model for 100 epochs and report the test set performance on the best checkpoint on the validation set.\n2D-TAN gives top-1 and top-5 recalls of 5.80%percent5.805.80\\% and 13.90%percent13.9013.90\\% at IoU=0.3, respectively.\nIn addition, we also ablate the model to obtain performance by randomizing the video features\n(âˆ’-visual) and textual features (âˆ’-text) for NLQ in Tab.Â 11. (b) Span-based Localization Network (VSLNet):\nUnlike traditional approaches in video natural language localization works, VSLNet treats the\ninput untrimmed video as a text passage, and uses a span-based approach to identify the relevant\nsections semantically related to the given natural language query.\nAt its core, VSLNet first encodes the natural language query and video features using a common,\nshared Transformer  network.\nNext, it uses the encoded query to then attend to the relevant parts of the video clip\n(akin to a text paragraph).\nThe attended sections are further refined using a query-guided highlighting (QGH) strategy by\nextending the selection foreground of the video by a hyperparamter to capture more visual\ncontext.\nPlease refer to  for more details on the motivation and architecture. For our experiments, we maintain consistency with the other NLQ baselines and use pre-extracted\nfeatures for both the video clip (SlowFast network ) and natural language query\n(BERT ).\nWe use the implementation provided by the authors151515https://github.com/IsaacChanghau/VSLNet with the following changes:\n(a) Set the video features size to 230423042304 dimensions to accommodate the features extracted from\nthe SlowFast network,\n(b) Replace the text encoder to a frozen, pretrained BERT  model,\n(c) Set the internal dimension of the multimodal network to 128128128, and project the pre-trained\nBERT features from 768768768 to 128128128.\nWe train the model for 200200200 epochs and pick the model with the best performance on val split.\nThe corresponding test performance of this VSLNet model is reported in\nTab. 11, along with visual and textual ablations. We formulate a moment queries baseline as a temporal action detection methodÂ [ref]141  [ref]237, plus simple post-processing. The MQ task only expects predictions for the query categories, whereas the temporal action detection task returns the predictions for all categories. Therefore, we can first use a temporal action detection method to predict for all categories, and only output the results corresponding to the query categories. To predict all categories, we adopt a recent method VSGNÂ [ref]237, which was designed for temporal action detection in third-person videos. We use VSGN without the VSS component. FigureÂ 34 illustrates the architecture. It takes a video ğ’±ğ’±\\mathcal{V} as input, extracts features for each snippet in the video using a network such as SlowFastÂ , and feeds these features into a graph pyramid network. The graph pyramid network contains a encoder and a decoder, where the encoder is comprised of multiple levels of graph convolutional networks, and the decoder is comprised of multiple levels of de-convolutional networks. It is an anchor-based method that pre-defines temporal segments for each feature level as prediction reference. It predicts the scores and refines the locations of the anchors in two stages. In the first stage, it uses a region proposal network (RPN) from the decoder to predict class labels and regress boundaries for each anchor; in the second stage, it applies a boundary adjustment module to refine the boundary offsets based on the updated anchors from the first stage. It also has startness/endness predictions to provide auxiliary supervision and supplement scores for each predicted segment. Its output predictions are formulated as Î¦={Ï•m=(tm,s,tm,e,cm,sm)}m=1MÎ¦superscriptsubscriptsubscriptitalic-Ï•ğ‘šsubscriptğ‘¡ğ‘šğ‘ subscriptğ‘¡ğ‘šğ‘’subscriptğ‘ğ‘šsubscriptğ‘ ğ‘šğ‘š1ğ‘€\\Phi=\\left\\{\\phi_{m}=\\left({t}_{m,s},{t}_{m,e},{c}_{m},{s}_{m}\\right)\\right\\}_{m=1}^{M}, where mğ‘šm is the number of predictions, tm,ssubscriptğ‘¡ğ‘šğ‘ {t}_{m,s} and tm,esubscriptğ‘¡ğ‘šğ‘’{t}_{m,e} are start time and end time of the mtâ€‹hsuperscriptğ‘šğ‘¡â„m^{th} prediction respectively, cmsubscriptğ‘ğ‘š{c}_{m} is the predicted category, and smsubscriptğ‘ ğ‘š{s}_{m} is the confidence score. For more details, please refer to [ref]237. Given a query category cğ‘c, the retrieval results for the moment queries task are obtained as follows For feature extraction, we use Ego4Dâ€™s provided pre-extracted features using a SlowFastÂ  network pre-trained on Kinects400Â  at 1.87 features per second. The feature dimension is 2304. Considering that the maximum clip length is 8 minutes, which has 897 features, we make the input length of our network 928 frames to cover the longest video clip. We have 5 levels in the graph pyramid network, each with temporal length 232, 116, 58, 29, and 14 respectively. We pre-define two base anchors of sizes 4 and 12 for Level 1 and increase the sizes by 2 for each deeper layer. We train for 30 epochs with a batch size 32 and learning rate 0.0001. In inference, we only apply per-category NMS with a confidence threshold 0.0005. We show our baseline performance in terms of mAP in TableÂ 12 and recall @ kâ€‹xğ‘˜ğ‘¥{kx}, tIoU=mğ‘š{m} in TableÂ 13. We provide further analysis on the average precision results using DETADÂ . In FigÂ 35, we illustrate the proportion of each error type for the false positive predictions. It shows that both localization and classification are responsible for the false positive, improving either can increase the overall performance by a nontrivial amount. In FigÂ 36, we demonstrate the performance of different groups of moment instances based on moment duration and number of instances belonging to the same category per video clip. We notice that short moments tend to have low performance even though they are large in number. When there are 2-3 instances in one video, they are easiest to detect. Visual queries presents a novel and challenging task for object localization in egocentric videos. While our proposed baseline achieves a reasonable success rate of 42.9%percent42.942.9\\%, it only achieves a localization performance of 0.130.130.13 tAP and 0.060.060.06 stAP. Furthermore, the best performance is achieved with 0%percent00\\% search efficiency, and naÃ¯ve techniques to improve the search efficiency lead to drastic performance reductions. We hope that this task will spur future research into accurate and efficient techniques for object search. Natural language queries is a challenging multimodal task that has wide\napplications in helping users search and retrieve relevant pieces of their\nepisodic memory, thanks to the flexibility of the queries.\nThe performance of the existing state-of-the-art video localization models\nhighlights the needle-in-a-haystack nature of the task, due to shorter response\nwindows of about 10â€‹s10ğ‘ 10s in a large video clip of 888 minutes.\nWe hope that the NLQ dataset opens the door to future research that specializes\nin identifying and retrieving a large diversity of language queries in longer\negocentric video clips, moving a step closer to augmenting a userâ€™s episodic memory. Moment queries in egocentric videos is a challenging task due to the long-tailed distribution of categories and the large variation in moment duration. Our baseline achieves a reasonable result according to the metric recall @kâ€‹xğ‘˜ğ‘¥kx, tIoU=mğ‘šm, which evaluates the performance of each query category and does not require correct classification of all categories. In contrast, its average mAP score of 5.96%percent5.965.96\\% is low when all categories are evaluated. According to the false positive analysis in FigÂ 36, errors caused by wrong labels are significant. A more sophisticated classifier for all candidate moments can be explored in future work. In addition, as shown in FigÂ 36, the performance of short moments, which occupy a large proportion in the dataset, is not as good as that of long moments. Therefore, improving short moments will significantly improve the overall performance. Kristen Grauman led the Episodic Memory benchmark and paper writing, wrote annotation instructions, contributed to data selection and taxonomy formation, and co-advised the VQ baseline development. Bernard Ghanem co-led the Episodic Memory benchmark, managed baseline development and evaluation for the MQ and NLQ tasks, and contributed to the annotation instructions, data selection, and taxonomy formation for the MQ and NLQ datasets. Jackson Hamburger contributed to the development of the annotation instructions and taxonomies of the NLQ, VQ, and MQ datasets along with the design of the early VQ baselines. Santhosh Kumar Ramakrishnan led VQ data selection, annotation, analysis and auditing, contributed to the formulation and annotation instructions of VQ, data selection, and implemented the VQ baseline.\nVince Cartillier contributed to the VQ-3D formulation and annotation instructions, led VQ-3D data selection, annotation, analysis and auditing, and implemented the VQ-3D baseline. Dhruv Batra co-mentored Vince Cartillier on developing baselines and provided guidance on 3D scans using Matterport. Hyun Soo Park contributed to 3D reconstruction of egocentric videos with respect to 3D Matterport scans. Tien Do developed algorithms to reconstruct 3D egocentric camera poses with respect to 3D Matterport scans. James Hillis provided background knowledge on human episodic memory function and contributed to early discussions on benchmark definition and annotation.\nSatwik Kottur led the design of NLQ data selection and annotation instructions, contributed to the NLQ task formulation, coordinated NLQ data annotation and data analysis, implemented the VSLNet NLQ baseline, wrote part of the NLQ sections. Menmeng Xu designed and implemented the experiment pipeline for the NLQ task, implemented several NLQ methods, did NLQ result analysis and visualization, and wrote part of the NLQ sections.\nMichael Wray contributed to early formulation of the benchmark tasks, definitions of NLQ queries and annotation instructions, provided input for dataset construction and evaluation metrics, and helped in the creation of the MQ taxonomy. Chen Zhao designed and implemented the MQ baseline, proposed and implemented the new metric for MQ, wrote the MQ sections, did MQ result analysis and visualization, contributed to the formulation, data selection and annotation instructions of MQ. Tushar Nagarajan contributed to the MQ formulation and annotation instructions, developed the MQ label taxonomy, and led the data selection and annotation of the MQ dataset. Merey Ramazanova managed the datasets for the experiments of MQ and NLQ baselines, and assisted with the taxonomy formation for the MQ baseline. Antonino Furnari provided keypoint feature extraction from the Matterport3D panoramas for the VQ3D baseline.\n\nReference markers found: [141]\n\nContext windows for each [ref]:\n[ref]141: come up with a sentence to describe what he/she wants, but instead can directly choose among the pre-defined categories. The moment queries task is related to the task of temporal action detectionÂ [ref]141  [ref]237, which aims to identify and localize all instances of all action categories that take place in a video. Both tasks have a list of action categories pre-defined, and both aim to\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 141,\n            \"titles\": [\n                \"Feature Pyramid Networks for Object Detection\",\n                \"Github repository of the ESPNet model zoo.\",\n                \"Kaldi English GLM file.\",\n                \"NIST SRE 2000 Evaluation Plan.\",\n                \"When will you do what? - Anticipating Temporal Occurrences of Activities\",\n                \"Deep Audio-Visual Speech Recognition\",\n                \"The Conversation: Deep Audio-Visual Speech Enhancement\",\n                \"Self-Supervised Learning of Audio-Visual Objects from Video\",\n                \"Joint Discovery of Object States and Manipulation Actions\",\n                \"Diagnosing Error in Temporal Action Detectors\",\n                \"Speaker diarization: A review of recent research.\",\n                \"Robust speaker diarization for meetings.\",\n                \"VQA: Visual Question Answering.\",\n                \"Multi-modal egocentric activity recognition using audio-visual features.\",\n                \"Objects that Sound\",\n                \"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\",\n                \"Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions.\",\n                \"The cocktail party problem: what is it? how can it be solved? and why should animal behaviorists study it?\",\n                \"Multiple object tracking performance metrics and evaluation in a smart room environment.\",\n                \"Evaluating multiple object tracking performance: the clear mot metrics.\",\n                \"First-Person Action-Object Detection with EgoNet\",\n                \"Prediction of the leadership style of an emergent leader using audio and visual nonverbal features.\",\n                \"Know Your Surroundings: Exploiting Scene Information for Object Tracking.\",\n                \"Learning 6d object pose estimation using 3d object coordinates.\",\n                \"Language Models are Few-Shot Learners\",\n                \"The yale human grasping dataset: Grasp, object, and task data in household and machine shop environments.\",\n                \"Understanding hand-object manipulation with grasp types and object attributes.\",\n                \"End-to-end object detection with transformers.\",\n                \"The AMI meeting corpus: A pre-announcement.\",\n                \"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\",\n                \"Procedure Planning in Instructional Videos\",\n                \"AVA-Speech: A Densely Labeled Dataset of Speech Activity in Movies\",\n                \"SoundSpaces: Audio-Visual Navigation in 3D Environments\",\n                \"Audio-visual embodied navigation.\",\n                \"GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio\",\n                \"Microsoft coco captions: Data collection and evaluation server.\",\n                \"Detection of eye contact with deep neural networks is as accurate as human experts.\",\n                \"Detecting Attended Visual Targets in Video\",\n                \"In defence of metric learning for speaker recognition\",\n                \"Spot the conversation: speaker diarisation in the wild\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]141 based on the context and research area of Ego4D: Around the World in 3,000 Hours of Egocentric Video. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}