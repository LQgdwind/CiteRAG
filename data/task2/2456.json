{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Learning to Adversarially Blur Visual Object Tracking\n\nPaper Abstract: \n\nPaper Section Text:\nTo allow efficient adversarial blur attack, we propose to predict the motion and accumulation weights (i.e., 𝒲tsubscript𝒲𝑡\\mathcal{W}_{t} and 𝒜tsubscript𝒜𝑡\\mathcal{A}_{t}) with a newly designed network denoted as joint adversarial motion and accumulation predictive network (JAMANet) in a one-step way, which is pre-trained through the objective function Eq. (3.2) and a naturalness-aware loss function.\nSpecifically, we use JAMANet to process the neighboring frames (i.e., 𝐈tsubscript𝐈𝑡\\mathbf{I}_{t} and 𝐈t−1subscript𝐈𝑡1\\mathbf{I}_{t-1}) and predict the 𝒲tsubscript𝒲𝑡\\mathcal{W}_{t} and 𝒜tsubscript𝒜𝑡\\mathcal{A}_{t}, respectively.\nMeanwhile, we also employ a pre-trained network to estimate the optical flow 𝐔tsubscript𝐔𝑡\\mathbf{U}_{t} between 𝐈tsubscript𝐈𝑡\\mathbf{I}_{t} and 𝐈t−1subscript𝐈𝑡1\\mathbf{I}_{t-1}. Here, we use the PWCNet [ref]43 since it achieves good results on diverse scenes.\nThen, with Eq. (2)-(3), we can obtain the motion-blurred frame 𝐈^tsubscript^𝐈𝑡\\hat{\\mathbf{I}}_{t}.\nAfter that, we feed 𝐈^tsubscript^𝐈𝑡\\hat{\\mathbf{I}}_{t} into the loss functions and calculate gradients of parameters of JAMANet to perform optimization.\nWe show the framework in Fig. 3. Architecture of JAMANet.\nWe first build two parameter sets with constant values, which are denoted as 𝒜norm={𝐀normi}subscript𝒜normsuperscriptsubscript𝐀norm𝑖\\mathcal{A}_{\\text{norm}}=\\{\\mathbf{A}_{\\text{norm}}^{i}\\} and 𝒲norm={𝐖normi}subscript𝒲normsuperscriptsubscript𝐖norm𝑖\\mathcal{W}_{\\text{norm}}=\\{\\mathbf{W}_{\\text{norm}}^{i}\\}.\nAll elements in 𝒜normsubscript𝒜norm\\mathcal{A}_{\\text{norm}} and 𝒲normsubscript𝒲norm\\mathcal{W}_{\\text{norm}} are fixed as 1N1𝑁\\frac{1}{N} and 1N−11𝑁1\\frac{1}{N-1}, respectively.\nWe then use 𝒲normsubscript𝒲norm\\mathcal{W}_{\\text{norm}}, 𝐈t−1subscript𝐈𝑡1\\mathbf{I}_{t-1}, and 𝐈tsubscript𝐈𝑡\\mathbf{I}_{t} to generate N𝑁N instant images through Eq. (2).\nJAMANet is built based on the U-Net architecture  but contains two decoder branches, which is fed with the N𝑁N instant images {𝐈ti}i=1Nsuperscriptsubscriptsuperscriptsubscript𝐈𝑡𝑖𝑖1𝑁\\{\\mathbf{I}_{t}^{i}\\}_{i=1}^{N} and outputs the offsets w.r.t. 𝒲normsubscript𝒲norm\\mathcal{W}_{\\text{norm}} and 𝒜normsubscript𝒜norm\\mathcal{A}_{\\text{norm}}. We name them as 𝒲offsubscript𝒲off\\mathcal{W}_{\\text{off}} and 𝒜offsubscript𝒜off\\mathcal{A}_{\\text{off}}.\nThe input {𝐈ti}i=1Nsuperscriptsubscriptsuperscriptsubscript𝐈𝑡𝑖𝑖1𝑁\\{\\mathbf{I}_{t}^{i}\\}_{i=1}^{N} is size of (N,3,H,W)𝑁3𝐻𝑊(N,3,H,W).\nWe resize it to (1,3​N,H,W)13𝑁𝐻𝑊(1,3N,H,W) and normalize the values to the range of -1 to 1.\nThe architecture is a fully convolutional encoder/decoder model with skip connections. In encoder stage, we use six convolutions with the kernel size 4x4 and the LeakyReLU  activation function. Unlike the standard U-Net, JAMANet has two decoders. Specifically, one branch is set to estimate the 𝒜offsubscript𝒜off\\mathcal{A}_{\\text{off}}, containing six transposed convolutions  with the latest activation function as TanhTanh\\mathrm{Tanh}.\nWe can calculate the final 𝒜tsubscript𝒜𝑡\\mathcal{A}_{t} through 𝒜t=𝒜norm+𝒜offsubscript𝒜𝑡subscript𝒜normsubscript𝒜off\\mathcal{A}_{t}=\\mathcal{A}_{\\text{norm}}+\\mathcal{A}_{\\text{off}}.\nAnother branch is to predict 𝒲offsubscript𝒲off\\mathcal{W}_{\\text{off}} and get 𝒲t=𝒲norm+𝒲offsubscript𝒲𝑡subscript𝒲normsubscript𝒲off\\mathcal{W}_{t}=\\mathcal{W}_{\\text{norm}}+\\mathcal{W}_{\\text{off}}. This architecture is the same with the previous one but following a SoftmaxSoftmax\\mathrm{Softmax} for catering to constraints of Eq. (3.2)111To let 𝒜tsubscript𝒜𝑡\\mathcal{A}_{t} also meet the constraints, for any pixel 𝐩𝐩\\mathbf{p}, we first select the element j=arg​mini,i∈[1,N]⁡𝐀offi​[𝐩]𝑗subscriptargmin𝑖𝑖1𝑁superscriptsubscript𝐀off𝑖delimited-[]𝐩j=\\operatornamewithlimits{arg\\,min}_{i,i\\in[1,N]}{\\mathbf{A}_{\\text{off}}^{i}[\\mathbf{p}]} and then set 𝐀tj​[𝐩]=1−∑i,i≠jN𝐀ti​[𝐩]superscriptsubscript𝐀𝑡𝑗delimited-[]𝐩1superscriptsubscript𝑖𝑖𝑗𝑁superscriptsubscript𝐀𝑡𝑖delimited-[]𝐩\\mathbf{A}_{t}^{j}[\\mathbf{p}]=1-\\sum_{i,i\\neq j}^{N}\\mathbf{A}_{t}^{i}[\\mathbf{p}]. . Loss functions. We train the JAMANet with two loss functions: where the first loss function, i.e., ℒadvsubscriptℒadv\\mathcal{L}_{\\text{adv}}, is set to the objective function in Eq. (3.2) to make sure the background content instead of the object be highlighted.\nNote that, this loss function means to enhance the capability of adversarial attack, that is, misleading the raw trackers.\nIt, however, neglects the naturalness of adversarial blur.\nTo this end, we set the loss function ℒnaturalsubscriptℒnatural\\mathcal{L}_{\\text{natural}} as This loss function encourages the estimated accumulation parameters to be similar to the normal ones, leading to natural motion blur. Training details. We use GOT-10K [ref]26 as our training\ndataset, which includes 10,000+ sequences and 500+ object classes. For each video in GOT-10K [ref]26, we set the first frame as template and take two adjacent frames as an image pair, i.e., (𝐈t−1,𝐈t)subscript𝐈𝑡1subscript𝐈𝑡(\\mathbf{I}_{t-1},\\mathbf{I}_{t}). We select eight image pairs from each video. The template and two adjacent frames make up a training sample.\nHere, we implement the OS-ABA for attacking two trackers, i.e., SiamRPN++  with ResNet50 and MobileNetv2, respectively. In the experiment, we show that OS-ABA has strong transferability against other state-of-the-art trackers.\nDuring the training iteration, we first calculate the template’s embedding to construct tracking model ϕθtsubscriptitalic-ϕsubscript𝜃𝑡\\phi_{\\theta_{t}} and the original response map 𝐘tsubscript𝐘𝑡\\mathbf{Y}_{t} (i.e., the positive activation map of SiamRPN++). Then, we get 𝐘t∗subscriptsuperscript𝐘𝑡\\mathbf{Y}^{*}_{t} and initialize the blurred frame via Blur​(𝐈t,𝐈^t−1)Blursubscript𝐈𝑡subscript^𝐈𝑡1\\text{Blur}(\\mathbf{I}_{t},\\hat{\\mathbf{I}}_{t-1}).\nWe can calculate the loss via Eq. (5) and obtain the gradients of the JAMANet via backpropagation for parameter updating.\nWe train the JAMANet for 10 epochs, requiring a total of about 9 hours on a single Nvidia RTX 2080Ti GPU. We use the Adam  with the learning rate of 0.0002 to optimize network parameters, and the loss weight λ𝜆\\lambda equals 0.001.\n\nReference markers found: [43]\n\nContext windows for each [ref]:\n[ref]43: we also employ a pre-trained network to estimate the optical flow 𝐔tsubscript𝐔𝑡\\mathbf{U}_{t} between 𝐈tsubscript𝐈𝑡\\mathbf{I}_{t} and 𝐈t−1subscript𝐈𝑡1\\mathbf{I}_{t-1}. Here, we use the PWCNet [ref]43 since it achieves good results on diverse scenes.\nThen, with Eq. (2)-(3), we can obtain the motion-blurred frame 𝐈^tsubscript^𝐈𝑡\\hat{\\mathbf{I}}_{t}.\nAfter that, we feed\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 43,\n            \"titles\": [\n                \"BIAS FIELD POSES A THREAT TO DNN-BASED X-RAY RECOGNITION\",\n                \"Recent advances in augmented reality\",\n                \"Fully-convolutional siamese networks for object tracking\",\n                \"Learning discriminative model prediction for tracking\",\n                \"Know your surroundings: Exploiting scene information for object tracking\",\n                \"Learning to Synthesize Motion Blur\",\n                \"Towards evaluating the robustness of neural networks\",\n                \"One-shot adversarial attacks on visual tracking with dual attention\",\n                \"Pasadena: Perceptually aware and stealthy adversarial denoise attack\",\n                \"High-Performance Long-Term Tracking with Meta-Updater\",\n                \"Probabilistic Regression for Visual Tracking\",\n                \"Triplet loss in siamese network for object tracking\",\n                \"Boosting Adversarial Attacks with Momentum\",\n                \"LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking\",\n                \"Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking\",\n                \"Making images undiscoverable from co-saliency detection\",\n                \"Explaining and Harnessing Adversarial Examples\",\n                \"Exploring the effects of blur and deblurring to visual object tracking\",\n                \"Learning dynamic Siamese network for visual object tracking\",\n                \"Selective spatial regularization by reinforcement learned decision making for object tracking\",\n                \"Watch out! motion is blurring the vision of your deep neural networks\",\n                \"SPARK: Spatial-aware Online Incremental Attack Against Visual Tracking\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Mobilenets: Efficient convolutional neural networks for mobile vision applications\",\n                \"GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild\",\n                \"FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\",\n                \"Adam: A method for stochastic optimization\",\n                \"The sixth visual object tracking vot2018 challenge results\",\n                \"The visual object tracking vot2015 challenge results\",\n                \"Siamrpn++: Evolution of siamese visual tracking with very deep networks\",\n                \"High performance visual tracking with siamese region proposal network\",\n                \"TEA: Temporal Excitation and Aggregation for Action Recognition\",\n                \"Rectifier nonlinearities improve neural network acoustic models\",\n                \"No-reference image quality assessment in the spatial domain\",\n                \"A benchmark and simulator for uav tracking\",\n                \"Multi-Modal Domain Adaptation for Fine-Grained Action Recognition\",\n                \"Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring\",\n                \"Motion blur rendering: State of the art\",\n                \"Motion Deblurring in the Wild\",\n                \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]43 based on the context and research area of Learning to Adversarially Blur Visual Object Tracking. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}