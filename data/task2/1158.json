{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Steady State Analysis of Episodic Reinforcement Learning\n\nPaper Abstract: \n\nPaper Section Text:\nMarkov chain.\nIn this paper, a markov chain is a homogeneous discrete-time stochastic process with countable (finite or infinite) state space. The state-transition probabilities are written into a transition matrix Mğ‘€M, where Mâ€‹(s,sâ€²)ğ‘€ğ‘ superscriptğ‘ â€²M(s,s^{\\prime}) is the entry of row sğ‘ s and column sâ€²superscriptğ‘ â€²s^{\\prime} which specifies â„™[st+1=sâ€²|st=s]â„™subscriptğ‘ ğ‘¡1conditionalsuperscriptğ‘ â€²subscriptğ‘ ğ‘¡ğ‘ \\operatorname*{\\mathbb{P}}[s_{t+1}=s^{\\prime}|s_{t}=s]. A rollout of Mğ‘€M generates a trajectory Î¶=(s0,s1,s2,â€¦)ğœsubscriptğ‘ 0subscriptğ‘ 1subscriptğ‘ 2â€¦\\zeta=(s_{0},s_{1},s_{2},\\dots) of infinite length. Let row vector Ï(t)superscriptğœŒğ‘¡\\rho^{\\scriptscriptstyle(t)} denote the marginal distribution of the state at time tğ‘¡t, so stâˆ¼Ï(t)=Ï(tâˆ’1)â€‹Msimilar-tosubscriptğ‘ ğ‘¡superscriptğœŒğ‘¡superscriptğœŒğ‘¡1ğ‘€s_{t}\\sim\\rho^{\\scriptscriptstyle(t)}=\\rho^{\\scriptscriptstyle(t-1)}M. The limiting distribution of stsubscriptğ‘ ğ‘¡s_{t}, if exists, is the marginal distribution at infinite time, i.e. limtâ†’âˆÏ(t)subscriptâ†’ğ‘¡superscriptğœŒğ‘¡\\lim_{t\\rightarrow\\infty}\\rho^{\\scriptscriptstyle(t)}, and the stationary (or steady-state) distribution is defined as the fixed-point distribution with Ï=Ïâ€‹MğœŒğœŒğ‘€\\rho=\\rho M. The existence and uniqueness of stationary and limiting distributions are characterized by the following well-known concepts and conditions: Given a markov chain with transition matrix Mğ‘€M, a state sğ‘ s is reachable from a state sÂ¯Â¯ğ‘ \\bar{s} if â„™[st=s|s0=sÂ¯]>0â„™subscriptğ‘ ğ‘¡conditionalğ‘ subscriptğ‘ 0Â¯ğ‘ 0\\operatorname*{\\mathbb{P}}[s_{t}=s|s_{0}=\\bar{s}]>0 for some tâ‰¥1ğ‘¡1t\\geq 1, and the markov chain Mğ‘€M is said irreducible if every state is reachable from any state in Mğ‘€M. The mean recurrence time of a state sğ‘ s is the expected number of steps for sğ‘ s to reach itself, denoted as ğ”¼[Ts]â‰âˆ‘t=1âˆtâ‹…â„™[st=sâ€‹Â andÂ â€‹sâˆ‰{s1:tâˆ’1}|s0=s]approaches-limitsubscriptğ”¼absentsubscriptğ‘‡ğ‘ superscriptsubscriptğ‘¡1â‹…ğ‘¡â„™subscriptğ‘ ğ‘¡ğ‘ Â andÂ ğ‘ conditionalsubscriptğ‘ :1ğ‘¡1subscriptğ‘ 0ğ‘ \\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}\\end{subarray}}[T_{s}]\\doteq\\sum_{t=1}^{\\infty}t\\cdot\\operatorname*{\\mathbb{P}}[s_{t}=s\\texttt{~{}and~{}}s\\not\\in\\{s_{1:t-1}\\}|s_{0}=s], and state sğ‘ s is said positive recurrent if ğ”¼[Ts]<âˆsubscriptğ”¼absentsubscriptğ‘‡ğ‘ \\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}\\end{subarray}}[T_{s}]<\\infty. The markov chain Mğ‘€M is positive recurrent if every state in Mğ‘€M is positive recurrent. Finally, a state sğ‘ s is aperiodic if sğ‘ s can reach itself in two trajectories with co-prime lengths, i.e. if gcdâ¡{t>0:â„™[st=s|s0=s]>0}=1:ğ‘¡0â„™subscriptğ‘ ğ‘¡conditionalğ‘ subscriptğ‘ 0ğ‘ 01\\gcd\\{t>0:\\operatorname*{\\mathbb{P}}[s_{t}=s|s_{0}=s]>0\\}=1. An irreducible markov chain Mğ‘€M has a unique stationary distribution ÏM=1/ğ”¼[Ts]subscriptğœŒğ‘€1subscriptğ”¼absentsubscriptğ‘‡ğ‘ \\rho_{\\scriptscriptstyle M}=1/\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}\\end{subarray}}[T_{s}] if and only if Mğ‘€M is positive recurrent.\n(, Theorem 54) An irreducible and positive-recurrent markov chain Mğ‘€M has a limiting distribution limtâ†’âˆÏ(t)=ÏMsubscriptâ†’ğ‘¡superscriptğœŒğ‘¡subscriptğœŒğ‘€\\lim\\limits_{t\\rightarrow\\infty}\\rho^{\\scriptscriptstyle(t)}=\\rho_{\\scriptscriptstyle M} if and only if there exists one aperiodic state in Mğ‘€M.\n(, Theorem 59) A markov chain satisfying the condition in Proposition 2 is called an ergodic markov chain. Markov Decision Process (MDP).\nAn MDP â„³=(SS,ğ’œ,R,P,Ï0)â„³SSğ’œğ‘…ğ‘ƒsubscriptğœŒ0\\mathcal{M}=(\\SS,\\mathcal{A},R,P,\\rho_{0}) is a sequential decision making model where SSSS\\SS is a countable state space, ğ’œğ’œ\\mathcal{A} a countable action space, Râ€‹(s)âˆˆ[rmâ€‹iâ€‹n,rmâ€‹aâ€‹x]ğ‘…ğ‘ subscriptğ‘Ÿğ‘šğ‘–ğ‘›subscriptğ‘Ÿğ‘šğ‘ğ‘¥R(s)\\in[r_{min},r_{max}] is a reward assigned to each state sâˆˆSSğ‘ SSs\\in\\SS, Pâ€‹(sâ€²|s,a)â‰â„™[st+1=sâ€²|st=s,at=a]approaches-limitğ‘ƒconditionalsuperscriptğ‘ â€²ğ‘ ğ‘â„™subscriptğ‘ ğ‘¡1conditionalsuperscriptğ‘ â€²subscriptğ‘ ğ‘¡ğ‘ subscriptğ‘ğ‘¡ğ‘P(s^{\\prime}|s,a)\\doteq\\operatorname*{\\mathbb{P}}[s_{t+1}=s^{\\prime}|s_{t}=s,a_{t}=a] specifies action-conditioned transition probabilities between states, and Ï0subscriptğœŒ0\\rho_{0} is the initial distribution with s0âˆ¼Ï0similar-tosubscriptğ‘ 0subscriptğœŒ0s_{0}\\sim\\rho_{0}. A policy function Ï€:SSÃ—ğ’œâ†’ :ğœ‹â†’SSğ’œ01\\pi:\\SS\\times\\mathcal{A}\\to  prescribes the probability to take action under each state. With â„™[at=a|st=s]=Ï€â€‹(s,a)â„™subscriptğ‘ğ‘¡conditionalğ‘subscriptğ‘ ğ‘¡ğ‘ ğœ‹ğ‘ ğ‘\\operatorname*{\\mathbb{P}}[a_{t}=a|s_{t}=s]=\\pi(s,a), every policy Ï€ğœ‹\\pi induces a markov chain with transition matrix MÏ€â€‹(s,sâ€²)=âˆ‘aÏ€â€‹(s,a)â€‹Pâ€‹(sâ€²|s,a)subscriptğ‘€ğœ‹ğ‘ superscriptğ‘ â€²subscriptğ‘ğœ‹ğ‘ ğ‘ğ‘ƒconditionalsuperscriptğ‘ â€²ğ‘ ğ‘M_{\\pi}(s,s^{\\prime})=\\sum_{a}\\pi(s,a)P(s^{\\prime}|s,a). Accordingly, row vectors ÏÏ€(t)subscriptsuperscriptğœŒğ‘¡ğœ‹\\rho^{\\scriptscriptstyle(t)}_{\\pi} and ÏÏ€subscriptğœŒğœ‹\\rho_{\\pi} denote the marginal and stationary distributions of stsubscriptğ‘ ğ‘¡s_{t} under policy Ï€ğœ‹\\pi, respectively, so ÏÏ€(t+1)=ÏÏ€(t)â€‹MÏ€subscriptsuperscriptğœŒğ‘¡1ğœ‹subscriptsuperscriptğœŒğ‘¡ğœ‹subscriptğ‘€ğœ‹\\rho^{\\scriptscriptstyle(t+1)}_{\\pi}=\\rho^{\\scriptscriptstyle(t)}_{\\pi}M_{\\pi}, and ÏÏ€=ÏÏ€â€‹MÏ€subscriptğœŒğœ‹subscriptğœŒğœ‹subscriptğ‘€ğœ‹\\rho_{\\pi}=\\rho_{\\pi}M_{\\pi}. An MDP is ergodic if for every policy Ï€ğœ‹\\pi the induced markov chain MÏ€subscriptğ‘€ğœ‹M_{\\pi} is ergodic, in which case the steady-state performanceÂ  of a policy Ï€ğœ‹\\pi is Jaâ€‹vâ€‹gâ€‹(Ï€)â‰limTâ†’âˆ1Tâ€‹âˆ‘t=1TRâ€‹(st)=ğ”¼sâˆ¼ÏÏ€[Râ€‹(s)]approaches-limitsubscriptğ½ğ‘ğ‘£ğ‘”ğœ‹subscriptâ†’ğ‘‡1ğ‘‡superscriptsubscriptğ‘¡1ğ‘‡ğ‘…subscriptğ‘ ğ‘¡subscriptğ”¼similar-toğ‘ subscriptğœŒğœ‹ğ‘…ğ‘ J_{avg}(\\pi)\\doteq\\lim\\limits_{T\\rightarrow\\infty}\\frac{1}{T}\\sum_{t=1}^{T}R(s_{t})=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\pi}\\end{subarray}}[R(s)].\nWhen a set of terminal states SSâŸ‚âŠ‚SSsubscriptSSperpendicular-toSS\\SS_{\\perp}\\subset\\SS is identified, a rollout trajectory Î¶ğœ\\zeta is said terminated when it reaches a terminal state. We use Tâ€‹(Î¶)â‰inf{tâ‰¥1:stâˆˆSSâŸ‚}approaches-limitğ‘‡ğœinfimumconditional-setğ‘¡1subscriptğ‘ ğ‘¡subscriptSSperpendicular-toT(\\zeta)\\doteq\\inf\\{t\\geq 1:s_{t}\\in\\SS_{\\perp}\\} to denote the termination time of Î¶ğœ\\zeta,\nand the episode-wise performance of a policy Ï€ğœ‹\\pi is defined as Jeâ€‹pâ€‹iâ€‹(Ï€)â‰ğ”¼Î¶âˆ¼MÏ€[âˆ‘t=1Tâ€‹(Î¶)Râ€‹(st)]approaches-limitsubscriptğ½ğ‘’ğ‘ğ‘–ğœ‹subscriptğ”¼similar-toğœsubscriptğ‘€ğœ‹superscriptsubscriptğ‘¡1ğ‘‡ğœğ‘…subscriptğ‘ ğ‘¡J_{epi}(\\pi)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}\\zeta\\sim M_{\\pi}\\end{subarray}}[\\sum_{t=1}^{T(\\zeta)}R(s_{t})]. A value function Q:SSÃ—ğ’œâ†’â„:ğ‘„â†’SSğ’œâ„Q:\\SS\\times\\mathcal{A}\\to\\mathbb{R} prescribes the â€œvalueâ€ of taking an action under a state.\nIn this paper, we consider the following product-form family of value functionsÂ        : in which Pâ€‹(s,a)ğ‘ƒğ‘ ğ‘P(s,a) and Ï€â€‹(sâ€²)ğœ‹superscriptğ‘ â€²\\pi(s^{\\prime}) are short-hands for the conditional distributions P(â‹…|s,a)P(\\cdot|s,a) and Ï€(â‹…|sâ€²)\\pi(\\cdot|s^{\\prime}), and Î³:SSâ†’ :ğ›¾â†’SS01\\gamma:\\SS\\to  is called the discounting function. Besides entailing the Bellman equation (2), the definition of QÏ€subscriptğ‘„ğœ‹Q_{\\pi}, i.e. (1), also induces the state-value function VÏ€â€‹(s)â‰ğ”¼aâˆ¼Ï€â€‹(s)[QÏ€â€‹(s,a)]approaches-limitsubscriptğ‘‰ğœ‹ğ‘ subscriptğ”¼similar-toğ‘ğœ‹ğ‘ subscriptğ‘„ğœ‹ğ‘ ğ‘V_{\\pi}(s)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}a\\sim\\pi(s)\\end{subarray}}[Q_{\\pi}(s,a)]. With Î³â€‹(s)=Î³cğ›¾ğ‘ subscriptğ›¾ğ‘\\gamma(s)=\\gamma_{c} for constant Î³câˆˆ[0,1)subscriptğ›¾ğ‘01\\gamma_{c}\\in[0,1), the product-form value functions QÏ€subscriptğ‘„ğœ‹Q_{\\pi} and VÏ€subscriptğ‘‰ğœ‹V_{\\pi} subsume the classic value functions that have been underlying much of the existing RL literature, and we will use QÏ€Î³csubscriptsuperscriptğ‘„subscriptğ›¾ğ‘ğœ‹Q^{\\scriptscriptstyle\\gamma_{c}}_{\\pi} and VÏ€Î³csubscriptsuperscriptğ‘‰subscriptğ›¾ğ‘ğœ‹V^{\\scriptscriptstyle\\gamma_{c}}_{\\pi} to refer to this particular form of value functions with constant discounting function.\nOn the other hand, with Î³â€‹(s)=ğŸ™â€‹(sâˆ‰SSâŸ‚)ğ›¾ğ‘ 1ğ‘ subscriptSSperpendicular-to\\gamma(s)=\\mathds{1}(s\\not\\in\\SS_{\\perp}), the episode-wise performance Jeâ€‹pâ€‹isubscriptğ½ğ‘’ğ‘ğ‘–J_{epi} can be recast as the product-form value of the initial stateÂ : Jeâ€‹pâ€‹iâ€‹(Ï€)=VÏ€â€‹(s0)subscriptğ½ğ‘’ğ‘ğ‘–ğœ‹subscriptğ‘‰ğœ‹subscriptğ‘ 0J_{epi}(\\pi)=V_{\\pi}(s_{0}) if Î³â€‹(s)=1ğ›¾ğ‘ 1\\gamma(s)=1 for sâˆ‰SSâŸ‚ğ‘ subscriptSSperpendicular-tos\\not\\in\\SS_{\\perp} and Î³â€‹(s)=0ğ›¾ğ‘ 0\\gamma(s)=0 for sâˆˆSSâŸ‚ğ‘ subscriptSSperpendicular-tos\\in\\SS_{\\perp}. We will call the function Î³â€‹(s)=ğŸ™â€‹(sâˆ‰SSâŸ‚)ğ›¾ğ‘ 1ğ‘ subscriptSSperpendicular-to\\gamma(s)=\\mathds{1}(s\\not\\in\\SS_{\\perp}), the episodic discounting function. See Appendix A for more discussions on the formulation choices behind the MDP model. Reinforcement Learning (RL).\nIn continual RL, we are given a decision task modeled by an ergodic MDP â„³Dsubscriptâ„³ğ·\\mathcal{M}_{D}, and the goal is to find good policy with respect to the steady-state performance Jaâ€‹vâ€‹gsubscriptğ½ğ‘ğ‘£ğ‘”J_{avg} through a single continual rollout of â„³Dsubscriptâ„³ğ·\\mathcal{M}_{D}. In episodic RL, the decision task â„³Dsubscriptâ„³ğ·\\mathcal{M}_{D} is a finite-horizon MDP with terminal states, and the goal is to optimize the episode-wise performance Jeâ€‹pâ€‹isubscriptğ½ğ‘’ğ‘ğ‘–J_{epi} through repeatedly rolling out â„³Dsubscriptâ„³ğ·\\mathcal{M}_{D} from the beginning. In this case, a special resetting procedure will intervene the learning process to start a new decision episode upon the end of the lastÂ . A common and basic idea, for both continual and episodic RL algorithms, is to use the rollout data to optimize a parameterized policy function Ï€â€‹(s,a;Î¸)ğœ‹ğ‘ ğ‘ğœƒ\\pi(s,a;\\theta) with respect to a surrogate objective J~â€‹(Î¸)~ğ½ğœƒ\\tilde{J}(\\theta) via the stochastic gradient method. At a update time tğ‘¡t, the gradient âˆ‡Î¸J~subscriptâˆ‡ğœƒ~ğ½\\nabla_{\\theta}\\tilde{J} is approximately computed as a sample mean of some computable function Fğ¹F over a â€œmini-batchâ€ ğ’Ÿtsubscriptğ’Ÿğ‘¡\\mathcal{D}_{t}, with\nâˆ‡Î¸J~â€‹(Î¸t)â‰ˆ1|ğ’Ÿt|â€‹âˆ‘(s,a)âˆˆğ’ŸtFâ€‹(s,a,Î¸t)subscriptâˆ‡ğœƒ~ğ½subscriptğœƒğ‘¡1subscriptğ’Ÿğ‘¡subscriptğ‘ ğ‘subscriptğ’Ÿğ‘¡ğ¹ğ‘ ğ‘subscriptğœƒğ‘¡\\nabla_{\\theta}\\tilde{J}(\\theta_{t})\\approx\\frac{1}{|\\mathcal{D}_{t}|}\\sum_{(s,a)\\in\\mathcal{D}_{t}}F(s,a,\\theta_{t}), where ğ’Ÿtsubscriptğ’Ÿğ‘¡\\mathcal{D}_{t} is a selected subset of all the rollout data up to time tğ‘¡t. For example, policy gradient algorithms, as a family of widely used RL algorithms, work on policy functions Ï€â€‹(s,a;Î¸)ğœ‹ğ‘ ğ‘ğœƒ\\pi(s,a;\\theta) that are directly differentiable, and typically choose J~pâ€‹gâ€‹(Î¸)â‰ğ”¼s0âˆ¼Ï0[VÏ€â€‹(Î¸)Î³câ€‹(s0)]approaches-limitsubscript~ğ½ğ‘ğ‘”ğœƒsubscriptğ”¼similar-tosubscriptğ‘ 0subscriptğœŒ0subscriptsuperscriptğ‘‰subscriptğ›¾ğ‘ğœ‹ğœƒsubscriptğ‘ 0\\tilde{J}_{pg}(\\theta)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s_{0}\\sim\\rho_{0}\\end{subarray}}[V^{\\scriptscriptstyle\\gamma_{c}}_{\\scriptscriptstyle\\pi(\\theta)}(s_{0})]\nÂ 111\nTo simplify notations we will write ÏÎ¸subscriptğœŒğœƒ\\rho_{\\theta}, QÎ¸subscriptğ‘„ğœƒQ_{\\theta}, VÎ¸subscriptğ‘‰ğœƒV_{\\theta}, Jâ€‹(Î¸)ğ½ğœƒJ(\\theta) for ÏÏ€â€‹(Î¸)subscriptğœŒğœ‹ğœƒ\\rho_{\\pi(\\theta)}, QÏ€â€‹(Î¸)subscriptğ‘„ğœ‹ğœƒQ_{\\pi(\\theta)}, VÏ€â€‹(Î¸)subscriptğ‘‰ğœ‹ğœƒV_{\\pi(\\theta)}, Jâ€‹(Ï€â€‹(Î¸))ğ½ğœ‹ğœƒJ(\\pi(\\theta)) from now on.\n\nin episodic tasks as the surrogate objective, whose gradient can be estimated by Fpâ€‹gâ€‹(s,a,Î¸)â‰QÎ¸Î³c^â€‹(s,a)â€‹âˆ‡logâ¡Ï€â€‹(s,a;Î¸)approaches-limitsubscriptğ¹ğ‘ğ‘”ğ‘ ğ‘ğœƒ^subscriptsuperscriptğ‘„subscriptğ›¾ğ‘ğœƒğ‘ ğ‘âˆ‡ğœ‹ğ‘ ğ‘ğœƒF_{pg}(s,a,\\theta)\\doteq\\widehat{Q^{\\scriptscriptstyle\\gamma_{c}}_{\\theta}}(s,a)\\nabla\\log\\pi(s,a;\\theta), where QÎ¸Î³c^â€‹(s,a)^subscriptsuperscriptğ‘„subscriptğ›¾ğ‘ğœƒğ‘ ğ‘\\widehat{Q^{\\scriptscriptstyle\\gamma_{c}}_{\\theta}}(s,a) is some practical estimation of QÎ¸Î³câ€‹(s,a)subscriptsuperscriptğ‘„subscriptğ›¾ğ‘ğœƒğ‘ ğ‘Q^{\\scriptscriptstyle\\gamma_{c}}_{\\theta}(s,a) for the specific (s,a)âˆˆğ’Ÿtğ‘ ğ‘subscriptğ’Ÿğ‘¡(s,a)\\in\\mathcal{D}_{t}.\nSubstituting J~pâ€‹gsubscript~ğ½ğ‘ğ‘”\\tilde{J}_{pg} and Fpâ€‹gsubscriptğ¹ğ‘ğ‘”F_{pg} to the general RL algorithmic framework above, yields where the equality part in (3) is known as the classic policy gradient theoremÂ . Policy gradient algorithms illustrate a general disparity observed in episodic RL between the â€œdesiredâ€ data distribution and the data actually collected. Specifically, according to (3), the policy gradient should be estimated by rolling out an episode using Ï€â€‹(Î¸)ğœ‹ğœƒ\\pi(\\theta) then summing over (Î³c)Ï„â‹…Fpâ€‹gâ‹…superscriptsubscriptğ›¾ğ‘ğœsubscriptğ¹ğ‘ğ‘”(\\gamma_{c})^{\\tau}\\cdot F_{pg} across all steps Ï„ğœ\\tau in the episode, which collectively provide one sample point to the right-hand side of (3). Many policy gradient algorithms indeed work this wayÂ [ref]5 6. However, some modern policy gradient algorithms, such as A3CÂ 4 or the variant of REINFORCE as described in , compute the policy gradient based on data only from a small time window, much smaller than the average episode length. Such an â€œonlineâ€ approachÂ [ref]5 has witnessed remarkable empirical success in practiceÂ 4. A popular explanation for the disparity above is to re-arrange the sum âˆ‘Ï„=0âˆ(Î³c)Ï„â€‹âˆ‘sÏ„ÏÎ¸(Ï„)â€‹(sÏ„)superscriptsubscriptğœ0superscriptsubscriptğ›¾ğ‘ğœsubscriptsubscriptğ‘ ğœsubscriptsuperscriptğœŒğœğœƒsubscriptğ‘ ğœ\\sum_{\\tau=0}^{\\infty}~{}(\\gamma_{c})^{\\tau}\\sum_{s_{\\tau}}\\rho^{\\scriptscriptstyle(\\tau)}_{\\theta}(s_{\\tau}) in (3) into âˆ‘s(âˆ‘Ï„=0âˆ(Î³c)Ï„â€‹ÏÎ¸(Ï„)â€‹(s))subscriptğ‘ superscriptsubscriptğœ0superscriptsubscriptğ›¾ğ‘ğœsubscriptsuperscriptğœŒğœğœƒğ‘ \\sum_{s}\\big{(}\\sum_{\\tau=0}^{\\infty}(\\gamma_{c})^{\\tau}\\rho^{\\scriptscriptstyle(\\tau)}_{\\theta}(s)\\big{)}, which is a weighted sum over states, thus can be interpreted as an expectation over a state distribution that we will call it the episode-wise visitation distribution Î¼Ï€Î³câ€‹(s)â‰âˆ‘Ï„=0âˆ(Î³c)Ï„â€‹ÏÏ€(Ï„)â€‹(s)/Zapproaches-limitsuperscriptsubscriptğœ‡ğœ‹subscriptğ›¾ğ‘ğ‘ superscriptsubscriptğœ0superscriptsubscriptğ›¾ğ‘ğœsubscriptsuperscriptğœŒğœğœ‹ğ‘ ğ‘\\mu_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s)\\doteq\\sum_{\\tau=0}^{\\infty}(\\gamma_{c})^{\\tau}\\rho^{\\scriptscriptstyle(\\tau)}_{\\pi}(s)/ZÂ  6 , where Zğ‘Z is a normalizing term. However, the visitation-distribution interpretation cannot explain why a single rollout data point stsubscriptğ‘ ğ‘¡s_{t} can serve as a faithful sample of Î¼Ï€Î³csubscriptsuperscriptğœ‡subscriptğ›¾ğ‘ğœ‹\\mu^{\\scriptscriptstyle\\gamma_{c}}_{\\pi}. For Î³c<1subscriptğ›¾ğ‘1\\gamma_{c}<1, we know that stsubscriptğ‘ ğ‘¡s_{t} is just biased to Î¼Ï€Î³csubscriptsuperscriptğœ‡subscriptğ›¾ğ‘ğœ‹\\mu^{\\scriptscriptstyle\\gamma_{c}}_{\\pi}Â ; for Î³c=1subscriptğ›¾ğ‘1\\gamma_{c}=1, we have Î¼Ï€1âˆğ”¼Ï€[âˆ‘tğŸ™â€‹(st=s)]proportional-tosubscriptsuperscriptğœ‡1ğœ‹subscriptğ”¼ğœ‹subscriptğ‘¡1subscriptğ‘ ğ‘¡ğ‘ \\mu^{1}_{\\pi}\\propto\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}\\pi\\end{subarray}}[\\sum_{t}\\mathds{1}(s_{t}=s)], which justifies the practice of taking long-run average across multiple episodesÂ [ref]5 6, but still cannot explain why a single step can represent this long-run averageÂ 4 . Importantly, we remark that the aforementioned disparity between desired data and actual data is observed not only in policy gradient algorithms, but also in other policy-based algorithms like PPO8, and in value-based and off-policy algorithms like Q-Learning2  too. The disparity between theory and practice in data collection is currently a general issue in the RL framework thus discussed (which subsumes all these algorithms, see Appendix B for more details).\n\nReference markers found: [5]\n\nContext windows for each [ref]:\n[ref]5: F_{pg} across all steps Ï„ğœ\\tau in the episode, which collectively provide one sample point to the right-hand side of (3). Many policy gradient algorithms indeed work this wayÂ [ref]5 6. However, some modern policy gradient algorithms, such as A3CÂ 4 or the variant of REINFORCE as described in , compute the policy gradient based on data only from a small time window, much\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 5,\n            \"titles\": [\n                \"Addressing Function Approximation Error in Actor-Critic Methods\",\n                \"Pybullet gymperium.\",\n                \"Dynamic programming and optimal control, volume 1.\",\n                \"Openai gym.\",\n                \"Off-policy actor-critic.\",\n                \"Benchmarking Deep Reinforcement Learning for Continuous Control\",\n                \"Continuous Deep Q-Learning with Model-based Acceleration\",\n                \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\n                \"A natural policy gradient.\",\n                \"Continuous control with deep reinforcement learning.\",\n                \"Simulation-based optimization of markov reward processes.\",\n                \"Markov chains and stochastic stability.\",\n                \"Human-level control through deep reinforcement learning.\",\n                \"Asynchronous methods for deep reinforcement learning.\",\n                \"Time Limits in Reinforcement Learning\",\n                \"Trust Region Policy Optimization\",\n                \"Equivalence Between Policy Gradients and Soft Q-Learning\",\n                \"Proximal policy optimization algorithms.\",\n                \"Basics of applied stochastic processes.\",\n                \"Deterministic policy gradient algorithms.\",\n                \"The predictron: End-to-end learning and planning.\",\n                \"A new q (lambda) with interim forward view and monte carlo equivalence.\",\n                \"Td models: modeling the world at a mixture of time scales.\",\n                \"Reinforcement learning: An introduction.\",\n                \"Policy gradient methods for reinforcement learning with function approximation.\",\n                \"Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction.\",\n                \"An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning\",\n                \"Bias in natural actor-critic algorithms.\",\n                \"On average versus discounted reward temporal-difference learning.\",\n                \"Deep Reinforcement Learning with Double Q-learning\",\n                \"Insights in reinforcement rearning: formal analysis and empirical evaluation of temporal-difference learning algorithms.\",\n                \"Learning from delayed rewards.\",\n                \"Unifying Task Specification in Reinforcement Learning\",\n                \"Simple statistical gradient-following algorithms for connectionist reinforcement learning.\",\n                \"On convergence of emphatic temporal-difference learning.\",\n                \"Generalized Off-Policy Actor-Critic\",\n                \"Generated Reference Title 36 for [ref]5\",\n                \"Generated Reference Title 37 for [ref]5\",\n                \"Generated Reference Title 38 for [ref]5\",\n                \"Generated Reference Title 39 for [ref]5\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]5 based on the context and research area of Steady State Analysis of Episodic Reinforcement Learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}