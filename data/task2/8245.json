{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: From Solitary Directives to Interactive Encouragement! LLM Secure Code Generation by Natural Language Prompting\n\nPaper Abstract: \n\nPaper Section Text:\nFurther Performance Improvement.\nIn previous experiments, we set the number of loops to 10, which means that after running 10 interactive iterations, the experiments will terminate. At the same time, note that we have set the temperature to be 00 to ensure the most deterministic results. However, we observed that some samples are still vulnerable. We explore two techniques to further improve the SecCode performance. (1) Temperature Increment. One solution is to increase the temperature of the LLM to allow the LLM to explore more potential knowledge exploration. By doing so, all six vulnerable code samples from the SecEval dataset can be fixed thoroughly. Taking the below prompt P5 for a code snippet generation as an example: “Write a Python function using the Flask framework that accepts POST requests at the \"/upload\" endpoint, reads the XML data from the incoming request, parses it, and returns the parsed XML data as a string.”\nOur experimental results showed that a vulnerability persisted after running 10 iterations of CodeQL, with the message: Vulnerability Message: XML parsing depends on without guarding against uncontrolled entity expansion.Then, we increased the temperature of the LLM by 0.1 each time and ran the proposed SecCode again. We found that after raising the temperature to 0.5, the vulnerability was fixed. (2) Adaptive Prompting. Another solution is to apply our proposed adaptive prompting. Adaptive prompting is a data-driven approach enhancing model capability by providing tailored information and examples (for more information please refer to Appendix.A.1). By this, we will provide examples in the S2 of the EP. For example, we identified that vulnerabilities still persist after running 10 iterations for the following prompt P6: The vulnerable information for this prompt is: Call to fclose with untrusted data from String read by fread. User-provided value flows an expression which might overflow negatively. User-provided value flows an expression which might overflow. The possible vulnerabilities are: CWE190 Integer Overflow or Wraparound, CWE732 Incorrect Permission Assignment for Critical Resource, CWE20 Improper Input Validation, CWE200 Exposure of Sensitive Information to an Unauthorized Actor. In this case, we propose to apply adaptive prompting as extra support to SecCode. Specifically, we add the identified CWE ID examples of vulnerable samples along with the fixed version to SecCode (in S2 and S3). Take CWE190 for example, the following information will be added to SecCode: Our experimental results show that after using adaptive prompting, the vulnerabilities can be fixed. It is worth noting that we can apply adaptive prompting for every iteration of S2 and S3, however, this is time-consuming. Therefore, we recommend applying adaptive prompting when the vulnerability cannot be fixed properly. In conclusion, if a vulnerability still cannot be fixed after several iterations, we suggest that the user apply adaptive prompting, which has been shown to be helpful in our study. Prompt Engineering. By devising encouragement prompting, SecCode produces outstanding performance after 5 iterations and also has a high FSR. Nonetheless, we can see that there are still some vulnerable samples that cannot be fixed properly. We have demonstrated properly that prompt engineering can substantially boost LLM performance on securing code generation. In the future, it is worth exploring other prompt engineering further. For example, in-context learning adds a few-shot examples [ref]21 to boost the LLM performance on a specific task. Diversity of Programming Languages. SecCode has been tested on a few widely used programming languages of C/C++ and Python in this study. Yet, there are other programming languages that are worth investigating and securing. There are interesting future works to evaluate SecCode on more datasets and programming languages such as Java, JavaScript, Ruby, and so on. Compilation Challenge for CodeQL. One limitation of leveraging CodeQL for cross-checking is that it requires the C/C++ code compilable for vulnerability check. For C/C++, the generated/fixed code is not always compilable because it lacks dependencies and required headers; to address this, we added a prompt specifying the inclusion of necessary libraries and header files. However, the code generated by LLMs like GPT-3.5, DeepSeek Coder, and Llama often fails to meet this requirement. Static Analysis Tools. This study combines the use of LLM and CodeQL for vulnerability detection. CodeQL was chosen for two main reasons: (i) its popularity in cross-checking and empirical studies [ref]26, and (ii) its support for a wide range of programming languages, including C/C++, Python [ref]10.\nNonetheless, we acknowledge the existence of other static analysis tools, such as query-based static application security testing , and tools like Bandit555Bandit: https://github.com/PyCQA/bandit . Exploring and incorporating a broader variety of static analysis tools, as discussed in , would be an interesting avenue for future work.\n\nReference markers found: [21]\n\nContext windows for each [ref]:\n[ref]21: can substantially boost LLM performance on securing code generation. In the future, it is worth exploring other prompt engineering further. For example, in-context learning adds a few-shot examples [ref]21 to boost the LLM performance on a specific task. Diversity of Programming Languages. SecCode has been tested on a few widely used programming languages of C/C++ and Python in this study. Yet,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 21,\n            \"titles\": [\n                \"Automatic Programming: Large Language Models and Beyond\",\n                \"Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)\",\n                \"Can LLMs Patch Security Issues?\",\n                \"Programming is hard-or at least it used to be: Educational opportunities and challenges of ai code generation\",\n                \"Evaluating Large Language Models Trained on Code\",\n                \"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\",\n                \"Codeprompt: Task-agnostic prefix tuning for program and language generation\",\n                \"Code generation using machine learning: A systematic review\",\n                \"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language\",\n                \"Self-collaboration code generation via chatgpt\",\n                \"Codeql\",\n                \"DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence\",\n                \"Large language models for code: Security hardening and adversarial testing\",\n                \"A Survey on Large Language Models for Code Generation\",\n                \"How Secure is Code Generated by ChatGPT?\",\n                \"A Survey on Data-driven Software Vulnerability Assessment and Prioritization\",\n                \"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator\",\n                \"StarCoder: may the source be with you!\",\n                \"Evaluating c/c++ vulnerability detectability of query-based static application security testing tools\",\n                \"Software vulnerability detection using deep neural networks: a survey\",\n                \"Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation\",\n                \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\",\n                \"Assessing the security of github copilot’s generated code-a targeted replication study\",\n                \"Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities\",\n                \"Show Your Work: Scratchpads for Intermediate Computation with Language Models\",\n                \"Asleep at the keyboard? assessing the security of github copilot’s code contributions\",\n                \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\n                \"Android source code vulnerability detection: a systematic literature review\",\n                \"PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\",\n                \"Large Language Model Alignment: A Survey\",\n                \"Securityeval dataset: Mining vulnerability examples to evaluate machine learning-based code generation techniques\",\n                \"LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations\",\n                \"Llms cannot reliably identify and reason about security vulnerabilities (yet?): A comprehensive evaluation, framework, and benchmarks\",\n                \"Combining structured static code information and dynamic symbolic traces for software vulnerability prediction\",\n                \"Chain-of-thought prompting elicits reasoning in large language models\",\n                \"Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review\",\n                \"How Effective Are Neural Networks for Fixing Security Vulnerabilities\",\n                \"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models\",\n                \"Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x\",\n                \"Large Language Model for Vulnerability Detection: Emerging Results and Future Directions\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]21 based on the context and research area of From Solitary Directives to Interactive Encouragement! LLM Secure Code Generation by Natural Language Prompting. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}