{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Evaluating the Social Impact of Generative AI Systems in Systems and Society\n\nPaper Abstract: \n\nPaper Section Text:\nGenerative AI systems are capable of exacerbating inequality, as seen in sections on 4.1.1 Bias, Stereotypes, and Representational Harms and 4.1.2 Cultural Values and Sensitive Content, and Disparate Performance. When deployed or updated, systems’ impacts on people and groups can directly and indirectly be used to harm and exploit vulnerable and marginalized groups. \n\nBiases in a system’s development process and safety provisions for generative AI systems, such as content moderation, can lead to community erasure . Avoiding the generation of the harms outlined is seen as a generally desirable outcome. However, the removal of harmful content can come with its own costs of lower general performances for sub-populations that use models for generation . Mitigation thus currently serves as a double-edged sword, where removal of toxic content also has negative implications, in particular for marginalized communities. Both the benefits and the costs of content moderation are unequally distributed. The automatic systems that remove undesirable content can perform next to randomly or be harmful for marginalized populations [ref]208, while the selection criteria for what constitutes safe content are aligned with technical safety and mitigation decisions. These impacts compound to make marginalized populations pay a greater cost for an intervention that they benefit from less. The production of harmful content is currently mitigated using combinations of four methods: data sourcing [ref]30; human moderation of content included in training data ; automated moderation of content included in training data 1; and keyword deny-lists . Given that the exclusion of harmful content within datasets stand to create distinct harms to marginalized communities, efforts towards mitigation of generating harmful content becomes a question of the politics of classification     and its potential harms. Evaluating Disparate Performance once systems have undergone safety provisions can give signal to possible erasure. Accounting for the demographics and composition of human crowdworkers can also provide information  about subsequent impacts. Longer-term impacts of erasure depend on the system’s deployment context, leading to opportunity loss or reinforced biases and norms. Better democratic processes for developing and deploying systems and safety provisions such as content moderation should work with marginalized populations. This should include more investment in representative crowdworkers and appropriate compensation and mental health support. Lessons from social media content moderation can apply, such as working with groups who have been erased and documenting patterns of erasure to improve future approaches . \n\nBiases, dominant cultural values, and disparate performance seen in lack of representation in training and development of generative AI systems can exacerbate marginalization when those systems are deployed. For example, increasing resourcing and performance for already highly resourced languages reinforces those languages’ dominance. Inclusion without consent can also harm marginalized groups. While some research strives to improve performance for underrepresented Indigenous languages , the same Indigenous groups resist AI approaches to use of their language . Profit from Indigenous languages and groups who have been systematically exploited continues directly and indirectly. Generative AI use in critical infrastructure that directly impacts human wellbeing can also be classified as high-risk use cases. This includes use in judicial systems, healthcare such as mental health and medical advice, and democratic processes, such as election or political information. An example is generative AI systems used to replace care work, such as crisis intervention and research  and action  to use chatbots for eating disorder prevention. Technical tooling used in human systems and processes that have long-recorded discrimination patterns  can instead exacerbate harm . Generative AI used in medical education and potentially in clinical decision-making will continue to underserve and expose institutionally marginalised individuals and communities to life-impacting risks. From inaccurate skin cancer diagnosis , to the scoring of Black patients in the U.S. medical system as less sick than the reality of their complex health and resource allocation needs , the use of generative AI in medical settings must be sensitive to existing challenges to equality within medical practice . Systems should again undergo Disparate Performance evaluations once updated for a high-risk task in critical infrastructure and account for the additional deployment context. Long-term impacts in addition to marginalization can include erasure. Evaluating marginalization will depend on context, and should account for marginalization when work by marginalized populations is less visible or uncredited . Evaluating marginalization impacts on individuals, such as through health , is ongoing research. Improving evaluation work for underrepresented populations and such as for low-resource languages, and crediting local researchers , can help give more information to disparate performance. Engagement with populations should be done in ways that embody local approaches . Policies should be crafted to better respect rights to refusal . Regulations for AI that address these discriminatory patterns should coordinate with other nations to ensure protections are global and regulations are not “patchworked”. When attempting to improve performance for underrepresented indigenous languages, it is important to adhere to established principles such as the Indigenous Data Sovereignty principles, e.g.: The CARE Principles for Indigenous Data Governance  or FAIR principles . Participatory methodologies in AI development have  included engaging locally led and compensated focus groups with impacted community members, in collaboration with engineers, to think through potential harmful outcomes. “Red-teaming” - testing AI models for potential vulnerabilities, biases, and weaknesses through real-world simulations is also an entry point for engaging the ‘epistemic privilege’  of those most affected by the social impacts of generative AI systems. Addressing barriers to evaluations are rendered difficult, and at times impossible, given that the model is enclosed in software or only available through an API. Therefore, given the overlaps in the public sphere, advocacy of open-sourced / licensed access are increasingly popular and compelling 1. Smuha  proposes accountability and monitoring mechanisms at a public oversight level, for example mandatory impact assessments of AI systems which incorporates opportunities for societal feedback. Smuha also emphasises the importance of independent information collection and distribution about AI’s societal impact. Further, it will be necessary to introduce procedural rights - for example \"right to access to information, access to justice, and participation in public decision-making on AI, regardless of the demonstration of individual harm\". \n\nGenerative AI systems can generate outputs that are used for abuse, constitute non-consensual content, or are threats of violence and harassment . Non-consensual sexual representations of people, include representations of minors as generative child sexual abuse material (CSAM) . Abuse and violence can disparately affect groups, such as women and girls . Sensitive topics and trauma’s impacts on people are by nature challenging to evaluate and must be done with care. Consequences of abuse of children and minors can be long-term or lifelong . Impacts and trauma can resurface throughout a person’s life in many aspects. Evaluations for generative AI impacts can overlap with similar harms such as image-based sexual abuse . As seen in 4.1.2 Cultural Values and Sensitive Content, consent from existing people should be evaluated with the person themselves. Research to detect, mitigate, and report abusive and violent content such as CSAM is ongoing  and tools specific to modalities such as images can help identify content that is not yet labeled as CSAM . Relevant regulation should be updated to address generated content that may not accurately portray an existing person or their body or self, but lead to real harms.\n\nReference markers found: [208]\n\nContext windows for each [ref]:\n[ref]208: benefits and the costs of content moderation are unequally distributed. The automatic systems that remove undesirable content can perform next to randomly or be harmful for marginalized populations [ref]208, while the selection criteria for what constitutes safe content are aligned with technical safety and mitigation decisions. These impacts compound to make marginalized populations pay a\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 208,\n            \"titles\": [\n                \"Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection\",\n                \"Analyse and automate every message\",\n                \"Global Comprehensive Privacy Law Mapping Chart\",\n                \"Methodologies and Tools to Evaluate Climate Change Impacts and Adaptation\",\n                \"California Consumer Privacy Act of 2018\",\n                \"AB-5 Worker status: employees and independent contractors.\",\n                \"DOD Adopts Ethical Principles for Artificial Intelligence\",\n                \"COM (2021) 762: Proposal for a DIRECTIVE OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL on improving working conditions in platform work\",\n                \"Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS\",\n                \"Americans’ Views on Generative Artificial Intelligence, Hate and Harassment\",\n                \"Cyber violence is a growing threat, especially for women and girls\",\n                \"REAIM 2023\",\n                \"A Systematic Review of the Empirical Literature Evaluating IRBs: What We Know and What We Still Need to Learn\",\n                \"Mirages: On Anthropomorphism in Dialogue Systems, May 2023\",\n                \"인공지능 기반 공격 그래프 생성\",\n                \"The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation\",\n                \"Behavioural consequences of child abuse\",\n                \"Protecting One’s Own Privacy in a Big Data Economy\",\n                \"The Limitations of Online Surveys\",\n                \"”What We Can’t Measure, We Can’t Understand”: Challenges to Demographic Data Procurement in the Pursuit of Fairness\",\n                \"The Labor Market Impacts of Technological Change: From Unbridled Enthusiasm to Qualified Optimism to Vast Uncertainty, May 2022\",\n                \"Marginalization: Conceptualizing patient vulnerabilities in the framework of social determinants of health – An integrative review\",\n                \"Differential Privacy Has Disparate Impact on Model Accuracy\",\n                \"T. Telleen-Lawton\",\n                \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\",\n                \"State of AI Report 2022\",\n                \"“It’s Complicated”: Negotiating Accessibility and (Mis)Representation in Image Descriptions of Race, Gender, and Disability\",\n                \"Digital Labour Platforms and the Future of Work: Towards Decent Work in the Online World\",\n                \"Bloom: A 176b-parameter open-access multilingual language model, 2023\",\n                \"Power to the People? Opportunities and Challenges for Participatory AI\",\n                \"The Forgotten Margins of AI Ethics\",\n                \"Language (Technology) is Power: A Critical Survey of “Bias” in NLP\",\n                \"On the Opportunities and Risks of Foundation Models, 2022-07-12\",\n                \"Sorting Things Out Classification and Its Consequences\",\n                \"Beyond ethnography: engagement and reciprocity as foundations for design research out here\",\n                \"F. Mireshghallah\",\n                \"Language Models are Few-Shot Learners, 2020-07-22\",\n                \"Truth, Lies, and Automation, May 2021\",\n                \"Do Opt-Outs Really Opt Me Out?\",\n                \"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]208 based on the context and research area of Evaluating the Social Impact of Generative AI Systems in Systems and Society. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}