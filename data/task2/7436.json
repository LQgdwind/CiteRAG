{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: On Computationally Efficient Multi-Class Calibration\n\nPaper Abstract: \n\nPaper Section Text:\nWe now formulate projected smooth calibration, a weighted calibration notion that satisfies our desiderata.\nAs discussed above, we want to ensure the following subset calibration guarantee:\nfor every subset TâŠ†[k]ğ‘‡delimited-[]ğ‘˜T\\subseteq[k] of labels, the probabilities assigned by our predictor to the event that the label belongs to Tğ‘‡T should be calibrated. Let ğ¯âˆˆÎ”kğ¯subscriptÎ”ğ‘˜\\mathbf{v}\\in\\Delta_{k} denote the prediction of our predictor. Letting ğŸTâŠ†{0,1}ksubscript1ğ‘‡superscript01ğ‘˜\\mathbf{1}_{T}\\subseteq\\{0,1\\}^{k} denote the indicator vector of Tğ‘‡T, the indicator for the event that the outcome ğ²ğ²\\mathbf{y} is in Tğ‘‡T is ğŸTâ‹…ğ²â‹…subscript1ğ‘‡ğ²\\mathbf{1}_{T}\\cdot\\mathbf{y}, whereas the predicted probability is ğŸTâ‹…ğ¯â‹…subscript1ğ‘‡ğ¯\\mathbf{1}_{T}\\cdot\\mathbf{v}.\nSay we want to enforce the calibration condition that when the predicted probability of belonging to Tğ‘‡T exceeds vâˆˆ [ref]1ğ‘£01v\\in [ref]1, the label indeed lies in Tğ‘‡T with roughly the predicted probability. We can view this as requiring a bound on where (ğ•€â€‹(ğŸTâ‹…ğ¯â‰¥v)â€‹ğŸT)âˆˆ{0,1}kğ•€â‹…subscript1ğ‘‡ğ¯ğ‘£subscript1ğ‘‡superscript01ğ‘˜(\\mathbb{I}(\\mathbf{1}_{T}\\cdot\\mathbf{v}\\geq v)\\mathbf{1}_{T})\\in\\{0,1\\}^{k} is a vector-valued function on Î”ksubscriptÎ”ğ‘˜\\Delta_{k}, which takes the value ğ•€â€‹(ğŸTâ‹…ğ¯â‰¥v)ğ•€â‹…subscript1ğ‘‡ğ¯ğ‘£\\mathbb{I}(\\mathbf{1}_{T}\\cdot\\mathbf{v}\\geq v) for coordinates in Tğ‘‡T, and the value 00 for coordinates outside Tğ‘‡T. The good news is that this setup fits the template of weighted calibration, where the class â„‹â„‹\\mathcal{H} contains all functions the form ğ•€â€‹(ğŸTâ‹…ğ¯â‰¥v)ğ•€â‹…subscript1ğ‘‡ğ¯ğ‘£\\mathbb{I}(\\mathbf{1}_{T}\\cdot\\mathbf{v}\\geq v) for Tâˆˆ{0,1}k,vâˆˆâ„formulae-sequenceğ‘‡superscript01ğ‘˜ğ‘£â„T\\in\\{0,1\\}^{k},v\\in\\mathbb{R}. The bad news is that we will show this problem is as hard as agnostically learning halfspaces. Daniely [Dan16] showed that, assuming the hardness of refuting random XorXor\\mathrm{Xor} CSPs, this problem cannot be solved in polynomial time. In projected smooth calibration, we replace the hard thresholds ğ•€â€‹(ğŸTâ‹…ğ¯â‰¥v)ğ•€â‹…subscript1ğ‘‡ğ¯ğ‘£\\mathbb{I}(\\mathbf{1}_{T}\\cdot\\mathbf{v}\\geq v) with the class â„‹pLip={Ï•â€‹(ğšâ‹…ğ¯)}subscriptâ„‹pLipitalic-Ï•â‹…ğšğ¯\\mathcal{H}_{\\mathrm{pLip}}=\\{\\phi({\\mathbf{a}}\\cdot\\mathbf{v})\\} where Ï•: [ref]1â†’[âˆ’1,1]:italic-Ï•â†’0111\\phi: [ref]1\\to[-1,1] is a Lipschitz continuous function and ğšâˆˆ[âˆ’1,1]kğšsuperscript11ğ‘˜{\\mathbf{a}}\\in[-1,1]^{k}.\nIn particular, this includes indicator vectors for subsets.\nProjected smooth calibration requires that the weighted calibration error for the weight family â„‹pLipksuperscriptsubscriptâ„‹pLipğ‘˜\\mathcal{H}_{\\mathrm{pLip}}^{k} is bounded. For a joint distribution ğ’Ÿğ’Ÿ\\mathcal{D} on predictions ğ¯ğ¯\\mathbf{v} and true outcomes ğ²ğ²\\mathbf{y}, the projected smooth calibration error is where â„‹pLipksuperscriptsubscriptâ„‹pLipğ‘˜\\mathcal{H}_{\\mathrm{pLip}}^{k} is the class of functions w:Î”kâ†’[âˆ’1,1]k:ğ‘¤â†’subscriptÎ”ğ‘˜superscript11ğ‘˜w:\\Delta_{k}\\to[-1,1]^{k} such that for every coordinate iğ‘–i in wğ‘¤wâ€™s output there is a 111-Lipschitz function Ï•(i): [ref]1â†’[âˆ’1,1]:superscriptitalic-Ï•ğ‘–â†’0111\\phi^{(i)}: [ref]1\\to[-1,1] s.t.\nw(i)â€‹(ğ¯)â‰¡Ï•(i)â€‹(ğšâ‹…ğ¯)superscriptğ‘¤ğ‘–ğ¯superscriptitalic-Ï•ğ‘–â‹…ğšğ¯w^{(i)}(\\mathbf{v})\\equiv\\phi^{(i)}({\\mathbf{a}}\\cdot\\mathbf{v}). A predictor satisfies projected smooth calibration if its error psCEpsCE\\mathrm{psCE} is bounded. We show that this definition satisfies all our desiderata: Property (1): expressive power. Projected smooth calibration guarantees that for every subset TâŠ†[k]ğ‘‡delimited-[]ğ‘˜T\\subseteq[k], the predicted probabilities for the binary classification task (namely, the outcome is in Tğ‘‡T) satisfy smooth calibration, a well-studied calibration notion with several desirable properties [KF08, GKSZ22, BGHN23]. In particular, this implies that the predicted probabilities ğŸTâ‹…ğ¯â‹…subscript1ğ‘‡ğ¯\\mathbf{1}_{T}\\cdot\\mathbf{v}\nthat the outcome will be in Tğ‘‡T are close to being perfectly calibrated. The proof builds on the work of [BGHN23]. In particular, for each such subset Tğ‘‡T, there exists a perfectly calibrated predictor pTâˆ—subscriptsuperscriptğ‘ğ‘‡p^{*}_{T} for the binary classification task of determining whether the outcome will land in Tğ‘‡T, whose predictions are close to ğŸTâ‹…ğ¯â‹…subscript1ğ‘‡ğ¯\\mathbf{1}_{T}\\cdot\\mathbf{v} in earthmover distance. Thus, we get meaningful guarantees for a rich collection of downstream binary classification tasks (including subset membership and more). Property (2): Computational efficiency. We show an efficient algorithm for auditing whether the projected smooth calibration error of a predictor is bounded. Here we state our result informally, the formal statement is in Theorem 7.1. There is an algorithm for deciding whether the projected smooth calibration error is at most Î±ğ›¼\\alpha, with sample complexity and running time Oâ€‹(kOâ€‹(1/Î±))ğ‘‚superscriptğ‘˜ğ‘‚1ğ›¼O(k^{O(1/\\alpha)}). The work of [SSS11] showed that agnostic learning halfspaces becomes tractable if we replace the hard thresholds used in halfspaces with Lipschitz transfer functions. Building on their techniques, and using Jacksonâ€™s Theorem on low-degree uniform approximations for Lipschitz functions, we show that auditing for projected smooth calibration is polynomial time solvable. Moreover, our auditing algorithm is quite simple and does not need to solve a convex program, generalizing results in [KSJ18, BGHN23]. Our algorithm in fact solves the associated search problem, if pğ‘p is not calibrated, it finds a witness to the lack of calibration, which can be used to post-process pğ‘p and redcuce its calibration error without increasing the squared loss. Defining a recalibration algorithm correctly is subtle, see Definition 4.1 and the discussion preceding it. Our algorithm has running time polynomial in kğ‘˜k for every fixed constant Î±ğ›¼\\alpha, in contrast with previous results for expressive notions of calibration. If we only care about auditing using Ï•â€‹(wâ‹…ğ¯)italic-Ï•â‹…ğ‘¤ğ¯\\phi(w\\cdot\\mathbf{v}) for vectors wğ‘¤w where âˆ¥wâˆ¥22â‰¤msubscriptsuperscriptdelimited-âˆ¥âˆ¥ğ‘¤22ğ‘š\\left\\lVert w\\right\\rVert^{2}_{2}\\leq m then the sample complexity can be bounded by mOâ€‹(1/Î±)superscriptğ‘šğ‘‚1ğ›¼m^{O(1/\\alpha)}. This gives a running time fixed polynomial in kğ‘˜k (but exponential in Î±ğ›¼\\alpha) for subset calibration where we only care about bounded size subsets.\nOne can also get better run times by restricting the family of Lipschitz functions Ï•italic-Ï•\\phi. By restricting to auditors of the form tanhâ¡(wâ‹…ğ¯)â‹…ğ‘¤ğ¯\\tanh(w\\cdot\\mathbf{v}) we get the weaker notion of sigmoid calibration, for which the auditor runs in time kOâ€‹(logâ¡(1/Î±))superscriptğ‘˜ğ‘‚1ğ›¼k^{O(\\log(1/\\alpha))}. This can be seen as a smooth relaxation of the intractable notion of halfspace auditing. However, the improved efficiency comes at the price of some expressivity, we do not get closeness to perfect calibration for downstream subset classification tasks. It is interesting to investigate whether the exponential dependence on 1/Î±1ğ›¼1/\\alpha in TheoremÂ 1.2 can be avoided. As a result in this direction, we show that the running time cannot be improved to polyâ¡(k,1/Î±)polyğ‘˜1ğ›¼\\operatorname{poly}(k,1/\\alpha): Under standard complexity-theoretic assumptions,\nthere is no algorithm that can decide whether the projected smooth calibration error is at most Î±ğ›¼\\alpha with sample complexity and running time kOâ€‹(log0.99â¡(1/Î±))superscriptğ‘˜ğ‘‚superscript0.991ğ›¼k^{O(\\log^{0.99}(1/\\alpha))}. We prove the theorem by showing a reduction from the task of refuting random XOR formulas. Getting the right exponent for kğ‘˜k as a function of 1/Î±1ğ›¼1/\\alpha is an interesting question for future work. Property (3): Robustness. The works of [KF08, FH18, BGHN23] advocate the use of Lipschitz functions in defining calibration since it results in robust measures that do not change drastically under small perturbations of the predictor. Since projected smooth calibration is defined using Lipschitz functions, it is a robust calibration measure.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: the predicted probability is ğŸTâ‹…ğ¯â‹…subscript1ğ‘‡ğ¯\\mathbf{1}_{T}\\cdot\\mathbf{v}.\nSay we want to enforce the calibration condition that when the predicted probability of belonging to Tğ‘‡T exceeds vâˆˆ [ref]1ğ‘£01v\\in [ref]1, the label indeed lies in Tğ‘‡T with roughly the predicted probability. We can view this as requiring a bound on where\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"A Unifying Theory of Distance from Calibration\",\n                \"A gentle introduction to conformal prediction and distribution-free uncertainty quantification\",\n                \"Practical Adversarial Multivalid Conformal Prediction\",\n                \"Agnostic boosting\",\n                \"Introduction to approximation theory\",\n                \"Complexity Theoretic Limitations on Learning Halfspaces\",\n                \"Objective probability forecasts\",\n                \"Present position and potential developments: Some personal views statistical theory the prequential approach\",\n                \"Imagenet: A large-scale hierarchical image database\",\n                \"Outcome Indistinguishability\",\n                \"Beyond bernoulli: Generating random outcomes that cannot be distinguished from nature\",\n                \"From pseudorandomness to multi-group fairness and back\",\n                \"From average case complexity to improper learning complexity\",\n                \"Relations between average case complexity and approximation complexity\",\n                \"Distribution-specific agnostic boosting\",\n                \"On agnostic learning of parities, monomials, and halfspaces\",\n                \"Smooth calibration, leaky forecasts, finite recall, and nash dynamics\",\n                \"Loss Minimization Through the Lens Of Outcome Indistinguishability\",\n                \"Online Multivalid Learning: Means, Moments, and Prediction Intervals\",\n                \"From Boltzmann Machines to Neural Networks and Back Again\",\n                \"Reliably learning the relu in polynomial time\",\n                \"Omnipredictors\",\n                \"Low-Degree Multicalibration\",\n                \"On Calibration of Modern Neural Networks\",\n                \"Hardness of learning halfspaces with noise\",\n                \"Top-label calibration and multiclass-to-binary reductions\",\n                \"Multicalibration: Calibration for the (computationally-identifiable) masses\",\n                \"Batch multivalid conformal prediction\",\n                \"Deterministic calibration and nash equilibrium\",\n                \"Novel decompositions of proper scoring rules for classification: Score adjustment as precursor to calibration\",\n                \"Potential-based agnostic boosting\",\n                \"Agnostically learning halfspaces\",\n                \"U-Calibration: Forecasting for an Unknown Agent\",\n                \"On agnostic boosting and parity learning\",\n                \"Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration\",\n                \"Trainable calibration measures for neural networks from kernel mean embeddings\",\n                \"On the Computational Efficiency of Training Neural Networks\",\n                \"Randomized algorithms\",\n                \"High-Dimensional Prediction for Sequential Decision Making\",\n                \"Making polynomials robust to noise\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of On Computationally Efficient Multi-Class Calibration. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}