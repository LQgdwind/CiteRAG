{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: On Computationally Efficient Multi-Class Calibration\n\nPaper Abstract: \n\nPaper Section Text:\nWe now formulate projected smooth calibration, a weighted calibration notion that satisfies our desiderata.\nAs discussed above, we want to ensure the following subset calibration guarantee:\nfor every subset T⊆[k]𝑇delimited-[]𝑘T\\subseteq[k] of labels, the probabilities assigned by our predictor to the event that the label belongs to T𝑇T should be calibrated. Let 𝐯∈Δk𝐯subscriptΔ𝑘\\mathbf{v}\\in\\Delta_{k} denote the prediction of our predictor. Letting 𝟏T⊆{0,1}ksubscript1𝑇superscript01𝑘\\mathbf{1}_{T}\\subseteq\\{0,1\\}^{k} denote the indicator vector of T𝑇T, the indicator for the event that the outcome 𝐲𝐲\\mathbf{y} is in T𝑇T is 𝟏T⋅𝐲⋅subscript1𝑇𝐲\\mathbf{1}_{T}\\cdot\\mathbf{y}, whereas the predicted probability is 𝟏T⋅𝐯⋅subscript1𝑇𝐯\\mathbf{1}_{T}\\cdot\\mathbf{v}.\nSay we want to enforce the calibration condition that when the predicted probability of belonging to T𝑇T exceeds v∈ [ref]1𝑣01v\\in [ref]1, the label indeed lies in T𝑇T with roughly the predicted probability. We can view this as requiring a bound on where (𝕀​(𝟏T⋅𝐯≥v)​𝟏T)∈{0,1}k𝕀⋅subscript1𝑇𝐯𝑣subscript1𝑇superscript01𝑘(\\mathbb{I}(\\mathbf{1}_{T}\\cdot\\mathbf{v}\\geq v)\\mathbf{1}_{T})\\in\\{0,1\\}^{k} is a vector-valued function on ΔksubscriptΔ𝑘\\Delta_{k}, which takes the value 𝕀​(𝟏T⋅𝐯≥v)𝕀⋅subscript1𝑇𝐯𝑣\\mathbb{I}(\\mathbf{1}_{T}\\cdot\\mathbf{v}\\geq v) for coordinates in T𝑇T, and the value 00 for coordinates outside T𝑇T. The good news is that this setup fits the template of weighted calibration, where the class ℋℋ\\mathcal{H} contains all functions the form 𝕀​(𝟏T⋅𝐯≥v)𝕀⋅subscript1𝑇𝐯𝑣\\mathbb{I}(\\mathbf{1}_{T}\\cdot\\mathbf{v}\\geq v) for T∈{0,1}k,v∈ℝformulae-sequence𝑇superscript01𝑘𝑣ℝT\\in\\{0,1\\}^{k},v\\in\\mathbb{R}. The bad news is that we will show this problem is as hard as agnostically learning halfspaces. Daniely [Dan16] showed that, assuming the hardness of refuting random XorXor\\mathrm{Xor} CSPs, this problem cannot be solved in polynomial time. In projected smooth calibration, we replace the hard thresholds 𝕀​(𝟏T⋅𝐯≥v)𝕀⋅subscript1𝑇𝐯𝑣\\mathbb{I}(\\mathbf{1}_{T}\\cdot\\mathbf{v}\\geq v) with the class ℋpLip={ϕ​(𝐚⋅𝐯)}subscriptℋpLipitalic-ϕ⋅𝐚𝐯\\mathcal{H}_{\\mathrm{pLip}}=\\{\\phi({\\mathbf{a}}\\cdot\\mathbf{v})\\} where ϕ: [ref]1→[−1,1]:italic-ϕ→0111\\phi: [ref]1\\to[-1,1] is a Lipschitz continuous function and 𝐚∈[−1,1]k𝐚superscript11𝑘{\\mathbf{a}}\\in[-1,1]^{k}.\nIn particular, this includes indicator vectors for subsets.\nProjected smooth calibration requires that the weighted calibration error for the weight family ℋpLipksuperscriptsubscriptℋpLip𝑘\\mathcal{H}_{\\mathrm{pLip}}^{k} is bounded. For a joint distribution 𝒟𝒟\\mathcal{D} on predictions 𝐯𝐯\\mathbf{v} and true outcomes 𝐲𝐲\\mathbf{y}, the projected smooth calibration error is where ℋpLipksuperscriptsubscriptℋpLip𝑘\\mathcal{H}_{\\mathrm{pLip}}^{k} is the class of functions w:Δk→[−1,1]k:𝑤→subscriptΔ𝑘superscript11𝑘w:\\Delta_{k}\\to[-1,1]^{k} such that for every coordinate i𝑖i in w𝑤w’s output there is a 111-Lipschitz function ϕ(i): [ref]1→[−1,1]:superscriptitalic-ϕ𝑖→0111\\phi^{(i)}: [ref]1\\to[-1,1] s.t.\nw(i)​(𝐯)≡ϕ(i)​(𝐚⋅𝐯)superscript𝑤𝑖𝐯superscriptitalic-ϕ𝑖⋅𝐚𝐯w^{(i)}(\\mathbf{v})\\equiv\\phi^{(i)}({\\mathbf{a}}\\cdot\\mathbf{v}). A predictor satisfies projected smooth calibration if its error psCEpsCE\\mathrm{psCE} is bounded. We show that this definition satisfies all our desiderata: Property (1): expressive power. Projected smooth calibration guarantees that for every subset T⊆[k]𝑇delimited-[]𝑘T\\subseteq[k], the predicted probabilities for the binary classification task (namely, the outcome is in T𝑇T) satisfy smooth calibration, a well-studied calibration notion with several desirable properties [KF08, GKSZ22, BGHN23]. In particular, this implies that the predicted probabilities 𝟏T⋅𝐯⋅subscript1𝑇𝐯\\mathbf{1}_{T}\\cdot\\mathbf{v}\nthat the outcome will be in T𝑇T are close to being perfectly calibrated. The proof builds on the work of [BGHN23]. In particular, for each such subset T𝑇T, there exists a perfectly calibrated predictor pT∗subscriptsuperscript𝑝𝑇p^{*}_{T} for the binary classification task of determining whether the outcome will land in T𝑇T, whose predictions are close to 𝟏T⋅𝐯⋅subscript1𝑇𝐯\\mathbf{1}_{T}\\cdot\\mathbf{v} in earthmover distance. Thus, we get meaningful guarantees for a rich collection of downstream binary classification tasks (including subset membership and more). Property (2): Computational efficiency. We show an efficient algorithm for auditing whether the projected smooth calibration error of a predictor is bounded. Here we state our result informally, the formal statement is in Theorem 7.1. There is an algorithm for deciding whether the projected smooth calibration error is at most α𝛼\\alpha, with sample complexity and running time O​(kO​(1/α))𝑂superscript𝑘𝑂1𝛼O(k^{O(1/\\alpha)}). The work of [SSS11] showed that agnostic learning halfspaces becomes tractable if we replace the hard thresholds used in halfspaces with Lipschitz transfer functions. Building on their techniques, and using Jackson’s Theorem on low-degree uniform approximations for Lipschitz functions, we show that auditing for projected smooth calibration is polynomial time solvable. Moreover, our auditing algorithm is quite simple and does not need to solve a convex program, generalizing results in [KSJ18, BGHN23]. Our algorithm in fact solves the associated search problem, if p𝑝p is not calibrated, it finds a witness to the lack of calibration, which can be used to post-process p𝑝p and redcuce its calibration error without increasing the squared loss. Defining a recalibration algorithm correctly is subtle, see Definition 4.1 and the discussion preceding it. Our algorithm has running time polynomial in k𝑘k for every fixed constant α𝛼\\alpha, in contrast with previous results for expressive notions of calibration. If we only care about auditing using ϕ​(w⋅𝐯)italic-ϕ⋅𝑤𝐯\\phi(w\\cdot\\mathbf{v}) for vectors w𝑤w where ∥w∥22≤msubscriptsuperscriptdelimited-∥∥𝑤22𝑚\\left\\lVert w\\right\\rVert^{2}_{2}\\leq m then the sample complexity can be bounded by mO​(1/α)superscript𝑚𝑂1𝛼m^{O(1/\\alpha)}. This gives a running time fixed polynomial in k𝑘k (but exponential in α𝛼\\alpha) for subset calibration where we only care about bounded size subsets.\nOne can also get better run times by restricting the family of Lipschitz functions ϕitalic-ϕ\\phi. By restricting to auditors of the form tanh⁡(w⋅𝐯)⋅𝑤𝐯\\tanh(w\\cdot\\mathbf{v}) we get the weaker notion of sigmoid calibration, for which the auditor runs in time kO​(log⁡(1/α))superscript𝑘𝑂1𝛼k^{O(\\log(1/\\alpha))}. This can be seen as a smooth relaxation of the intractable notion of halfspace auditing. However, the improved efficiency comes at the price of some expressivity, we do not get closeness to perfect calibration for downstream subset classification tasks. It is interesting to investigate whether the exponential dependence on 1/α1𝛼1/\\alpha in Theorem 1.2 can be avoided. As a result in this direction, we show that the running time cannot be improved to poly⁡(k,1/α)poly𝑘1𝛼\\operatorname{poly}(k,1/\\alpha): Under standard complexity-theoretic assumptions,\nthere is no algorithm that can decide whether the projected smooth calibration error is at most α𝛼\\alpha with sample complexity and running time kO​(log0.99⁡(1/α))superscript𝑘𝑂superscript0.991𝛼k^{O(\\log^{0.99}(1/\\alpha))}. We prove the theorem by showing a reduction from the task of refuting random XOR formulas. Getting the right exponent for k𝑘k as a function of 1/α1𝛼1/\\alpha is an interesting question for future work. Property (3): Robustness. The works of [KF08, FH18, BGHN23] advocate the use of Lipschitz functions in defining calibration since it results in robust measures that do not change drastically under small perturbations of the predictor. Since projected smooth calibration is defined using Lipschitz functions, it is a robust calibration measure.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: the predicted probability is 𝟏T⋅𝐯⋅subscript1𝑇𝐯\\mathbf{1}_{T}\\cdot\\mathbf{v}.\nSay we want to enforce the calibration condition that when the predicted probability of belonging to T𝑇T exceeds v∈ [ref]1𝑣01v\\in [ref]1, the label indeed lies in T𝑇T with roughly the predicted probability. We can view this as requiring a bound on where\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"A Unifying Theory of Distance from Calibration\",\n                \"A gentle introduction to conformal prediction and distribution-free uncertainty quantification\",\n                \"Practical Adversarial Multivalid Conformal Prediction\",\n                \"Agnostic boosting\",\n                \"Introduction to approximation theory\",\n                \"Complexity Theoretic Limitations on Learning Halfspaces\",\n                \"Objective probability forecasts\",\n                \"Present position and potential developments: Some personal views statistical theory the prequential approach\",\n                \"Imagenet: A large-scale hierarchical image database\",\n                \"Outcome Indistinguishability\",\n                \"Beyond bernoulli: Generating random outcomes that cannot be distinguished from nature\",\n                \"From pseudorandomness to multi-group fairness and back\",\n                \"From average case complexity to improper learning complexity\",\n                \"Relations between average case complexity and approximation complexity\",\n                \"Distribution-specific agnostic boosting\",\n                \"On agnostic learning of parities, monomials, and halfspaces\",\n                \"Smooth calibration, leaky forecasts, finite recall, and nash dynamics\",\n                \"Loss Minimization Through the Lens Of Outcome Indistinguishability\",\n                \"Online Multivalid Learning: Means, Moments, and Prediction Intervals\",\n                \"From Boltzmann Machines to Neural Networks and Back Again\",\n                \"Reliably learning the relu in polynomial time\",\n                \"Omnipredictors\",\n                \"Low-Degree Multicalibration\",\n                \"On Calibration of Modern Neural Networks\",\n                \"Hardness of learning halfspaces with noise\",\n                \"Top-label calibration and multiclass-to-binary reductions\",\n                \"Multicalibration: Calibration for the (computationally-identifiable) masses\",\n                \"Batch multivalid conformal prediction\",\n                \"Deterministic calibration and nash equilibrium\",\n                \"Novel decompositions of proper scoring rules for classification: Score adjustment as precursor to calibration\",\n                \"Potential-based agnostic boosting\",\n                \"Agnostically learning halfspaces\",\n                \"U-Calibration: Forecasting for an Unknown Agent\",\n                \"On agnostic boosting and parity learning\",\n                \"Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration\",\n                \"Trainable calibration measures for neural networks from kernel mean embeddings\",\n                \"On the Computational Efficiency of Training Neural Networks\",\n                \"Randomized algorithms\",\n                \"High-Dimensional Prediction for Sequential Decision Making\",\n                \"Making polynomials robust to noise\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of On Computationally Efficient Multi-Class Calibration. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}