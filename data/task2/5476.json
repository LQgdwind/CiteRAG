{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Ethical Considerations for Responsible Data Curation\n\nPaper Abstract: \n\nPaper Section Text:\nRepresentational and historical biases.\nThe Council of Europe have expressed concerns about the threat posed by AI systems to equality and non-discrimination principles . Many dataset creators often prioritize protected attributes, i.e., gender, race, and age, as key factors of dataset diversity . Nevertheless, most HCCV datasets exhibit historical and representational biases     . These biases can be pernicious, particularly when models learn and amplify them. For instance, image captioning models may rely on contextual cues related to activities like shopping  and laundry  to generate gendered captions. Spurious correlations are detrimental, as they are not causally related and perpetuate harmful associations  . In addition, prominent examples in HCCV research demonstrate disparate algorithmic performance based on race and skin color              . Most recently, autonomous robots have displayed racist, sexist, and physiognomic stereotypes . Furthermore, face detection models have shown lower accuracy when processing images of older individuals compared to younger individuals . While not endorsing these applications, discrepancies have also been observed in facial emotion recognition services for children in both commercial and research systems  , as well as age estimation   . Despite concerns regarding privacy, liability, and public relations, the collection of special and sensitive category data is crucial for bias assessments . GDPR guidance from the UK ICO confirms that sensitive attributes can be collected for fairness purposes . However, obtaining this information presents challenges, such as historical mistrust in clinical research among African-Americans   or the social stigma of being photographed that some women face . Nonetheless, marginalized communities may require explicit explanations and assurances about data usage to address concerns related to service provision, security, allocation, and representation 9. This is particularly important as remaining unseen does not protect against being mis-seen 9. The digital divide and accessibility.\nHealthcare datasets often lack representation of minority populations, compromising the reliability of automated decisions 6. The World Health Organization (WHO) emphasizes the need for data accuracy, completeness, and diversity, particularly regarding age, in order to address ageism in AI 5. ML systems may prioritize younger populations for resource allocation, assuming they would benefit the most in terms of life expectancy 5. The digital divide further exacerbates the underrepresentation of vulnerable groups, including older generations, low-income school-aged children, and children in East Asia and the South Pacific who lack access to digital technology 4 . Insufficient access to digital technology hampers the representation of vulnerable persons in datasets , leading to outcome homogenization—i.e., the systematic failing of the same individuals or groups . Confused taxonomies.\nSex and gender are often used interchangeably, treating gender as a consequence of one’s assigned sex at birth . However, this approach erases intersex individuals who possess non-binary physiological sex characteristics . Treating sex and gender as interchangeable perpetuates normative views by casting gender as binary, immutable, and solely based on biological sex [ref]179. This perspective disregards transgender and gender nonconforming individuals. Moreover, sex, like gender, is a social construct, as sexed bodies do not exist outside of their social context . Similar to sex and gender, race and ethnicity are often used synonymously . Nations employ diverse census questions to ascertain ethnic group composition, encompassing factors such as nationality, race, color, language, religion, customs, and tribe . However, these categories and their definitions lack consistency over time and geography, often influenced by political agendas and socio-cultural shifts . This variability makes it challenging to collect globally representative and meaningful data on ethnic groups. Consequently, several HCCV datasets have incorporated inconsistent and arbitrary racial categorization systems  [ref]374  . For instance, the FairFace dataset [ref]170 creators reference the US Census Bureau’s racial categories without considering the social definition of race they represent . The US Census Bureau explicitly states that their categories reflect a social definition rather than a biological, anthropological, or genetic one. Consequently, labeling the “physical race” of image subjects based on nonphysiological categories is contradictory. Furthermore, the FairFace creators do not disclose the demographics or cultural compatibility of their annotators. Own-anchor bias.\n\nHCCV approaches for encoding age in datasets vary, using either integer labels        [ref]374  or group labels    . Age groupings are often preferred when collecting unconstrained images from the web, as human annotators must infer subjects’ ages, which is challenging . This is evident in crowdsourced annotations, where 40.2% of individuals in the OpenImages MIAP dataset  could not be categorized into an age group. Factors unrelated to age, such as facial expression    and makeup   , influence age perception. Furthermore, annotators have exhibited lower accuracy when labeling people outside of their own demographic group       . Post hoc rationalization of the use of physiological markers.\n\nGender information about data subjects is obtained through inference [ref]170  [ref]374    [ref]374      or self-identification     . Inference raises concerns as it assumes that gender can be determined solely from imagery without consent or consultation with the subject, which is noninclusive and harmful [ref]179  7. Even when combined with non-image-based information, inferred gender fails to account for the fluidity of identity, potentially mislabeling subjects at the time of image capture  . Moreover, physical traits are just one of many dimensions, including posture, clothing, and vocal cues, used to infer not only gender but also race [ref]179  . Erasure of nonstereotypical individuals.\n\nHCCV datasets frequently adopt a US-based racial schema [ref]170     , which may oversimplify and essentialize groups . This approach may not align with other more nuanced models, e.g., the continuum-based color system used in Brazil, which considers a wide range of physical characteristics. Nonconsensual image datasets rely on annotators to assign semantic categories, perpetuating stereotypes and disseminating them beyond their cultural context . Notably, images without label consensus are often discarded [ref]170  , potentially excluding individuals who defy stereotypes, such as multi-ethnic individuals . Phenotypic attributes.\nProtected attributes may not be the most appropriate criteria for evaluating HCCV models . Social constructs like race and gender lack clear delineations for subgroup membership based on visible or invisible characteristics. These labels capture invisible aspects of identity that are not solely determined by visible appearance. Moreover, the phenotypic characteristics within and across subgroups exhibit significant variability   6   . Environment and instrument.\nThe image capture device and environmental conditions significantly influence model performance, and their impact should be considered . Factors such as camera software, hardware, and environmental conditions affect HCCV model robustness in various settings 3        . Understanding performance differences is crucial from ethical and scientific perspectives. For example, sensitivity to illumination or white balance may be linked to sensitive attributes, e.g., skin tone    , while available instruments or environmental co-occurrences may correlate with demographic attributes   . Annotator positionality.\n\nPsychological research highlights the influence of annotators’ sociocultural background on their visual perception      . However, recent empirical studies have evidenced a lack of regard for the impact an annotator’s social identity has on data 9 . Only a handful of HCCV datasets provide annotator demographic details    . Recruitment and compensation.\nData collected without consent patently lacks compensation. Balancing between excessive and deficient payment is crucial to avoid coercion and exploitation  . An additional concern is the employment of remote workers from disadvantaged regions , often with low wages and fast-paced work conditions 1   . This can lead to arbitrary denial of payment based on opaque quality criteria 8 and prevents union formation , creating a sense of invisibility and uncertainty for workers .\n\nReference markers found: [179]\n\nContext windows for each [ref]:\n[ref]179: non-binary physiological sex characteristics . Treating sex and gender as interchangeable perpetuates normative views by casting gender as binary, immutable, and solely based on biological sex [ref]179. This perspective disregards transgender and gender nonconforming individuals. Moreover, sex, like gender, is a social construct, as sexed bodies do not exist outside of their social\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 179,\n            \"titles\": [\n                \"One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision\",\n                \"Announcing the NeurIPS Code of Ethics\",\n                \"OpenAI and Microsoft Sued for $3 Billion Over Alleged ChatGPT ’Privacy Violations’ — vice.com\",\n                \"Shared, but not up for grabs\",\n                \"What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance\",\n                \"Data Readiness Report\",\n                \"Gendered Differences in Face Recognition Accuracy Explained by Hairstyles, Makeup, and Facial Morphology\",\n                \"Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings\",\n                \"An own-age bias in face recognition for children and older adults\",\n                \"Evidence for an own-age bias in face recognition\",\n                \"The human in emotion recognition on social media: Attitudes, outcomes, risks\",\n                \"The hidden fingerprint inside your photos\",\n                \"A View From Somewhere: Human-Centric Face Representations\",\n                \"2d human pose estimation: New benchmark and state of the art analysis\",\n                \"Working to address algorithmic bias? don’t overlook the role of demographic data\",\n                \"”What We Can’t Measure, We Can’t Understand”: Challenges to Demographic Data Procurement in the Pursuit of Fairness\",\n                \"Pruning training sets for learning of object categories\",\n                \"Operationalising the collection of ethnicity data in studies of the sociology of health and illness\",\n                \"Towards causal benchmarking of bias in face analysis algorithms\",\n                \"Human phenotypic diversity\",\n                \"Students can adjust their pronouns and gender identity in bu’s updated data system\",\n                \"What’s Your Pronoun?: Beyond He and She\",\n                \"Google mistakenly tags Black people as ‘gorillas,’ showing limits of algorithms\",\n                \"Skin Deep: Investigating Subjectivity in Skin Tone Annotations for Computer Vision Benchmark Datasets\",\n                \"Principles of biomedical ethics: marking its fortieth anniversary\",\n                \"A survey on facial soft biometrics for video surveillance and forensic applications\",\n                \"Recognition in Terra Incognita\",\n                \"Data statements for natural language processing: Toward mitigating system bias and enabling better science\",\n                \"Racial categories in machine learning\",\n                \"Detecting discriminatory risk through data annotation based on Bayesian inferences\",\n                \"Representation in ai evaluations\",\n                \"Children and the data cycle: Rights and ethics in a big data world\",\n                \"Algorithmic colonization of africa\",\n                \"Automating ambiguity: Challenges and pitfalls of artificial intelligence\",\n                \"Large image datasets: A pyrrhic win for computer vision?\",\n                \"Multimodal datasets: misogyny, pornography, and malignant stereotypes\",\n                \"Why is data on disability so hard to collect and understand?\",\n                \"Making Intelligence: Ethical Values in IQ and ML Benchmarks\",\n                \"Picking on the same person: Does algorithmic monoculture lead to outcome homogenization?\",\n                \"Respondent burden: health survey research methods\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]179 based on the context and research area of Ethical Considerations for Responsible Data Curation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}