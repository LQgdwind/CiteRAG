{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: LPC-AD: Fast and Accurate Multivariate Time Series Anomaly Detection via Latent Predictive Coding\n\nPaper Abstract: \n\nPaper Section Text:\nMTS anomaly detection is a challenging problem, and\nit has been studied extensively.\nFrom a methodological perspective,\nprevious works can be categorized into two research lines,\ni.e., (1) classical methods and (2) deep learning based methods.\nNotable classical methods include\n     ,\njust to name a few.\nOne limitation of classical methods is that the expressiveness is low,\nresulting low anomaly detection accuracy .\nDeep learning based methods address this limitation.\nOur work falls into the research line of deep learning based methods.\nMethodologically, previous works on deep learning based\nMTS anomaly detection methods\ncan be categorized into four groups:\n(1) forecasting based methods, (2) reconstruction based methods,\n(3) hybrid methods combining forecasting and reconstruction\n,\nand\n (4) miscellaneous methods.\n Forecasting based MTS anomaly detection methods    [ref]13 \nuse a prediction function to capture normal spatial and temporal\ndependence in MTS,\nand they identify abnormal dependence, i.e., report anomaly,\nwhen the prediction error is relatively large.\nThe LSTM-AD  uses a stacked LSTM to\npredict future data points from historical data points.\nIt uses the likelihood of the prediction error to quantify the anomaly\nscore of a data point,\nand it reports an anomaly whenever the anomaly score exceeds a given alert threshold.\nChauhan et al. applied the LSTM-AD to detect anomalies in electrocardiography time series .\nBontemps et al.  used the vanilla LSTM for forecasting\nand quantified the anomaly score via the prediction errors of several steps.\nLSTM-NDT[ref]13 also uses the vanilla LSTM for forecasting and uses the prediction error of\na data point to quantify the anomaly score.\nIt develops a non-parameterized method to select the alert threshold.\nHowever, the detection accuracy of\nthese LSTM based MTS anomaly detection methods\n   [ref]13\nis not high enough, because LSTM alone lacks expressiveness to capture complex dependence in MTS [ref]37.\nDeng et al.  proposed\na graph neural network to capture spatial dependence\nof multi-time series explicitly\nand used a graph attention network to predict future data points\nfrom historical data points.\nHowever, the graph attention network makes this method\nunable to capture long-term dependence well.\nThese forecasting based methods have a relatively fast training speed,\nbut their detection accuracy is greatly outperformed by reconstruction\nbased methods   [ref]37. Reconstruction based methods\ncapture the normal spatial and temporal dependence in multi-time series\nvia a low dimensional latent representation,\nand they identify abnormal dependence, i.e., report anomaly,\nwhen the reconstruction from the low dimensional latent representation differs from the actual observation.\nFrom a model architecture’s perspective,\nprevious works can be categorized into\nautoencoder based ones [ref]4   [ref]37  \nand variational autoencoder based ones     . Autoencoder (AE) based methods.\nThe EncDec-AD \ninstantiates the autoencoder architecture with the vanilla LSTM for MTS\nanomaly detection.\nThe reconstruction error of a data point quantifies the anomaly score,\nand EncDec-AD reports an anomaly whenever the anomaly score exceeds a given threshold.\nThe MSCRED  improves the expressiveness of EncDec-AD.\nIt uses a convolutional network to capture spatial dependence and\na convolutional LSTM to capture temporal dependence.\nHowever, it is time-consuming to train the model, and it requires\na large amount of training data to fit the model well.\nRecent autoencoder based methods [ref]4  [ref]37\ncontribute simple models with a fast training speed,\nwhile retaining a fairly high detection accuracy.\nThe openGauss  instantiates the autoencoder architecture with\na tree-based LSTM for MTS anomaly detection.\nIt has a fast training speed and low memory requirement.\nUSAD [ref]4 further improves the\ntraining speed via adversarial training.\nIt has one simple encoder and two decoders.\nTwo decoders share the same encoder, and\nthe purpose of two decoders is to enable adversarial training,\nwhich amplifies the temporal anomaly.\nThe encoder and two decoders are composed of several linear layers,\nenabling a fast training speed.\nHowever, simple linear layers cannot capture temporal dependence well\nbecause they can not process longer input.\nTranAD [ref]37 replaced the linear layer of USAD\nwith transformer network\nto capture temporal dependence better.\nTranAD contributes a slightly more sophisticated model than USAD.\nIt is shown to have a higher detection accuracy than USAD,\nwhile retaining a fast training speed.\nCompared to these works, our work presents a new architecture,\nwhich combines autoencoder,\npredictive coding and randomized perturbation.\nFurthermore, our work outperforms them in both training speed\nand detection accuracy. Variational autoencoder (VAE) based methods.:\nThe Donut  extends the vanilla VAE by\nchanging the linear layers to represent the variance of latent variables to\na layer with the soft plus operation.\nThe LSTM-VAE  instantiates the VAE architecture by\nreplacing the feed-forward network of VAE with a vanilla LSTM network to\ncapture the temporal dependence in MTS.\nTo improve the expressiveness of Donut and LSTM-VAE,\nOmniAnomaly  changes the latent variables’ distribution of\nLSTM-VAE from standard Gaussian distribution\nto a more sophisticated distribution.\nThe sophisticated distribution in OmniAnomaly is a combination of\na linear Gaussian state space model \nand a normalizing flow .\nOmniAnomaly further uses latent variable linking to enhance the temporal dependence between latent variables.\nInterFusion  improves OmniAnomaly by\nusing an anomaly pre-filtering algorithm to filter out potential anomalous data in the training set.\nIt also compresses the multi-time series\nvia two-view embedding.\nTo better capture spatial dependence in MTS,\nthe SDFVAE \ninstantiates the VAE architecture with a convolutional neural network, a BiLSTM, and recurrent VAE.\nAlthough OmniAnomaly, InterFusion and SDFVAE have high expressiveness\nbecause of their sophisticated models,\ntheir training processes are very resource-intensive and time-consuming.\nDifferent from them, our work is an autoencoder based method,\nand it outperforms them in both training speed\nand detection accuracy. Hybrid methods combine forecasting-based methods and reconstruction based methods\nin the sense that they jointly optimize a forecasting model and a reconstruction model.\nThey attempt to overcome the shortcomings that forecasting based methods\nand reconstruction based methods suffer on their own .\nOne notable architecture of hybrid methods is proposed by Srivastava et al. \nThis architecture contains one encoder and two decoders sharing the same encoder.\nOne of the decoders is designed for forecasting purposes,\nand the other is designed for reconstruction purposes.\nIn the seminal work on this architecture, Srivastava et al. \ninstantiated this architecture with the vanilla LSTM.\nMedel et al.  instantiated this architecture\nwith convolutional LSTM to have a high expressiveness capability.\nZhao et al.  instantiated this architecture\nwith a 3D convolutional neural network\nfor better detection of video series anomalies.\nThese methods are mainly designed for video series,\nand they do not capture the spatial and temporal dependence in multi-time series well.\nThe MTAD-GAT  is composed of a graph attention network for forecasting,\nwhich captures the spatial dependence,\nand a vanilla VAE for reconstruction, which captures the likelihood of a data point.\nIt jointly optimizes those two models and quantifies the anomaly score with\nboth the prediction error and the reconstruction error for anomaly detection.\nThe CAEM  is composed of a convolutional autoencoder\nfor reconstruction and a BiLSTM for forecasting.\nIt also jointly optimizes those two models and quantifies the anomaly score with\nboth the prediction error and the reconstruction error for anomaly detection.\nHowever, these methods have a high computational cost in training\nand low scalability for high-dimensional datasets.\nThey are outperformed by autencoder based methods\nlike TranAD [ref]37. Miscellaneous methods refer to notable related works that do not fall into the above research lines.\nThe DAGMM  is composed of a simple autoencoder model for dimensionality reduction\nand a Gaussian Mixture Model  for density estimation.\nThe density estimated by the GMM is used to predict the next data point.\nHowever, it is computationally expensive for training and unable to capture the spatial-temporal dependence well.\nRen et al.  proposed estimators for the implicit likelihoods of GANs,\nand they showed that such estimators could be applied to detect anomalies\nin multi-time series data.\nThe MAD-GAN  instantiated the vanilla GAN architecture with LSTM-RNN.\nKoopman operator is a classical method in control theory .\nIt has a similar idea of latent predictive coding.\nTheoretically, in an infinite-dimensional latent state space,\nthe next state can be a linear function of the states in a recent time window .\nIn particular, when we instantiate the predictor in our LPC-AD architecture as a linear function,\nwe get a deep Koopman operator .\nFor practical applications where we cannot have finite-dimensional states,\nnon-linear predictor function is needed for state transitions in latent space.\nThis is validated by our experiment in Figure 11,\nwhere we observe that using non-linear predictor functions\nimproves the accuracy of anomaly detection. Last, in terms of design objective, our work is closely related to USAD [ref]4 and TranAD [ref]37.\nTo the best of our knowledge, USAD and TranAD are the only two works that\naim to reduce training time while retaining a fairly high detection accuracy.\nWe proposed a generic architecture and show that simple instantiations of this\narchitecture can outperform USAD and TranAD in training speed\nand outperform SOAT methods (including sophisticated models) in detection accuracy.\nThe design of our architecture reveals new insights in exploring latent predictive coding\nand randomized perturbation for MTS anomaly detection.\n\nReference markers found: [13]\n\nContext windows for each [ref]:\n[ref]13: based methods, (2) reconstruction based methods,\n(3) hybrid methods combining forecasting and reconstruction\n,\nand\n (4) miscellaneous methods.\n Forecasting based MTS anomaly detection methods    [ref]13 \nuse a prediction function to capture normal spatial and temporal\ndependence in MTS,\nand they identify abnormal dependence, i.e., report anomaly,\nwhen the prediction error is relatively\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 13,\n            \"titles\": [\n                \"Elle: Inferring Isolation Anomalies from Experimental Observations\",\n                \"Perturbation techniques in online learning and optimization\",\n                \"Wadi: a water distribution testbed for research in the design of secure cyber physical systems\",\n                \"Adaptive predictive coding of speech signals\",\n                \"Usad: unsupervised anomaly detection on multivariate time series\",\n                \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n                \"Efficient anomaly detection by isolation using nearest neighbour ensemble\",\n                \"Collective anomaly detection based on long short-term memory recurrent neural networks\",\n                \"Anomaly detection in ecg time signals via deep long short-term memory networks\",\n                \"Sdfvae: Static and dynamic factorized vae for anomaly detection of multivariate cdn kpis\",\n                \"Graph Neural Network-Based Anomaly Detection in Multivariate Time Series\",\n                \"Unsupervised Visual Representation Learning by Context Prediction\",\n                \"Hitanomaly: Hierarchical transformers for anomaly detection in system log\",\n                \"Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding\",\n                \"Linear gaussian state space modeling\",\n                \"MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks\",\n                \"opengauss: An autonomous database system\",\n                \"Multivariate time series anomaly detection and interpretation using hierarchical inter-metric and temporal embedding\",\n                \"MicroHECL: High-Efficient Root Cause Localization in Large-Scale Microservice Systems\",\n                \"Isolation forest\",\n                \"Deep learning for universal linear embeddings of nonlinear dynamics\",\n                \"Diagnosing root causes of intermittent slow queries in cloud databases\",\n                \"Lstm-based encoder-decoder for multi-sensor anomaly detection\",\n                \"Swat: A water treatment testbed for research and training on ics security\",\n                \"Anomaly detection in video using predictive convolutional long short-term memory networks\",\n                \"Localizing failure root causes in a microservice through causality inference\",\n                \"Efficient estimation of word representations in vector space\",\n                \"A Multimodal Anomaly Detector for Robot-Assisted Feeding Using an LSTM-based Variational Autoencoder\",\n                \"An overview of anomaly detection techniques: Existing solutions and latest technological trends\",\n                \"A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction\",\n                \"Estimate the implicit likelihoods of gans with application to anomaly detection\",\n                \"Gaussian mixture models\",\n                \"Variational inference with normalizing flows\",\n                \"Anomaly detection using autoencoders with nonlinear dimensionality reduction\",\n                \"Unsupervised learning of video representations using lstms\",\n                \"Robust anomaly detection for multivariate time series through stochastic recurrent neural network\",\n                \"TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data\",\n                \"Attention Is All You Need\",\n                \"Real-time sensor anomaly detection and recovery in connected automated vehicle sensors\",\n                \"Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]13 based on the context and research area of LPC-AD: Fast and Accurate Multivariate Time Series Anomaly Detection via Latent Predictive Coding. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}