{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Runtime analysis of a coevolutionary algorithm on impartial combinatorial games\n\nPaper Abstract: \n\nPaper Section Text:\nRather than storing a population as a set of points in the search space, as is the case for most EAs, an estimation of distribution algorithm (EDA) represents its population as a probability distribution over the search spaceÂ . Whereas most algorithms sample candidates for selection from their current population uniformly at random, an EDA instead samples from its probability distribution. After selection has been completed, the selected individuals are then used to update the probability distribution for the next generation. Much of the existing runtime analysis for EDAs (seeÂ     ) has emphasised the benefit provided by the high level of diversity among generated search points. This is also the case in the recent first runtime analysis of a coevolutionary EDAÂ , wherein the difficulty presented by locally intransitive payoff landscapes could be provably averted by evaluating strategies against a diverse set of opponents generated by an EDA. As intransitivity is also apparent in impartial combinatorial games, a coevolutionary EDA is a good candidate for a first runtime analysis on this topic too. Most existing theoretical analysis of EDAs concerns those operating over bitstrings â€“ that is, {0,1}nsuperscript01ğ‘›\\{0,1\\}^{n} is the search domain. However, as outlined in SectionÂ 2, our formulation of strategies gives rise to a more complicated search domain. For a parent set Sğ‘†S, we are considering search domains of the form ğ’³=âˆiâˆˆISiğ’³subscriptproductğ‘–ğ¼subscriptğ‘†ğ‘–\\mathcal{X}=\\prod_{i\\in I}S_{i}, where Iğ¼I is an indexing set and SiâŠ†Ssubscriptğ‘†ğ‘–ğ‘†S_{i}\\subseteq S for each iâˆˆIğ‘–ğ¼i\\in I. Given a tuple pâˆˆâˆiâˆˆIğ’«â€‹(Si)ğ‘subscriptproductğ‘–ğ¼ğ’«subscriptğ‘†ğ‘–p\\in\\prod_{i\\in I}\\mathcal{P}(S_{i}), let Univâ€‹(ğ’³,p)Univğ’³ğ‘\\text{Univ}(\\mathcal{X},p) denote the probability distribution over ğ’³ğ’³\\mathcal{X} such that, if xâˆ¼Univâ€‹(ğ’³,p)similar-toğ‘¥Univğ’³ğ‘x\\sim\\text{Univ}(\\mathcal{X},p), then for any yâˆˆğ’³ğ‘¦ğ’³y\\in\\mathcal{X}, For notational convenience, given a tuple pâˆˆâˆiâˆˆIğ’«â€‹(Si)ğ‘subscriptproductğ‘–ğ¼ğ’«subscriptğ‘†ğ‘–p\\in\\prod_{i\\in I}\\mathcal{P}(S_{i}) we will often write for iâˆˆIğ‘–ğ¼i\\in I and sâˆˆSğ‘ ğ‘†s\\in S, The EDA we consider will represent its current population as an element pâˆˆâˆiâˆˆIğ’«â€‹(Si)ğ‘subscriptproductğ‘–ğ¼ğ’«subscriptğ‘†ğ‘–p\\in\\prod_{i\\in I}\\mathcal{P}(S_{i}), with individuals being generated according to Univâ€‹(ğ’³,p)Univğ’³ğ‘\\text{Univ}(\\mathcal{X},p). In the case where I=[n]ğ¼delimited-[]ğ‘›I=[n] and Si={0,1}subscriptğ‘†ğ‘–01S_{i}=\\{0,1\\} for each iâˆˆ[n]ğ‘–delimited-[]ğ‘›i\\in[n], we recover the standard framework for univariate EDAs operating over bitstrings. For these EDAs, the tuple pâˆˆâˆiâˆˆ[n]ğ’«â€‹({0,1})ğ‘subscriptproductğ‘–delimited-[]ğ‘›ğ’«01p\\in\\prod_{i\\in[n]}\\mathcal{P}(\\{0,1\\}) is often represented as a frequency vector (pâ€‹(1),â€¦,pâ€‹(n))âˆˆ[ref]0 nğ‘1â€¦ğ‘ğ‘›superscript01ğ‘›(p(1),\\ldots,p(n))\\in[ref]0 ^{n}, where pâ€‹(i)ğ‘ğ‘–p(i) is the probability that xâˆ¼Univâ€‹({0,1}n,p)similar-toğ‘¥Univsuperscript01ğ‘›ğ‘x\\sim\\text{Univ}(\\{0,1\\}^{n},p) has a 111-bit in position iğ‘–i. A common feature for EDAs operating over bitstrings is to constrain these frequencies to the interval [Î³,1âˆ’Î³]ğ›¾1ğ›¾[\\gamma,1-\\gamma] for some small Î³ğ›¾\\gamma at the end of each generation. For the general case, where we track a tuple pâˆˆâˆiâˆˆIğ’«â€‹(Si)ğ‘subscriptproductğ‘–ğ¼ğ’«subscriptğ‘†ğ‘–p\\in\\prod_{i\\in I}\\mathcal{P}(S_{i}), we need to constrain each pâ€‹(i)âˆˆğ’«â€‹(Si)ğ‘ğ‘–ğ’«subscriptğ‘†ğ‘–p(i)\\in\\mathcal{P}(S_{i}) to the set ğ’«Î³â€‹(Si)subscriptğ’«ğ›¾subscriptğ‘†ğ‘–\\mathcal{P}_{\\gamma}(S_{i}). To achieve this, we adopt the following minor variation of the multi-valued EDA framework proposed by Ben Jedidia, Doerr, and KrejcaÂ . Given Î³âˆˆ[0,1|S|)ğ›¾01ğ‘†\\gamma\\in[0,\\textstyle{\\frac{1}{|S|}}) and pâˆˆğ’«â€‹(S)ğ‘ğ’«ğ‘†p\\in\\mathcal{P}(S), let so that where (2) follows from (1) using the fact that 1âˆ’Î³â€‹|S|>01ğ›¾ğ‘†01-\\gamma|S|>0. Let Ï€Î³S:ğ’«â€‹(S)â†’ğ’«Î³â€‹(S):superscriptsubscriptğœ‹ğ›¾ğ‘†â†’ğ’«ğ‘†subscriptğ’«ğ›¾ğ‘†\\pi_{\\gamma}^{S}:\\mathcal{P}(S)\\to\\mathcal{P}_{\\gamma}(S) then be the function given by If pâˆˆğ’«â€‹(S)ğ‘ğ’«ğ‘†p\\in\\mathcal{P}(S), then setting S+={sâˆˆS:pâ€‹(s)â©¾Î³}superscriptğ‘†conditional-setğ‘ ğ‘†ğ‘ğ‘ ğ›¾S^{+}=\\{s\\in S:p(s)\\geqslant\\gamma\\} and Sâˆ’=Sâˆ–S+superscriptğ‘†ğ‘†superscriptğ‘†S^{-}=S\\setminus S^{+} we have and so Ï€Î³Ssuperscriptsubscriptğœ‹ğ›¾ğ‘†\\pi_{\\gamma}^{S} indeed defines a function ğ’«â€‹(S)â†’ğ’«Î³â€‹(S)â†’ğ’«ğ‘†subscriptğ’«ğ›¾ğ‘†\\mathcal{P}(S)\\to\\mathcal{P}_{\\gamma}(S). It is not difficult to deduce that if pâ€‹(s)â©¾Î³ğ‘ğ‘ ğ›¾p(s)\\geqslant\\gamma, then (1âˆ’Î³â€‹|S|)â€‹pâ€‹(s)â©½Ï€Î³Sâ€‹(p)â€‹(s)â©½pâ€‹(s)1ğ›¾ğ‘†ğ‘ğ‘ superscriptsubscriptğœ‹ğ›¾ğ‘†ğ‘ğ‘ ğ‘ğ‘ (1-\\gamma|S|)p(s)\\leqslant\\pi_{\\gamma}^{S}(p)(s)\\leqslant p(s). For the case |S|=2ğ‘†2|S|=2 we obtain and so this model fits the usual method for constraining univariate EDAs over bitstrings. (Despite some differences in notation, the function Ï€Î³Ssuperscriptsubscriptğœ‹ğ›¾ğ‘†\\pi_{\\gamma}^{S} is nearly identical the restriction described inÂ . In the context of [1, SectionÂ 4.2], our only modification is to forego an initial clamping of probabilities to the interval [Î³,1âˆ’(|S|âˆ’1)â€‹Î³]ğ›¾1ğ‘†1ğ›¾[\\gamma,1-(|S|-1)\\gamma], as the upper border of 1âˆ’(|S|âˆ’1)â€‹Î³1ğ‘†1ğ›¾1-(|S|-1)\\gamma is already implied by the fact that the remaining steps produce an element of ğ’«Î³â€‹(S)subscriptğ’«ğ›¾ğ‘†\\mathcal{P}_{\\gamma}(S). Indeed, an actual difference between the two methods only arises for inputs pğ‘p satisfying maxsâˆˆSâ¡pâ€‹(s)>1âˆ’(|S|âˆ’1)â€‹Î³subscriptğ‘ ğ‘†ğ‘ğ‘ 1ğ‘†1ğ›¾\\max_{s\\in S}p(s)>1-(|S|-1)\\gamma, and even in such cases the difference is not significant.) We next collect some simple properties of Ï€Î³Ssuperscriptsubscriptğœ‹ğ›¾ğ‘†\\pi_{\\gamma}^{S} that will be useful in the proofs later. If pâ€‹(s)â©¾Î³ğ‘ğ‘ ğ›¾p(s)\\geqslant\\gamma, then (1âˆ’Î²Î³âˆ’â€‹(p)1âˆ’Î³â€‹|S|)â€‹pâ€‹(s)â©½Ï€Î³Sâ€‹(p)â€‹(s)â©½pâ€‹(s)1superscriptsubscriptğ›½ğ›¾ğ‘1ğ›¾ğ‘†ğ‘ğ‘ superscriptsubscriptğœ‹ğ›¾ğ‘†ğ‘ğ‘ ğ‘ğ‘ \\left(1-\\textstyle{\\frac{\\beta_{\\gamma}^{-}(p)}{1-\\gamma|S|}}\\right)p(s)\\leqslant\\pi_{\\gamma}^{S}(p)(s)\\leqslant p(s). For any SiâŠ†Ssubscriptğ‘†ğ‘–ğ‘†S_{i}\\subseteq S, pâˆˆğ’«â€‹(Si)ğ‘ğ’«subscriptğ‘†ğ‘–p\\in\\mathcal{P}(S_{i}) and sâˆˆSğ‘ ğ‘†s\\in S, Ï€Î³Siâ€‹(p)â€‹(s)â©½maxâ¡{Î³,pâ€‹(s)}superscriptsubscriptğœ‹ğ›¾subscriptğ‘†ğ‘–ğ‘ğ‘ ğ›¾ğ‘ğ‘ \\pi_{\\gamma}^{S_{i}}(p)(s)\\leqslant\\max{\\{\\gamma,p(s)\\}}. For any Si,AâŠ†Ssubscriptğ‘†ğ‘–ğ´ğ‘†S_{i},A\\subseteq S and pâˆˆğ’«â€‹(Si)ğ‘ğ’«subscriptğ‘†ğ‘–p\\in\\mathcal{P}(S_{i}), Ï€Î³Siâ€‹(p)â€‹(A)â©½pâ€‹(A)+Î³â€‹|Si|superscriptsubscriptğœ‹ğ›¾subscriptğ‘†ğ‘–ğ‘ğ´ğ‘ğ´ğ›¾subscriptğ‘†ğ‘–\\pi_{\\gamma}^{S_{i}}(p)(A)\\leqslant p(A)+\\gamma|S_{i}|. B1: If pâ€‹(s)â©¾Î³ğ‘ğ‘ ğ›¾p(s)\\geqslant\\gamma, then by setting Î±=Î²Î³âˆ’â€‹(p)/Î²Î³+â€‹(p)ğ›¼superscriptsubscriptğ›½ğ›¾ğ‘superscriptsubscriptğ›½ğ›¾ğ‘\\alpha=\\beta_{\\gamma}^{-}(p)/\\beta_{\\gamma}^{+}(p), and so B1 holds. B2: If pâ€‹(s)â©½Î³ğ‘ğ‘ ğ›¾p(s)\\leqslant\\gamma then Ï€Î³Siâ€‹(p)â€‹(s)â©½Î³=maxâ¡{Î³,pâ€‹(s)}superscriptsubscriptğœ‹ğ›¾subscriptğ‘†ğ‘–ğ‘ğ‘ ğ›¾ğ›¾ğ‘ğ‘ \\pi_{\\gamma}^{S_{i}}(p)(s)\\leqslant\\gamma=\\max{\\{\\gamma,p(s)\\}}. On the hand, if pâ€‹(s)â©¾Î³ğ‘ğ‘ ğ›¾p(s)\\geqslant\\gamma, then B1 implies that Ï€Î³Siâ€‹(p)â©½pâ€‹(s)=maxâ¡{Î³,pâ€‹(s)}superscriptsubscriptğœ‹ğ›¾subscriptğ‘†ğ‘–ğ‘ğ‘ğ‘ ğ›¾ğ‘ğ‘ \\pi_{\\gamma}^{S_{i}}(p)\\leqslant p(s)=\\max{\\{\\gamma,p(s)\\}}. In either case, B2 holds. B3: We can compute as required.\nâˆ A description of the algorithm we analyse is now provided by AlgorithmÂ 1, which effectively generalises the version appearing inÂ  (which applied only to bitstrings and omitted the step involving Ï€Î³Sisuperscriptsubscriptğœ‹ğ›¾subscriptğ‘†ğ‘–\\pi_{\\gamma}^{S_{i}}). Note that due to the use of Ï€Î³Sisuperscriptsubscriptğœ‹ğ›¾subscriptğ‘†ğ‘–\\pi_{\\gamma}^{S_{i}} in lineÂ 23, we always have ptâˆˆâˆiâˆˆIğ’«Î³â€‹(Si)subscriptğ‘ğ‘¡subscriptproductğ‘–ğ¼subscriptğ’«ğ›¾subscriptğ‘†ğ‘–p_{t}\\in\\prod_{i\\in I}\\mathcal{P}_{\\gamma}(S_{i}). A key step towards analysing the performance of AlgorithmÂ 1 on impartial combinatorial games is understanding the distribution of a selected individual Pt+1â€‹(j)subscriptğ‘ƒğ‘¡1ğ‘—P_{t+1}(j). This will be handled by the following lemma. Its conclusion gives an exact expression for how the probability a selected individual would choose to move from uğ‘¢u to vğ‘£v compares to the probability a sampled individual would choose to move from uğ‘¢u to vğ‘£v (where a selected individual is simply the winner of a game played between two independent sampled individuals). Let GğºG be an impartial combinatorial game, and let pâˆˆâˆvâˆˆIntâ€‹(G)ğ’«â€‹(Fâ€‹(v))ğ‘subscriptproductğ‘£Intğºğ’«ğ¹ğ‘£p\\in\\prod_{v\\in\\text{\\emph{Int}}(G)}\\mathcal{P}(F(v)). Suppose that x,yâˆ¼Univâ€‹(ğ’³G,p)similar-toğ‘¥ğ‘¦Univsubscriptğ’³ğºğ‘x,y\\sim\\text{\\emph{Univ}}(\\mathcal{X}_{G},p) are independent, and Then, for any uâˆˆVğ‘¢ğ‘‰u\\in V and vâˆˆFâ€‹(u)ğ‘£ğ¹ğ‘¢v\\in F(u), For an intuition behind (3), the comparative factor has effectively three terms (here interpreted in the context of AlgorithmÂ 1): â„™â€‹(uâˆˆPathGâ€‹(x,y))â„™ğ‘¢subscriptPathğºğ‘¥ğ‘¦\\mathbb{P}(u\\in\\text{{Path}}_{G}(x,y)), the probability the algorithm encounters position uğ‘¢u; â„™â€‹(fGuâ€‹(x,y)=1)â„™superscriptsubscriptğ‘“ğºğ‘¢ğ‘¥ğ‘¦1\\mathbb{P}(f_{G}^{u}(x,y)=1), the probability that uğ‘¢u is observed as a winning position; and 1âˆ’â„™â€‹(fGvâ€‹(x,y)=1)1â„™superscriptsubscriptğ‘“ğºğ‘£ğ‘¥ğ‘¦11-\\mathbb{P}(f_{G}^{v}(x,y)=1), the probability that vğ‘£v is observed as a losing position. If it is likely for vğ‘£v to be observed as a losing position, but unlikely for uğ‘¢u to be observed as a winning position, then it is beneficial to deliberately move from uğ‘¢u to vğ‘£v (placing your opponent in a likely losing position) rather than play out with whatever the current strategy is from uğ‘¢u (where you are unlikely to win), thus incurring an increase in the prevalence of zâ€‹(u)=vğ‘§ğ‘¢ğ‘£z(u)=v among selected individuals. On the other hand, if the reverse is true, then it is beneficial to deliberately avoid moving from uğ‘¢u to vğ‘£v and instead play out normally, thus incurring a decrease in prevalence of zâ€‹(u)=vğ‘§ğ‘¢ğ‘£z(u)=v. This helps motivate the effect of the term 1âˆ’â„™â€‹(fGvâ€‹(x,y)=1)âˆ’â„™â€‹(fGuâ€‹(x,y)=1)1â„™superscriptsubscriptğ‘“ğºğ‘£ğ‘¥ğ‘¦1â„™superscriptsubscriptğ‘“ğºğ‘¢ğ‘¥ğ‘¦11-\\mathbb{P}(f_{G}^{v}(x,y)=1)-\\mathbb{P}(f_{G}^{u}(x,y)=1). The magnitude of this effect scales with the relative frequency with which uğ‘¢u is encountered as a game position, which corresponds to â„™â€‹(uâˆˆPathGâ€‹(x,y))â„™ğ‘¢subscriptPathğºğ‘¥ğ‘¦\\mathbb{P}(u\\in\\text{{Path}}_{G}(x,y)). First, we will introduce some notation to assist with this proof. Let us write r=â„™â€‹(uâˆˆPathGâ€‹(x,y))ğ‘Ÿâ„™ğ‘¢subscriptPathğºğ‘¥ğ‘¦r=\\mathbb{P}(u\\in\\text{Path}_{G}(x,y)), su=â„™â€‹(fGuâ€‹(x,y)=1)subscriptğ‘ ğ‘¢â„™superscriptsubscriptğ‘“ğºğ‘¢ğ‘¥ğ‘¦1s_{u}=\\mathbb{P}(f_{G}^{u}(x,y)=1), and sv=â„™â€‹(fGvâ€‹(x,y)=1)subscriptğ‘ ğ‘£â„™superscriptsubscriptğ‘“ğºğ‘£ğ‘¥ğ‘¦1s_{v}=\\mathbb{P}(f_{G}^{v}(x,y)=1). Let us also write and note that Ağ´A, BğµB, and {u}ğ‘¢\\{u\\} are pairwise disjoint sets. Finally, if we have Pathâ€‹(x,y)=v0â€‹v1â€‹â€¦â€‹vâ„“Pathğ‘¥ğ‘¦subscriptğ‘£0subscriptğ‘£1â€¦subscriptğ‘£â„“\\text{Path}(x,y)=v_{0}v_{1}\\ldots v_{\\ell} when regarded as a directed path (where here and throughout we drop the subscript from PathGsubscriptPathğº\\text{Path}_{G} to simplify notation), then we will define Note that because Pathâ€‹(x,y)Pathğ‘¥ğ‘¦\\text{Path}(x,y) is the disjoint union of Path1â€‹(x,y)superscriptPath1ğ‘¥ğ‘¦\\text{Path}^{1}(x,y) and Path2â€‹(x,y)superscriptPath2ğ‘¥ğ‘¦\\text{Path}^{2}(x,y), we have The event zâ€‹(u)=vğ‘§ğ‘¢ğ‘£z(u)=v can be written as the disjoint union of the following six events. Let us examine the probability of each of these events occurring. For E1subscriptğ¸1E_{1}, the event uâˆ‰Pathâ€‹(x,y)âˆ§fGâ€‹(x,y)=1ğ‘¢Pathğ‘¥ğ‘¦subscriptğ‘“ğºğ‘¥ğ‘¦1u\\notin\\text{Path}(x,y)\\wedge f_{G}(x,y)=1 can be determined using only (xâ€‹(w))wâ‰ usubscriptğ‘¥ğ‘¤ğ‘¤ğ‘¢(x(w))_{w\\neq u} and (yâ€‹(w))wâ‰ usubscriptğ‘¦ğ‘¤ğ‘¤ğ‘¢(y(w))_{w\\neq u}, and so is independent of the event xâ€‹(u)=vğ‘¥ğ‘¢ğ‘£x(u)=v. Similarly, in E2subscriptğ¸2E_{2} the event uâˆ‰Pathâ€‹(x,y)âˆ§fGâ€‹(x,y)=âˆ’1ğ‘¢Pathğ‘¥ğ‘¦subscriptğ‘“ğºğ‘¥ğ‘¦1u\\notin\\text{Path}(x,y)\\wedge f_{G}(x,y)=-1 is independent of the event yâ€‹(u)=vğ‘¦ğ‘¢ğ‘£y(u)=v. Therefore, For E3subscriptğ¸3E_{3}, the event uâˆˆPath1â€‹(x,y)ğ‘¢superscriptPath1ğ‘¥ğ‘¦u\\in\\text{Path}^{1}(x,y) can be determined using only (xâ€‹(w))wâˆˆAsubscriptğ‘¥ğ‘¤ğ‘¤ğ´(x(w))_{w\\in A} and (yâ€‹(w))wâˆˆAsubscriptğ‘¦ğ‘¤ğ‘¤ğ´(y(w))_{w\\in A}, and the event fGvâ€‹(x,y)=1superscriptsubscriptğ‘“ğºğ‘£ğ‘¥ğ‘¦1f_{G}^{v}(x,y)=1 can be determined using only (xâ€‹(w))wâˆˆBsubscriptğ‘¥ğ‘¤ğ‘¤ğµ(x(w))_{w\\in B} and (yâ€‹(w))wâˆˆBsubscriptğ‘¦ğ‘¤ğ‘¤ğµ(y(w))_{w\\in B}. Therefore, all three component events in E3subscriptğ¸3E_{3} are independent of each other. The same is also true of E5subscriptğ¸5E_{5}. Therefore, noting that â„™â€‹(fGvâ€‹(x,y)=âˆ’1)=â„™â€‹(fGvâ€‹(y,x)=âˆ’1)â„™superscriptsubscriptğ‘“ğºğ‘£ğ‘¥ğ‘¦1â„™superscriptsubscriptğ‘“ğºğ‘£ğ‘¦ğ‘¥1\\mathbb{P}(f_{G}^{v}(x,y)=-1)=\\mathbb{P}(f_{G}^{v}(y,x)=-1), we can write For E4subscriptğ¸4E_{4}, the event uâˆˆPath1â€‹(x,y)ğ‘¢superscriptPath1ğ‘¥ğ‘¦u\\in\\text{Path}^{1}(x,y) can be determined using only (xâ€‹(w))wâˆˆAsubscriptğ‘¥ğ‘¤ğ‘¤ğ´(x(w))_{w\\in A} and (yâ€‹(w))wâˆˆAsubscriptğ‘¦ğ‘¤ğ‘¤ğ´(y(w))_{w\\in A}, and the event fGuâ€‹(x,y)=âˆ’1superscriptsubscriptğ‘“ğºğ‘¢ğ‘¥ğ‘¦1f_{G}^{u}(x,y)=-1 can be determined using only (xâ€‹(w))wâˆˆ{u}âˆªBsubscriptğ‘¥ğ‘¤ğ‘¤ğ‘¢ğµ(x(w))_{w\\in\\{u\\}\\cup B} and (yâ€‹(w))wâˆˆBsubscriptğ‘¦ğ‘¤ğ‘¤ğµ(y(w))_{w\\in B}. Therefore, all three component events in E4subscriptğ¸4E_{4} are independent of each other. The same is also true of E6subscriptğ¸6E_{6}. Therefore, noting that â„™â€‹(fGuâ€‹(x,y)=âˆ’1)=â„™â€‹(fGuâ€‹(y,x)=âˆ’1)â„™superscriptsubscriptğ‘“ğºğ‘¢ğ‘¥ğ‘¦1â„™superscriptsubscriptğ‘“ğºğ‘¢ğ‘¦ğ‘¥1\\mathbb{P}(f_{G}^{u}(x,y)=-1)=\\mathbb{P}(f_{G}^{u}(y,x)=-1), we can write We can now combine these observations to obtain as required.\nâˆ As an aside, we note here a parallel with evolutionary game theory. Consider the discrete time replicator equation with nonlinear payoff functions (see (2.1) ofÂ ; also  for the more standard continuous and linear versions), where we interpret qisubscriptğ‘ğ‘–q_{i} as the proportion of type iğ‘–i in a population and aisubscriptğ‘ğ‘–a_{i} as the fitness of a type iğ‘–i individual. If uğ‘¢u is fixed, then by enumerating Fâ€‹(u)={v1,â€¦,vk}ğ¹ğ‘¢subscriptğ‘£1â€¦subscriptğ‘£ğ‘˜F(u)=\\{v_{1},\\ldots,v_{k}\\}, identifying qi=pâ€‹(u,vi)subscriptğ‘ğ‘–ğ‘ğ‘¢subscriptğ‘£ğ‘–q_{i}=p(u,v_{i}) and ai=â„™â€‹(uâˆˆPathGâ€‹(u,vi))â‹…(1âˆ’â„™â€‹(fGviâ€‹(x,y)=1))subscriptğ‘ğ‘–â‹…â„™ğ‘¢subscriptPathğºğ‘¢subscriptğ‘£ğ‘–1â„™superscriptsubscriptğ‘“ğºsubscriptğ‘£ğ‘–ğ‘¥ğ‘¦1a_{i}=\\mathbb{P}(u\\in\\text{Path}_{G}(u,v_{i}))\\cdot(1-\\mathbb{P}(f_{G}^{v_{i}}(x,y)=1)), and applying the identity then (3) can be seen to be of the form provided by (8). In this sense, in an application of AlgorithmÂ 1 to a game GğºG, the evolution of the distribution pâ€‹(u,â‹…)ğ‘ğ‘¢â‹…p(u,\\,\\cdot\\,) at each vertex uğ‘¢u of GğºG stochastically emulates these replicator dynamics. (However, a key difference is that in standard evolutionary game theory, each fitness function ai:=aiâ€‹(q1,â€¦,qk)assignsubscriptğ‘ğ‘–subscriptğ‘ğ‘–subscriptğ‘1â€¦subscriptğ‘ğ‘˜a_{i}:=a_{i}(q_{1},\\ldots,q_{k}) typically depends only on the distribution of types in the population; whereas the expression ai=â„™â€‹(uâˆˆPathGâ€‹(u,vi))â‹…(1âˆ’â„™â€‹(fGviâ€‹(x,y)=1))subscriptğ‘ğ‘–â‹…â„™ğ‘¢subscriptPathğºğ‘¢subscriptğ‘£ğ‘–1â„™superscriptsubscriptğ‘“ğºsubscriptğ‘£ğ‘–ğ‘¥ğ‘¦1a_{i}=\\mathbb{P}(u\\in\\text{Path}_{G}(u,v_{i}))\\cdot(1-\\mathbb{P}(f_{G}^{v_{i}}(x,y)=1)) depends on the distribution of â€˜typesâ€™ not just at the node uğ‘¢u, but also at possibly all other nodes as well, and so the dynamics of each node cannot be considered in isolation.)\n\nReference markers found: [0]\n\nContext windows for each [ref]:\n[ref]0: over bitstrings. For these EDAs, the tuple pâˆˆâˆiâˆˆ[n]ğ’«â€‹({0,1})ğ‘subscriptproductğ‘–delimited-[]ğ‘›ğ’«01p\\in\\prod_{i\\in[n]}\\mathcal{P}(\\{0,1\\}) is often represented as a frequency vector (pâ€‹(1),â€¦,pâ€‹(n))âˆˆ[ref]0 nğ‘1â€¦ğ‘ğ‘›superscript01ğ‘›(p(1),\\ldots,p(n))\\in[ref]0 ^{n}, where pâ€‹(i)ğ‘ğ‘–p(i) is the probability that xâˆ¼Univâ€‹({0,1}n,p)similar-toğ‘¥Univsuperscript01ğ‘›ğ‘x\\sim\\text{Univ}(\\{0,1\\}^{n},p) has a 111-bit in\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 0,\n            \"titles\": [\n                \"Estimation-of-Distribution Algorithms for Multi-Valued Decision Variables\",\n                \"Runtime analysis of coevolutionary algorithms on a class of symmetric zero-sum games\",\n                \"Winning Ways for Your Mathematical Plays, volume 3\",\n                \"Introductory Combinatorics\",\n                \"Evolving neural networks to play checkers without relying on expert knowledge\",\n                \"Concentration inequalities and martingale inequalities: a survey\",\n                \"On Numbers and Games\",\n                \"Simplified runtime analysis of estimation of distribution algorithms\",\n                \"The significance of Kasparov versus DEEP BLUE and the future of computer chess\",\n                \"Playing games with algorithms: Algorithmic combinatorial game theory\",\n                \"The runtime of the compact genetic algorithm on jump functions\",\n                \"A survey on recent progress in the theory of evolutionary algorithms for discrete optimization\",\n                \"A rigorous analysis of the compact genetic algorithm for linear functions\",\n                \"Upper and lower bounds for randomized search heuristics in black-box optimization\",\n                \"The Spragueâ€“Grundy function for some nearly disjunctive sums of nim and silver dollar games\",\n                \"Using genetic programming to evolve board evaluation functions\",\n                \"Solution concepts in coevolutionary algorithms\",\n                \"A self-learning evolutionary chess program\",\n                \"Scenic trails ascending from sea-level nim to alpine chess and back\",\n                \"Computing a perfect strategy for n Ã— n chess requires time exponential in n\",\n                \"A curious nim-type game\",\n                \"Chomp on generalized Kneser graphs and others\",\n                \"Genetic programming and coevolution to play the Bombermanâ„¢ video game\",\n                \"Deciding the winner of an arbitrary finite poset game is PSPACE-complete\",\n                \"Mathematics and games\",\n                \"Impartial games\",\n                \"What is a game?\",\n                \"Competitive coevolution for defense and security: Elo-based similar-strength opponent sampling\",\n                \"Evolving search heuristics for combinatorial games with genetic programming\",\n                \"GP-EndChess: using genetic programming to evolve chess endgame players\",\n                \"How fitness aggregation methods affect the performance of competitive CoEAs on bilinear problems\",\n                \"Runtime analysis of a co-evolutionary algorithm: Overcoming negative drift in maximin-optimisation\",\n                \"Evolutionary game dynamics\",\n                \"The cooperative coevolutionary (1+1) EA\",\n                \"Fitnessless coevolution\",\n                \"Multi-criteria comparison of coevolution and temporal difference learning on othello\",\n                \"Context and driving forces in the development of the early computer game Nimbi\",\n                \"Solving complex problems with coevolutionary algorithms\",\n                \"Co-evolution of spies and resistance fighters\",\n                \"Runtime analysis of competitive co-evolutionary algorithms for maximin optimisation of a bilinear function\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]0 based on the context and research area of Runtime analysis of a coevolutionary algorithm on impartial combinatorial games. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}