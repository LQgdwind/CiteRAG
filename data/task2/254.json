{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Bias in Machine Learning - What is it Good for?\n\nPaper Abstract: \n\nPaper Section Text:\nThe result from an inductive learning process, i.e. the function f∗superscript𝑓f^{*} in Eq. 1), is often referred to as a ‘model’. As described in the previous sections, bias may propagate from the biased world, through a biased data generation, to the learning step with its inevitable inductive bias and other biases. The observed bias of a resulting model is often simply denoted ‘bias’    .\nTo distinguish this from other types of bias discussed in this paper, we propose using the term model bias to refer to bias as it appears and is analyzed in the final model.\nAn alternative would be the existing term algorithmic bias . However, typical usage of that term usually refers to the societal effects of biased systems , while our notion of bias is broader. Nevertheless, most suggestions on how to define model bias statistically consider such societal effects: how classification rates differ for groups of people with different values on a protected attribute such as race, color, religion, gender, disability, or family status .\nAs we will see in the following, classification rates may differ in very many respects, and a large number of bias types have been defined based on the condition that should hold for a model not to have that particular type of bias.\nFor a binary classifier we can for example require that the overall misclassification rate (OMR) is independent of a certain protected attribute A𝐴A (that takes the values 0 or 1). The corresponding condition for a classifier not being biased in this respect is : where Y^^𝑌\\widehat{Y} is the classifier output f​(x)𝑓𝑥f(x) (see Eq. 2), and y𝑦y is the correct classification for input x𝑥x. Both Y^^𝑌\\widehat{Y} and y𝑦y take the values 0 or 1.\nFor example, the fact that a person is female (A=0𝐴0A=0) should not increase or decrease the risk of incorrectly being refused, or allowed, to borrow money at the bank.\nSeveral similar conditions can be defined to describe other types of unwanted bias in a classifier model : false positive rate (FPR): false negative rate (FNR): false omission rate (FOR): false discovery rate (FDR): Each one of these equations focuses on that an incorrect (Y^≠y)^𝑌𝑦(\\widehat{Y}\\neq y) classification should be independent of A𝐴A and a specific value of Y^^𝑌\\widehat{Y} or y𝑦y.\nThe advantageous classifier output (for example being accepted a loan) is here coded as 1.\nA classifier that does not satisfy one of these equations is said to be biased in the corresponding sense777In practise, the requirement is usually that the left and right hand side of the equation should be approximate equal..\nFor example, a classifier is biased with respect to FDR if the value of A𝐴A affects the probability of incorrectly being allowed to borrow money. A related condition is the equalized odds, which appears in the literature with slightly different definitions (see  and ). In , equalized odds is defined by the following two conditions (slightly modified notation): and Note that Eq. 8 is equivalent to FPR in Eq. 4, and Eq. 9 is equivalent to TPR in Eq. 5. Several other indicators of model bias have been proposed. Loftus et al.  define Calibration, Demographic Parity/Disparate Impact, and Individual Fairness.\nFor a binary classification Y^^𝑌\\widehat{Y}, and a binary protected group A𝐴A, demographic parity is defined as follows: That is, Y^^𝑌\\widehat{Y} should be independent of A𝐴A, such that the classifier in average gives the same predictions to different groups. If the equality does not hold, this is referred to as disparate impact.\nAn example is a software company that wants to reach a better gender balance among their, mainly male, programmers. By following the principle of demographic parity, when recruiting, the same proportion of female applicants as male applicants are hired. Taken all together we conclude that there is a large number of different types of model biases, each one with its own focus on unwanted behavior of a classifier. Furthermore, many of these biases are related, and it can also be shown that several of them are conflicting in the sense that they cannot be avoided simultaneously   .\nHence, it is problematic to talk about ‘fair’ or ‘unbiased’ classifiers, at least without clearly defining the meaning of the terms. It can also be argued that a proper notion of fairness must be task-specific [ref]16.\n\nReference markers found: [16]\n\nContext windows for each [ref]:\n[ref]16: problematic to talk about ‘fair’ or ‘unbiased’ classifiers, at least without clearly defining the meaning of the terms. It can also be argued that a proper notion of fairness must be task-specific [ref]16.\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 16,\n            \"titles\": [\n                \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\",\n                \"False positives, false negatives, and false analyses: A rejoinder to ”machine bias: There’s software used across the country to predict future criminals. and it’s biased against blacks”\",\n                \"Learning de-biased representations with biased representations\",\n                \"Big data’s disparate impact\",\n                \"Hyper-parameter optimization in deep learning and transfer learning : applications to medical imaging\",\n                \"Man is to computer programmer as woman is to homemaker? debiasing word embeddings\",\n                \"Understanding the Origins of Bias in Word Embeddings\",\n                \"Semantics derived automatically from language corpora necessarily contain human biases\",\n                \"Ai now 2017 report\",\n                \"CRISP-DM 1.0: Step-by-step data mining guide\",\n                \"Fair prediction with disparate impact: A study of bias in recidivism prediction instruments\",\n                \"Algorithmic discrimination is an information problem\",\n                \"Think again: Big data\",\n                \"Algorithmic bias in autonomous systems\",\n                \"Proxy non-discrimination in data-driven systems\",\n                \"Chapter 6 research design - sampling bias\",\n                \"Fairness through awareness\",\n                \"Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them\",\n                \"European Union regulations on algorithmic decision-making and a “right to explanation”\",\n                \"Truth and Method\",\n                \"Equality of Opportunity in Supervised Learning\",\n                \"Handbook of Research on Machine Learning Innovations and Trends\",\n                \"Privacy as protection of the incomputable self: From agnostic to agonistic machine learning\",\n                \"Detecting biased statements in wikipedia\",\n                \"Towards bias detection in online text corpora\",\n                \"Machine bias: There’s software used across the country to predict future criminals. and it’s biased against blacks\",\n                \"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems\",\n                \"Inherent Trade-Offs in the Fair Determination of Risk Scores\",\n                \"Generalization versus discrimination in machine learning\",\n                \"Repair: Removing representation bias by dataset resampling\",\n                \"Causal Reasoning for Algorithmic Fairness\",\n                \"Returns from investing in equity mutual funds 1971 to 1991\",\n                \"Efficient estimation of word representations in vector space\",\n                \"The need for biases in learning generalizations\",\n                \"Social data: Biases, methodological pitfalls, and ethical boundaries\",\n                \"Artificial intelligence and algorithmic bias: implications for health systems\",\n                \"Discrimination-aware data mining\",\n                \"Glove: Global vectors for word representation\",\n                \"A Dictionary of Epidemiology\",\n                \"Linguistic models for analyzing and detecting biased language\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]16 based on the context and research area of Bias in Machine Learning - What is it Good for?. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}