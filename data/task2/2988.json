{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Meta-Forecasting by combining Global Deep Representations with Local Adaptation\n\nPaper Abstract: \n\nPaper Section Text:\nIn all our experiments, we train on a source dataset and measure predictive performance on a target dataset.\nTo compare with NBEATS, we use the same source-target combinations reported by Oreshkin et¬†al. : we train on one of the M4 datasets and use datasets with same frequency as target datasets whenever possible (see Table¬†6 in the Appendix).\nWe stress that when source and target dataset are not the same, the model computes the forecasts for each TS in the target dataset without access to the other TS in the target dataset.\nThis is different from domain adaptation settings where source and target dataset are available at training time.\nM4 contains a large number of diverse TS, thus there is higher chance for it to share some properties with other datasets.\nAn ideal source dataset would contain a very diverse set of time series from many real-world applications from different domains (and different frequencies). Benchmark datasets. We use four datasets in our experiments222Oreshkin et¬†al.  use an additional dataset called FRED as the source dataset.\nHowever, this dataset is not publicly available and can only be retrieved by crawling an ever changing website over the course of days.\nSince this procedure does not result in a reproducible dataset, we do not consider it here.\n.\nelectr contains hourly time series of the electricity consumption of 370 customers .\ntraff time series measures hourly occupancy rate, between 0 and 1, of 963 San Francisco car lanes .\ntourism is a collection of yearly, monthly and quarterly series of indicators related to tourism activities .\nM3 and M4 are two collection of time series that were used in recent forecasting competitions  6.\nM3 comprises 645 yearly, 756 quarterly, 1428 monthly as well as 174 TS without time granularity (M3-other). Finally M4 contains 100K time series in total of yearly, quarterly, monthly, weekly, daily and hourly granularities. Both M3 and M4 contain heterogeneous TS types from business, financial and economic domains. All datasets are accessed through the GluonTS library . Baselines. We report the top baselines from Oreshkin et¬†al. , namely Theta (local) , auto-ARIMA (local), NBEATS and DeepAR 5. We also report the results from DeepAR from Salinas et¬†al. 5 and train and evaluate the DeepAR model using a procedure similar to our method. We recall that local methods do not require training on the source dataset and their parameters are estimated directly on each TS of the target dataset.\nOther meta-learning methods used primarily in computer-vision like MAML [ref]11, REPTILE 9 or ANIL 3 can in principle be applied to TS forecasting.\n]RG: added the following discussion on meta-learning baselines. is this the right spot? Ok, but conclusion might be better.\nHowever, all require an additional hyperparameter: the number of steps of gradient descent used to learn the local parameters, which can‚Äôt be easily meta-learned. Furthermore, both MAML and REPTILE have as local parameters all the network weights, hence they would be significantly slower and occupy more memory than our method during training and prediction. On the other hand, ANIL adapts only the last layer and thus should have a similar time and memory cost than our method. Applying these methods to time-series forecasting is an interesting future work. Transfer from M4. In Table 1, for both Meta-GlAR and DeepAR and for each frequency of the M4 dataset we repeat the following model selection. ]RG: we should motivate the model selection\nWe evaluate 200 random hyperparameter configurations and pick the 10 configurations with the lowest sMAPE on a subset of 8‚ÄãK8ùêæ8K training TS. The hyperparameters for the random search are described in the Appendix (Table¬†8). A single Meta-GLAR model (Meta-GLAR-top10) is competitive with Theta and ARIMA and always outperforms DeepAR.\nEnsembling can improve performance for both DeepAR and Meta-GLAR. An ensemble of 10 Meta-GLAR models (Meta-GLAR-ens10) outperforms NBEATS, which consists in an ensemble of 90 models, on electr, as well as DeepAR trained on the target dataset on electr and M3. NBEATS is still superior on traff and tourism while the gap is much closer on M3. \nAs shown in Table¬†2, our method has a substantially lower global parameter count than NBEATS: from 353535 to 100010001000 times less parameters for the monthly frequency.]following paragraph added after rebuttal\nWe did not notice performance improvements of our method by increasing the number of layers or the hidden dimensions. This is a common behavior of one-step-ahead RNN based forecasting models (see e.g. Salinas et¬†al. 5), whose accuracy does not increase much beyond a certain model size. By contrast, NBEATS is a multi-step ahead residual network that computes a sequence of predictions in a single forward pass and hence benefits from a larger number of parameters. More detailed results are in Appendix¬†A.1. Ablation analysis. In Figure¬†1, we perform an ablation analysis to evaluate the benefits of different elements of our method (Meta+RNN+ITF). These are the closed-form adaptation (Meta), the RNN network for the representation and the use of iterated forecast during training (ITF). We perform combinations of the following ablations. Replacing the closed-form adaptation with a global linear layer (no Meta in the name). Using the observations in the horizon to compute predictions during training instead of iterated forecasts (no ITF in the name). Replacing the RNN with a stateless feed-forward model (FF) and a linear (Lin) model.\nThe results show that all the three ingredients are necessary to achieve the best performance. In addition, we note that in this scenario, using iterated forecasts during training without the closed-form adaptation (RNN+ITF) performs worse than the auto-regressive RNN model. Replacing the last global linear layer with the closed-form adaptation gives our model more ‚Äúmemory‚Äù even when we use a feed-forward or linear backbone, since the last linear layer is computed using past representations in the context, while an autoregressive stateless model only uses the last representation in the context. Hence, it is interesting to see that the RNN backbone performs the best even in this setting. Ablation analysis on the hourly frequency is provided in Appendix¬†A.2. Comparison with fine-tuning. Recently, Tian et¬†al. 4 criticized meta-learning approaches in computer vision and achieved state-of-the-art results for few-shot image classification only by learning a good representation and then using it with a local model at prediction time, avoiding any local learning or adaptation during training.\nWe explored this approach by training a standard global RNN model on the source dataset.\nFor prediction on the target dataset, we then kept the network weights fixed, but replaced the last layer with the closed-form adaption layer (with Œ≥=1ùõæ1\\gamma=1), corresponding to the setting of 4 of a fixed globally representation trained without meta learning.\nThis method reaches a median sMAPE333We computed the median on the 10 over 100 random search runs and with lowest sMAPE on a subset of 8‚ÄãK8ùêæ8K training TS. of 14.1614.1614.16, 15.4015.4015.40 (ITF) and 14.1314.1314.13, 15.7415.7415.74 (ITF) for M4 and M3 monthly respectively, which is far higher than all the other methods in Figure¬†1.\nWhile we used the closed-form adaption layer in our experiments, the solution is equivalent to training the last layer from scratch using SGD.\nWe also tried a variant of this experiment where we use the regularization Œ≥‚Äã‚à•w‚àíwglobal‚à•ùõædelimited-‚à•‚à•ùë§subscriptùë§global\\gamma\\lVert{w-w_{\\text{global}}}\\rVert where wglobalsubscriptùë§globalw_{\\text{global}} are the parameters of the global last layer. This is equivalent to gradient descent fine-tuning of the last layer starting from wglobalsubscriptùë§globalw_{\\text{global}}. This variant reaches 14.8614.8614.86 and 16.0616.0616.06 median sMAPE respectively on M4 and M3 monthly (no ITF).\nThese experiments show that in the forecasting setting it is important to meta-learn the representation parameters such that the local model works well, in contrast to the results of Tian et¬†al. 4. Learning curves. In Figure¬†2, we compare the performance during training of our method with that of NBEATS with shared weights (NBEATS-SH) and the global RNN method from our ablation analysis. Meta-GLAR and RNN use the same model and training hyperparameters, while NBEATS uses an 888 times larger minibatch size (the same used by Oreshkin et¬†al. ).\nThe gap in performance is large at the beginning of the training. We believe this is due to the local closed-form adaptation, which allows to have greater performance even with random global parameters.\nWe also note that the performance of Meta-GLAR is more stable during training and that the training time (on CPU) is only 17%percent1717\\% higher than that of the non-adaptive RNN. The prediction time for Meta-GLAR is only marginally higher than that of a the non-adaptive RNN, i.e. it takes 3:21 minutes instead of 3:19 to forecast the full M4 test set, while NBEATS takes 3:15 minutes (we set the minibatch size to 1K for all methods during prediction).\n\nReference markers found: [11]\n\nContext windows for each [ref]:\n[ref]11: do not require training on the source dataset and their parameters are estimated directly on each TS of the target dataset.\nOther meta-learning methods used primarily in computer-vision like MAML [ref]11, REPTILE 9 or ANIL 3 can in principle be applied to TS forecasting.\n]RG: added the following discussion on meta-learning baselines. is this the right spot? Ok, but conclusion might be\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 11,\n            \"titles\": [\n                \"Unsupervised Scalable Representation Learning for Multivariate Time Series\",\n                \"Differentiable convex optimization layers\",\n                \"Gluonts: Probabilistic and neural time series modeling in python\",\n                \"The theta model: a decomposition approach to forecasting\",\n                \"The tourism forecasting competition\",\n                \"Winning methods for forecasting tourism time series\",\n                \"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\",\n                \"Meta-learning with differentiable closed-form solvers\",\n                \"Rule-based forecasting: Development and validation of an expert systems approach to combining time series extrapolations\",\n                \"Uci machine learning repository\",\n                \"Classical and contemporary approaches to big time series forecasting\",\n                \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Long short-term memory\",\n                \"Forecasting: principles and practice\",\n                \"Another look at measures of forecast accuracy\",\n                \"Few-shot Learning for Time-series Forecasting\",\n                \"Adam: A method for stochastic optimization\",\n                \"Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations\",\n                \"Reconstruction and regression loss for time-series transfer learning\",\n                \"Meta-learning for time series forecasting and forecast combination\",\n                \"Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting\",\n                \"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\",\n                \"Meta Learning with Differentiable Closed-form Solver for Fast Video Object Segmentation\",\n                \"The m3-competition: results, conclusions and implications\",\n                \"The m4 competition: 100,000 time series and 61 forecasting methods\",\n                \"Representation based meta-learning for few-shot spoken intent recognition\",\n                \"Fforma: Feature-based forecast model averaging\",\n                \"On First-Order Meta-Learning Algorithms\",\n                \"N-BEATS: Neural basis expansion analysis for interpretable time series forecasting\",\n                \"Meta-learning framework with applications to zero-shot time-series forecasting\",\n                \"Forecasting: theory and practice\",\n                \"Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML\",\n                \"Optimization as a model for few-shot learning\",\n                \"DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks\",\n                \"Evolutionary principles in self-referential learning. on learning how to learn: The meta-meta-meta‚Ä¶-hook\",\n                \"Bayesian intermittent demand forecasting for large inventories\",\n                \"Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting\",\n                \"A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting\",\n                \"Prototypical Networks for Few-shot Learning\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]11 based on the context and research area of Meta-Forecasting by combining Global Deep Representations with Local Adaptation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}