{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Image Captioning using Deep Stacked LSTMs, Contextual Word Embeddings and Data Augmentation\n\nPaper Abstract: \n\nPaper Section Text:\nGeneration of captions for images is a vital task for scene understanding which is one of the primary aims of Computer Vision.\nBut in addition to recognizing objects that appear in the image, the caption generation model must also be powerful enough to recognize the relationships among them and express these relationships in a natural language. And this requirement, which is to be able to mimic the human ability to transform the information from visual domain to the domain of natural languages, is what makes it an important challenge for deep learning research. Over the last few years, the problem has been tackled by using Convolutional Neural Networks (convnets) to extract an image representation and different versions of Recurrent Neural Networks to generate the caption sentences one word at a time. For extracting the image representations, pretrained Convolutional Neural Networks (CNNs) are used which are first trained on the ImageNet Dataset for the object recognition task. Generally, the image representations used are of two types: (a) the ones which are extracted from the last few fully connected layers of the CNN which distill the information about the salient objects into a fixed length vector such as those used in Vinyals et. al. [ref]2 and Mao et. al. , and (b) the ones which are extracted from the Convolutional layers of the CNN which produce a set of vectors each of which contain information about an aspect or region of the image such as the those used in Xu et. al. [ref]4. We use the latter approach as the former condenses the information of an image into a single vector which may lead to loss of information about specific regions or aspects of the image. To produce the captions the decoder will use the image representations generated by the decoder along with the previous generated words (to be used as contextual information). There are two main ways to use the image representation generated by the CNN: (a) the image representation is mapped to the same vector space as the hidden state representation of the Recurrent Neural Networks (RNN) decoder and used as the initial hidden and cell state of the decoder as proposed in Vinyals et. al. [ref]2 or, (b) the image representation is mapped to the vector space of input word vector representations and then merged with the word embedding representation of the sequence generated until the previous time-step (by concatenating or adding, element-wisely, the two vectors), as proposed in Mao et. al.. For our work we use the Inception-ResNet-v2 Convolutional Neural Network as proposed in Szegedy et. al.. For producing the captions, we use a Multilayer Long Short Term Memory Network (LSTM) such that input to the network is the input to the first LSTM layer and output of a LSTM layer is used as input for the next LSTM layer. The output of the last LSTM layer is used as input to the softmax activation layer which produces a vector containing probability of occurrence of each word in the vocabulary. We use the description of LSTM as proposed in Hochreiter et. al . At each time-step, in addition to the image representation provided as initial hidden and cell states, the words generated till the previous time-step are used as input to the LSTM. The words are passed through an Embedding Layer which produces dense vector representation of the words. We use the Word Embedding framework proposed in Komninos et. al.[ref]7. Henceforth, in this article, we will refer to these embeddings as Komninos Embeddings. In automatic image caption generation process, using the image representation generated by the CNN encoder, the decoder will recurrently generate the caption one word at each time step. Precisely, if we have the input image I and the target sentence S = w1subscriptùë§1w_{1}, w2subscriptùë§2w_{2}, ‚Ä¶,w|S|subscriptùë§ùëÜw_{|S|}, then we formulate a probability distribution over SùëÜS in such a way that at each time step tksubscriptùë°ùëòt_{k} during the training phase, we train the model to maximize the posterior probability of sub-sequence w1subscriptùë§1w_{1}, w2subscriptùë§2w_{2},‚Ä¶ wk‚àí1subscriptùë§ùëò1w_{k-1}, given the image IùêºI. The probability distribution can be stated as:\n with w0subscriptùë§0w_{0} being the <S‚ÄãT‚ÄãA‚ÄãR‚ÄãT>expectationùëÜùëáùê¥ùëÖùëá<START> token which is pre-fixed with the sequence as an signal to begin the generation process. Hence, the generation of next word is conditioned on the words  w0:t‚àí1subscriptùë§:0ùë°1w_{0:t-1}  that have been generated previously and the input image. Since the Equation 1 is recursive in nature, the solution can be implemented as a Recurrent Neural Network decoder. Each factor in the equation represents one step of the word generation process. The core contributions of this work are as follows: We utilise Inception-ResNet-v2 (Szegedy et. al.) as encoder for extracting the image feature representation, and a Deep LSTM (Hochreiter et. al.) network for generating the caption. Compared with other similar approaches, feature extraction using Inception-ResNet-v2 allows us to gain significant improvements in performance of our models. Also the Deep LSTM network allows learning of more complex relationships as compared to a single layered network. We employ pre-trained word embeddings proposed in Komninos et. al. [ref]7 which employ word and dependency context features to generate the word representations. To do this, the authors consider word co-occurrences between pairs of words, words and dependency context features, and between different dependency context features. We explain the word embeddings in detail in Section 2.2. In order to further improve the performance of system, we fine-tune the parameters of Word Embedding layer and the Feature Extractor (CNN) layer. We perform fine-tuning after our models shows breakdown of co-relation between validation set log-likelihood error and the image captioning evaluation metrics (BLEU-4 used in our experiments): We fine-tune the words embeddings (that we use as word representations). In other words, unlike the previous works we do not use the pre-trained word vectors as fixed weights in the word embedding layer to generate a feature representation for input words. Instead, we allow the network to learn the weights of the word embedding layer after initializing the weights with the pre-trained word embeddings proposed in Komninos et. al.[ref]7. We adopt a learning rate for the word embedding layer which is 10 times lower than the learning rate of the decoder layer. Since we are using transfer learning with pre-trained CNN to extract features from the images, following the observations of Yosinski et. al.  where they find that transferability of Deep Neural Networks is negatively affected primarily by the specialization of higher layer neurons, we perform fine-tuning on the CNN. We freeze the weights of the first few layers and allow the model to learn the weights of higher layers using back-propagation. The learning rate for the CNN is 10 times lower than the learning rate for the decoder layer. We use image data augmentation to prevent overfitting since the datasets that are commonly used in Image Captioning have very limited number of images and their annotated descriptions. Image Data Augmentation has been used in Image Captioning in Wang et. al.  where the authors have explored random cropping of the images, multi-scaling and vertical flipping of the images. However, cropping the images may leave out some important information from some images where the objects are present close to the edges of the images. Also, vertical flipping the images may not account for spatial relationships among the objects between the training data and test set as data in the test set will not be augmented. Hence, we explore horizontal flipping of the images and Perspective Transformation to create more image-sentence pairs. Horizontal flipping preserves more spatial relationships and perspective transformation of images introduces slight distortion in images with respect to the reference plane of the image but preserves all spatial relationships. We explain Image Data Augmentation in detail in Section 3.7. We evaluate our approach with two generation methods:\n(a) Encoder-Decoder Approach or Inject Approach: The image feature representations are used as initial hidden and cell states of the decoder and decoder generates the caption, one word a time, by taking the generated caption sub-sequence until the last time step. This approach is similar to the approach proposed in Vinyals et. al.[ref]2. However, our approach differs from their approach in following respects: We extract a set of feature vectors from an intermediate convolutional layer of the CNN as explained in Section 3.2 unlike extracting a feature vector from the final fully connected layer of the CNN as has been done in Vinyals et. al.[ref]2. We have found that this approach aids in generating better captions because it contains information about the spatial relationships between objects in the image. As mentioned earlier, we utilise Inception-ResNet-v2 CNN as encoder and Deep LSTM network as decoder along with word embeddings proposed in Komninos et. al. [ref]7 unlike the VGG-16 encoder and a single LSTM layered decoder in Vinyals et. al.[ref]2. Also the authors of Vinyals et. al. [ref]2 don‚Äôt use pre-trained word embeddings as they find that using pretrained embeddings from Mikolov et. al. 0 doesn‚Äôt improve the performance of their model. The work of Vinyals et. al. [ref]2 uses a model ensemble which allows them to achieve a performance gain of a few points worth in terms of BLEU score. Since we evaluate our approach with respect to two methods (Encoder-Decoder Approach and Soft Attention), in order to ensure a fair comparison between the two, we do not use model ensembles. And lastly, unlike Vinyals et. al. [ref]2, we do not use batch normalization on the inputs because it does not help us to achieve tangible performance gains of our models. One reason for this may be that, since batch normalization is a technique to prevent overfitting of the model to training data, use of data augmentation helps us to alleviate overfitting and thus batch normalization is no longer needed. (b) Attention based approach: The image feature representations, after being mapped to the same vector space as the hidden representations of the LSTM layer, are used as first state of the decoder. This is similar to the Encoder-Decoder approach, but at each time step the decoder also gets, as input, a context vector, s~tsubscript~sùë°\\widetilde{\\textbf{s}}_{t} which is a dynamic representation of the input image at that time step containing information about the relevant portions of the image. Our attention-based method is similar to the method proposed by Xu et. al. [ref]4 with some differences: We utilise Inception-ResNet-v2 as encoder and Deep LSTM network as decoder along with word embeddings proposed in Komninos et. al. [ref]7 unlike the VGG-16 encoder and a single LSTM layered decoder used by Xu et. al.[ref]4. In addition, for feeding the text data into the Deep LSTM layer, we use the word representations proposed in Komninos et. al[ref]7.\n\nReference markers found: [2]\n\nContext windows for each [ref]:\n[ref]2: ones which are extracted from the last few fully connected layers of the CNN which distill the information about the salient objects into a fixed length vector such as those used in Vinyals et. al. [ref]2 and Mao et. al. , and (b) the ones which are extracted from the Convolutional layers of the CNN which produce a set of vectors each of which contain information about an aspect or region of\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 2,\n            \"titles\": [\n                \"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\n                \"ImageNet Large Scale Visual Recognition Challenge\",\n                \"Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge\",\n                \"Show, attend and tell: Neural image caption generation with visual attention\",\n                \"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\",\n                \"Long short-term memory\",\n                \"Dependency based embeddings for sentence classification tasks\",\n                \"How transferable are features in deep neural networks?\",\n                \"Image Captioning with Deep Bidirectional LSTMs and Multi-Task Learning\",\n                \"Efficient estimation of word representations in vector space\",\n                \"Composing simple image descriptions using web-scale n-grams\",\n                \"Babytalk: Understanding and generating simple image descriptions\",\n                \"Collective generation of natural image descriptions\",\n                \"Imagenet classifcation with deep convolutional neural networks\",\n                \"Sequence to sequence learning with neural networks\",\n                \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n                \"Unifying visual-semantic embeddings with multimodal neural language models\",\n                \"Grounded compositional semantics for fnding and describing images with sentences\",\n                \"Image Captioning with Semantic Attention\",\n                \"Convolutional sequence to sequence learning\",\n                \"Convolutional Image Captioning\",\n                \"Distributed representations\",\n                \"Large language models in machine translation\",\n                \"Framing image description as a ranking task: Data, models and evaluation metrics\",\n                \"Glove: Global vectors for word representation\",\n                \"Improving distributional similarity with lessons learned from word embeddings\",\n                \"Improving Deep Learning using Generic Data Augmentation\",\n                \"Estimation of number of people in crowded scenes using perspective transformation\",\n                \"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\",\n                \"Microsoft COCO: Common Objects in Context\",\n                \"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\n                \"Microsoft coco captions: Data collection and evaluation server\",\n                \"Long-term Recurrent Convolutional Networks for Visual Recognition and Description\",\n                \"Image caption generation with part of speech guidance\",\n                \"Generated Reference Title 34 for [ref]2\",\n                \"Generated Reference Title 35 for [ref]2\",\n                \"Generated Reference Title 36 for [ref]2\",\n                \"Generated Reference Title 37 for [ref]2\",\n                \"Generated Reference Title 38 for [ref]2\",\n                \"Generated Reference Title 39 for [ref]2\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]2 based on the context and research area of Image Captioning using Deep Stacked LSTMs, Contextual Word Embeddings and Data Augmentation. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}