{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: First-order Policy Optimization for Robust Markov Decision Process This research was partially supported by NSF DMS-2134037 and AFOSR FA9550-22-1-0447.\n\nPaper Abstract: \n\nPaper Section Text:\nTo proceed, we introduce a characterization that upper bounds the difference of robust values for two policies, and serves as a keystone in the ensuing algorithmic developments. For any pair of policies π,π′𝜋superscript𝜋′\\pi,\\pi^{\\prime}, we have Let ξuπ′​(s)≔{(st,at)}t≥0≔subscriptsuperscript𝜉superscript𝜋′𝑢𝑠subscriptsubscript𝑠𝑡subscript𝑎𝑡𝑡0\\xi^{\\pi^{\\prime}}_{u}(s)\\coloneqq\\left\\{(s_{t},a_{t})\\right\\}_{t\\geq 0} denote the trajectory generated from by π′superscript𝜋′\\pi^{\\prime} within MDP ℳuπ′subscriptℳsubscript𝑢superscript𝜋′\\mathcal{M}_{u_{\\pi^{\\prime}}}, with initial state set as s𝑠s.\nThat is, at∼π′(⋅|st)a_{t}\\sim\\pi^{\\prime}(\\cdot|s_{t}), st+1∼ℙuπ′(⋅|st,at)s_{t+1}\\sim\\mathbb{P}_{u_{\\pi}^{\\prime}}(\\cdot|s_{t},a_{t}) for all t≥0𝑡0t\\geq 0 and s0=ssubscript𝑠0𝑠s_{0}=s.\nWe know that where (a)𝑎(a) follows from (2.3); (b)𝑏(b) follows from moving Vrπ​(s0)subscriptsuperscript𝑉𝜋𝑟subscript𝑠0V^{\\pi}_{r}(s_{0}) outside the summation;\n(c)𝑐(c) follows from definition that s0=ssubscript𝑠0𝑠s_{0}=s; (e)𝑒(e) follows from the definition of dsπ,uπsubscriptsuperscript𝑑𝜋subscript𝑢𝜋𝑠d^{\\pi,u_{\\pi}}_{s} in (1.7), and the relation between Vrπsubscriptsuperscript𝑉𝜋𝑟V^{\\pi}_{r} and Qrπsubscriptsuperscript𝑄𝜋𝑟Q^{\\pi}_{r} in (2.6).\nIt remains to show that (d)𝑑(d) holds. For any t≥0𝑡0t\\geq 0, we have where (a′)superscript𝑎′(a^{\\prime}) follows from the definition of ξuπ′​(s)subscriptsuperscript𝜉superscript𝜋′𝑢𝑠\\xi^{\\pi^{\\prime}}_{u}(s), and (b′)superscript𝑏′(b^{\\prime}) follows from the property of Qrπsubscriptsuperscript𝑄𝜋𝑟Q^{\\pi}_{r} (2.5).\nThus relation (d)𝑑(d) follows immediately from the prior observation and the linearity of expectation.\n∎ By applying Lemma 2.6, we know that for any policy π∈Π𝜋Π\\pi\\in\\Pi and an optimal robust policy π∗superscript𝜋\\pi^{*}, we have where ℱs:Π×Π→ℝ|𝒮|​|𝒜|:subscriptℱ𝑠→ΠΠsuperscriptℝ𝒮𝒜\\mathcal{F}_{s}:\\Pi\\times\\Pi\\to\\mathbb{R}^{\\lvert{\\mathcal{S}}\\rvert\\lvert\\mathcal{A}\\rvert} is defined by Given the optimality of π∗superscript𝜋\\pi^{*}, we then know that\n⟨ℱ​(π,π∗),π−π∗⟩≥0ℱ𝜋superscript𝜋𝜋superscript𝜋0\\langle\\mathcal{F}(\\pi,\\pi^{*}),\\pi-\\pi^{*}\\rangle\\geq 0\nfor all π∈Π𝜋Π\\pi\\in\\Pi.\nNow for any π∈Π𝜋Π\\pi\\in\\Pi, let α∈(0,1)𝛼01\\alpha\\in(0,1), we define πα=α​π+(1−α)​π∗subscript𝜋𝛼𝛼𝜋1𝛼superscript𝜋\\pi_{\\alpha}=\\alpha\\pi+(1-\\alpha)\\pi^{*}.\nSubstituting παsubscript𝜋𝛼\\pi_{\\alpha} into the prior relation, we obtain\n⟨ℱs​(πα,π∗),πα−π∗⟩=α​⟨ℱs​(πα,π∗),π−π∗⟩≥0.subscriptℱ𝑠subscript𝜋𝛼superscript𝜋subscript𝜋𝛼superscript𝜋𝛼subscriptℱ𝑠subscript𝜋𝛼superscript𝜋𝜋superscript𝜋0\\langle\\mathcal{F}_{s}(\\pi_{\\alpha},\\pi^{*}),\\pi_{\\alpha}-\\pi^{*}\\rangle=\\alpha\\langle\\mathcal{F}_{s}(\\pi_{\\alpha},\\pi^{*}),\\pi-\\pi^{*}\\rangle\\geq 0.\nConsequently, for any α∈(0,1)𝛼01\\alpha\\in(0,1).\nIt is straightforward to verify that the visitation measure dsπ,usuperscriptsubscript𝑑𝑠𝜋𝑢d_{s}^{\\pi,u} is a continuous function of (π,u)𝜋𝑢(\\pi,u) (see proof of Lemma 2.1).\nOn the other hand, note that given (2.2), the worst-case uncertainty uπ(⋅|s,a)∈∂σ𝒰s,a(Vrπ)u_{\\pi}(\\cdot|s,a)\\in\\partial\\sigma_{\\mathcal{U}_{s,a}}(V^{\\pi}_{r}), where σX​(⋅)subscript𝜎𝑋⋅\\sigma_{X}(\\cdot) denotes the support function of set X𝑋X, and ∂f𝑓\\partial f denotes the subdifferential of function f𝑓f.\nFor simplicity, let us assume that ∂σ𝒰s,a​(Vrπ)subscript𝜎subscript𝒰𝑠𝑎subscriptsuperscript𝑉𝜋𝑟\\partial\\sigma_{\\mathcal{U}_{s,a}}(V^{\\pi}_{r}) is always a singleton for any π𝜋\\pi (See Lemma B.4 for example). Now let α↓0↓𝛼0\\alpha\\downarrow 0, we have πα→π∗→subscript𝜋𝛼superscript𝜋\\pi_{\\alpha}\\to\\pi^{*}.\nSince Vrπsubscriptsuperscript𝑉𝜋𝑟V^{\\pi}_{r} is Lipschitz continuous in π𝜋\\pi (see Lemma B.6 for an elementary proof), then Vrπα→Vrπ∗→subscriptsuperscript𝑉subscript𝜋𝛼𝑟subscriptsuperscript𝑉superscript𝜋𝑟V^{\\pi_{\\alpha}}_{r}\\to V^{\\pi^{*}}_{r}.\nThus given that σ𝒰s,asubscript𝜎subscript𝒰𝑠𝑎\\sigma_{\\mathcal{U}_{s,a}} is a closed and proper convex function, and the fact that the subdifferential map of a closed convex function is closed (Theorem 24.4, [ref]35), we know that any\nlimit point of uπα(⋅|s,a)u_{\\pi_{\\alpha}}(\\cdot|s,a) is also a subgradient of σ𝒰s,a​(Vrπ∗)subscript𝜎subscript𝒰𝑠𝑎subscriptsuperscript𝑉superscript𝜋𝑟\\sigma_{\\mathcal{U}_{s,a}}(V^{\\pi^{*}}_{r}), and hence the limit point is indeed unique and the worst-case uncertainty for π∗superscript𝜋\\pi^{*}.\nDenoting this limit point as uπ∗subscript𝑢superscript𝜋u_{\\pi^{*}}, then by taking α↓0↓𝛼0\\alpha\\downarrow 0 in (2.15), we obtain In view of the discussions in the last two paragraphs, (2.16) suggests us to find the optimal robust policy via solving the following variational inequality (VI), Interested readers might find that VI (2.18) has a close analogy for solving non-robust MDPs, constructed in .\nSpecifically, for a non-robust, discounted finite MDP, the optimal policy satisfies the following VI, where ν∗superscript𝜈\\nu^{*} denotes the stationary state distribution induced by the optimal policy π∗superscript𝜋\\pi^{*}.\nOne can clearly see that the only difference between the non-robust VI (2.19) and the robust VI (2.18) is the additional role of worst-case environment for the latter.\nTo solve the non-robust version of VI constructed therein,  exploits the key fact that the constructed VI satisfies the so-called generalized monotonicity: for any policy π∉Π∗𝜋superscriptΠ\\pi\\notin\\Pi^{*}. However, we next show that unlike the performance difference lemma for standard MDPs  , for which\n(2.13) holds with equality,\nthe inequality in Lemma 2.6 seems unavoidable in the robust setting.\nConsequently, the VI for the robust MDP (2.18) no longer satisfies the generalized monotonicity as defined in (2.20). There exists a robust MDP instance ℳ𝒰subscriptℳ𝒰\\mathcal{M}_{\\mathcal{U}} such that holds for at least one state s∈𝒮𝑠𝒮s\\in{\\mathcal{S}}.\nIn particular, π∗superscript𝜋\\pi^{*} corresponds to a solution of (1.4), and π𝜋\\pi is a strictly suboptimal policy.\nMoreover, denoting ν∗≔νπ∗,uπ∗≔superscript𝜈superscript𝜈superscript𝜋subscript𝑢superscript𝜋\\nu^{*}\\coloneqq\\nu^{\\pi^{*},u_{\\pi^{*}}} as the stationary state distribution of the optimal policy π∗superscript𝜋\\pi^{*} within its worst-case MDP ℳuπ∗subscriptℳsubscript𝑢superscript𝜋\\mathcal{M}_{u_{\\pi^{*}}}, we have Consider the MDP ℳℳ\\mathcal{M} that contains three states 𝒮={Sa,Sb,Sc}𝒮subscript𝑆𝑎subscript𝑆𝑏subscript𝑆𝑐{\\mathcal{S}}=\\left\\{S_{a},S_{b},S_{c}\\right\\}, and each state is associated with two actions 𝒜={L,R}𝒜𝐿𝑅\\mathcal{A}=\\left\\{L,R\\right\\}.\nThe transition of ℳℳ\\mathcal{M}, denoted by ℙℙ\\mathbb{P}, is fully deterministic, and is illustrated in Figure 2. Each edge starts from the current state, and ends at the next state, with its arc (a,c)𝑎𝑐(a,c) consisting of the action a𝑎a, and the cost c𝑐c associated with the action a𝑎a. We assume such a cost occurs immediately after the action is made, and is independent of the next state.\nIn addition, we assume C>1𝐶1C>1. Let us consider the scenario where the uncertain environment can arbitrarily manipulate the transition probabilities at state Scsubscript𝑆𝑐S_{c} except there is no returning to itself, and has no manipulation strength at other states.\nThat is, for any a∈𝒜𝑎𝒜a\\in\\mathcal{A}, we have 𝒫Sc,a=Δ{Sa,Sb}subscript𝒫subscript𝑆𝑐𝑎subscriptΔsubscript𝑆𝑎subscript𝑆𝑏\\mathcal{P}_{S_{c},a}=\\Delta_{\\left\\{S_{a},S_{b}\\right\\}}.\nOn the other hand, for any s≠Sc𝑠subscript𝑆𝑐s\\neq S_{c}, and for any a∈𝒜𝑎𝒜a\\in\\mathcal{A}, we have 𝒰s,a=𝟎subscript𝒰𝑠𝑎0\\mathcal{U}_{s,a}=\\mathbf{0}. We consider a pair of policies π,π∗𝜋superscript𝜋\\pi,\\pi^{*}, defined by Since C>1𝐶1C>1, and the fact that π​(L|Sa)=1𝜋conditional𝐿subscript𝑆𝑎1\\pi(L|S_{a})=1, it should be clear that for policy π𝜋\\pi, the worst case transition ℙuπsubscriptℙsubscript𝑢𝜋\\mathbb{P}_{u_{\\pi}} should satisfy Consequently, the robust value of π𝜋\\pi is given by On the other hand, for policy π∗superscript𝜋\\pi^{*}, since π∗​(R|Sa)=1superscript𝜋conditional𝑅subscript𝑆𝑎1\\pi^{*}(R|S_{a})=1 and 1>0101>0, the worst case transition ℙuπ∗subscriptℙsubscript𝑢superscript𝜋\\mathbb{P}_{u_{\\pi^{*}}} should satisfy It should also be clear that π∗superscript𝜋\\pi^{*} is an optimal robust policy.\nFrom the previous observations, simple calculation yields Now let ξuπ∗​(Sc)subscriptsuperscript𝜉superscript𝜋𝑢subscript𝑆𝑐\\xi^{\\pi^{*}}_{u}(S_{c}) denote the trajectory generated by π∗superscript𝜋\\pi^{*} within ℳπ∗subscriptℳsuperscript𝜋\\mathcal{M}_{\\pi^{*}}, starting from state Scsubscript𝑆𝑐S_{c}.\nThen we have where the strict inequality follows from observation (2.23) and (2.24).\nThus by repeating the same argument in the proof of Lemma 2.6, we know that inequality (d)𝑑(d) in (2.14) holds with strict inequality for s=Sc𝑠subscript𝑆𝑐s=S_{c}.\nSince (d)𝑑(d) is the only inequality in the proof of Lemma 2.6, we conclude that We proceed to show that 𝔼s′∼dScπ∗,uπ∗​⟨Qrπ,π∗−π⟩s′=0subscript𝔼similar-tosuperscript𝑠′subscriptsuperscript𝑑superscript𝜋subscript𝑢superscript𝜋subscript𝑆𝑐subscriptsubscriptsuperscript𝑄𝜋𝑟superscript𝜋𝜋superscript𝑠′0\\mathbb{E}_{s^{\\prime}\\sim d^{\\pi^{*},u_{\\pi^{*}}}_{S_{c}}}\\langle Q^{\\pi}_{r},\\pi^{*}-\\pi\\rangle_{s^{\\prime}}=0 and hence establishing (2.21).\nTo see this, note that\nQrπ​(Sc,⋅)=γ​Vrπ​(Sa)subscriptsuperscript𝑄𝜋𝑟subscript𝑆𝑐⋅𝛾subscriptsuperscript𝑉𝜋𝑟subscript𝑆𝑎Q^{\\pi}_{r}(S_{c},\\cdot)=\\gamma V^{\\pi}_{r}(S_{a}), and hence\n⟨Qrπ,π∗−π⟩Sc=0subscriptsubscriptsuperscript𝑄𝜋𝑟superscript𝜋𝜋subscript𝑆𝑐0\\langle Q^{\\pi}_{r},\\pi^{*}-\\pi\\rangle_{S_{c}}=0.\nIn addition, we also have Qrπ​(Sb,⋅)=1+γ​Vrπ​(Sc)subscriptsuperscript𝑄𝜋𝑟subscript𝑆𝑏⋅1𝛾subscriptsuperscript𝑉𝜋𝑟subscript𝑆𝑐Q^{\\pi}_{r}(S_{b},\\cdot)=1+\\gamma V^{\\pi}_{r}(S_{c}), and hence\n⟨Qrπ,π∗−π⟩Sb=0subscriptsubscriptsuperscript𝑄𝜋𝑟superscript𝜋𝜋subscript𝑆𝑏0\\langle Q^{\\pi}_{r},\\pi^{*}-\\pi\\rangle_{S_{b}}=0.\nFinally, note that dScπ∗,uπ∗​(Sa)=0superscriptsubscript𝑑subscript𝑆𝑐superscript𝜋subscript𝑢superscript𝜋subscript𝑆𝑎0d_{S_{c}}^{\\pi^{*},u_{\\pi^{*}}}(S_{a})=0, and hence 𝔼s′∼dScπ∗,uπ∗​⟨Qrπ,π∗−π⟩s′=0subscript𝔼similar-tosuperscript𝑠′subscriptsuperscript𝑑superscript𝜋subscript𝑢superscript𝜋subscript𝑆𝑐subscriptsubscriptsuperscript𝑄𝜋𝑟superscript𝜋𝜋superscript𝑠′0\\mathbb{E}_{s^{\\prime}\\sim d^{\\pi^{*},u_{\\pi^{*}}}_{S_{c}}}\\langle Q^{\\pi}_{r},\\pi^{*}-\\pi\\rangle_{s^{\\prime}}=0. To show (2.22), it suffices to note that ν∗=Unif​({Sb,Sc})superscript𝜈Unifsubscript𝑆𝑏subscript𝑆𝑐\\nu^{*}=\\mathrm{Unif}(\\left\\{S_{b},S_{c}\\right\\}) is the stationary state distribution of π∗superscript𝜋\\pi^{*} within MDP ℳuπ∗subscriptℳsubscript𝑢superscript𝜋\\mathcal{M}_{u_{\\pi^{*}}}.\nWith the same arguments as in the last paragraph we can show that 𝔼s′∼dSbπ∗,uπ∗​⟨Qrπ,π∗−π⟩s′=0subscript𝔼similar-tosuperscript𝑠′subscriptsuperscript𝑑superscript𝜋subscript𝑢superscript𝜋subscript𝑆𝑏subscriptsubscriptsuperscript𝑄𝜋𝑟superscript𝜋𝜋superscript𝑠′0\\mathbb{E}_{s^{\\prime}\\sim d^{\\pi^{*},u_{\\pi^{*}}}_{S_{b}}}\\langle Q^{\\pi}_{r},\\pi^{*}-\\pi\\rangle_{s^{\\prime}}=0.\nHence we obtain where the strict inequality follows from (2.25).\nThe claim then follows from the definition of 𝒢ssubscript𝒢𝑠\\mathcal{G}_{s} in (2.17).\n∎ The construction of strict inequality in Proposition 2.3 suggests that (2.13) in Lemma 2.6 fails to characterize even the simplest change of robust values when switching the policy.\nIn particular, as illustrated in (2.21), improvement of value when switching from π𝜋\\pi to the optimal π∗superscript𝜋\\pi^{*} seems not being captured by the aggregated inner product defined in the second term of (2.21) at all.\nA closer look into the constructed example shows that the state Sasubscript𝑆𝑎S_{a} is the only state where the inner product is nonzero (positive), but when changing to the optimal policy, the state Sasubscript𝑆𝑎S_{a} is never visited by the optimal policy via ds~π∗,uπ∗superscriptsubscript𝑑~𝑠superscript𝜋subscript𝑢superscript𝜋d_{\\widetilde{s}}^{\\pi^{*},u_{\\pi^{*}}}, for any s~≠Sa~𝑠subscript𝑆𝑎\\widetilde{s}\\neq S_{a}. It seems to suggest that the fundamental difficulty causing the insufficiency of Lemma 2.6 is due to the fact that the very state (i.e., state Sasubscript𝑆𝑎S_{a}) that leads to the improvement of policy is never visited by the improved policy π∗superscript𝜋\\pi^{*} within its worst-case environment ℳuπ∗subscriptℳsuperscript𝑢superscript𝜋\\mathcal{M}_{u^{\\pi^{*}}}. The following lemma aims to address the previously observed difficulty. For any policy π𝜋\\pi, let uπsubscript𝑢𝜋u_{\\pi} denote its worst-case uncertainty defined in (2.2), then we have We have where (a)𝑎(a) follows from (2.7),\n(c)𝑐(c) follows from the (2.3),\n(d)𝑑(d) follows from the definition of Vrπ∗=maxu∈𝒰⁡Vuπ∗subscriptsuperscript𝑉superscript𝜋𝑟subscript𝑢𝒰subscriptsuperscript𝑉superscript𝜋𝑢V^{\\pi^{*}}_{r}=\\max_{u\\in\\mathcal{U}}V^{\\pi^{*}}_{u}.\nWe now proceed to establish (b)𝑏(b). The proof of (b)𝑏(b) follows similar lines as in the proof of Lemma 2.6.\nLet ξuππ∗​(s)≔{(st,at)}t≥0≔subscriptsuperscript𝜉superscript𝜋subscript𝑢𝜋𝑠subscriptsubscript𝑠𝑡subscript𝑎𝑡𝑡0\\xi^{\\pi^{*}}_{u_{\\pi}}(s)\\coloneqq\\left\\{(s_{t},a_{t})\\right\\}_{t\\geq 0} denote the trajectories generated by policy π∗superscript𝜋\\pi^{*}, within MDP ℳuπsubscriptℳsubscript𝑢𝜋\\mathcal{M}_{u_{\\pi}}, with initial state set as s𝑠s.\nThat is, at∼π∗(⋅|st)a_{t}\\sim\\pi^{*}(\\cdot|s_{t}), st+1∼ℙuπ(⋅|st,at)s_{t+1}\\sim\\mathbb{P}_{u_{\\pi}}(\\cdot|s_{t},a_{t}) for all t≥0𝑡0t\\geq 0 and s0=ssubscript𝑠0𝑠s_{0}=s.\nWe then have where (e)𝑒(e) follows from (1.6).\n∎ By comparing Proposition 2.3 and Lemma 2.7, it should be clear that the key to inequality (2.26) is the combination of policy and the environment when choosing the state-visitation measure.\nInstead of choosing the worst-case environment uπ∗subscript𝑢superscript𝜋u_{\\pi^{*}} for π∗superscript𝜋\\pi^{*} as in Proposition 2.3 (Lemma 2.6), we choose the worst-case environment uπsubscript𝑢𝜋u_{\\pi} of the current policy π𝜋\\pi.\nIn particular, returning back to the constructed example in Proposition 2.3, we see that the state (i.e., state Sasubscript𝑆𝑎S_{a}) that leads to the improvement of policy can be now visited by the improved policy π∗superscript𝜋\\pi^{*}, if the environment is still fixed as the worst-case environment ℳuπsubscriptℳsubscript𝑢𝜋\\mathcal{M}_{u_{\\pi}} for the original policy π𝜋\\pi. Before we end our discussion in this section, it is worth mentioning some similarities that robust state-action value function Qrπsubscriptsuperscript𝑄𝜋𝑟Q^{\\pi}_{r} shares with the (sub)gradient in the convex optimization literature, in the sense that with proper aggregation over states:\n(1) it provides an upper bound on the local value changes, as shown in Lemma 2.6, much similar to using gradient-based local quadratic approximation for smooth objectives  (with the quadratic term being identically 00);\n(2) its inner product with the direction to the optimal policy is lower bounded by the optimality gap, as shown in Lemma 2.7, similar to the (sub)-gradient for convex objectives  .\nThese observations thus suggest using Qrπsubscriptsuperscript𝑄𝜋𝑟Q^{\\pi}_{r} as the first-order information to update the policy.\nNevertheless, it should be noted that the objective (1.5) is neither convex nor smooth [ref]1 .\nIn the following section, we develop the robust policy mirror descent method that formalizes this intuition, and establish its computational efficiency in finding an optimal policy.\n\nReference markers found: [35]\n\nContext windows for each [ref]:\n[ref]35: that σ𝒰s,asubscript𝜎subscript𝒰𝑠𝑎\\sigma_{\\mathcal{U}_{s,a}} is a closed and proper convex function, and the fact that the subdifferential map of a closed convex function is closed (Theorem 24.4, [ref]35), we know that any\nlimit point of uπα(⋅|s,a)u_{\\pi_{\\alpha}}(\\cdot|s,a) is also a subgradient of\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 35,\n            \"titles\": [\n                \"Reinforcement Learning under Model Mismatch\",\n                \"On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift\",\n                \"Robust Reinforcement Learning using Least Squares Policy Iteration with Provable Performance Guarantees\",\n                \"Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization\",\n                \"A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants\",\n                \"The theory of max-min and its application to weapons allocation problems\",\n                \"Twice regularized MDPs and the equivalence between robustness and regularization\",\n                \"A first-order approach to accelerated value iteration\",\n                \"Robust markov decision processes: Beyond rectangularity\",\n                \"Scalable First-Order Methods for Robust MDPs\",\n                \"Partial policy iteration for l1-robust markov decision processes\",\n                \"Robust estimation of a location parameter\",\n                \"Robust dynamic programming\",\n                \"Approximately optimal approximate reinforcement learning\",\n                \"A natural policy gradient\",\n                \"Robust modified policy iteration\",\n                \"On the linear convergence of natural policy gradient algorithm\",\n                \"Risk-Averse Learning by Temporal Difference Methods\",\n                \"On fréchet subdifferentials\",\n                \"Efficient Policy Iteration for Robust Markov Decision Processes via Regularization\",\n                \"First-order and stochastic optimization methods for machine learning\",\n                \"Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes\",\n                \"Markov chains and mixing times, volume 107\",\n                \"Policy mirror descent inherently explores action space\",\n                \"Implicit bias of gradient descent based adversarial training on separable data\",\n                \"Homotopic policy mirror descent: Policy convergence, implicit regularization, and improved sample complexity\",\n                \"Distributionally robust q𝑞q-learning\",\n                \"Distributionally Robust Offline Reinforcement Learning with Linear Function Approximation\",\n                \"Overcoming Exploration in Reinforcement Learning with Demonstrations\",\n                \"Introductory lectures on convex optimization: A basic course, volume 87\",\n                \"Robust control of markov decision processes with uncertain transition matrices\",\n                \"Sample complexity of robust reinforcement learning with a generative model\",\n                \"Curiosity-driven Exploration by Self-supervised Prediction\",\n                \"Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703\",\n                \"Markov decision processes: discrete stochastic dynamic programming\",\n                \"Convex analysis, volume 18\",\n                \"Risk-averse dynamic programming for markov decision processes\",\n                \"Adaptive Trust Region Policy Optimization: Global Convergence and Faster Rates for Regularized MDPs\",\n                \"Deep Reinforcement Learning with Robust and Smooth Policy\",\n                \"Lectures on Geometric Measure Theory, volume 3\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]35 based on the context and research area of First-order Policy Optimization for Robust Markov Decision Process This research was partially supported by NSF DMS-2134037 and AFOSR FA9550-22-1-0447.. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}