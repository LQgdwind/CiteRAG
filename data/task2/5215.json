{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Objects as Spatio-Temporal 2.5D points\n\nPaper Abstract: \n\nPaper Section Text:\nExisting literature in 2D+ object detection can be categorized into LiDAR-based methods   , that take raw or processed LiDAR 3D points as inputs, monocular (camera-based) methods  , or a hybrid of the two 8 [ref]42 .\nWhile LiDAR based methods are generally more accurate in determining the 3D (x, y & z) and BEV positions of objects in a scene, they also require that data from LiDAR sensor during inference, which is quite expensive and often unavailable for our offline perception use cases. On the other hand, cameras are relatively cheaper sensors, but they lose 3D information due to the way an image is captured while preserving the rich textural cues.\nThus, evidently, monocular camera-based methods fall short in predicting the accurate depth (z) of objects in a scene in comparison with their LiDAR-based counterparts, but are relatively more accurate when it comes to predicting (x & y) of the scene. Attempts have been made to combine both modalities, in a hybrid setting, to exploit the best of both worlds 8. But most of them require the LiDAR data as input at query time. While there are a few methods 8 [ref]42  that predict depth and use them for LiDAR object detection, they rely heavily on the availability of 3D object annotations. 3D object annotations are challenging for two main reasons, they are challenging to annotate, and they often cover very less object classes. The method proposed follows a hybrid approach, uses both camera data and LiDAR point cloud data for training, but there are two major differences (1) the LiDAR data is not used as an input at test time and thus LiDAR sensor is not required in the test vehicle, and (2) the method relies on camera 2D object detections alone instead of 3D annotations (as in 8 [ref]42). 3D annotations are expensive to annotate and are often restricted to a small number of classes, and (3) the method combines object detection and tracking into a single step feedforward pass ensuring spatio-temporally smooth object detections. The proposed method is also relevant to 8 [ref]42, where instead of building a second object detection network on top of the regression depth, which further adds to the already computationally intensive depth prediction process, we propose a single network that, given a camera image as input, can learn both object detection and dense depth in a multimodal regression setting, where the ground truth LiDAR data is used at training time only to compute depth regression loss. Since, the network uses as input, the camera image alone, at test time, the network can predict both object detections and dense depth of the scene. Monocular 2D Object Detection:\n2D Object detection networks can be divided into anchor-based     or anchor free architectures    [ref]41. Anchor-based architectures are further categorized into either two-staged       or computationally efficient single stage     . On the other hand, anchor-free detectors  [ref]41 directly find objects either using a key-point or center-point based approaches. CornetNet  models each object as a pair of keypoints placed at the corners of the bounding boxes and CenterNet [ref]41 combines   with center-point  based approaches by adding an additional keypoint at the center of the bounding box effectively modeling each object as a keypoint triplet. The proposed method builds over center point-based single-stage architecture [ref]41 for 2D object detection. Monocular 3D Object Detection:\nMonocular 3D object detection methods    regress complete 3D object information such as the center of the object in 3D, offsets of the 3D bounding box along with its orientation, all from a monocular image.\nCenterTrack  builds over CenterNet  which models objects as centers, by jointly detecting and tracking object instances in 3D using a tracking conditioned on detection method. The proposed method further extends center-based method [ref]41 to spatio-temporally model objects as points in 3D, in a weakly-supervised manner, aiming to achieve 3D representation for objects, without relying on a comprehensive 3D object detection architectures that require laborious and exorbitant bounding boxes and orientation annotations. Monocular Depth Prediction:\nMost literature for predicting pixel-wise depth map from monocular image can be categorized into supervised  , Semi-supervised   and self-supervised    7.\nSelf-supervised methods rely on   7, to train the depth prediction network using Image consistency loss computed between frames of image sequence , weakly-supervised or semi-supervised methods rely on the stereopsis  . On the other hand, supervised methods often train convolutional auto-encoders  , trains a two-network stack with full LiDAR supervision, with second network refining the coarse depth predictions of the first.\nSophisticated loss functions and backbones have been explored, such as ordinal regression loss in DORN , surface normals to model geometry 0  and more recently transformer based  7.\nThe proposed method combines object detection [ref]41 and supervised depth prediction  into a unified single-network learning task by adding a depth prediction head to the network backbone. Results on Table 1 adds strength to our argument that forcing object tracking network to learn depth additionally, results in improved 2D object detection and tracking performance. Multi-Object Tracking:\nSeveral methods have attempted coupling tracking and object detection into one framework by learning them jointly 2  such that the tracking algorithm can leverage both appearances and motion dynamics for modeling objects over time. Schulter et al. 2 models pair-wise association between objects as a function of appearance using network flow graph, Sharma et al. 4 introduced reconstruction-based tracking method, and Fang et al.  proposes a recurrent neural networks to model long and short term memory. On the other hand, recently a few methods have been revisiting the traditional filtering based methods for data association and have shown state-of-the-art performances with them, such as   . Weng et al.  uses a simple Kalman filter & Hungarian distance-based method to compute data association on 3D bounding boxes regressed using an object detection algorithm, and demonstrate superior performances. The proposed method extends  architecture that performs tracking-by-detection, using a point-based framework that feeds tracklets (represented as gaussians of object centerpoints) of the previous frames back into the neural network, effectively forcing the network to learn to predict associations between two adjacent image frames. Monocular BEV Object Detection:\nBird’s Eye View (BEV)-object detection  relies on global and local geometric cues of point clouds, often procured using LiDAR sensors. Although LiDAR based BEV object detection methods    [ref]41 2 achieves higher accuracies, the cost of LiDAR sensor(s) mounted on the car during inference is exorbitant. Pseudo-LiDAR based methods 8 [ref]42  bridge the gap between BEV and camera based methods by lifting the camera images to BEV and employ BEV object detection upon it. This involves two stages, (1) learn to regress pixel-wise depth map using techniques like , and (2) convert the depth map to Bird’s Eye View and employ a BEV object detection  upon it. Unlike  [ref]42 8, our method uses a single stage architecture which learns to regress both 2D Object detections and depthmap jointly, giving per pixel depth value; using regressed detection and depth maps the method renders the object as a point in 3D. While the estimated 3D object position may not necessarily be the actual center of the object, the application we are targeting does not require the accurate estimation of actual 3D object centers. Our technique overcomes the challenge of missing smaller objects, often not captured in sparse LiDAR data. We also bypass the requirement for ground-truth 3D annotations, a challenging and expensive task, by using raw LiDAR data with 2D annotations and training the network to regress depthmap (and thus x,y & z (3D position) of the object).\n\nReference markers found: [42]\n\nContext windows for each [ref]:\n[ref]42: literature in 2D+ object detection can be categorized into LiDAR-based methods   , that take raw or processed LiDAR 3D points as inputs, monocular (camera-based) methods  , or a hybrid of the two 8 [ref]42 .\nWhile LiDAR based methods are generally more accurate in determining the 3D (x, y & z) and BEV positions of objects in a scene, they also require that data from LiDAR sensor during\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 42,\n            \"titles\": [\n                \"Deep Layer Aggregation\",\n                \"AdaBins: Depth Estimation using Adaptive Bins\",\n                \"Towards Accurate One-Stage Object Detection with AP-Loss\",\n                \"Monocular 3d object detection for autonomous driving\",\n                \"Open Street Maps\",\n                \"Depth Map Prediction from a Single Image using a Multi-Scale Deep Network\",\n                \"Recurrent Autoregressive Networks for Online Multi-Object Tracking\",\n                \"Deep Ordinal Regression Network for Monocular Depth Estimation\",\n                \"Vision meets robotics: The kitti dataset\",\n                \"Are we ready for autonomous driving? the kitti vision benchmark suite\",\n                \"Unsupervised Monocular Depth Estimation with Left-Right Consistency\",\n                \"3d packing for self-supervised monocular depth estimation\",\n                \"PCT: Point Cloud Transformer\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Bounding box regression with uncertainty for accurate object detection\",\n                \"Joint 3D Proposal Generation and Object Detection from View Aggregation\",\n                \"Semi-Supervised Deep Learning for Monocular Depth Map Prediction\",\n                \"PointPillars: Fast Encoders for Object Detection from Point Clouds\",\n                \"CornerNet: Detecting Objects as Paired Keypoints\",\n                \"From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation\",\n                \"Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs\",\n                \"Receptive Field Block Net for Accurate and Fast Object Detection\",\n                \"SSD: Single Shot MultiBox Detector\",\n                \"SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation\",\n                \"MOT16: A Benchmark for Multi-Object Tracking\",\n                \"Libra R-CNN: Towards Balanced Learning for Object Detection\",\n                \"ThunderNet: Towards Real-time Generic Object Detection on Mobile Devices\",\n                \"Vision Transformers for Dense Prediction\",\n                \"You Only Look Once: Unified, Real-Time Object Detection\",\n                \"Yolov3: An incremental improvement\",\n                \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\n                \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\",\n                \"Deep Network Flow for Multi-Object Tracking\",\n                \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\",\n                \"Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking\",\n                \"Complex-yolo: real-time 3d object detection on point clouds. arxiv 2018\",\n                \"Learning to rank proposals for object detection\",\n                \"Learning depth from monocular videos using direct methods\",\n                \"Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving\",\n                \"Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]42 based on the context and research area of Objects as Spatio-Temporal 2.5D points. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}