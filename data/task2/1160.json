{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Steady State Analysis of Episodic Reinforcement Learning\n\nPaper Abstract: \n\nPaper Section Text:\nThis section provides more discussions about the MDP formulation as introduced in Section 2. A faithful implementation of this formulation was used to generate all the results reported in Section 6 and Section F. Our MDP formulation assumes both SSSS\\SS and ğ’œğ’œ\\mathcal{A} are countable sets. This is mainly for aligning to the standard markov chain theory, and also for enabling convenient notations like transition matrix Mğ‘€M and summation âˆ‘s,asubscriptğ‘ ğ‘\\sum_{s,a}. Results in this paper are readily generalizable to uncountable action spaces (which still induce countable transitions after marginalizing over the actions), and may also be generalized to uncountable state spaces based on the theory of general-state-space markov chainsÂ . Our MDP formulation also assumes state-based deterministic reward function Rğ‘…R. While this formulation was used in many previous worksÂ [ref]16, some literatureÂ  explicitly assign stochastic rewards to transitions, in the form of Râ€‹(r|s,a,sâ€²)=â„™[rt+1=r|st=s,at=a,st+1=sâ€²]ğ‘…conditionalğ‘Ÿğ‘ ğ‘superscriptğ‘ â€²â„™subscriptğ‘Ÿğ‘¡1conditionalğ‘Ÿsubscriptğ‘ ğ‘¡ğ‘ subscriptğ‘ğ‘¡ğ‘subscriptğ‘ ğ‘¡1superscriptğ‘ â€²R(r|s,a,s^{\\prime})=\\operatorname*{\\mathbb{P}}[r_{t+1}=r|s_{t}=s,a_{t}=a,s_{t+1}=s^{\\prime}]. Our reward-function formulation has no loss of generality here, as one can think of a â€œstateâ€ in our formulation as a (s,r)ğ‘ ğ‘Ÿ(s,r) pair in stochastic-reward models, with the transition-dependent stochasticity of the reward captured by the transition function Pğ‘ƒP. Our MDP formulation has replaced the (often included) discounting constant Î³csubscriptğ›¾ğ‘\\gamma_{c} with the (often excluded) initial state distribution Ï0subscriptğœŒ0\\rho_{0}. Similar perspective to â€œdowngradeâ€ the discounting constant was discussed in  (Chapter 10.4). As will become evident later in the paper, the discounting constant is neither necessary for the purpose of defining the problem, nor is it necessary for treating the problem. On the other hand, an explicit specification of Ï0subscriptğœŒ0\\rho_{0} is necessary to define the episode-wise performance Jeâ€‹pâ€‹isubscriptğ½ğ‘’ğ‘ğ‘–J_{epi} as used in all episodic tasks, and will be also needed to define the terminal states in the formulation of episodic learning process proposed in this paper. A finite-horizon task can have degenerated steady states if it is formulated as an infinite-horizon MDP with an imaginary absorbing state sâ–¡subscriptğ‘ â–¡s_{\\scriptscriptstyle\\Box}. In the absorbing MDP formulation, any finite episode will end up with moving from its terminal state to the absorbing state sâ–¡subscriptğ‘ â–¡s_{\\scriptscriptstyle\\Box}, from there the rollout is trapped in the absorbing state forever without generating effective rewardÂ  . As a result, the stationary (and limiting) distribution of the absorbing MDP of any finite-horizon task concentrates fully and only to the absorbing state, making it of limited use for designing and analyzing RL algorithms. The definition of the steady-state performance measure Jaâ€‹vâ€‹gâ€‹(Ï€)â‰limTâ†’âˆ1Tâ€‹âˆ‘t=1TRâ€‹(st)=ğ”¼sâˆ¼ÏÏ€[Râ€‹(s)]approaches-limitsubscriptğ½ğ‘ğ‘£ğ‘”ğœ‹subscriptâ†’ğ‘‡1ğ‘‡superscriptsubscriptğ‘¡1ğ‘‡ğ‘…subscriptğ‘ ğ‘¡subscriptğ”¼similar-toğ‘ subscriptğœŒğœ‹ğ‘…ğ‘ J_{avg}(\\pi)\\doteq\\lim\\limits_{T\\rightarrow\\infty}\\frac{1}{T}\\sum_{t=1}^{T}R(s_{t})=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\pi}\\end{subarray}}[R(s)], as introduced in Section 2, is based on the following ergodic theorem of markov chain: In ergodic markov chain Mğ‘€M with stationary distribution ÏMsubscriptğœŒğ‘€\\rho_{\\scriptscriptstyle M}, let fâ€‹(s)ğ‘“ğ‘ f(s) be any function such that ğ”¼sâˆ¼ÏM[|fâ€‹(s)|]<âˆsubscriptğ”¼similar-toğ‘ subscriptğœŒğ‘€ğ‘“ğ‘ \\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\scriptscriptstyle M}\\end{subarray}}[~{}|f(s)|~{}]<\\infty, then the time-average of fğ‘“f converges almost surely to the state-average of fğ‘“f, i.e., limTâ†’âˆâ„™[1Tâ€‹âˆ‘t=1Tfâ€‹(st)=ğ”¼sâˆ¼ÏM[fâ€‹(s)]]=1subscriptâ†’ğ‘‡â„™1ğ‘‡superscriptsubscriptğ‘¡1ğ‘‡ğ‘“subscriptğ‘ ğ‘¡subscriptğ”¼similar-toğ‘ subscriptğœŒğ‘€ğ‘“ğ‘ 1\\lim\\limits_{T\\rightarrow\\infty}\\operatorname*{\\mathbb{P}}[~{}\\frac{1}{T}\\sum_{t=1}^{T}f(s_{t})=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s\\sim\\rho_{\\scriptscriptstyle M}\\end{subarray}}[f(s)]~{}]=1.\n(, Theorem 74) The value function Qğ‘„Q is often more specifically called the action-value function. We called the Qğ‘„Q-function just value function because of its symmetric role with policy function Ï€ğœ‹\\pi. Moreover, in classic literature, the term â€œaction-value functionâ€ was often referred specifically to\nQÏ€Î³câ€‹(s,a)â‰ğ”¼s1âˆ¼Pâ€‹(s,a),{sâ‰¥2}âˆ¼MÏ€[âˆ‘t=1âˆRâ€‹(st)â‹…Î³ctâˆ’1]approaches-limitsubscriptsuperscriptğ‘„subscriptğ›¾ğ‘ğœ‹ğ‘ ğ‘subscriptğ”¼formulae-sequencesimilar-tosubscriptğ‘ 1ğ‘ƒğ‘ ğ‘similar-tosubscriptğ‘ absent2subscriptğ‘€ğœ‹superscriptsubscriptğ‘¡1â‹…ğ‘…subscriptğ‘ ğ‘¡superscriptsubscriptğ›¾ğ‘ğ‘¡1Q^{\\scriptscriptstyle\\gamma_{c}}_{\\pi}(s,a)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s_{1}\\sim P(s,a),\\{s_{\\geq 2}\\}\\sim M_{\\pi}\\end{subarray}}[\\sum_{t=1}^{\\infty}R(s_{t})\\cdot\\gamma_{c}^{t-1}]\n, in which the constant 0â‰¤Î³c<10subscriptğ›¾ğ‘10\\leq\\gamma_{c}<1 is called the discounting factor. The specialized definition of QÏ€Î³csuperscriptsubscriptğ‘„ğœ‹subscriptğ›¾ğ‘Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}} entails the specialized version of Bellman equation\nQÏ€Î³câ€‹(s,a)=ğ”¼sâ€²âˆ¼Pâ€‹(s,a),aâ€²âˆ¼Ï€â€‹(sâ€²)[Râ€‹(sâ€²)+Î³câ‹…QÏ€Î³câ€‹(sâ€²,aâ€²)],superscriptsubscriptğ‘„ğœ‹subscriptğ›¾ğ‘ğ‘ ğ‘subscriptğ”¼formulae-sequencesimilar-tosuperscriptğ‘ â€²ğ‘ƒğ‘ ğ‘similar-tosuperscriptğ‘â€²ğœ‹superscriptğ‘ â€²ğ‘…superscriptğ‘ â€²â‹…subscriptğ›¾ğ‘superscriptsubscriptğ‘„ğœ‹subscriptğ›¾ğ‘superscriptğ‘ â€²superscriptğ‘â€²Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s,a)=\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}s^{\\prime}\\sim P(s,a),a^{\\prime}\\sim\\pi(s^{\\prime})\\end{subarray}}[R(s^{\\prime})+\\gamma_{c}\\cdot Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s^{\\prime},a^{\\prime})],\nand induces the specialized version of state-value function\nVÏ€Î³câ€‹(s)â‰ğ”¼aâˆ¼Ï€â€‹(s)[QÏ€Î³câ€‹(s,a)].approaches-limitsuperscriptsubscriptğ‘‰ğœ‹subscriptğ›¾ğ‘ğ‘ subscriptğ”¼similar-toğ‘ğœ‹ğ‘ superscriptsubscriptğ‘„ğœ‹subscriptğ›¾ğ‘ğ‘ ğ‘V_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s)\\doteq\\operatorname*{\\mathbb{E}}_{\\begin{subarray}{c}a\\sim\\pi(s)\\end{subarray}}[Q_{\\pi}^{\\scriptscriptstyle\\gamma_{c}}(s,a)]. The product of Î³ğ›¾\\gamma in the general-form value functions (1) was originally proposed as a virtual probabilistic termination in the agentâ€™s mindÂ ,\nbut was latter found useful to account for the real episode-boundaries when the MDP is used to model multi-episode rollouts  .\nOur paper uses product-form value functions for the same purpose as in  (Section 2.1.1) and . However, both  and  used transition-based Î³ğ›¾\\gamma-discounting, while the value function in our treatment uses state-based discounting and is probably closer to the method of Figure 1(c) in , which was considered an sub-optimal design in that paper. In fact,  attributes much of its main technical contribution to the adoption and contrastive analysis of transition-based discounting (against state-based discounting) which concludes that â€œtransition-based discounting is necessary to enable the unified specification of episodic and continuing tasksâ€ ( Section 2.1, see also Section 6 and B). Despite this major technical disparity, we nevertheless think the approach as described in Section 3 of our paper actually aligns with  (Section 2.1.1) and  in terms of the bigger idea that connecting finite episodes into a single infinite-horizon MDP greatly helps with unifying the episodic and continual formalisms.\n\nReference markers found: [16]\n\nContext windows for each [ref]:\n[ref]16: based on the theory of general-state-space markov chainsÂ . Our MDP formulation also assumes state-based deterministic reward function Rğ‘…R. While this formulation was used in many previous worksÂ [ref]16, some literatureÂ  explicitly assign stochastic rewards to transitions, in the form of\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 16,\n            \"titles\": [\n                \"Equivalence Between Policy Gradients and Soft Q-Learning\",\n                \"Pybullet gymperium.\",\n                \"Dynamic programming and optimal control, volume 1.\",\n                \"Openai gym.\",\n                \"Off-policy actor-critic.\",\n                \"Benchmarking Deep Reinforcement Learning for Continuous Control\",\n                \"Addressing Function Approximation Error in Actor-Critic Methods\",\n                \"Continuous Deep Q-Learning with Model-based Acceleration\",\n                \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\n                \"A natural policy gradient.\",\n                \"Continuous control with deep reinforcement learning.\",\n                \"Simulation-based optimization of markov reward processes.\",\n                \"Markov chains and stochastic stability.\",\n                \"Human-level control through deep reinforcement learning.\",\n                \"Asynchronous methods for deep reinforcement learning.\",\n                \"Time Limits in Reinforcement Learning\",\n                \"Trust Region Policy Optimization\",\n                \"Proximal policy optimization algorithms.\",\n                \"Basics of applied stochastic processes.\",\n                \"Deterministic policy gradient algorithms.\",\n                \"The predictron: End-to-end learning and planning.\",\n                \"A new q (lambda) with interim forward view and monte carlo equivalence.\",\n                \"Td models: modeling the world at a mixture of time scales.\",\n                \"Reinforcement learning: An introduction.\",\n                \"Policy gradient methods for reinforcement learning with function approximation.\",\n                \"Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction.\",\n                \"An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning\",\n                \"Bias in natural actor-critic algorithms.\",\n                \"On average versus discounted reward temporal-difference learning.\",\n                \"Deep Reinforcement Learning with Double Q-learning\",\n                \"Insights in reinforcement rearning: formal analysis and empirical evaluation of temporal-difference learning algorithms.\",\n                \"Learning from delayed rewards.\",\n                \"Unifying Task Specification in Reinforcement Learning\",\n                \"Simple statistical gradient-following algorithms for connectionist reinforcement learning.\",\n                \"On convergence of emphatic temporal-difference learning.\",\n                \"Generalized Off-Policy Actor-Critic\",\n                \"Generated Reference Title 36 for [ref]16\",\n                \"Generated Reference Title 37 for [ref]16\",\n                \"Generated Reference Title 38 for [ref]16\",\n                \"Generated Reference Title 39 for [ref]16\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]16 based on the context and research area of Steady State Analysis of Episodic Reinforcement Learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}