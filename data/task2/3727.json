{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Factored Adaptation for Non-stationary Reinforcement Learning\n\nPaper Abstract: \n\nPaper Section Text:\nWe evaluate our approach on four well-established benchmarks, including Half-Cheetah-V3 from MuJoCo  [ref]21, Sawyer-Reaching and Sawyer-Peg from Sawyer  [ref]18, and Minitaur . We modified these tasks to test several non-stationary RL scenarios with continuous and discrete changes.\nThe results suggest that FANS-RL can (1) obtain high rewards, (2) learn meaningful mappings to the change factors with compact latent vectors, and (3) be robust to different non-stationary levels, different functional forms (e.g. piecewise linear, sine, damped sine) and multiple concurrent changes. For space limits, we only highlight a subset of results, and report the full results in Appendix C.4. The implementation will be open-sourced at https://bit.ly/3erKoWm. Half-Cheetah-v3 in MuJoCo.\nIn this task, the agent is moving forward using the joint legs and the objective is to achieve the target velocity vgsuperscript𝑣𝑔{v}^{g}.\nWe consider both the changes in the dynamics (change of the wind forces fwsuperscript𝑓𝑤f^{w}, change of gravity) and reward functions (change of target velocity vgsuperscript𝑣𝑔{v}^{g}). We also consider changes on the agent’s mechanism, where one random joint is disabled. The reward function is rt=−‖vto−vtg‖2−0.05​‖at‖2subscript𝑟𝑡subscriptnormsubscriptsuperscript𝑣𝑜𝑡subscriptsuperscript𝑣𝑔𝑡20.05subscriptnormsubscript𝑎𝑡2r_{t}=-\\left\\|v^{o}_{t}-v^{g}_{t}\\right\\|_{2}-0.05\\|{a}_{t}\\|_{2}, where vosuperscript𝑣𝑜v^{o} and atsubscript𝑎𝑡a_{t} are the agent’s velocity and action, respectively, at timestep t𝑡t. The number of time steps in each episode is 505050. For dynamics, we change the wind forces fwsuperscript𝑓𝑤f^{w} in the environment. Moreover, in terms of the reward functions, we change the target velocity vgsuperscript𝑣𝑔{v}^{g} to be a time-dependent variable.\nThe change function in the dynamics fwsuperscript𝑓𝑤f^{w} can be either continuous or discrete, and discrete changes can happen both within and across episodes. Similarly to LILAC [ref]7, we choose different functions (piecewise linear, sine and damped sine), besides allowing the change at specified intervals, we also allow it to change smoothly.\nThe change in the reward function vgsuperscript𝑣𝑔v^{g} is not generally stable in the continuous case, so we only consider the discrete and across episode change functions vgsuperscript𝑣𝑔v^{g} for the reward.\nWe also design a scenario where dynamic and reward functions change concurrently.\nWe report all equations for gssuperscript𝑔𝑠g^{s} and grsuperscript𝑔𝑟g^{r} in Appendix C.1. Sawyer.\nWe consider two robotic manipulation tasks, Sawyer-Reaching and Sawyer-Peg. We describe the non-stationary settings in Appendix C.2. In Sawyer-Reaching, the sawyer arm is trained to reach a target position 𝒔gsuperscript𝒔𝑔\\bm{s}^{g}.\nThe reward rtsubscript𝑟𝑡r_{t} is the difference between the current position 𝒔tsubscript𝒔𝑡\\bm{s}_{t} and the target position rt=−‖𝒔t−𝒔g‖2subscript𝑟𝑡subscriptnormsubscript𝒔𝑡superscript𝒔𝑔2r_{t}=-\\left\\|\\bm{s}_{t}-\\bm{s}^{g}\\right\\|_{2}. In this task, we cannot directly modify the dynamics in the simulator, so consider a reward-varying scenario, where the target location changes across each episode following a periodic function. In Sawyer-Peg, the robot arm is trained to insert a peg into a designed target location 𝒔gsuperscript𝒔𝑔\\bm{s}^{g}. In this task, following , we consider a reward-varying scenario, where the target location changes across each episode following a periodic function. In order to compare with similar approaches, e.g., ZeUS , CADM , Hyperdynamics and Meld [ref]18, we evaluate our method on raw pixels. Following  , we consider discrete across-episode changes, where the target location can change in each episode, and is randomly sampled from a small interval. Minitaur. A minitaur robot is a simulated quadruped robot with eight direct-drive actuators. The minitaur is trained to move at the target speed st,vsubscript𝑠𝑡𝑣s_{t,v}. The reward is rt=0.3−|0.3−𝒔t,v|−0.01⋅‖𝒂t−2​𝒂t−1+𝒂t−2‖1subscript𝑟𝑡0.30.3subscript𝒔𝑡𝑣⋅0.01subscriptnormsubscript𝒂𝑡2subscript𝒂𝑡1subscript𝒂𝑡21r_{t}=0.3-\\left|0.3-\\bm{s}_{t,v}\\right|-0.01\\cdot\\left\\|\\bm{a}_{t}-2\\bm{a}_{t-1}+\\bm{a}_{t-2}\\right\\|_{1}.\nWe modify (1) the mass m𝑚m (dynamics) and (2) target speed st,vsubscript𝑠𝑡𝑣s_{t,v} (reward) of the minitaur. We consider continuous, discrete across-episode and discrete within-episode changes for the dynamics, and across-episode changes for the reward. We describe the settings in Appendix C.3. Baselines.\nWe compare our approach with a meta-RL approach for stationary RL, VariBAD , a meta-RL approach for non-stationary RL, TRIO , as well as with two representative task embedding approaches, LILAC [ref]7 and ZeUS .\nThe details on the meta-learning setups are given in Appendix D.4.\nWe also compare with stationary RL method, SAC , which will be our lower-bound, and compare with an oracle agent that has access to the full information of non-stationarity (e.g., the wind forces) and can use it to learn a policy, which will be our upper-bound.\nFor all baselines, we use SAC for policy learning.\nWe compare with the baselines on average return and the compactness of the latent space in varying degrees and functional forms of non-stationarity. Experimental results.\nFig. 2(a)-(c) shows the smoothed curves of average return across timesteps in a subsection of Half-Cheetah, Sawyer-Reaching, and Minitaur experiments.\nSmoothed curves of other experiments are given in Appendix Fig. A2 and A3.\nFor continuous changes, we only compare with LILAC [ref]7 since other approaches are not applicable. We smooth the learning curves by uniformly selecting a few data points for readability. Table 1 shows the results on Sawyer-Peg experiments using raw pixels as input, based on the reported results in .\nThe learning curve is given in Appendix Fig. A9. For a fair comparison, we indicate the best and final average return for each baseline in Fig. A9.\nFull quantitative results of all experiemnts are given in Appendix C.4, including significance tests for Wilcoxon signed-rank test at α=0.05𝛼0.05\\alpha=0.05 showing that FANS-RL is significantly better than baselines. Finally, we conduct ablation studies on each component of FANS-RL and report some key results in Fig. 2(d). Full results are in Appendix C.4. Ablation studies. The ablation studies verify the effectiveness of all components, including binary masks/structure, smoothness loss, sparsity loss, reward prediction or state prediction. The results show that the largest gain is provided by the factored representation, validating our original hypothesis, followed by state prediction. As expected, reward prediction is also important when there is a nonstationary reward, while smoothness is important for within-episode changes. The disentangled design of CF inference networks is valuable when there are changes on both dynamics and reward functions. Full results are in Appendix C.5. Visualization on the learned θ𝜃\\bm{\\theta}.\nTo verify that the learned 𝜽𝜽\\bm{\\theta} can capture the true change factors, we compute the pairwise distance between learned 𝜽rsuperscript𝜽𝑟\\bm{\\theta}^{r} at different time steps. We randomly sample 101010 time steps from the Half-Cheetah experiment with across-episode changes on reward functions.\nFig. 2(e) gives the pairwise distance of 𝜽rsuperscript𝜽𝑟\\bm{\\theta}^{r} among 555 time steps (from episode 148,279,155,159,230148279155159230148,279,155,159,230) with different target speed values (0.04,0.48,0.89,2.06,2.850.040.480.892.062.850.04,0.48,0.89,2.06,2.85), respectively. We can find that there is a positive correlation between the distance of learned 𝜽rsuperscript𝜽𝑟\\bm{\\theta}^{r} and values of change factors. Meanwhile, we also sample 5 time steps from episodes 31,188,234,4083118823440831,188,234,408 with target values around 1.671.671.67. Fig. 2(f) shows that the distance of 𝜽rsuperscript𝜽𝑟\\bm{\\theta}^{r} among these 555 time steps is very small, indicating that the learned 𝜽rsuperscript𝜽𝑟\\bm{\\theta}^{r} are almost the same for the similar values of change factors at different time steps. The visualization suggests that the learned 𝜽𝜽\\bm{\\theta} can capture meaningful mappings from the time-varying factors. Varying latent dimensions, non-stationary levels and functional forms. Fig. 2(g) shows the normalized averaged return versus the number of two latent features on Half-Cheetah. Our framework can learn a better policy with relatively smaller feature dimensions in the latent space than other approaches. As we show in Appendix C.4, Saywer and Minitaur have the similar trend, where we learn a better policy than the baselines with fewer latent features.\nWe also vary the non-stationary levels in Half-Cheetah with discrete across-episode changes on both dynamics and rewards. A higher non-stationary degree indicates a faster change rate. Fig. 2(h) shows that FANS-RL achieves the highest return across all tested non-stationary degrees and that the gap increases with the non-stationarity.\nWe also test FANS-RL together with all baselines on different non-stationary functions, including piecewise linear, damping-like and sinusoid waves. Fig. 2(i) displays the results, which indicate that FANS-RL can generally outperform the baselines on diverse non-stationary function forms. Detailed function equations and experimental setups can be referred to Appendix C. Multiple change factors We consider different numbers and types of changes to verify the benefits from the factored structure in FANS-RL. We conduct experiments with 1) only change wind forces (1D); 2) change wind forces and gravity concurrently (2D); 3) change wind force and target speed (1D+1R); and 4) change wind force, gravity, and target speed together (2D+1R) in an across-episode way in Half-Cheetah. From Fig. 2(j), we find that, thanks to the factored representation, FANS-RL performs better in those more complicated scenarios with multiple numbers and types of changes.\n\nReference markers found: [21]\n\nContext windows for each [ref]:\n[ref]21: We evaluate our approach on four well-established benchmarks, including Half-Cheetah-V3 from MuJoCo  [ref]21, Sawyer-Reaching and Sawyer-Peg from Sawyer  [ref]18, and Minitaur . We modified these tasks to test several non-stationary RL scenarios with continuous and discrete changes.\nThe results\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 21,\n            \"titles\": [\n                \"Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning\",\n                \"Reinforcement learning: An introduction\",\n                \"Challenges of real-world reinforcement learning: definitions, benchmarks and analysis\",\n                \"A survey of reinforcement learning algorithms for dynamically varying environments\",\n                \"Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments\",\n                \"Meta-Reinforcement Learning by Tracking Task Non-stationarity\",\n                \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\",\n                \"Deep Reinforcement Learning amidst Lifelong Non-Stationarity\",\n                \"Block Contextual MDPs for Continual Learning\",\n                \"Causal discovery from heterogeneous/nonstationary data\",\n                \"Domain Adaptation as a Problem of Inference on Graphical Models\",\n                \"Stochastic dynamic programming with factored representations\",\n                \"Efficient reinforcement learning in factored mdps\",\n                \"Near-optimal Reinforcement Learning in Factored MDPs\",\n                \"AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning\",\n                \"Long short-term memory\",\n                \"Auxiliary tasks in multi-task learning\",\n                \"Recurrent World Models Facilitate Policy Evolution\",\n                \"MELD: Meta-Reinforcement Learning from Images via Latent State Models\",\n                \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\n                \"Mujoco: A physics engine for model-based control\",\n                \"Openai gym\",\n                \"Sim-to-Real: Learning Agile Locomotion For Quadruped Robots\",\n                \"Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning\",\n                \"Hyperdynamics: Meta-learning object and agent dynamics with hypernetworks\",\n                \"Varibad: Variational bayes-adaptive deep rl via meta-learning\",\n                \"Dealing with non-stationary environments using context detection\",\n                \"On the role of tracking in stationary environments\",\n                \"An environment model for nonstationary reinforcement learning\",\n                \"Solving hidden-semi-markov-mode markov decision problems\",\n                \"Optimizing for the Future in Non-Stationary MDPs\",\n                \"Minimum-delay adaptation in non-stationary reinforcement learning via online high-confidence change-point detection\",\n                \"Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning\",\n                \"A relational intervention approach for unsupervised dynamics generalization in model-based reinforcement learnings\",\n                \"Off-policy model-based learning under unknown factored dynamics\",\n                \"Factoredrl: Leveraging factored graphs for deep reinforcement learning\",\n                \"Working Memory Graphs\",\n                \"Nervenet: Learning structured policy with graph neural networks\",\n                \"Factorized world models for learning causal relationships\",\n                \"Policy Architectures for Compositional Generalization in Control\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]21 based on the context and research area of Factored Adaptation for Non-stationary Reinforcement Learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}