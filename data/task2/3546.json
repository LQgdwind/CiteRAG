{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Complexity of a Class of First-Order Objective-Function-Free Optimization Algorithms\n\nPaper Abstract: \n\nPaper Section Text:\nWe now provide some numerical illustration on problems which are commonly used for\nthe evaluation of optimization algorithms. For the sake of clarity and conciseness,\nwe needed to keep the list of algorithmic variants reported here reasonably limited,\nand have taken the following considerations into account for our choice. Both weights’ definitions (3.1) and (4.1) are illustrated.\nMoreover, since the Adam algorithm using (3.32) is so commonly used the\nstochastic context, we also included it in the comparison. Despite Theorems 3.2 and 4.2 covering a\nwide choice of the parameters μ𝜇\\mu and ν𝜈\\nu, we have chosen to focus here on the\nmost common choice for (3.1) and (3.32) (i.e. μ=12𝜇12\\mu={\\scriptstyle\\frac{1}{2}} and\nβ2=910subscript𝛽2910\\beta_{2}={\\scriptstyle\\frac{9}{10}}, corresponding to Adagrad, Adagrad-Norm and Adam). When\nusing (4.1), we have also restricted our comparison to the single\nchoice of μ𝜇\\mu and ν𝜈\\nu used (with reasonable success) in ,\nnamely μ=ν=110𝜇𝜈110\\mu=\\nu={\\scriptstyle\\frac{1}{10}}.\nBy contrast, we have included results for the variants of the weights’ definitions\nfor values of θ=1𝜃1\\theta=1 (the standard choice) and θ=n𝜃𝑛\\theta=\\sqrt{n} suggested in\n(3.33) for unknown Γ0subscriptΓ0\\Gamma_{0}. In order to be able to test enough algorithmic variants on enough problems in\nreasonable computing time, we have decided to focus our experiments on\nlow-dimensional problems in the case where ϑ=1italic-ϑ1\\vartheta=1. We have nevertheless considered a few\nlarge-scale instance for the purpose of illustrating the effect of the scaling\nθ=n𝜃𝑛\\theta=\\sqrt{n} which is only expected to be significant for such instances. We have chosen to define the step sksubscript𝑠𝑘s_{k} in Step 3 of the ASTR1 algorithm\nby approximately minimizing the quadratic model (2.10) within the\nℓ∞subscriptℓ\\ell_{\\infty} trust-region using a projected truncated conjugate-gradient\napproach  [ref]40 which is terminated as soon as We also considered an alternative,\nnamely that of minimizing the quadratic model in an Euclidean ℓ2subscriptℓ2\\ell_{2} trust region\n(with the same accuracy requirement) using a Generalized Lanczos Trust\nRegion (GLTR) technique . We thought it would be interesting to compare “purely first-order”\nvariants (that is variants for which Bk=0subscript𝐵𝑘0B_{k}=0 for all k𝑘k) with methods\nusing some kind of Hessian approximation. Among many possibilities, we selected\nthree types of approximations of interest. The first is the diagonal Barzilai-Borwein\napproximation  where Insubscript𝐼𝑛I_{n} is the identity matrix of dimension n𝑛n, yk=gk+1−gksubscript𝑦𝑘subscript𝑔𝑘1subscript𝑔𝑘y_{k}=g_{k+1}-g_{k} and\nykT​sk≥10−15​‖sk‖22superscriptsubscript𝑦𝑘𝑇subscript𝑠𝑘superscript1015superscriptsubscriptnormsubscript𝑠𝑘22y_{k}^{T}s_{k}\\geq 10^{-15}\\|s_{k}\\|_{2}^{2}. The second\nis limited-memory BFGS approximations 7, where a small number (3)\nof BFGS updates are added to the matrix (5.1), each update corresponding\nto a secant pair (yk,sk)subscript𝑦𝑘subscript𝑠𝑘(y_{k},s_{k}) with ykT​sk≥10−15​‖sk‖22superscriptsubscript𝑦𝑘𝑇subscript𝑠𝑘superscript1015superscriptsubscriptnormsubscript𝑠𝑘22y_{k}^{T}s_{k}\\geq 10^{-15}\\|s_{k}\\|_{2}^{2}. The third is not to approximate the Hessian\nat all, but to use its exact value, that is Bk=∇x2f​(xk)subscript𝐵𝑘superscriptsubscript∇𝑥2𝑓subscript𝑥𝑘B_{k}=\\nabla_{x}^{2}f(x_{k}) for all k𝑘k. Given these considerations, we have selected the algorithmic\nASTR1 variants using γk=1subscript𝛾𝑘1\\gamma_{k}=1 and detailed in\nTable 1, where the second column indicates the norm used\nto define the trust-region. \n\n\n\nName\nNorm\nwi,ksubscript𝑤𝑖𝑘w_{i,k} definition (i∈{1,…,n}𝑖1…𝑛i\\in\\{1,\\ldots,n\\})\nBk+1subscript𝐵𝑘1B_{k+1}\nparams\n\nadagnorm\n∥⋅∥2\\|\\cdot\\|_{2}\nwi,k=[1100+∑j=0k‖gj‖22]12subscript𝑤𝑖𝑘superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscriptnormsubscript𝑔𝑗2212w_{i,k}=\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}\\|g_{j}\\|_{2}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\n0\n\n\nadagrad\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=[1100+∑j=0kgi,j2]12subscript𝑤𝑖𝑘superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝑔𝑖𝑗212w_{i,k}=\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}g_{i,j}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\n0\n\n\nadamnorm\n∥⋅∥2\\|\\cdot\\|_{2}\nwi,k=[1100+∑j=0kβ2k−j​‖gj‖22]12subscript𝑤𝑖𝑘superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝛽2𝑘𝑗superscriptsubscriptnormsubscript𝑔𝑗2212w_{i,k}=\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}\\beta_{2}^{k-j}\\|g_{j}\\|_{2}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\n0\nβ=910𝛽910\\beta={\\scriptstyle\\frac{9}{10}}\n\nadam\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=[1100+∑j=0kβ2k−j​gi,j2]12subscript𝑤𝑖𝑘superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝛽2𝑘𝑗superscriptsubscript𝑔𝑖𝑗212w_{i,k}=\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}\\beta_{2}^{k-j}g_{i,j}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\n0\nβ=910𝛽910\\beta={\\scriptstyle\\frac{9}{10}}\n\nmaxgnorm\n∥⋅∥2\\|\\cdot\\|_{2}\nwi,k=(k+1)110​max⁡[1100,maxj∈{0,…,k}⁡‖gj‖2]subscript𝑤𝑖𝑘superscript𝑘11101100subscript𝑗0…𝑘subscriptnormsubscript𝑔𝑗2w_{i,k}=(k+1)^{\\scriptstyle\\frac{1}{10}}\\max\\Big{[}{\\scriptstyle\\frac{1}{100}},\\displaystyle\\max_{j\\in\\{0,\\ldots,k\\}}\\|g_{j}\\|_{2}\\Big{]}\n0\n\n\nmaxg\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=(k+1)110​max⁡[1100,maxj∈{0,…,k}⁡|gi,j|]subscript𝑤𝑖𝑘superscript𝑘11101100subscript𝑗0…𝑘subscript𝑔𝑖𝑗w_{i,k}=(k+1)^{\\scriptstyle\\frac{1}{10}}\\max\\Big{[}{\\scriptstyle\\frac{1}{100}},\\displaystyle\\max_{j\\in\\{0,\\ldots,k\\}}|g_{i,j}|\\Big{]}\n0\n\n\nadagbb\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=[1100+∑j=0kgi,j2]12subscript𝑤𝑖𝑘superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝑔𝑖𝑗212w_{i,k}=\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}g_{i,j}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\n(5.1)\n\n\nadagbfgs3\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=[1100+∑j=0kgi,j2]12subscript𝑤𝑖𝑘superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝑔𝑖𝑗212w_{i,k}=\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}g_{i,j}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\nLBFGS(3)\n\n\nadagH\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=[1100+∑j=0kgi,j2]12subscript𝑤𝑖𝑘superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝑔𝑖𝑗212w_{i,k}=\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}g_{i,j}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\n∇x2f​(xk+1)superscriptsubscript∇𝑥2𝑓subscript𝑥𝑘1\\nabla_{x}^{2}f(x_{k+1})\n\n\nadagrads\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=n​[1100+∑j=0kgi,j2]12subscript𝑤𝑖𝑘𝑛superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝑔𝑖𝑗212w_{i,k}=\\sqrt{n}\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}g_{i,j}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\n0\n\n\nadams\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=n​[1100+∑j=0kβ2k−j​gi,j2]12subscript𝑤𝑖𝑘𝑛superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝛽2𝑘𝑗superscriptsubscript𝑔𝑖𝑗212w_{i,k}=\\sqrt{n}\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}\\beta_{2}^{k-j}g_{i,j}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\n0\nβ=910𝛽910\\beta={\\scriptstyle\\frac{9}{10}}\n\nmaxgs\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=n​(k+1)110​max⁡[1100,maxj∈{0,…,k}⁡|gi,j|]subscript𝑤𝑖𝑘𝑛superscript𝑘11101100subscript𝑗0…𝑘subscript𝑔𝑖𝑗w_{i,k}=\\sqrt{n}(k+1)^{\\scriptstyle\\frac{1}{10}}\\max\\Big{[}{\\scriptstyle\\frac{1}{100}},\\displaystyle\\max_{j\\in\\{0,\\ldots,k\\}}|g_{i,j}|\\Big{]}\n0\n\n\nadagbbs\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=n​[1100+∑j=0kgi,j2]12subscript𝑤𝑖𝑘𝑛superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝑔𝑖𝑗212w_{i,k}=\\sqrt{n}\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}g_{i,j}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\n(5.1)\n\n\nadagbfgs3s\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=n​[1100+∑j=0kgi,j2]12subscript𝑤𝑖𝑘𝑛superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝑔𝑖𝑗212w_{i,k}=\\sqrt{n}\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}g_{i,j}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\nLBFGS(3)\n\n\nadagHs\n∥⋅∥∞\\|\\cdot\\|_{\\infty}\nwi,k=n​[1100+∑j=0kgi,j2]12subscript𝑤𝑖𝑘𝑛superscriptdelimited-[]1100superscriptsubscript𝑗0𝑘superscriptsubscript𝑔𝑖𝑗212w_{i,k}=\\sqrt{n}\\left[{\\scriptstyle\\frac{1}{100}}+\\sum_{j=0}^{k}g_{i,j}^{2}\\right]^{\\scriptstyle\\frac{1}{2}}\n∇x2f​(xk+1)superscriptsubscript∇𝑥2𝑓subscript𝑥𝑘1\\nabla_{x}^{2}f(x_{k+1})\n\n\nsdba\nstandard steepest-descent with\nbacktracking (e.g. [10, Algorithm 2.2.1])\n\n Note that all variants for which Bk+1=0subscript𝐵𝑘10B_{k+1}=0 are ”purely first-order” in the sense discussed above.\nNote also that, under AS.3, maxgnorm and maxg satisfy (4.1)\nwith μ=ν=110𝜇𝜈110\\mu=\\nu={\\scriptstyle\\frac{1}{10}}, ςi=ς=1100subscript𝜍𝑖𝜍1100\\varsigma_{i}=\\varsigma={\\scriptstyle\\frac{1}{100}} and κw=κgsubscript𝜅𝑤subscript𝜅𝑔\\kappa_{w}=\\kappa_{g}.\nAll algorithms were run(7)(7)(7)In\nMatlab® running under Ubuntu on a Dell Precision with 16 cores and 64\nGB of memory. on the low dimensional instances of the\nproblems(8)(8)(8)From their standard starting point. of the OPM\ncollection 3 (April 2023), a subset of widely used CUTEst testing environment .\nThe instances are listed with their dimension in\nTable 2, until either ‖∇x1f​(xk)‖2≤10−6subscriptnormsuperscriptsubscript∇𝑥1𝑓subscript𝑥𝑘2superscript106\\|\\nabla_{x}^{1}f(x_{k})\\|_{2}\\leq 10^{-6}, or a\nmaximum of 100000 iterations was reached, or evaluation of the derivatives\nreturned an error. Before considering the results, we make two additional comments. The first is\nthat very few of the test functions have bounded gradients on the whole of IRnsuperscriptIR𝑛\\hbox{I\\hskip-2.0ptR}^{n}.\nWhile this is usually not a problem when testing standard first-order descent\nmethods (because it may then be true in the level set determined by the\nstarting point), this is no longer the case for significantly non-monotone\nmethods like the ones tested here. As a consequence, it may (and does) happen\nthat the gradient evaluation is attempted at a point where its value exceeds\nthe Matlab overflow limit, causing the algorithm to fail on the\nproblem. The second comment is that the (sometimes quite wild)\nnon-monotonicity of the methods considered here has another practical\nconsequence: it happens on several nonconvex problems(9)(9)(9)broyden3d,\nbroydenbd, curly10, gottfr, hairy, indef, jensmp, osborneb, sensors,\nwmsqrtals, wmsqrtbls, woods.  that convergence of different algorithmic\nvariants occurs to points with gradient norm within termination tolerance (the\nmethods are thus achieving their objective), but these points can be quite far\napart and may have very different function values. We discuss the results of our tests from the efficiency and reliability points\nof view. Efficiency is measured in number of derivatives’ evaluations\n(or, equivalently, iterations)(10)(10)(10)For sdba, gradient and\nobjective-function evaluations.: the fewer evaluations the more efficient the\nalgorithm. Because the standard performance profiles\n for our selection of 16 algorithms would be too crowded to read,\nwe follow  and consider the derived\n“global” measure π𝚊𝚕𝚐𝚘subscript𝜋𝚊𝚕𝚐𝚘\\pi_{\\tt algo} to be 150150{\\scriptstyle\\frac{1}{50}} of the area below the curve\ncorresponding to algo in this performance profile, for abscissas in the\ninterval [ref]1 [ref]1 . The larger this area and closer π𝚊𝚕𝚐𝚘subscript𝜋𝚊𝚕𝚐𝚘\\pi_{\\tt algo} to one,\nthe closer the curve to the right and top borders of the plot and the better\nthe global performance. When reporting reliability, we say that the run of an\nalgorithmic variant on a specific test problem is successful if the gradient\nnorm tolerance has been achieved.\nIn what follows, ρ𝚊𝚕𝚐𝚘subscript𝜌𝚊𝚕𝚐𝚘\\rho_{\\tt algo} denotes the percentage of successful runs taken on\nall problems. Table 3 presents\nthe values of these statistics in two columns: for easier reading, the\nvariants are sorted by decreasing global performance (π𝚊𝚕𝚐𝚘subscript𝜋𝚊𝚕𝚐𝚘\\pi_{\\tt algo}) in the first, and\nby decreasing reliability (ρ𝚊𝚕𝚐𝚘subscript𝜌𝚊𝚕𝚐𝚘\\rho_{\\tt algo}) in the second.\nA total of 18 problems(11)(11)(11)\nbiggs5, brownbs, cliff, genhumps, gulf,\nheart6ls, heart8ls, himm29, mexhat, meyer3,\nnondquar, osbornea, penalty2, powellbs, powellsg,\nscurly10, watson, yfitu.\n could not be successfully solved by any of the above algorithms, we believe\nmostly because of ill-conditioning. The authors are of course aware that the very limited experiments presented\nhere do not replace extended numerical practice and could be completed in\nvarious ways. They nevertheless suggest the following (very tentative)\ncomments. There often seems to be a definite advantage in using the ∥⋅∥∞\\|\\cdot\\|_{\\infty}\nnorm over ∥⋅∥2\\|\\cdot\\|_{2}, as can be seen by comparing adagnorm with adagrad and maxgnorm with maxg. While this may be due in part to the fact that the trust region\nin ℓ∞subscriptℓ\\ell_{\\infty} norm is larger than that in ℓ2subscriptℓ2\\ell_{2} norm (and thus allows\nlarger steps), it is also the case that the disaggregate definition of the\nscaling factors wi,ksubscript𝑤𝑖𝑘w_{i,k} ((3.1), (3.32) or\n(4.1)) used in conjunction with the ℓ∞subscriptℓ\\ell_{\\infty} norm may\nallow a better exploitation of differences of scale between coordinates. Among the ”purely first-order” methods, sdba, maxg and\nadagrad are almost undistinguishable form the performance point of\nview, with a reliability advantage for adagrad (the most reliable\nmethod in our tests). This means that, at least in those experiments, the\nsuggestion resulting from the theory that OFFO methods may perform\ncomparably to standard first-order methods seems vindicated. The Adam variants (adamnorm and adam) are clearly\noutperformed in our tests by the Adagrad ones (adagnorm and adagrad). We recall that analytical examples where Adam fails do exist,\nwhile the convergence of Adagrad is guaranteed. The theoretical difference in global rate of convergence between adagrad and maxg does not seem to have much impact on the relative\nperformance of these two methods. The use of limited memory Hessian approximation (adagbfgs3) appears\nto enhance the performance of adagrad, but this is not the case of the\nBarzilai-Borwein approximation (adagbb) or, remarkably, for the use\nof the exact Hessian (adagH). When these methods fail, this is\noften because the steplength is too small to allow the truncated\nconjugate-gradient solver to pick up second-order information in other\ndirections than the negative gradient. What favours the limited memory\napproach remains unclear at this stage. We also note that the variants scaled with (3.33) (with\nΓ0subscriptΓ0\\Gamma_{0} unknown) did not perform better on small dimensional\nproblems; possibly because the factor n𝑛n does not dominate the\ncomplexity bounds in this case. To illustrate\nthe impact of this scaling in larger cases, we ran a subset of six\nmethods which performed well in Table 3 (namely adagnorm, adagrad, adagrads, maxgnorm, maxg\nand maxgs) on the broyden3d and nlminsurf problems with\nincreasing problem dimension (we used ϵ1=10−3subscriptitalic-ϵ1superscript103\\epsilon_{1}=10^{-3}).\nThe results are reported in Table 4. Despite the improvement in complexity due to choosing θ>1𝜃1\\theta>1 being\ntheoretical (and applies to the worst-case performance), we may still\nnote some positive (if not completely uniform) effect in this table.\nThe interpretation is also blurred somewhat by the fact that maxg and adagrad converged to local minimas of broyden3d\nrather than the global one. We nevertheless note the consistently\nbetter performance of adagnorm compared to adagrad and\nadagrads, possibly illustrating the fact that its complexity\nbound does not explicitly involve n𝑛n. Finally, and although this is a slight digression from the paper’s main topic,\nwe report in Table 5 how reliability of our selection of OFFO\nvariants is impacted by noise. To obtain these results, we ran the considered\nmethods on all test problems where the evaluations (function(12)(12)(12)For sdba. and derivatives) are contaminated by 5, 15, 25 or 50 % of\nrelative Gaussian noise with unit variance. The reliability percentages in\nthe table result from averaging results obtained for ten independent runs. As can be seen in the table, the reliability of the sdba methods\ndramatically drops as soon as noise is present, while that of the other OFFO\nmethods is barely affected and remains globally unchanged(13)(13)(13)\nIt is interesting that reliability is slightly better for the noisy\ncases and the better OFFO methods.\nfor increasing noise\nlevel. This is consistent with widespread experience in the deep learning\ncontext, where noise is caused by sampling among the very large number of\nterms defining the objective function. This observation vindicates\nthe popularity of methods such as Adagrad in the noisy context and suggests\nthat the new OFFO algorithms may have some practical potential. We conclude by noting that the algorithms’ reliability (ρ𝚊𝚕𝚐𝚘subscript𝜌𝚊𝚕𝚐𝚘\\rho_{\\tt algo}\nis (expectedly) better for ϵ1=10−3subscriptitalic-ϵ1superscript103\\epsilon_{1}=10^{-3} (first column of\nTable 5) than for ϵ1=10−6subscriptitalic-ϵ1superscript106\\epsilon_{1}=10^{-6} (Table 3),\nbut that the improvement remains modest, the reliability of Adagrad\ndecreasing marginally slower.\n\nReference markers found: [40]\n\nContext windows for each [ref]:\n[ref]40: in Step 3 of the ASTR1 algorithm\nby approximately minimizing the quadratic model (2.10) within the\nℓ∞subscriptℓ\\ell_{\\infty} trust-region using a projected truncated conjugate-gradient\napproach  [ref]40 which is terminated as soon as We also considered an alternative,\nnamely that of minimizing the quadratic model in an Euclidean ℓ2subscriptℓ2\\ell_{2} trust region\n(with the same accuracy\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 40,\n            \"titles\": [\n                \"Variants of RMSProp and Adagrad with Logarithmic Regret Bounds\",\n                \"Private Adaptive Gradient Methods for Convex Optimization\",\n                \"SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance\",\n                \"Two-point step size gradient method\",\n                \"First-order Methods in Optimization\",\n                \"The Impact of Noise on Evaluation Complexity: The Deterministic Trust-Region Case\",\n                \"Nonlinear Programming\",\n                \"Convergence rate analysis of a stochastic trust region method via supermartingales\",\n                \"On the complexity of steepest descent, Newton’s and regularized Newton’s methods for nonconvex unconstrained optimization\",\n                \"Worst-case evaluation complexity and optimality of second-order methods for nonconvex smooth optimization\",\n                \"Evaluation complexity of algorithms for nonconvex optimization\",\n                \"Generalized AdaGrad (G-AdaGrad) and Adam: A State-Space Perspective\",\n                \"Bounds on the Lambert function and their application to the outage analysis of user cooperation\",\n                \"Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks\",\n                \"Trust-Region Methods\",\n                \"On the Lambert W function\",\n                \"Concise complexity analyses for trust region methods\",\n                \"A Simple Convergence Proof of Adam and Adagrad\",\n                \"Optimality measures for performance profiles\",\n                \"Adaptive subgradient methods for online learning and stochastic optimization\",\n                \"Estimation, optimization, and parallelism when data is sparse\",\n                \"A stochastic trust region algorithm based on careful step normalization\",\n                \"A new trust region algorithm with trust region radius converging to zero\",\n                \"Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD\",\n                \"The power of adaptivity in SGD: Self-tuning step sizes with unbounded gradients and affine variance\",\n                \"Stochastic first- and zeroth-order methods for nonconvex stochastic programming\",\n                \"Solving the trust-region subproblem using the Lanczos method\",\n                \"CUTEst: a constrained and unconstrained testing environment with safe threads for mathematical optimization\",\n                \"An adaptive trust-region method without function evaluation\",\n                \"First-order objective-function-free optimization algorithms and their complexity\",\n                \"Parametric complexity analysis for a class of first-order Adagrad-like algorithms\",\n                \"Convergence properties of an objective-function-free optimization regularization algorithm, including an 𝒪​(ϵ−3/2)𝒪superscriptitalic-ϵ32\\\\mathcal{O}(\\\\epsilon^{-3/2}) complexity bound\",\n                \"Recursive trust-region methods for multiscale nonlinear optimization\",\n                \"OPM, a collection of optimization problems in Matlab\",\n                \"Online Adaptive Methods, Universality and Acceleration\",\n                \"Adam: A method for stochastic optimization\",\n                \"On the convergence of stochastic gradient descent with adaptive stepsizes\",\n                \"On the limited memory BFGS method for large scale optimization\",\n                \"Adaptive bound optimization for online convex optimization\",\n                \"Algorithms for bound constrained quadratic programming problems\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]40 based on the context and research area of Complexity of a Class of First-Order Objective-Function-Free Optimization Algorithms. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}