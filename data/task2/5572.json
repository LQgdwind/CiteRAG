{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Generative Models of Huge Objects\n\nPaper Abstract: \n\nPaper Section Text:\nSome desirable properties of a generative model are global and cannot be enforced using computationally bounded distinguishers alone.\nThis motivated [GGN10] to introduce the notion of truthfulness that ensures such global properties beyond indistinguishability.\nHere we focus on a natural global property of a binary function f:X→{0,1}:𝑓→𝑋01f:X\\to\\{0,1\\}: the size of its support 𝗌𝗎𝗉𝗉​(f):={x∈X:f​(x)=1}assign𝗌𝗎𝗉𝗉𝑓conditional-set𝑥𝑋𝑓𝑥1{\\mathsf{supp}}(f):=\\{x\\in X:f(x)=1\\}.\nThe model M𝑀M we create in Section 3.1 using the multiaccurate predictor p𝑝p from Lemma 3.3 may generate functions f𝑓f with support size different from the target function f∗superscript𝑓f^{*}. Indeed, even if ∑x∈Xp​(x)=|𝗌𝗎𝗉𝗉​(f∗)|subscript𝑥𝑋𝑝𝑥𝗌𝗎𝗉𝗉superscript𝑓\\sum_{x\\in X}p(x)=|{\\mathsf{supp}}(f^{*})|, a random function f𝑓f with each entry f​(x)𝑓𝑥f(x) independently drawn from 𝖡𝖾𝗋​(p​(x))𝖡𝖾𝗋𝑝𝑥{\\mathsf{Ber}}(p(x)) is not guaranteed to satisfy |𝗌𝗎𝗉𝗉​(f)|=|𝗌𝗎𝗉𝗉​(f∗)|𝗌𝗎𝗉𝗉𝑓𝗌𝗎𝗉𝗉superscript𝑓|{\\mathsf{supp}}(f)|=|{\\mathsf{supp}}(f^{*})|. Now we show an efficient learner that outputs truthful models that preserve the support size of the generated functions. Let the distinguisher class 𝒟𝒟\\mathcal{D} and the function class 𝒢𝒢\\mathcal{G} be defined as in Section 3.1. Let ε,γ,δ,δ′∈(0,1/2)𝜀𝛾𝛿superscript𝛿′012\\varepsilon,\\gamma,\\delta,\\delta^{\\prime}\\in(0,1/2) be parameters satisfying γ≤ε𝛾𝜀\\gamma\\leq\\varepsilon and δ′≤c​δ​γ2superscript𝛿′𝑐𝛿superscript𝛾2\\delta^{\\prime}\\leq c\\delta\\gamma^{2} for a sufficiently small absolute constant c>0𝑐0c>0. Let ℬℬ\\mathcal{B} be the class of sample-access objects induced by binary functions f:X→{0,1}:𝑓→𝑋01f:X\\to\\{0,1\\} satisfying |𝗌𝗎𝗉𝗉​(f)|=k𝗌𝗎𝗉𝗉𝑓𝑘|{\\mathsf{supp}}(f)|=k, where X={0,1}n𝑋superscript01𝑛X=\\{0,1\\}^{n}. Let ΛΛ\\Lambda be an (ε,γ,δ′)𝜀𝛾superscript𝛿′(\\varepsilon,\\gamma,\\delta^{\\prime})-auditor for 𝒢𝒢\\mathcal{G} (Definition 3.1). Then there exists an (ε,δ)𝜀𝛿(\\varepsilon,\\delta)-learner L𝐿L for ℬℬ\\mathcal{B} w.r.t. 𝒟𝒟\\mathcal{D} and the learner always outputs a random-oracle implementation of a model M𝑀M that is truthful w.r.t. ℬℬ\\mathcal{B}.\nMoreover, if the auditor ΛΛ\\Lambda runs in time at most W1subscript𝑊1W_{1} and always outputs a function with circuit size at most W2subscript𝑊2W_{2}, then the learner L𝐿L runs in time poly​(n,γ−1,log⁡(δ−1),W1)poly𝑛superscript𝛾1superscript𝛿1subscript𝑊1\\mathrm{poly}(n,\\gamma^{-1},\\log(\\delta^{-1}),W_{1}) and always outputs an implementation with circuit complexity poly​(n,γ−1,W2)poly𝑛superscript𝛾1subscript𝑊2\\mathrm{poly}(n,\\gamma^{-1},W_{2}). We prove Theorem 3.4 using Lemma 3.3 and the following lemma: Define X={0,1}n𝑋superscript01𝑛X=\\{0,1\\}^{n} for a positive integer n𝑛n.\nLet k𝑘k be an integer satisfying 0≤k≤|X|0𝑘𝑋0\\leq k\\leq|X| and let ε∈(0,1)𝜀01\\varepsilon\\in(0,1) be a parameter. There exists a poly​(n,ε−1)poly𝑛superscript𝜀1\\mathrm{poly}(n,\\varepsilon^{-1})-time implementation T𝑇T such that T𝑇T takes an arbitrary predictor p:X→ [ref]1:𝑝→𝑋01p:X\\to [ref]1 as an oracle and T𝑇T implements a model M𝑀M such that each object B∼Msimilar-to𝐵𝑀B\\sim M is the sample-access object induced by a binary function fB:X→{0,1}:subscript𝑓𝐵→𝑋01f_{B}:X\\to\\{0,1\\}. Moreover, for every predictor p:X→ [ref]1:𝑝→𝑋01p:X\\to [ref]1, the resulting model M𝑀M satisfies PrB∼M⁡[∑x∈XfB​(x)=k]=1subscriptPrsimilar-to𝐵𝑀subscript𝑥𝑋subscript𝑓𝐵𝑥𝑘1\\Pr_{B\\sim M}[\\sum_{x\\in X}f_{B}(x)=k]=1; ∑x∈X|𝔼B∼M⁡fB​(x)−p​(x)|≤|k−∑x∈Xp​(x)|+ε​|X|subscript𝑥𝑋subscript𝔼similar-to𝐵𝑀subscript𝑓𝐵𝑥𝑝𝑥𝑘subscript𝑥𝑋𝑝𝑥𝜀𝑋\\sum_{x\\in X}|\\operatorname{\\mathbb{E}}_{B\\sim M}f_{B}(x)-p(x)|\\leq|k-\\sum_{x\\in X}p(x)|+\\varepsilon|X|. We first prove Theorem 3.4 using Lemma 3.5 and then prove Lemma 3.5. We start with the observation that using ΛΛ\\Lambda, we can build an auditor Λ′superscriptΛ′\\Lambda^{\\prime} for 𝒢∪{g1}𝒢subscript𝑔1\\mathcal{G}\\cup\\{g_{1}\\}, where g1subscript𝑔1g_{1} is the constant 111 function: g1​(x)=1subscript𝑔1𝑥1g_{1}(x)=1 for every x∈X𝑥𝑋x\\in X. This is because we can accurately estimate 𝔼⁡[f∗​(x)​g1​(x)]−𝔼⁡[p​(x)​g1​(x)]𝔼superscript𝑓𝑥subscript𝑔1𝑥𝔼𝑝𝑥subscript𝑔1𝑥\\operatorname{\\mathbb{E}}[f^{*}(x)g_{1}(x)]-\\operatorname{\\mathbb{E}}[p(x)g_{1}(x)] using random examples (x,f∗​(x),p​(x))𝑥superscript𝑓𝑥𝑝𝑥(x,f^{*}(x),p(x)) for x𝑥x drawn uniformly from X𝑋X. Let B∗superscript𝐵B^{*} be the sample-access object induced by a function f∗:X→{0,1}:superscript𝑓→𝑋01f^{*}:X\\to\\{0,1\\} satisfying 𝗌𝗎𝗉𝗉​(f∗)=k𝗌𝗎𝗉𝗉superscript𝑓𝑘{\\mathsf{supp}}(f^{*})=k. By Lemma 3.3, we can learn a predictor p:X→{0,1}:𝑝→𝑋01p:X\\to\\{0,1\\} such that In particular, choosing g=g1𝑔subscript𝑔1g=g_{1}, we have We apply Lemma 3.5 on p𝑝p to obtain an implementation T𝑇T of a model M𝑀M such that each object B∼Msimilar-to𝐵𝑀B\\sim M is the sample-access object induced by a binary function fB:X→{0,1}:subscript𝑓𝐵→𝑋01f_{B}:X\\to\\{0,1\\}. Lemma 3.5 allows us to ensure that It remains to prove that M𝑀M is ε𝜀\\varepsilon-indistinguishable from B∗superscript𝐵B^{*} w.r.t. 𝒟𝒟\\mathcal{D} and that M𝑀M is truthful w.r.t. ℬℬ\\mathcal{B}. The truthfulness follows immediately from (6). We prove indistinguishability below. For every g∈𝒢𝑔𝒢g\\in\\mathcal{G}, we have where the last inequality follows from (7).\nCombining (5) and (8), for every g∈𝒢𝑔𝒢g\\in\\mathcal{G}, By 3.1, this implies that M𝑀M is ε𝜀\\varepsilon-indistinguishable from B∗superscript𝐵B^{*} w.r.t. 𝒟𝒟\\mathcal{D}.\n∎ In the following sub-sections, we prove Lemma 3.5 starting with an inefficient implementation and building to more and more efficient ones, where we use N=2n𝑁superscript2𝑛N=2^{n} to denote the domain size |X|𝑋|X|. For convenience, we sometimes take [N]:={1,…,N}assigndelimited-[]𝑁1…𝑁[N]:=\\{1,\\ldots,N\\} to be our domain in place of X={0,1}n𝑋superscript01𝑛X=\\{0,1\\}^{n}, in which case we allow N𝑁N to be not necessarily a power of 222.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: ε∈(0,1)𝜀01\\varepsilon\\in(0,1) be a parameter. There exists a poly​(n,ε−1)poly𝑛superscript𝜀1\\mathrm{poly}(n,\\varepsilon^{-1})-time implementation T𝑇T such that T𝑇T takes an arbitrary predictor p:X→ [ref]1:𝑝→𝑋01p:X\\to [ref]1 as an oracle and T𝑇T implements a model M𝑀M such that each object B∼Msimilar-to𝐵𝑀B\\sim M is the sample-access object induced by a binary function\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"Generalization and Equilibrium in Generative Adversarial Nets (GANs)\",\n                \"Mixed membership stochastic blockmodels\",\n                \"Learnability and the Vapnik-Chervonenkis dimension\",\n                \"How to generate cryptographically strong sequences of pseudorandom bits\",\n                \"Randomness-efficient oblivious sampling\",\n                \"On the complexity of simulating auxiliary input\",\n                \"Universal classes of hash functions\",\n                \"Outcome Indistinguishability\",\n                \"New insights into multi-calibration\",\n                \"Quick approximation to matrices and applications\",\n                \"How to construct random functions\",\n                \"On the implementation of huge random objects\",\n                \"Loss Minimization Through the Lens Of Outcome Indistinguishability\",\n                \"Omnipredictors\",\n                \"Low-Degree Multicalibration\",\n                \"Generative adversarial nets\",\n                \"Multicalibration: Calibration for the (computationally-identifiable) masses\",\n                \"Stochastic blockmodels: First steps\",\n                \"Omnipredictors for Constrained Optimization\",\n                \"Expander graphs and their applications\",\n                \"Comparative Learning: A Sample Complexity Theory for Two Hypothesis Classes\",\n                \"Metric Entropy Duality and the Sample Complexity of Outcome Indistinguishability\",\n                \"Lecture on learning models: connections between boosting, hard-core distributions, dense models, GAN, and regularity I\",\n                \"How to fake auxiliary input\",\n                \"Multiaccuracy: Black-Box Post-Processing for Fairness in Classification\",\n                \"Universal adaptability: Target-independent inference that competes with propensity scoring\",\n                \"Making Decisions under Outcome Performativity\",\n                \"Szemerédi’s regularity lemma and quasi-randomness\",\n                \"Results on learnability and the Vapnik-Chervonenkis dimension\",\n                \"How to construct pseudorandom permutations from pseudorandom functions\",\n                \"A Brief History of Generative Models for Power Law and Lognormal Distributions\",\n                \"Small-bias probability spaces: Efficient constructions and applications\",\n                \"Implementing huge sparse random graphs\",\n                \"Efficiently constructible huge graphs that preserve first order properties of random graphs\",\n                \"Constructing pseudo-random permutations with a prescribed structure\",\n                \"Szemerédi’s regularity lemma for matrices and sparse graphs\",\n                \"Simulating Auxiliary Inputs, Revisited\",\n                \"A subgradient algorithm for computational distances and applications to cryptography\",\n                \"A cryptographic view of regularity lemmas: simpler unified proofs and refined bounds\",\n                \"Regularity, boosting, and efficiently simulating every high-entropy distribution\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of Generative Models of Huge Objects. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}