{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Deep Attributed Network Representation Learning via Attribute Enhanced Neighborhood\n\nPaper Abstract: \n\nPaper Section Text:\n1) A Joint Optimization Framework of the DANRL-ANE Model The entire model consists of three coupled modules, which share the same encoder component. The encoder aims at mapping the input data into the representation space by one or multiple layers of non-linear functions. In our model, the input-output relationship of each layer of the encoder is defined as\n where Risubscriptğ‘…ğ‘–R_{i} is the iğ‘–i-th row of the input matrix Rğ‘…R, and records the reconstructed neighbor relationship of node visubscriptğ‘£ğ‘–v_{i}. The symbol Î´(.)\\delta(.) denotes the non-linear activation functions, and we choose the best suitable one based on their performance on different tasks and datasets. The model parameters W(k)superscriptğ‘Šğ‘˜W^{(k)} and b(k)superscriptğ‘ğ‘˜b^{(k)} represent the transformation matrix and bias vector in the kğ‘˜k-th layer, respectively, and Kğ¾K is the layer number of the encoder. The joint optimization objective of the DANRL-ANE model with where LFâ€‹oâ€‹Psubscriptğ¿ğ¹ğ‘œğ‘ƒL_{FoP}, Laâ€‹eMsuperscriptsubscriptğ¿ğ‘ğ‘’ğ‘€L_{ae}^{M}, Lsâ€‹gNâ€‹Ssuperscriptsubscriptğ¿ğ‘ ğ‘”ğ‘ğ‘†L_{sg}^{NS} are the loss function of the self-built first-order proximity preserved module, the attribute enhanced neighborhood autoencoder module and the community-aware skip-gram module, respectively, as well as Î±ğ›¼\\alpha and Î²ğ›½\\beta are the hyperparameters that balance the effect of the modules. The hyperparameter Î³ğ›¾\\gamma is the coefficient of the l2subscriptğ‘™2l_{2} norm regularizer Lrâ€‹eâ€‹gsubscriptğ¿ğ‘Ÿğ‘’ğ‘”L_{reg} that is used to prevent overfitting. The number of layers in the encoder and decoder is Kğ¾K. The matrix W(k)superscriptğ‘Šğ‘˜W^{(k)} and W^(k)superscript^ğ‘Šğ‘˜\\hat{W}^{(k)} represents the weight matrix of the encoder and decoder in the kğ‘˜k-th layer, respectively. We optimize Equation (4) by the stochastic gradient algorithm adopted by ANRL [ref]18. Next, we give the detailed description of each module. 2) Self-built First-order Proximity Preserved Module\nThe first-order proximity can reveal the similarity between nodes intuitively and simply. We here propose a self-built first-order proximity preserved module. To model the first-order proximity, inspired by the LINE  and DANE [ref]15 model, we define the joint probability wiâ€‹jâ€‹(vi,vj)subscriptğ‘¤ğ‘–ğ‘—subscriptğ‘£ğ‘–subscriptğ‘£ğ‘—w_{ij}(v_{i},v_{j}) between nodes visubscriptğ‘£ğ‘–v_{i} and vjsubscriptğ‘£ğ‘—v_{j} with the sigmoid function Ïƒâ€‹(x)=11+expâ¡(âˆ’x)ğœğ‘¥11ğ‘¥\\sigma(x)=\\frac{1}{1+\\exp(-x)}. Let x=yi(K)â€‹yj(K)Tğ‘¥superscriptsubscriptğ‘¦ğ‘–ğ¾superscriptsubscriptğ‘¦ğ‘—superscriptğ¾Tx=y_{i}^{(K)}y_{j}^{(K)^{\\mathrm{T}}}, then we obtain where yi(K)superscriptsubscriptğ‘¦ğ‘–ğ¾y_{i}^{(K)} and yj(K)superscriptsubscriptğ‘¦ğ‘—ğ¾y_{j}^{(K)} denote the representation of nodes visubscriptğ‘£ğ‘–v_{i} and vjsubscriptğ‘£ğ‘—v_{j}, respectively. Essentially, the first-order proximity on undirected and unweighted networks describes the existence or non-existence of edge between node pairs, which is equivalent to a binomial classification problem. The sigmoid cross-entropy loss function is a typical objective function for a binomial classification, which is defined as\n where tğ‘¡t represents the label of a sample, and can be either 111 or 00. If the sample belongs to the positive class, t=1ğ‘¡1t=1, else t=0ğ‘¡0t=0. The probability psâ€‹(s)subscriptğ‘ğ‘ ğ‘ p_{s}(s) indicates the possibility that the sample is predicted to belong to the positive class, and is calculated by the sigmoid function. Here, t=aiâ€‹jğ‘¡subscriptğ‘ğ‘–ğ‘—t=a_{ij} and psâ€‹(s)=wiâ€‹jâ€‹(vi,vj)subscriptğ‘ğ‘ ğ‘ subscriptğ‘¤ğ‘–ğ‘—subscriptğ‘£ğ‘–subscriptğ‘£ğ‘—p_{s}(s)=w_{ij}(v_{i},v_{j}), equivalently. Because only the information about the existing edges is useful for the network representation, we set t=1ğ‘¡1t=1, and take the first term of Equation (7) into consideration. We average all the losses when the edges exist, and thus obtain the objective of the self-built first-order proximity preserved module where aiâ€‹jsubscriptğ‘ğ‘–ğ‘—a_{ij} represents the element of the iğ‘–i-th row and the jğ‘—j-th column of the adjacency matrix Ağ´A. 3) Attribute Enhanced Neighborhood Autoencoder Module The deep autoencoder model is widely used to mine the proximity between the neighborhood structure of node pairs , since it could smoothly capture the data manifolds, and preserve the similarity between samples. The autoencoder consists of two parts, i.e. the encoder and decoder. The decoder is the inverse calculation process of the encoder. The representation in the representation space is mapped into the reconstruction space in the decoder process. The purpose of the autoencoder is to minimize the reconstruction error between the input data and the reconstructed data, so that the abstract representation of the mid-layer output can capture the manifold structure in the input data. To be specific, the objective of the autoencoder is where nğ‘›n is the number of the nodes in the networks, and Ri^^subscriptğ‘…ğ‘–\\hat{R_{i}} represents the reconstructed output of the input data Risubscriptğ‘…ğ‘–R_{i}. Although the direct neighborhood of each node gets enhanced after integrating the attribute semantics and adjacency matrix, the reconstructed adjacency matrix Rğ‘…R could still be a sparse matrix, that is, the number of non-zero elements is far less than that of zero elements. A sparse input matrix could further make the autoencoder tend to preserve more zero elements. Since the non-zero elements record the connections between nodes, it is more essential to preserve non-zero elements as much as possible instead of zero elements. Inspired by SDNE , we employ the Hadamard product as the penalty factor, and extend the loss function of the autoencoder. The modified objective is\n where âŠ™direct-product\\odot indicates the Hadamard product, bi={bi,j}j=1nsubscriptğ‘ğ‘–superscriptsubscriptsubscriptğ‘ğ‘–ğ‘—ğ‘—1ğ‘›b_{i}=\\{b_{i,j}\\}_{j=1}^{n}. If Ri,j=0subscriptğ‘…ğ‘–ğ‘—0R_{i,j}=0, bi,j=1subscriptğ‘ğ‘–ğ‘—1b_{i,j}=1, else bi,j=Ï‡>1subscriptğ‘ğ‘–ğ‘—ğœ’1b_{i,j}=\\chi>1. The attribute enhanced neighborhood autoencoder module takes each row vector of the reconstructed adjacency matrix Rğ‘…R as the sample input, where the row vector denotes the neighbor structural information with the attribute semantics of the corresponding node. In other words, the autoencoder could preserve the second-order proximity. 4) Community-aware skip-gram module Inspired by DeepWalk , we design the community-aware skip-gram module for capturing the high-order proximity in this work. To reduce the time complexity, we adopt the node sequences sampling procedure performed by node2vec[ref]3, where the return parameter pn=1.0subscriptğ‘ğ‘›1.0p_{n}=1.0 and in-out parameter qn=1.0subscriptğ‘ğ‘›1.0q_{n}=1.0, and use the negative sampling to approximate the following loss function where nğ‘›n is the number of nodes in the network, câˆˆCğ‘ğ¶c\\in C denotes the node sequences sampled by the random walk, bğ‘b is the size of the window. The input data Risubscriptğ‘…ğ‘–R_{i} occupies the iğ‘–i-th row of the input matrix Rğ‘…R. The node vi+jsubscriptğ‘£ğ‘–ğ‘—v_{i+j} is the context node of the current node visubscriptğ‘£ğ‘–v_{i} located in the generated random sequences in the window bğ‘b. The node representation yi(K)superscriptsubscriptğ‘¦ğ‘–ğ¾y_{i}^{(K)} is the output of the sample input Risubscriptğ‘…ğ‘–R_{i} through the Kğ¾K layer encoder. The matrix Hâ€²superscriptğ»â€²H^{\\prime} is the transition matrix between the representation output layer of the autoencoder and the output layer of the skip-gram, and hiâ€²superscriptsubscriptâ„ğ‘–â€²h_{i}^{\\prime} is in Row iğ‘–i of the transition matrix Hâ€²superscriptğ»â€²H^{\\prime}. Then, we obtain the following Equation (12) where Ïƒâ€‹(x)=11+expâ¡(âˆ’x)ğœğ‘¥11ğ‘¥\\sigma(x)=\\frac{1}{1+\\exp(-x)} is the sigmoid function, |nâ€‹eâ€‹g|ğ‘›ğ‘’ğ‘”|neg| denotes the number of the sampled negative samples. The sampling distribution Pnâ€‹(v)âˆdv3/4proportional-tosubscriptğ‘ƒğ‘›ğ‘£superscriptsubscriptğ‘‘ğ‘£34P_{n}(v)\\propto d_{v}^{3/4} is set as suggested in  and dvsubscriptğ‘‘ğ‘£d_{v} represents the degree of node vnsubscriptğ‘£ğ‘›v_{n}. Minimizing Equation (12), we can get the result that if the two nodes co-occur, they have similar embedding vectors. Algorithm 1 describes the learning process of the entire model and all model parameters are denoted as Î˜Î˜\\Theta.\n\nReference markers found: [18]\n\nContext windows for each [ref]:\n[ref]18: represents the weight matrix of the encoder and decoder in the kğ‘˜k-th layer, respectively. We optimize Equation (4) by the stochastic gradient algorithm adopted by ANRL [ref]18. Next, we give the detailed description of each module. 2) Self-built First-order Proximity Preserved Module\nThe first-order proximity can reveal the similarity between nodes intuitively and\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 18,\n            \"titles\": [\n                \"Network Representation Learning: A Survey\",\n                \"Unifying Virtual and Physical World: Learning Toward Local and Global Consistency\",\n                \"Anomaly Detection: A Survey\",\n                \"node2vec: Scalable Feature Learning for Networks\",\n                \"DeepWalk: Online Learning of Social Representations\",\n                \"Collective Classification in Network Data Articles\",\n                \"Label-dependent Node Classification in The Network\",\n                \"The Link-Prediction Problem for Social Networks\",\n                \"On the Relation between Low Density Separation, Spectral Clustering and Graph Cuts\",\n                \"Visualizing Data using GTSNE\",\n                \"Graph Embedding and Extensions: A General Framework for Dimensionality Reduction\",\n                \"Structural Deep Network Embedding\",\n                \"Network Studies of Social Influence\",\n                \"Birds of A Feather: Homophily in Social Networks\",\n                \"Homogeneity in Confiding Relations\",\n                \"Deep Attributed Network Embedding\",\n                \"Attributed Social Network Embedding\",\n                \"Multimodal Deep Network Embedding with Integrated Structure and Attribute Information\",\n                \"ANRL: Attributed Network Representation Learning via Deep Neural Networks\",\n                \"GraRep: Learning Graph Representations with Global Structural Information\",\n                \"Graph Embedding Techniques, Applications, and Performance: A Survey\",\n                \"Nonlinear Dimensionality Reduction by Locally Linear Embedding\",\n                \"A Global Geometric Framework for Nonlinear Dimensionality Reduction\",\n                \"BiasedWalk: Biased Sampling for Representation Learning on Graphs\",\n                \"LINE: Large-scale Information Network Embedding\",\n                \"GAT2VEC: Representation Learning for Attributed Graphs\",\n                \"A Scalable Attribute-aware Network Embedding System\",\n                \"Impact of Similarity Measures on Web-page Clustering\",\n                \"Semantic Hashing\",\n                \"Efficient Estimation of Word Representations in Vector Space\",\n                \"Label Informed Attributed Network Embedding\",\n                \"Representation Learning on Graphs: Methods and Applications\",\n                \"An Introduction to ROC Analysis\",\n                \"Generated Reference Title 33 for [ref]18\",\n                \"Generated Reference Title 34 for [ref]18\",\n                \"Generated Reference Title 35 for [ref]18\",\n                \"Generated Reference Title 36 for [ref]18\",\n                \"Generated Reference Title 37 for [ref]18\",\n                \"Generated Reference Title 38 for [ref]18\",\n                \"Generated Reference Title 39 for [ref]18\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]18 based on the context and research area of Deep Attributed Network Representation Learning via Attribute Enhanced Neighborhood. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}