{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects1footnote 11footnote 1Minor Revision, AIAA Journal of Guidance, Control, and Dynamics (YouTube video: https://youtu.be/AhDPE-R5GZ4)\n\nPaper Abstract: \n\nPaper Section Text:\nSince an SN-DNN is Lipschitz bounded by design and robust to perturbation, the optimality gap of the guidance framework uâ„“subscriptğ‘¢â„“u_{\\ell} introduced in Sec.Â 3.2 can be bounded as in the following theorem, where the notations are summarized in TableÂ 2 and the proof concept is illustrated in Fig.Â 5. Suppose that umpcsubscriptğ‘¢mpcu_{\\mathrm{mpc}} is Lipschitz with its 222-norm Lipschitz constant Lmpcâˆˆâ„>0subscriptğ¿mpcsubscriptâ„absent0L_{\\mathrm{mpc}}\\in\\mathbb{R}_{>0}. If uâ„“subscriptğ‘¢â„“u_{\\ell} is trained using the empirical loss functionÂ (9) of RemarkÂ 3 to have âˆƒÏµtrainâˆˆâ„â‰¥0subscriptitalic-Ïµtrainsubscriptâ„absent0\\exists\\epsilon_{\\mathrm{train}}\\in\\mathbb{R}_{\\geq 0} s.t. then we have the following bound: where râ€‹(x,Å“,t,Ï)=infiâˆˆâ„•âˆª[1,Nd]â€–xÂ¯iâˆ’xâ€–2+â€–Å“Â¯iâˆ’Å“â€–2+(tÂ¯iâˆ’t)2+â€–ÏÂ¯iâˆ’Ïâ€–2ğ‘Ÿğ‘¥Å“ğ‘¡ğœŒsubscriptinfimumğ‘–â„•1subscriptğ‘ğ‘‘superscriptnormsubscriptÂ¯ğ‘¥ğ‘–ğ‘¥2superscriptnormsubscriptÂ¯Å“ğ‘–Å“2superscriptsubscriptÂ¯ğ‘¡ğ‘–ğ‘¡2superscriptnormsubscriptÂ¯ğœŒğ‘–ğœŒ2r(x,\\textit{\\oe},t,\\rho)=\\inf_{i\\in\\mathbb{N}\\cup[1,N_{d}]}\\sqrt{\\|\\bar{x}_{i}-x\\|^{2}+\\|\\bar{\\textit{\\oe}}_{i}-\\textit{\\oe}\\|^{2}+(\\bar{t}_{i}-t)^{2}+\\|\\bar{\\rho}_{i}-\\rho\\|^{2}}. Let Î·=(x,Å“,t,Ï)ğœ‚ğ‘¥Å“ğ‘¡ğœŒ\\eta=(x,\\textit{\\oe},t,\\rho) be a test element in ğ’®testsubscriptğ’®test\\mathcal{S}_{\\mathrm{test}} (i.e., Î·âˆˆğ’®testğœ‚subscriptğ’®test\\eta\\in\\mathcal{S}_{\\mathrm{test}}) and let Î¶Â¯jâ€‹(Î·)subscriptÂ¯ğœğ‘—ğœ‚\\bar{\\zeta}_{j}(\\eta) be the training data point in Î trainsubscriptÎ train\\Pi_{\\mathrm{train}} (i.e., Î¶Â¯jâ€‹(Î·)âˆˆÎ trainsubscriptÂ¯ğœğ‘—ğœ‚subscriptÎ train\\bar{\\zeta}_{j}(\\eta)\\in\\Pi_{\\mathrm{train}}) that achieves the infimum of râ€‹(Î·)ğ‘Ÿğœ‚r(\\eta) with i=jğ‘–ğ‘—i=j for a given Î·âˆˆğ’®testğœ‚subscriptğ’®test\\eta\\in\\mathcal{S}_{\\mathrm{test}}. Since uâ„“subscriptğ‘¢â„“u_{\\ell} and umpcsubscriptğ‘¢mpcu_{\\mathrm{mpc}} are Lipschitz by design and by assumption, respectively, we have for any Î·âˆˆğ’®testğœ‚subscriptğ’®test\\eta\\in\\mathcal{S}_{\\mathrm{test}} that where the second inequality follows from the definition of Î¶Â¯jâ€‹(Î·)subscriptÂ¯ğœğ‘—ğœ‚\\bar{\\zeta}_{j}(\\eta) and the third inequality follows fromÂ (10) and the fact that Î¶Â¯jâ€‹(Î·)âˆˆÎ trainsubscriptÂ¯ğœğ‘—ğœ‚subscriptÎ train\\bar{\\zeta}_{j}(\\eta)\\in\\Pi_{\\mathrm{train}}. This relation leads to the desired resultÂ (11) as it holds for any Î·âˆˆğ’®testğœ‚subscriptğ’®test\\eta\\in\\mathcal{S}_{\\mathrm{test}}.\nâˆ The SN-DNN provides a verifiable optimality gap even for data points not in its training set, which indicates that it still benefits from the near-optimal guarantee of the MPC in terms of dynamic regret as discussed belowÂ (5). As illustrated in Fig.Â 5, each term in the optimality gapÂ (11) can be interpreted as follows: Ïµtrainsubscriptitalic-Ïµtrain\\epsilon_{\\mathrm{train}} ofÂ (10) is the training error of the SN-DNN, which is expected to decrease as the learning proceeds using SGD. In general, the value of Ïµtrainsubscriptitalic-Ïµtrain\\epsilon_{\\mathrm{train}} is affected by the choice of Lipschitz constant Lâ„“subscriptğ¿â„“L_{\\ell} (seeÂ [ref]27 for more details). râ€‹(Î·)ğ‘Ÿğœ‚r(\\eta) is the closest distance from the test element Î·âˆˆğ’®testğœ‚subscriptğ’®test\\eta\\in\\mathcal{S}_{\\mathrm{test}} to a training data point in Î trainsubscriptÎ train\\Pi_{\\mathrm{train}}, expected to decrease as the number of data points Ndsubscriptğ‘ğ‘‘N_{d} in Î trainsubscriptÎ train\\Pi_{\\mathrm{train}} increases and as the training set ğ’®trainsubscriptğ’®train\\mathcal{S}_{\\mathrm{train}} gets larger. The Lipschitz constant Lâ„“subscriptğ¿â„“L_{\\ell} is a design parameter we could arbitrarily choose when constructing the SN-DNN. Note that the SN-DNN is Lipschitz by design by spectrally normalizing the weights of the neural networkÂ , and thus we can freely choose Lâ„“subscriptğ¿â„“L_{\\ell} independently of the choice of the neural net parameters. We could also treat Lmpcsubscriptğ¿mpcL_{\\mathrm{mpc}} as a design parameter by adding a Lipschitz constraint, â€–âˆ‚umpc/âˆ‚Î·â€–â‰¤Lmpcnormsubscriptğ‘¢mpcğœ‚subscriptğ¿mpc\\|\\partial u_{\\mathrm{mpc}}/\\partial\\eta\\|\\leq L_{\\mathrm{mpc}}, in solvingÂ (3). Note that Ïµtrainsubscriptitalic-Ïµtrain\\epsilon_{\\mathrm{train}} and râ€‹(Î·)ğ‘Ÿğœ‚r(\\eta) can always be computed numerically for a given Î·ğœ‚\\eta as the dataset Î trainsubscriptÎ train\\Pi_{\\mathrm{train}} only has a finite number of data points. The obvious downsides of the optimality gapÂ (11) are that (i) it still suffers the generalization issue, which is common in the field of machine learning, and (ii) we may have to empirically find the region that the MPC is locally Lipschitz. Also, in practice, although the SN-DNN is originally designed for a better generalization property, a smaller Lipschitz constant Lâ„“subscriptğ¿â„“L_{\\ell} could lead to a larger training error Ïµtrainsubscriptitalic-Ïµtrain\\epsilon_{\\mathrm{train}}, requiring some tuning by trading-off the training error and test error in real-world simulations (see Sec.Â 6 and Sec.Â 7)Â [ref]27. The benefit of having LemmaÂ 1 is to explicitly describe all the assumptions we need to mathematically obtain the bound on the learning error, without treating it like a black box. For controllers whose optimal solutions are not necessarily Lipsthiz (e.g., impulsive-type controllers), the SN-DNN is not a suitable representation due to its Lipschitz assumption, which is one of the limitations of the bound in LemmaÂ 1. Although we have demonstrated the practicality of this assumption for our cases in Sec.Â 7 (and inÂ  with the comparison of our method with impulsive maneuvers), improving the expressibility of our method is required for its more general applications with more general mathematical guarantees. This is left as one of our future works. Obtaining a tighter and more general optimality gap of the learned control and state trajectories has been an active field of research. Particularly for a neural network equipped with a dynamical structure as in our proposed approach, we could refer to generalization bounds for the neural ordinary differential equationsÂ [ref]6, or use systems and control theoretical methods to augment it with stability and robustness propertiesÂ [ref]28 . We could also consider combining the SN-DNN with neural networks that have enhanced structural guarantees of robustness, including robust implicit networksÂ  and robust equilibrium networksÂ . The optimality gap discussed in LemmaÂ 1Â andÂ in RemarkÂ 5 is useful in that they provide mathematical guarantees for learning-based frameworks only with the Lipschitz assumption (e.g., it could prove safety in the learning-based MPC frameworkÂ ). However, when the system is perturbed by the state uncertainty as inÂ (1), we could only guarantee that the distance between the state trajectories controlled by umpcsubscriptğ‘¢mpcu_{\\mathrm{mpc}} ofÂ (5) and uâ„“subscriptğ‘¢â„“u_{\\ell} of LemmaÂ 1 is bounded by a function that increases exponentially with timeÂ . In Sec.Â 4Â andÂ 5, we will see how such a conservative bound can be replaced by a decreasing counterpart.\n\nReference markers found: [27]\n\nContext windows for each [ref]:\n[ref]27: decrease as the learning proceeds using SGD. In general, the value of Ïµtrainsubscriptitalic-Ïµtrain\\epsilon_{\\mathrm{train}} is affected by the choice of Lipschitz constant Lâ„“subscriptğ¿â„“L_{\\ell} (seeÂ [ref]27 for more details). râ€‹(Î·)ğ‘Ÿğœ‚r(\\eta) is the closest distance from the test element Î·âˆˆğ’®testğœ‚subscriptğ’®test\\eta\\in\\mathcal{S}_{\\mathrm{test}} to a training data point in\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 27,\n            \"titles\": [\n                \"Robust Classification using Contractive Hamiltonian Neural ODEs\",\n                \"A Brief Visit From a Red and Extremely Elongated Interstellar Asteroid\",\n                \"Initial Characterization of Interstellar Comet 2I/Borisov\",\n                \"Mission Design Study for Objects on Oort Cloud Comet Orbits\",\n                \"Spectral Normalization for Generative Adversarial Networks\",\n                \"The Power of Predictions in Online Control\",\n                \"Neural Ordinary Differential Equations\",\n                \"Adaptive-Control-Oriented Meta-Learning for Nonlinear Systems\",\n                \"Learning-based Model Predictive Control: Toward Safe Learning in Control\",\n                \"On Contraction Analysis for Nonlinear Systems\",\n                \"Applied Nonlinear Control\",\n                \"Interstellar Object Accessibility and Mission Design\",\n                \"Model Predictive Control: Past, Present and Future\",\n                \"A Six Degree-of-freedom Spacecraft Dynamics Simulator for Formation Control Research\",\n                \"Adaptive Dynamic Programming: An Introduction\",\n                \"Differential Dynamic Programming with Nonlinear Constraints\",\n                \"Neural-Fly Enables Rapid Learning for Agile Flight in Strong Winds\",\n                \"Autonomous Optical Navigation (AutoNav) DS1 Technology Validation Report\",\n                \"Navigation Evaluation for Fast Interstellar Object Flybys\",\n                \"Robust Model Predictive Control: A Survey\",\n                \"Contraction Theory for Nonlinear Stability Analysis and Learning-based Control: A Tutorial Overview\",\n                \"Swarm-keeping Strategies for Spacecraft under J2subscriptğ½2J_{2} and Atmospheric Drag Perturbations\",\n                \"Analytical Mechanics of Space Systems\",\n                \"Application of Synchronization to Formation Flying Spacecraft: Lagrangian Approach\",\n                \"A Contraction Theory-based Analysis of the Stability of the Deterministic Extended Kalman Filter\",\n                \"Information-based Guidance and Control Architecture for Multi-Spacecraft On-Orbit Inspection\",\n                \"Nonautonomous Dynamical Systems\",\n                \"Spectrally-normalized margin bounds for neural networks\",\n                \"LyaNet: A Lyapunov Framework for Training Neural ODEs\",\n                \"Robust Implicit Networks via Non-Euclidean Contractions\",\n                \"Recurrent Equilibrium Networks: Unconstrained Learning of Stable and Robust Dynamical Models\",\n                \"A Review of Space Robotics Technologies for On-orbit Servicing\",\n                \"Trajectory Generation and Control for Precise Aggressive Maneuvers with Quadrotors\",\n                \"Rapidly Exponentially Stabilizing Control Lyapunov Functions and Hybrid Zero Dynamics\",\n                \"Stochastic Stability and Control\",\n                \"Control Barrier Function Based Quadratic Programs with Application to Adaptive Cruise Control\",\n                \"Robust Controller Design for Stochastic Nonlinear Systems via Convex Optimization\",\n                \"Trajectory Optimization of Chance-Constrained Nonlinear Stochastic Systems for Motion Planning Under Uncertainty\",\n                \"GLAS: Global-to-Local Safe Autonomy Synthesis for Multi-Robot Motion Planning with End-to-End Learning\",\n                \"Swarm Assignment and Trajectory Optimization using Variable-swarm, Distributed Auction Assignment and Sequential Convex Programming\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]27 based on the context and research area of Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects1footnote 11footnote 1Minor Revision, AIAA Journal of Guidance, Control, and Dynamics (YouTube video: https://youtu.be/AhDPE-R5GZ4). The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}