{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Critical Analysis of Classifier Selection in Learned Bloom Filters\n\nPaper Abstract: \n\nPaper Section Text:\nStarting from an initial list of classifiersâ€”without presuming to be exhaustiveâ€”we performed a set of preliminary experiments, from which we received indications about the classifiersâ€™ families to be further analyzed, based on their trade-off between performance and space requirements111The experiments and data about this preliminary part are available upon request.. Namely, from the initial list we have removed the following classifiers: Logistic RegressionÂ , Naive BayesÂ  and Recurrent Neural NetworksÂ , due to their poor trade-off performance, confirming the results of a preliminary studyÂ . The remaining ones are briefly described in the following paragraphs. Since our evaluation considers both balanced and unbalanced classification problems, we also detail how their inference is managed in an unbalanced context.\nThe hyperparameters of the corresponding learning algorithms are distinguished between regular and key hyperparameters, the latter affecting the space occupancy of the classifier. The model selection phase only involves non-key hyperparameters, while different configurations for key hyperparameters are analysed in dedicated experiments aiming at studying the interplay among FPR, space occupancy and reject time of Learned Bloom Filters.\nHereafter, when not differently specified, the space occupancy serves as a proxy for the complexity of a classifier.\n In order to fix notation, we assume U=â„qğ‘ˆsuperscriptâ„ğ‘U=\\mathbb{R}^{q} as universe, and we refer to DâŠ‚Uğ·ğ‘ˆD\\subset U as a set of dğ‘‘d labeled instances, denoting by ğ’™âˆˆDğ’™ğ·\\boldsymbol{x}\\in D a training instance and by yxâˆˆ{0,1}subscriptğ‘¦ğ‘¥01y_{x}\\in\\{0,1\\} its label (with a slightly different notation for SVMs, as explained here below). Classification in Linear Support Vector Machines (SVMs) for a given instance ğ’™ğ’™\\boldsymbol{x} is usually based on the optimal hyperplane (ğ°,bğ°ğ‘\\mathbf{w},b) and the sign of fâ€‹(ğ’™)=ğ°â‹…ğ’™+bğ‘“ğ’™â‹…ğ°ğ’™ğ‘f(\\boldsymbol{x})=\\mathbf{w}\\cdot\\boldsymbol{x}+b. To fall in the setting defined for a LBF, we need to transform the SVM prediction into a score in   . To this end, we use fâ€‹(ğ’™)ğ‘“ğ’™f(\\boldsymbol{x}) as argument to a sigmoid function.\nThe optimal hyperplane (ğ°,bğ°ğ‘\\mathbf{w},b) is learned via maximization of the margin between the two classes, by solving where misclassification is allowed by the slack variables Î¾xsubscriptğœ‰ğ‘¥\\xi_{x} and penalized using the hyperparameter c>0ğ‘0c>0 (note that in this case yxâˆˆ{âˆ’1,1}subscriptğ‘¦ğ‘¥11y_{x}\\in\\{-1,1\\}). Nonlinear SVMs might have been used here, but the need of storing the kernel matrix, e.g., a Gaussian kernel, alongside the hyperplane parameters results in an unacceptable size for the learned filters. For the linear case we have chosen, we have only one non-key hyperparameter, namely cğ‘c. When dealing with unbalanced labels, we consider the cost-senstive SVM version described in . We also consider Feed-Forward neural networksÂ   accepting instances as inputs and equipped with lğ‘™l hidden layers, respectively having h1,â€¦,hlsubscriptâ„1â€¦subscriptâ„ğ‘™h_{1},\\ldots,h_{l} hidden units\n(NN-h1,â€¦,hlsubscriptâ„1â€¦subscriptâ„ğ‘™h_{1},\\dots,h_{l} for short).\nOne output unit completes the network topology.\nAs usual, training involves the estimation of unit connections and biases from data. In this case, the hisubscriptâ„ğ‘–h_{i}s are key hyperparameters, while the learning rate lâ€‹rğ‘™ğ‘Ÿlr acts as a regular hyperparameter to be tuned. It is worth noting that we fix the activation functions for all network units (although the former are tunable in principle), to limit the size of the already massive set of experiments. Precisely, as typically done, we use ReLU and Sigmoid activations for hidden and output units, respectively.\nWhere appropriate, label imbalance is dealt using a cost-sensitive model variantÂ . Finally, we consider also Random ForestsÂ , shortened as RF-tğ‘¡t to make explicit that this model is an ensemble of tğ‘¡t classification treesÂ . Each such tree is trained on a different bootstrap subset of Dğ·D randomly extracted with replacement. Analogously, the splitting functions at the tree nodes are chosen from a random subset of the available attributes. The RF aggregates classifications uniformly across trees, computing for each instance the fraction of trees that output a positive classification. To address the case where labels are unbalanced, we adopt an imbalance-aware variant of RFsÂ  [ref]26\nin which, during the growth of each tree, the bootstrap sample is not drawn uniformly over Dğ·D, but by selecting an instance ğ’™ğ’™\\boldsymbol{x} with probability where D+={ğ’™âˆˆD|yx=1}subscriptğ·conditional-setğ’™ğ·subscriptğ‘¦ğ‘¥1D_{+}=\\{\\boldsymbol{x}\\in D|y_{x}=1\\}, and Dâˆ’=Dâˆ–D+subscriptğ·ğ·subscriptğ·D_{-}=D\\setminus D_{+}. In this way, the probabilities of extracting a positive or a negative example are both 1/2121/2, and the trees are trained on balanced subsets. The key hyperparameter of a RF is tğ‘¡t, directly impacting on the classifier size. The non-key hyperparameter which we consider here is the minimum number Î´ğ›¿\\delta of samples in a leaf, which allows to control the depth of the individual trees. It is worth observing that this hyperparameter can also slightly affect the tree size, but in our setting the adopted grid of values only yields a negligible difference in size (see SectionÂ 4.3).\n\nReference markers found: [26]\n\nContext windows for each [ref]:\n[ref]26: across trees, computing for each instance the fraction of trees that output a positive classification. To address the case where labels are unbalanced, we adopt an imbalance-aware variant of RFsÂ  [ref]26\nin which, during the growth of each tree, the bootstrap sample is not drawn uniformly over Dğ·D, but by selecting an instance ğ’™ğ’™\\boldsymbol{x} with probability where\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 26,\n            \"titles\": [\n                \"RadixSpline: A Single-Pass Learned Index\",\n                \"On learning algorithm selection for classification\",\n                \"Learned sorted table search and static indexes in small model space (Extended Abstract)\",\n                \"Space/time trade-offs in hash coding with allowable errors\",\n                \"A â€œlearnedâ€ approach to quicken and compress rank/select dictionaries\",\n                \"Random Forests\",\n                \"Classification and regression trees\",\n                \"Network Applications of Bloom Filters: A Survey\",\n                \"Classification of imbalanced remote-sensing data by neural networks\",\n                \"Analysis of data complexity measures for classification\",\n                \"Universal classes of hash functions\",\n                \"On the properties of neural machine translation: Encoderâ€“decoder approaches\",\n                \"Genomic DNA k-mer spectra: models and modalities\",\n                \"The regression analysis of binary sequences\",\n                \"Adaptive learned bloom filter (ada-bf): Efficient utilization of the classifier\",\n                \"Adaptive Learned Bloom Filter (Ada-BF): Efficient utilization of the classifier with application to real-time information filtering on the web\",\n                \"Pattern Classification and Scene Analysis\",\n                \"Pattern Classification, 2nd Edition\",\n                \"To petabytes and beyond: recent advances in probabilistic and signal processing algorithms and their application to metagenomics\",\n                \"On the performance of learned data structures\",\n                \"The PGM-index: a fully-dynamic compressed learned index with provable worst-case bounds\",\n                \"Domains of competence of the semi-naive bayesian network classifiers\",\n                \"Statistical Models : Theory and Practice\",\n                \"On the Choice of General Purpose Classifiers in Learned Bloom Filters: An Initial Analysis Within Basic Filters\",\n                \"Neural networks: a comprehensive foundation\",\n                \"Learning from imbalanced data\",\n                \"Predicting disease risks from highly imbalanced data using random forest\",\n                \"Sapling: accelerating suffix array queries with learned data models\",\n                \"Towards instance-optimized data systems\",\n                \"The Case for Learned Index Structures\",\n                \"Stable learned bloom filters for data streams\",\n                \"How Complex is your classification problem? A survey on measuring classification complexity\",\n                \"An automatic extraction method of the domains of competence for learning classifiers using data complexity measures\",\n                \"A Critical Analysis of Recursive Model Indexes\",\n                \"Benchmarking Learned Indexes\",\n                \"CDFShop: Exploring and optimizing learned index structures\",\n                \"Deep neural networks compression: a comparative survey and choice recommendations\",\n                \"A Model for Learned Bloom Filters, and Optimizing by Sandwiching\",\n                \"Algorithms with predictions\",\n                \"Combining statistical learning with a knowledge-based approach: a case study in intensive care monitoring\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]26 based on the context and research area of A Critical Analysis of Classifier Selection in Learned Bloom Filters. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}