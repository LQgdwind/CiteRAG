{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Low-Degree Multicalibration\n\nPaper Abstract: \n\nPaper Section Text:\nMachine learning models are increasingly used to aid decision-making in professional, personal, medical, and legal spheres.\nThis ubiquity has brought increased concern about whether these models make fair predictions, especially on underrepresented subpopulations.\nTypically in supervised learning, models are trained to minimize the expected loss over the entire population, and can be less accurate on such subpopulations.\nA particular fairness concern is that predictive models may commit algorithmic stereotyping, where every member of a subpopulation receives similar predictions, despite internal diversity within the subpopulation.\nSuch shortcomings of the standard supervised learning framework have been documented extensively within the research community and in the popular press.\nIn response, a growing area of research investigates multi-group fairness notions, that require predictions to perform well not simply overall, but even when restricting attention to structured subgroups [HKRR18, KNRW18, KRR18, KGZ19, SCM20, BL20, DKR+21, JLP+21, RY21, TH21, GJN+21, DKR+22]. Central within the study of multi-group fairness is the notion of multicalibration.\nCalibration is a classic notion from the forecasting literature [Daw82] that was introduced to the literature on fairness in prediction tasks by [KMR17].\nFor a binary prediction task, calibration requires that amongst the individuals which receive prediction f​(x)=v𝑓𝑥𝑣f(x)=v, the true expectation is v𝑣v.\nDefined by [HKRR18], multicalibration strengthens the classic notion, requiring a predictor to be calibrated simultaneously across a large, possibly-overlapping collection of subpopulations.\nWe model the collection by a hypothesis class 𝒞⊆{c:𝒳→{0,1}}𝒞conditional-set𝑐→𝒳01\\mathcal{C}\\subseteq\\left\\{c:\\mathcal{X}\\to\\left\\{0,1\\right\\}\\right\\} and say that a predictor f𝑓f is mutlicalibrated over 𝒞𝒞\\mathcal{C} if, for all predicted values v∈ [ref]1𝑣01v\\in [ref]1, and for all c∈𝒞𝑐𝒞c\\in\\mathcal{C} Intuitively, an expressive class 𝒞𝒞\\mathcal{C} will contain subpopulations that go beyond “protected groups” (typically defined marginally in terms of a single attribute).\nIn this way, while calibration provides only marginal guarantees, multicalibration requires predictors to capture the variation within subpopulations, to give confident (but not overconfident) predictions,\nthus providing strong protections against algorithmic stereotyping. Beyond its origins as a notion of fairness, multicalibration has proved surprisingly versatile and powerful in diverse contexts.\nThe notion of multicalibration makes no mention of loss minimization, yet the work of [GKR+22] shows that multicalibrated predictors implicitly obtain optimal loss, simultaneously for all Lipschitz, convex losses.\nSpecifically, given any (fixed) 𝒞𝒞\\mathcal{C}-multicalibrated predictor f𝑓f, for every such loss ℓℓ\\ell, the predictor f𝑓f guarantees loss competitive with the hypothesis cℓ∈𝒞subscript𝑐ℓ𝒞c_{\\ell}\\in\\mathcal{C} chosen to the minimize the loss over 𝒞𝒞\\mathcal{C} (in fact, over linear combinations of hypotheses in 𝒞𝒞\\mathcal{C}).\nThis property leads to an omniprediction guarantee, where one can learn a single predictor f𝑓f, without knowledge of the choice of loss at the time of learning. In another direction, [DKR+21] demonstrate that the multicalibration framework is equivalent to a certain pseudorandomness condition, which they call outcome indistinguishability.\nIntuitively, a predictor f𝑓f is outcome indistinguishable to a family of distinguisher algorithms 𝒜𝒜\\mathcal{A} if given a sample (𝐱,𝐲)𝐱𝐲(\\mathbf{x},\\mathbf{y}) no distinguisher A∈𝒜𝐴𝒜A\\in\\mathcal{A} can tell whether 𝐲𝐲\\mathbf{y} was sampled from Nature’s true conditional distribution of 𝐲|𝐱conditional𝐲𝐱\\mathbf{y}|\\mathbf{x}, or according to the predicted probability f​(𝐱)𝑓𝐱f(\\mathbf{x}).\nThis indistinguishability perspective has seen application in characterizing the feasibility of multi-group strengthenings of agnostic PAC learnability [RY21].\nMulticalibration has also been extended to diverse settings of uncertainty quantification for real-valued outcomes [JLP+21], importance weights [GRSW21], online prediction [GJN+21], and adaptation under covariate shift [KKG+22]. It is perhaps not a surprise that the power of multicalibration comes at a cost—both in terms of samples and computation.\nComputationally, [HKRR18] show that a weak agnostic learner for the class 𝒞𝒞\\mathcal{C} is necessary and sufficient to learn 𝒞𝒞\\mathcal{C}-multicalibrated predictors.\nUsing the weak learner, they design a boosting-style algorithm that produces a multicalibrated predictor by combining hypotheses c∈𝒞𝑐𝒞c\\in\\mathcal{C} using nontrivial Boolean logic.111In fact, multicalibration has been shown to be tightly connected to the boosting-by-branching-programs framework of [MM02, KMV08]. See [GKR+22] for a discussion.\nThe number of calls to the weak learner and the sample complexity are governed by an approximation parameter α𝛼\\alpha which quantifies the deviation from perfect multicalibration.\nThe sample complexity depends inverse polynomially in the parameter α𝛼\\alpha, with fairly large exponent.\nThis dependence becomes even worse when we generalize multicalibration to the multi-class setting with l>2𝑙2l>2 class labels, where a single prediction is a vector of probabilities in l𝑙l dimensions. Reasoning about expectations conditioned on the predictions leads to complexity that scales as 1/αΩ​(ℓ)1superscript𝛼Ωℓ1/\\alpha^{\\Omega(\\ell)}, exponential in the number of class labels.\nConsequently, achieving multicalibration is practically infeasible with more than a few classes. In the work defining multicalibration, [HKRR18] also introduced a weaker fairness notion known as multiaccuracy, which only requires that f​(𝐱)𝑓𝐱f(\\mathbf{x}) and 𝐲𝐲\\mathbf{y} have similar expectations over subpopulations in 𝒞𝒞\\mathcal{C}, without conditioning on the predicted values. The simpler notion of multiaccuracy is comparatively easier to obtain, quantitatively and qualitatively. One can view 𝒞𝒞\\mathcal{C}-multiaccuracy as a first-order optimality condition on λ∈ℝ|𝒞|𝜆superscriptℝ𝒞\\lambda\\in{\\mathbb{R}}^{\\left|\\mathcal{C}\\right|} for predictors of the form f​(x)=∑c∈𝒞λc⋅c​(x)𝑓𝑥subscript𝑐𝒞⋅subscript𝜆𝑐𝑐𝑥f(x)=\\sum_{c\\in\\mathcal{C}}\\lambda_{c}\\cdot c(x).\nThus, while multiaccuracy also requires a weak learning oracle, one can learn a multiaccurate predictor simply by minimizing the squared or logistic loss over linear combinations of c∈𝒞𝑐𝒞c\\in\\mathcal{C} (without specialized boolean logic), using techniques like coordinate ascent or gradient boosting.\nFurther, standard concentration inequalities demonstrate that the sample complexity to obtain multiaccuracy scales as α−2superscript𝛼2\\alpha^{-2}, even as the number of classes l𝑙l grows. In exchange for its efficiency, multiaccuracy is known to provide considerably weaker guarantees than multicalibration.\nMany of the most desirable fairness and accuracy properties that can be derived from multicalibration cannot be derived from multiaccuracy alone, including its guarantees for loss minimization [HKRR18, RY21, GKR+22], the fairness properties of the ranking induced by predictions [DKR+19], and multi-group confidence intervals [JLP+21].\nThis tension—between guarantees and efficiency—brings us to the motivating question behind our work: Are there notions that retain important properties of multicalibration,\n\nbut are computable much more efficiently (comparable to multiaccuracy)?\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: and say that a predictor f𝑓f is mutlicalibrated over 𝒞𝒞\\mathcal{C} if, for all predicted values v∈ [ref]1𝑣01v\\in [ref]1, and for all c∈𝒞𝑐𝒞c\\in\\mathcal{C} Intuitively, an expressive class 𝒞𝒞\\mathcal{C} will contain subpopulations that go beyond “protected groups” (typically defined marginally in\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"Advancing subgroup fairness via sleeping experts\",\n                \"The limits of distribution-free conditional predictive inference\",\n                \"Agnostic boosting\",\n                \"Algorithmic stability for adaptive data analysis\",\n                \"Developing a COVID-19 mortality risk prediction model when individual-level data are not available\",\n                \"Verification of forecasts expressed in terms of probability\",\n                \"Sum-of-Squares Proofs and the Quest toward Optimal Algorithms\",\n                \"Addressing bias in prediction models by improving subpopulation calibration\",\n                \"Objective probability forecasts\",\n                \"Present position and potential developments: Some personal views statistical theory the prequential approach\",\n                \"Preserving statistical validity in adaptive data analysis\",\n                \"Learning from outcomes: Evidence-based rankings\",\n                \"Outcome Indistinguishability\",\n                \"Beyond bernoulli: Generating random outcomes that cannot be distinguished from nature\",\n                \"Calibrating noise to sensitivity in private data analysis\",\n                \"Distribution-specific agnostic boosting\",\n                \"Smooth calibration, leaky forecasts, finite recall, and nash dynamics\",\n                \"Asymptotic calibration\",\n                \"Online Multivalid Learning: Means, Moments, and Prediction Intervals\",\n                \"Tracking and improving information in the service of fairness\",\n                \"Omnipredictors\",\n                \"On Calibration of Modern Neural Networks\",\n                \"Multicalibrated Partitions for Importance Weights\",\n                \"Multicalibration: Calibration for the (computationally-identifiable) masses\",\n                \"Equality of Opportunity in Supervised Learning\",\n                \"A new analysis of differential privacy’s generalization guarantees\",\n                \"Moment multicalibration for uncertainty estimation\",\n                \"Deterministic calibration and nash equilibrium\",\n                \"Novel decompositions of proper scoring rules for classification: Score adjustment as precursor to calibration\",\n                \"Multiaccuracy: Black-Box Post-Processing for Fairness in Classification\",\n                \"A complexity-theoretic perspective on fairness\",\n                \"Potential-based agnostic boosting\",\n                \"Universal adaptability: Target-independent inference that competes with propensity scoring\",\n                \"Inherent Trade-Offs in the Fair Determination of Risk Scores\",\n                \"On agnostic boosting and parity learning\",\n                \"Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness\",\n                \"Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration\",\n                \"Fairness Through Computationally-Bounded Awareness\",\n                \"Pairwise independence and derandomization\",\n                \"Boosting using branching programs\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of Low-Degree Multicalibration. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}