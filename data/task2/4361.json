{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: The Lottery Ticket Hypothesis for Self-attention in Convolutional Neural Network♣\n\nPaper Abstract: \n\nPaper Section Text:\nLottery Ticket Hypothesis for Self-attention Networks is proposed in this paper, which is supported by numerical and theoretical evidence. Then, to find a ticket, we propose an effective connection scheme searching method based on policy gradient as a baseline to find a ticket. The self-attention network found by our method can maintain accuracy, reduce parameters and accelerate the inference speed. Besides, we illustrate that the found network has the capacity of capturing the informative features and transferring to other computer vision tasks. [Proof of Theorem 1] Theorem 1. A 1-hidden-layer feed-forward NN is defined as N​N​(x)=W2​σ​(W1​x)𝑁𝑁𝑥superscript𝑊2𝜎superscript𝑊1𝑥NN(x)=W^{2}\\sigma(W^{1}x), where input x∈ℝd×1𝑥superscriptℝ𝑑1x\\in\\mathds{R}^{d\\times 1} with ‖x‖2≤1subscriptnorm𝑥21\\|x\\|_{2}\\leq 1, W1superscript𝑊1W^{1} is of size m×d𝑚𝑑m\\times d, W2superscript𝑊2W^{2} is of size 1×m1𝑚1\\times m and σ𝜎\\sigma is ReLU activation. Wi,j1subscriptsuperscript𝑊1𝑖𝑗W^{1}_{i,j} is initialized i.i.d. by the Gaussian distribution 𝒩​(0,(1m)2)𝒩0superscript1𝑚2\\mathcal{N}(0,{(\\frac{1}{\\sqrt{m}})}^{2}), and W1,j2subscriptsuperscript𝑊21𝑗W^{2}_{1,j} is initialized by the uniform distribution U​n​i​f​o​r​m​{1,−1}𝑈𝑛𝑖𝑓𝑜𝑟𝑚11Uniform\\{1,-1\\}. Let 𝒫​(d−1,ϵ)𝒫𝑑1italic-ϵ\\mathcal{P}(d-1,\\epsilon) be ℙ​{χ2​(d−1)≥ϵ2}ℙsuperscript𝜒2𝑑1superscriptitalic-ϵ2\\mathbb{P}\\{\\chi^{2}(d-1)\\geq\\epsilon^{2}\\}, where χ2​(d−1)superscript𝜒2𝑑1\\chi^{2}(d-1) is a chi-square variable with d−1𝑑1d-1 degree of freedom. Then for any ϵ,δ>0italic-ϵ𝛿0\\epsilon,\\delta>0, when the number of hidden neurons m>ln⁡(δ)ln⁡(𝒫​(d−1,ϵ))𝑚𝛿𝒫𝑑1italic-ϵm>\\frac{\\ln(\\delta)}{\\ln(\\mathcal{P}(d-1,\\epsilon))}, then there exists the row j𝑗j of W1superscript𝑊1W^{1} such that when we set the row j𝑗j to be zero, i.e., Bj​W1subscript𝐵𝑗superscript𝑊1B_{j}W^{1} with Bj=d​i​a​g​{1,⋯,1,0,1,⋯,1}subscript𝐵𝑗𝑑𝑖𝑎𝑔1⋯101⋯1B_{j}=diag\\{1,\\cdots,1,0,1,\\cdots,1\\} (the jthsuperscript𝑗thj^{\\rm th} entry is 0), we have with probability higher than 1−δ1𝛿1-\\delta. Since W1,j2∼U​n​i​f​o​r​m​{−1,1}similar-tosubscriptsuperscript𝑊21𝑗𝑈𝑛𝑖𝑓𝑜𝑟𝑚11W^{2}_{1,j}\\sim Uniform\\{-1,1\\}, we have ‖W2‖2=msubscriptnormsubscript𝑊22𝑚\\|W_{2}\\|_{2}=\\sqrt{m}. We denote the row s𝑠s of W1superscript𝑊1W^{1} as Ws:1subscriptsuperscript𝑊1:𝑠absentW^{1}_{s:}. Let’s consider the following probability, Let 𝒫​(d−1,ϵ)m<ϵ𝒫superscript𝑑1italic-ϵ𝑚italic-ϵ\\mathcal{P}(d-1,\\epsilon)^{m}<\\epsilon, and then we have m>ln⁡(δ)ln⁡(𝒫​(d−1,ϵ))𝑚𝛿𝒫𝑑1italic-ϵm>\\frac{\\ln(\\delta)}{\\ln(\\mathcal{P}(d-1,\\epsilon))}. Therefore, when m>ln⁡(δ)ln⁡(𝒫​(d−1,ϵ))𝑚𝛿𝒫𝑑1italic-ϵm>\\frac{\\ln(\\delta)}{\\ln(\\mathcal{P}(d-1,\\epsilon))}, with probability greater than 1−δ1𝛿1-\\delta, there exists j𝑗j such that ‖Wj:1‖2<ϵmsubscriptnormsubscriptsuperscript𝑊1:𝑗absent2italic-ϵ𝑚\\|W^{1}_{j:}\\|_{2}<\\frac{\\epsilon}{\\sqrt{m}}. Let G=d​i​a​g​{W1​x≥0}𝐺𝑑𝑖𝑎𝑔superscript𝑊1𝑥0G=diag\\{W^{1}x\\geq 0\\}, where the s𝑠s-th diagonal component of G𝐺G is 1 if Ws:1​x≥0subscriptsuperscript𝑊1:𝑠absent𝑥0W^{1}_{s:}x\\geq 0 else 0. Then σ​(W1​x)=G​W1​x𝜎superscript𝑊1𝑥𝐺superscript𝑊1𝑥\\sigma(W^{1}x)=GW^{1}x and σ​(Bj​W1​x)=G​Bj​W1​x𝜎subscript𝐵𝑗superscript𝑊1𝑥𝐺subscript𝐵𝑗superscript𝑊1𝑥\\sigma(B_{j}W^{1}x)=GB_{j}W^{1}x. Finally, we obtain with probability greater than 1−δ1𝛿1-\\delta.\n∎ [Proof of Theorem 2] Theorem 2. Let T​(x)𝑇𝑥T(x) be a Lipschitz continuous and Lebesgue integrable function in d𝑑d-dimensional compact set K𝐾K. And Rfull​(x,θfull)subscript𝑅full𝑥subscript𝜃fullR_{\\text{full}}(x,\\theta_{\\text{full}}) is a ReLU ResNet structure with parameters θfullsubscript𝜃full\\theta_{\\text{full}}. Let ϵ0>0subscriptitalic-ϵ00\\epsilon_{0}>0 be a fixed constant. Suppose that there exists θfull0superscriptsubscript𝜃full0\\theta_{\\text{full}}^{0} such that\n∫K|Rfull​(x,θfull0)−T|​𝑑x≤ϵ02.subscript𝐾subscript𝑅full𝑥superscriptsubscript𝜃full0𝑇differential-d𝑥subscriptitalic-ϵ02\\int_{K}|R_{\\text{full}}(x,\\theta_{\\text{full}}^{0})-T|dx\\leq\\frac{\\epsilon_{0}}{2}.\nIf the width of each layer in Rfull​(x,θfull)subscript𝑅full𝑥subscript𝜃fullR_{\\text{full}}(x,\\theta_{\\text{full}}) is larger than d𝑑d and the depth of Rfull​(x,θfull)subscript𝑅full𝑥subscript𝜃fullR_{\\text{full}}(x,\\theta_{\\text{full}}) is larger than a constant that depends on ϵ0subscriptitalic-ϵ0\\epsilon_{0}, then for any ϵ∈(ϵ0,1)italic-ϵsubscriptitalic-ϵ01\\epsilon\\in(\\epsilon_{0},1), there exists a subnetwork Rsub​(x)subscript𝑅sub𝑥R_{\\text{sub}}(x) of Rfull​(x,θfull)subscript𝑅full𝑥subscript𝜃fullR_{\\text{full}}(x,\\theta_{\\text{full}}) such that  For any d∈ℕ𝑑ℕd\\in\\mathbb{N}, the family of ResNet with one-neuron hidden layers and ReLU activation function can universally approximate any Lebesgue integrable function f𝑓f. In other words, for any ϵ>0italic-ϵ0\\epsilon>0, there is a ResNet R𝑅R with finitely many layers and width not larger than d𝑑d such that See .\n∎ We use the notation dep​(⋅)dep⋅{\\rm dep}(\\cdot) to denote the depth of a network. (Extension strategy) Let T𝑇T be a Lebesgue integrable d𝑑d-dimensional function. For an ϵ>0italic-ϵ0\\epsilon>0, a ReLU ResNet g​(x,θg0)𝑔𝑥superscriptsubscript𝜃𝑔0g(x,\\theta_{g}^{0}) with parameters θg0superscriptsubscript𝜃𝑔0\\theta_{g}^{0} satisfies ∫ℝd|g​(x,θg0)−T|​𝑑x≤ϵsubscriptsuperscriptℝ𝑑𝑔𝑥superscriptsubscript𝜃𝑔0𝑇differential-d𝑥italic-ϵ\\int_{\\mathds{R}^{d}}\\left|g\\left(x,\\theta_{g}^{0}\\right)-T\\right|dx\\leq\\epsilon, then there exists f​(x,θf0)𝑓𝑥superscriptsubscript𝜃𝑓0f(x,\\theta_{f}^{0}) with dep​(f)>dep​(g)dep𝑓dep𝑔{\\rm dep}(f)>{\\rm dep}(g) such that Here f​(x,θf0)𝑓𝑥superscriptsubscript𝜃𝑓0f(x,\\theta_{f}^{0}) is obtained by adding some layers to the last layer of g​(x,θg0)𝑔𝑥superscriptsubscript𝜃𝑔0g(x,\\theta_{g}^{0}). As shown in Fig.  7, we can expand g𝑔g by adding some skip connection layers to its last layer. And then we set all the values of the parameters of the extra layers to zeros. Through the skip connections, we have g​(x,θg0)=f​(x,θf0)𝑔𝑥superscriptsubscript𝜃𝑔0𝑓𝑥superscriptsubscript𝜃𝑓0g(x,\\theta_{g}^{0})=f(x,\\theta_{f}^{0}) and dep​(g)<dep​(f)dep𝑔dep𝑓\\text{dep}(g)<\\text{dep}(f). Therefore, ∎ Let f𝑓f be the Lebesgue integrable function defined on d𝑑d-dimensional compact set K⊂ℝd𝐾superscriptℝ𝑑K\\subset\\mathds{R}^{d}. ∀ϵ>0for-allitalic-ϵ0\\forall\\epsilon>0, according to Lemma 1, there is a ResNet R​(x)𝑅𝑥R(x) with finitely many layers and width not larger than d𝑑d such that ∫K|f​(x)−R​(x)|​𝑑x≤ϵsubscript𝐾𝑓𝑥𝑅𝑥differential-d𝑥italic-ϵ\\int_{K}|f(x)-R(x)|dx\\leq\\epsilon. Then the depth of R​(x)𝑅𝑥R(x) is O​(1/rd)𝑂1superscript𝑟𝑑O(1/r^{d}), where r𝑟r satisfies ωK​(r)≤ϵ/Vol​(K)subscript𝜔𝐾𝑟italic-ϵVol𝐾\\omega_{K}(r)\\leq\\epsilon/\\text{Vol}(K) with ωK​(r)subscript𝜔𝐾𝑟\\omega_{K}(r) defined by Refer to .\n∎ Now, we prove Theorem 2 through the lemmas above. First, T​(x)𝑇𝑥T(x) is a Lipschitz continuous function, so for x,y∈K𝑥𝑦𝐾x,y\\in K, where L𝐿L is a constant. Then we have Let ωK​(r)≤L​r=ϵ/Vol​(K)subscript𝜔𝐾𝑟𝐿𝑟italic-ϵVol𝐾\\omega_{K}(r)\\leq Lr=\\epsilon/\\text{Vol}(K), and then we have When r=ϵ/(Vol​(K)⋅L)𝑟italic-ϵ⋅Vol𝐾𝐿r=\\epsilon/(\\text{Vol}(K)\\cdot L), then for any ϵ∈(ϵ0,1)italic-ϵsubscriptitalic-ϵ01\\epsilon\\in(\\epsilon_{0},1), where C𝐶C is a constant.\nTherefore, according to Lemma 3, ∀ϵ∈(ϵ0,1)for-allitalic-ϵsubscriptitalic-ϵ01\\forall\\epsilon\\in(\\epsilon_{0},1), there exist a ResNet Rshort​(x)subscript𝑅short𝑥R_{\\text{short}}(x) with width not greater than d𝑑d and the depth of at most C​(Lϵ0)d𝐶superscript𝐿subscriptitalic-ϵ0𝑑C(\\frac{L}{\\epsilon_{0}})^{d} such that ∫K|T​(x)−Rshort​(x)|​𝑑x≤ϵ/2subscript𝐾𝑇𝑥subscript𝑅short𝑥differential-d𝑥italic-ϵ2\\int_{K}|T(x)-R_{\\text{short}}(x)|dx\\leq\\epsilon/2. When the depth of Rfull​(x,θfull)subscript𝑅full𝑥subscript𝜃fullR_{\\text{full}}(x,\\theta_{\\text{full}}) is greater than C​(Lϵ0)d𝐶superscript𝐿subscriptitalic-ϵ0𝑑C(\\frac{L}{\\epsilon_{0}})^{d}, we can use Lemma 2 (extension strategy) to construct a function Rlong​(x)subscript𝑅long𝑥R_{\\text{long}}(x) such that and the width of Rlongsubscript𝑅longR_{\\text{long}} is not greater than d𝑑d. Also, for any x∈K𝑥𝐾x\\in K, Rlong​(x)=Rshort​(x)subscript𝑅long𝑥subscript𝑅short𝑥R_{\\text{long}}(x)=R_{\\text{short}}(x). So we have Then Note that dep​(Rlong)=dep​(Rfull)depsubscript𝑅longdepsubscript𝑅full\\text{dep}(R_{\\text{long}})=\\text{dep}(R_{\\text{full}}). Also, Rlongsubscript𝑅longR_{\\text{long}} is a ResNet with width not larger than d𝑑d while the width of Rfullsubscript𝑅fullR_{\\text{full}} is greater than d𝑑d. Therefore, Rlongsubscript𝑅longR_{\\text{long}} is a subnetwork of Rfullsubscript𝑅fullR_{\\text{full}} and satisfies the inequality (20). ∎ [Different types of SAMs]\nIn this part, we review the SAMs used in our paper, i.e., SE [ref]2, SGE [ref]4 and DIA [ref]5. We follow some notations of Section II. Let xℓsubscript𝑥ℓx_{\\ell} be the input of the ℓthsuperscriptℓth\\ell^{\\text{th}} block, fℓ​(⋅)subscript𝑓ℓ⋅f_{\\ell}(\\cdot) be the residual mapping, and M​(⋅;Wℓ)𝑀⋅subscript𝑊ℓM(\\cdot;W_{\\ell}) be the SAM in the ℓthsuperscriptℓth\\ell^{\\text{th}} block with the parameters Wℓsubscript𝑊ℓW_{\\ell}. The attention is formulated as M​(fℓ​(xℓ);Wℓ)𝑀subscript𝑓ℓsubscript𝑥ℓsubscript𝑊ℓM(f_{\\ell}(x_{\\ell});W_{\\ell}). We denote fℓ​(xℓ)subscript𝑓ℓsubscript𝑥ℓf_{\\ell}(x_{\\ell}) as X(ℓ)superscript𝑋ℓX^{(\\ell)} of size C×H×W𝐶𝐻𝑊C\\times H\\times W, where C,H𝐶𝐻C,H and W𝑊W denote channel, height and width, respectively. For simplicity, we denote Xc​h​wℓ=Xℓ​[c,h,w]superscriptsubscript𝑋𝑐ℎ𝑤ℓsuperscript𝑋ℓ𝑐ℎ𝑤X_{chw}^{\\ell}=X^{\\ell}[c,h,w] as the value of pixel (h,w)ℎ𝑤(h,w) at the channel c𝑐c and Xcℓ=Xℓ​[c,:,:]superscriptsubscript𝑋𝑐ℓsuperscript𝑋ℓ𝑐::X_{c}^{\\ell}=X^{\\ell}[c,:,:] as the tensor at the channel c𝑐c. SE Module. SE module utilizes average pooling to extract the features and processes the extracted features by a one-hidden-layer fully connected network. First, the SE module squeezes the information of channels by the average pooling, where c=1,⋯,C𝑐1⋯𝐶c=1,\\cdots,C. Then, an one-hidden-layer fully connected network FC​(⋅;Wℓ)FC⋅subscript𝑊ℓ\\text{FC}(\\cdot;W_{\\ell}) with ReLU activation is used to fuse the information of all the channels and here Wℓsubscript𝑊ℓW_{\\ell} is the parameter. The hidden layer node size is C//rC//r, where “/⁣///” is exact division and “r” denotes reduction rate. The reduction rate is 16 in our experiments. Finally, a sigmoid function (i.e., sig​(z)=1/(1+e−z)sig𝑧11superscript𝑒𝑧\\text{sig}(z)=1/(1+e^{-z})) is applied to the processed features and we get the attention as follows, DIA Module. DIA module integrates the block-wise information by an LSTM (Long Short-Term Memory). Let mcℓsuperscriptsubscript𝑚𝑐ℓm_{c}^{\\ell} be the output of average pooling as Eq.21.\nThen mcℓsuperscriptsubscript𝑚𝑐ℓm_{c}^{\\ell} is passed to LSTM along with a hidden state vector hℓ−1subscriptℎℓ1h_{\\ell-1} and a cell state vector cℓ−1subscript𝑐ℓ1c_{\\ell-1}, where h0subscriptℎ0h_{0} and c0subscript𝑐0c_{0} are initialized as zero vectors. The LSTM generates hℓsubscriptℎℓh_{\\ell} and cℓsubscript𝑐ℓc_{\\ell} at the ℓthsuperscriptℓth\\ell^{\\text{th}} block, i.e., where W𝑊W is the trainable parameter of the LSTM. The hidden state vector htsubscriptℎ𝑡h_{t} is used as attention to recalibrate feature maps. The reduction ratio within LSTM introduced in  [ref]5 is 4 for CIFAR100 or 20 for ImageNet2012. SGE Module. SGE divides the feature maps into different groups and then utilizes the global information from the group to recalibrate its features. Let G be the number of groups and then each group has C//GC//G feature maps. Denote Yℓsuperscript𝑌ℓY^{\\ell} of size (C//G)×H×W(C//G)\\times H\\times W as a group of feature maps within Xℓsuperscript𝑋ℓX^{\\ell}. The extracted feature for the group Yℓsuperscript𝑌ℓY^{\\ell} is Let g𝑔g be [g1ℓ;⋯;gC⁣/⁣/Gℓ]superscriptsubscript𝑔1ℓ⋯superscriptsubscript𝑔𝐶absent𝐺ℓ[g_{1}^{\\ell};\\cdots;g_{C//G}^{\\ell}]. The importance coefficient for each pixel (h,w)ℎ𝑤(h,w) is defined as where ⋅⋅\\cdot is dot product. Then ph​wsubscript𝑝ℎ𝑤p_{hw} is normalized by where the mean μ𝜇\\mu and variance σ2superscript𝜎2\\sigma^{2} are defined by An additional pair of parameters (γ,β)𝛾𝛽(\\gamma,\\beta) are introduced for the group Yℓsuperscript𝑌ℓY^{\\ell} to rescale and shift the normalized features, and SGE modules get the attention for Y​[:,h,w]𝑌:ℎ𝑤Y[:,h,w] as follows, G𝐺G is 4 for CIFAR100 and 64 for ImageNet2012 experiments. [Training details for supernet]\nIn Alg.1, we have presented the training strategy for supernet briefly. We show this process in an intuitive way in Fig. 8. First, for each step t𝑡t, we can sample one connection scheme from a [B​e​r​n​o​u​l​l​i​(β)]msuperscriptdelimited-[]𝐵𝑒𝑟𝑛𝑜𝑢𝑙𝑙𝑖𝛽𝑚[Bernoulli(\\beta)]^{m} distribution. Next, based on this sampled connection scheme, we can obtain a subnetwork from supernet. Then, we train this subnetwork on the training set Dtrainsubscript𝐷trainD_{\\text{train}}. [Training details for Controller]\nIn this part, we provide the training details for the controller. The training process is shown in Fig. 9. The reward function of the connection scheme consists of three parts, i.e., sparsity reward, validation reward, and curiosity bonus. Besides, we supplement some details of Alg. 1. Sparsity Reward gspasubscript𝑔spag_{\\text{spa}}. One of our goals is to accelerate the inference of the Full-SA network. To achieve it, we complement a sparsity reward gspasubscript𝑔spag_{\\text{spa}} to encourage the controller to generate the schemes with fewer connections between SAMs and backbone. We define gspasubscript𝑔spag_{\\text{spa}} by where ∥⋅∥0\\left\\|\\cdot\\right\\|_{0} is a zero norm that counts the number of non-zero entities, and m𝑚m is the number of blocks. Validation Reward gvalsubscript𝑔valg_{\\text{val}}. Another goal is to find the schemes with which the networks can maintain the original accuracy. Hence, we use the validation accuracy of the subnetwork Ω​(𝐱|𝐚)Ωconditional𝐱𝐚\\Omega(\\mathbf{x}|\\mathbf{a}) sampled from the supernet as a reward, which depicts the performance of its structure. The accuracy of Ω​(𝐱|𝐚)Ωconditional𝐱𝐚\\Omega(\\mathbf{x}|\\mathbf{a}) on Dvalsubscript𝐷valD_{\\text{val}} is denoted as gvalsubscript𝑔valg_{\\text{val}}. In fact, it is popular to use validation accuracy of a candidate network as a reward signal in NAS     . Furthermore, it has been empirically proven that the validation performance of the subnetworks sampled from a supernet can be positively correlated to their stand-alone performance . We evaluate the correlation between the validation accuracy of subnetworks sampled from a supernet and their stand-alone performance on CIFAR100 with ResNet and SE module over 42 samples and obtain the Pearson coefficient is 0.71, which again confirms the strong correlation as shown in the previous works. Curiosity Bonus grndsubscript𝑔rndg_{\\text{rnd}}. To encourage the controller to explore more potentially useful connection schemes, we add the Random Network Distillation (RND) curiosity bonus  in our reward.\nTwo extra networks with input 𝐚𝐚\\mathbf{a} are involved in the RND process, including a target network σ1​(⋅)subscript𝜎1⋅\\sigma_{1}(\\cdot) and a predictor network σ2​(⋅;ϕ)subscript𝜎2⋅italic-ϕ\\sigma_{2}(\\cdot;\\phi), where ϕitalic-ϕ\\phi is the parameter set. The parameters of σ1​(⋅)subscript𝜎1⋅\\sigma_{1}(\\cdot) are randomly initialized and fixed after initialization, while σ2​(⋅;ϕ)subscript𝜎2⋅italic-ϕ\\sigma_{2}(\\cdot;\\phi) is trained with the connection schemes collected by the controller. The basic idea of RND is to minimize the difference between the outputs of these two networks, which is denoted by term σϕ​(⋅)=‖σ1​(⋅)−σ2​(⋅;ϕ)‖22subscript𝜎italic-ϕ⋅superscriptsubscriptnormsubscript𝜎1⋅subscript𝜎2⋅italic-ϕ22\\sigma_{\\phi}(\\cdot)=\\left\\|\\sigma_{1}(\\cdot)-\\sigma_{2}(\\cdot;\\phi)\\right\\|_{2}^{2}, over the seen connection schemes.\nIf the controller generates a new scheme 𝐚𝐚\\mathbf{a}, σϕ​(𝐚)subscript𝜎italic-ϕ𝐚\\sigma_{\\phi}(\\mathbf{a}) is expected to be larger because the predictor σ2​(⋅;ϕ)subscript𝜎2⋅italic-ϕ\\sigma_{2}(\\cdot;\\phi) never trains on scheme 𝐚𝐚\\mathbf{a}. Then, we denote the term ‖σ1​(𝐚)−σ2​(𝐚;ϕ)‖22superscriptsubscriptnormsubscript𝜎1𝐚subscript𝜎2𝐚italic-ϕ22\\left\\|\\sigma_{1}(\\mathbf{a})-\\sigma_{2}(\\mathbf{a};\\phi)\\right\\|_{2}^{2} as grndsubscript𝑔rndg_{\\text{rnd}}, which is used as curiosity bonus to reward the controller for exploring a new scheme. Besides, in Fig. 5, we empirically show that RND bonus mitigates the fast convergence of early training iterations, leading to exploration for more schemes. To sum up, our reward G​(𝐚)𝐺𝐚G(\\mathbf{a}) becomes where λ1,λ2,λ3subscript𝜆1subscript𝜆2subscript𝜆3\\lambda_{1},\\lambda_{2},\\lambda_{3} are the coefficients for each reward. Data Reuse. To improve the utilization efficiency of sampled connection schemes and speed up the training of the controller, we incorporate Proximal Policy Optimization (PPO)  in our method. As shown in Alg. 1, after the update of parameter θ𝜃\\theta and ϕitalic-ϕ\\phi, we put the tuple (𝐩θ,𝐚,G​(𝐚))subscript𝐩𝜃𝐚𝐺𝐚(\\mathbf{p}_{\\theta},~{}\\mathbf{a},~{}G(\\mathbf{a})) into a buffer. At the later step, we retrieve some used connection schemes and update θ𝜃\\theta as follows: where η𝜂\\eta is the earning rate and the θo​l​dsubscript𝜃𝑜𝑙𝑑\\theta_{old} denotes the θ𝜃\\theta sampled from buffer. Note that we do not update with PPO until the controller is updated for hℎh times. Finally, we give hyper-parameter settings for training a controller on different datasets. CIFAR100. We optimize the controller for 1000 iterations with momentum SGD. The learning rate is set to be 5×10−2absentsuperscript102\\times 10^{-2}. The time step hℎh to apply PPO is 10. ImageNet2012. We optimize the controller for 300 iterations with momentum SGD. The learning rate is set to be 5×10−2absentsuperscript102\\times 10^{-2}. The time step hℎh to apply PPO is 10. [Training details for Stand-alone Performance]\nIn this part, we introduce the parameter setting for the model trained from scratch. In our experiments, we use cross-entropy loss and optimize the model by SGD with momentum 0.9 and initial learning rate 0.1. The weight decay is set to be 10−4.superscript10410^{-4}. The results for all search methods reported are the best out of three candidates with the highest reward (lowest validation loss for DARTS) in one search. CIFAR100. When ResNet164 is used, the model is trained for 164 epochs with the learning rate dropped by 0.1 at 81, 122 epochs. When ResNet38 is used, the model is trained for 100 epochs with the learning rate following cosine learning rate decay. In order to mitigate the over-fitting problems faced by the deep networks, ResNet164 is trained with random flipping and cropping. ResNet38 is trained with random flipping. ImageNet2012. We use the ResNet50 backbone for ImageNet experiments. The network is trained for 120 epochs with the learning rate dropped by 0.1 at every 30 epochs.\n\nReference markers found: [2]\n\nContext windows for each [ref]:\n[ref]2: is a subnetwork of Rfullsubscript𝑅fullR_{\\text{full}} and satisfies the inequality (20). ∎ [Different types of SAMs]\nIn this part, we review the SAMs used in our paper, i.e., SE [ref]2, SGE [ref]4 and DIA [ref]5. We follow some notations of Section II. Let xℓsubscript𝑥ℓx_{\\ell} be the input of the ℓthsuperscriptℓth\\ell^{\\text{th}} block, fℓ​(⋅)subscript𝑓ℓ⋅f_{\\ell}(\\cdot) be\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 2,\n            \"titles\": [\n                \"CBAM: Convolutional Block Attention Module\",\n                \"Instance enhancement batch normalization: An adaptive regulator of batch noise.\",\n                \"Squeeze-and-Excitation Networks\",\n                \"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks\",\n                \"DIANet: Dense-and-Implicit Attention Network\",\n                \"GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond\",\n                \"Non-local Neural Networks\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Benchmark Analysis of Representative Deep Neural Network Architectures\",\n                \"SRM : A Style-based Recalibration Module for Convolutional Neural Networks\",\n                \"Cap: Context-aware pruning for semantic segmentation\",\n                \"Rethinking the Value of Network Pruning\",\n                \"Pruning Convolutional Neural Networks for Resource Efficient Inference\",\n                \"Multi-objective Evolution for Deep Neural Network Architecture Search\",\n                \"Single Path One-Shot Neural Architecture Search with Uniform Sampling\",\n                \"Gradient Descent Provably Optimizes Over-parameterized Neural Networks\",\n                \"Rethinking the Pruning Criteria for Convolutional Neural Network\",\n                \"Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration\",\n                \"AlphaNet: Improved Training of Supernets with Alpha-Divergence\",\n                \"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet\",\n                \"FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search\",\n                \"Drop-Activation: Implicit Parameter Reduction and Harmonious Regularization\",\n                \"Exploration by Random Network Distillation\",\n                \"Learning multiple layers of features from tiny images\",\n                \"ImageNet Large Scale Visual Recognition Challenge\",\n                \"Efficient Neural Architecture Search via Parameter Sharing\",\n                \"Darts: Differentiable architecture search\",\n                \"Convolution-weight-distribution assumption: Rethinking the criteria of channel pruning\",\n                \"Neural Architecture Search with Reinforcement Learning\",\n                \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\",\n                \"Single-image crowd counting via multi-column convolutional neural network\",\n                \"Scale aggregation network for accurate and efficient crowd counting\",\n                \"CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes\",\n                \"Crowd Counting Using Scale-Aware Attention Networks\",\n                \"The pascal visual object classes (voc) challenge\",\n                \"Mri image synthesis with dual discriminator adversarial learning and difficulty-aware attention mechanism for hippocampal subfields segmentation\",\n                \"A novel adversarial learning strategy for medical image classification\",\n                \"Efficient Crowd Counting via Structured Knowledge Transfer\",\n                \"The pascal visual object classes challenge: A retrospective\",\n                \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]2 based on the context and research area of The Lottery Ticket Hypothesis for Self-attention in Convolutional Neural Network♣. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}