{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Learning Logic Programs by Combining Programs\n\nPaper Abstract: \n\nPaper Section Text:\nILP is a form of rule mining.\nA notable rule mining approach is AMIE+ .\nComparing combo with AMIE+ is difficult.\nAMIE+ adopts an open-world assumption.\nBy contrast, combo adopts the closed-world assumption.\nMoreover, combo can learn programs with relations of arity greater than two, which AMIE+ cannot, i.e. AMIE+ can only use unary and binary relations.\nThis difference is important as AMIE+ cannot be used on most of the datasets in our experiments.\nFor instance, all the IGGP tasks  and many of the program synthesis tasks use relations of arity greater than 2, such next_cell/3, true_cell/3 and does_jump/4 in the iggp-coins task.\nLikewise, many of the program synthesis tasks use append/3 or sum/3.\nFinally, AMIE+ requires facts as input, which can be difficult to provide, especially when learning from infinite domains such as the program synthesis domain.\nBy contrast, combo takes as input a definite program as BK. tilde  is a divide-and-conquer approach.\nProgol  is a separate-and-conquer approach that has inspired many other approaches [ref]38 [ref]2 [ref]39, notably Aleph 1.\nAlthough both approaches can learn programs with many rules and literals, they struggle to learn recursive and optimal programs and cannot perform predicate invention 2. Scalability can be on many dimensions.\nFor instance, many systems, such as QuickFOIL 4, focus on scaling to handle millions of training examples and background facts.\nScaling to millions of examples is not a goal of this work (although we show that combo can handle hundreds of thousands of examples).\nInstead, our goal is to scale to scale to large hypothesis spaces, which is difficult for many existing systems, notably rule selection approaches, outlined in the following paragraph. Many systems formulate the ILP problem as a rule selection problem    .\nThese approaches precompute every possible rule in the hypothesis space and then search for a subset that covers the examples, frequently using ASP to perform the search.\nThe major limitation of precomputation approaches is scalability in terms of (i) the size of rules, and (ii) the number of possible rules.\nAs these approaches precompute every possible rule, they cannot scale to rules with more than a few body literals because the number of rules is exponential in the number of body literals.\nSimilarly, as they perform a combinatorial search over every possible rule, they cannot scale to problems with many possible rules.\nFor instance, Prosynth  and difflog 0 consider at most 1000 and 1267 candidate rules respectively.\nHowever, our simplest experiment (trains1) requires 31,860 candidate rules.\nOur coins-goal experiment requires approximately 1015superscript101510^{15} candidate rules, which is infeasible for these approaches.\nMoreover, our approach differs in many ways.\nWe do not precompute every possible rule, which allows us to learn rules with many body literals.\nIn addition, we only search over promising programs (programs known to cover at least one positive and no negative example), which allows us to scale to problems with many possible rules. Rather than precompute every possible rule, the key idea of Popper is to discover constraints from smaller programs (potentially with multiple rules) to rule out larger programs.\nHowever, Popper struggles to learn large programs with many rules and many literals because it tries to generate a single program that covers all the examples.\nWe differ by (i) only generating non-separable programs in the generate step, and (ii) adding a combine step.\ndcc  combines classical divide-and-conquer search with modern constraint-driven ILP.\ndcc learns a program for each example separately.\nAs these programs are likely to be overly specific, dcc iteratively tries to learn more general programs.\nTo improve performance, dcc reuses knowledge between iterations.\nOur approach is completely different from dcc.\ndcc tries to generate a single program, potentially separable, that covers all the examples and has no combine stage.\nBy contrast, combo never generates a separable program and searches for combinations of programs in the combine stage.\nhopper  extends Popper to learn higher-order programs.\nAlthough our implementation only learns first-order programs, the approach should directly transfer to hopper.\n\nReference markers found: [38]\n\nContext windows for each [ref]:\n[ref]38: domain.\nBy contrast, combo takes as input a definite program as BK. tilde  is a divide-and-conquer approach.\nProgol  is a separate-and-conquer approach that has inspired many other approaches [ref]38 [ref]2 [ref]39, notably Aleph 1.\nAlthough both approaches can learn programs with many rules and literals, they struggle to learn recursive and optimal programs and cannot perform predicate\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Best-Effort Inductive Logic Programming via Fine-grained Cost-based Hypothesis Generation The Inspire System at the Inductive Logic Programming Competition\",\n                \"Explainable pathfinding for inscrutable planners with inductive logic programming\",\n                \"Efficient program synthesis using constraint satisfaction in inductive logic programming\",\n                \"Towards meta-interpretive learning of programming language semantics\",\n                \"Top-down induction of first-order logical decision trees\",\n                \"Grounding compositional hypothesis generation in specific instances\",\n                \"Inductive logic programming in answer set programming\",\n                \"Learning logic programs though divide, constrain, and conquer\",\n                \"Inductive logic programming at 30: A new introduction\",\n                \"Inductive general game playing\",\n                \"Learning programs by learning from failures\",\n                \"Predicate invention by learning from failures\",\n                \"Logical reduction of metarules\",\n                \"Abductive Knowledge Induction From Raw Data\",\n                \"Learning relational representations with auto-encoding logic programs\",\n                \"Learning Explanatory Rules from Noisy Data\",\n                \"Making sense of sensory input\",\n                \"Fast rule mining in ontological knowledge bases with AMIE+\",\n                \"Answer Set Solving in Practice\",\n                \"Clingo = ASP + Control: Preliminary Report\",\n                \"The international general game playing competition\",\n                \"Neuro-Symbolic Hierarchical Rule Induction\",\n                \"Complete bottom-up predicate invention in meta-interpretive learning\",\n                \"Ilp-assisted de novo drug design\",\n                \"Meta-interpretive learning using hex-programs\",\n                \"Inductive inference of VL decision rules\",\n                \"Inductive learning of answer set programs\",\n                \"Foundations of logic programming\",\n                \"Mapping and revising markov logic networks for transfer learning\",\n                \"Inductive logic programming\",\n                \"Inverse entailment and progol\",\n                \"ILP turns 20 - biography and future challenges\",\n                \"Meta-interpretive learning of higher-order dyadic Datalog: predicate invention revisited\",\n                \"Maximum satisfiability using core-guided maxsat resolution\",\n                \"Top Program Construction and Reduction for polynomial time Meta-Interpretive Learning\",\n                \"Automatic Methods of Inductive Inference\",\n                \"Learning higher-order logic programs from failures\",\n                \"Provenance-guided synthesis of datalog programs\",\n                \"Nonmonotonic abductive inductive learning\",\n                \"Synthesizing Datalog Programs Using Numerical Relaxation\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]38 based on the context and research area of Learning Logic Programs by Combining Programs. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}