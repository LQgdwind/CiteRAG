{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Sequential Learning in the Dense Associative Memory\n\nPaper Abstract: \n\nPaper Section Text:\nThe classical Hopfield network is a relatively simple artificial neural network that operates as a model of autoassociative memory . Although earlier autoassociative memories exist    , the Hopfield network is perhaps the most studied in the class. Typically, a network of dimension N𝑁N operates over states that are drawn from a discrete domain, usually the binary ({0,1}Nsuperscript01𝑁\\{0,1\\}^{N}) or bipolar ({−1,1}Nsuperscript11𝑁\\{-1,1\\}^{N}). The network can be operated over a continuous domain, but it has been shown that the interesting behavior of the network is the same whether operating over continuous or discrete domains , and hence the simpler discrete domain is more common. The network usually employs the Hebbian learning rule , or a variation of the Hebbian such as the Widrow-Hof 2 or Storkey  learning rule. The classical Hopfield network has been analyzed extensively from the perspective of statistical physics, where the model is equivalent to that of a Sherrington-Kirkpatrick spin-glass system    . The long range spinglass is a mature model with well studied classes of states, phase transitions, and dynamics which can all be applied nearly directly to the Hopfield network, although statistical physics rarely looks at the equivalent of sequential learning. The Hebbian learning rule has been shown to give rise to a network capacity of only 0.14​N0.14𝑁0.14N states from both the computer science   and physics perspectives  [ref]7. This makes the classical Hopfield network difficult to work with, as the linear network capacity is made impractical by the quadratic weight matrix size N2superscript𝑁2N^{2}. More relevant to our work, there has been significant analysis of the network in the field of psychology thanks to its biological plausibility  [ref]39 9 . Of note is the work on prototype formation in the Hopfield network, which may help explain prototype learning in the human brain 0 1. The Dense Associative Memory abstracts the Hopfield network by effectively allowing variation in the steepness of the energy wells forming the attractor space . This is done by altering a hyperparameter known as the interaction vertex n∈ℝ+𝑛superscriptℝn\\in\\mathbb{R}^{+}. It has been shown that the Dense Associative Memory has the same behavior as the classical network for n=2𝑛2n=2  , meaning the modern network could in theory replace the classical network in all results. However, the modern network has carved its own distinct niche thanks to several other alterations. The shift from a Hebbian based learning rule to a gradient descent has removed a significant portion of the biological plausibility 6, making the network potentially less attractive in the study of psychology and neuroscience. This is compounded by a change from a weight matrix (which can describe the connection strength between a set of N𝑁N neurons) to a set of memory vectors that are isolated from one another. However, as a model of autoassociative memory, the gains to the network capacity are hard to ignore. Specifically, Krotov and Hopfield find a network capacity of: meaning the network capacity is superlinear for n>2𝑛2n>2. Furthermore, the network memories have been observed to have interesting behaviors between low and high interaction vertices. The memories appear to take on feature-like values at low interaction vertices, but transition to prototype-like values at high interaction vertices, indicating the network is learning to model the training data differently as the interaction vertex changes. This is entirely different to the prototype behavior observed in the classical Hopfield network, which emerges from properties of the Hebbian learning rule 1. Despite the steps away from biological plausibility, the Dense Associative Memory has still been used in studies of psychology and neuroscience, with links made to the human brain 3 . The main focus of research around the Dense Associative Memory recently, however, has been on the correspondence to the attention mechanism used in transformers and Large Language Models 0 6. This shows there is an interest in autoassociative memories and the possibility for biologically plausible explanations of network mechanics. Moreover, LLMs and attention based models often undergo a fine-tuning step in which the model learns on a small set of data corresponding to a new, specific task without retraining on the massive amount of general data seen previously. This is very similar to a sequential learning environment, and our work may have consequences for how fine-tuning is applied in the Dense Associative Memory equivalent attention mechanism.\n\nReference markers found: [7]\n\nContext windows for each [ref]:\n[ref]7: of sequential learning. The Hebbian learning rule has been shown to give rise to a network capacity of only 0.14​N0.14𝑁0.14N states from both the computer science   and physics perspectives  [ref]7. This makes the classical Hopfield network difficult to work with, as the linear network capacity is made impractical by the quadratic weight matrix size N2superscript𝑁2N^{2}. More relevant to\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 7,\n            \"titles\": [\n                \"Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without Catastrophic Forgetting\",\n                \"Elastic Weight Consolidation (EWC): Nuts and Bolts\",\n                \"Optuna: A Next-generation Hyperparameter Optimization Framework\",\n                \"Memory Aware Synapses: Learning what (not) to forget\",\n                \"11 - Psychology, Neurobiology and Modeling: The Science of Hebbian Reverberations\",\n                \"Spin-glass models of neural networks\",\n                \"Storing Infinite Numbers of Patterns in a Spin-Glass Model of Neural Networks\",\n                \"Statistical mechanics of neural networks near saturation\",\n                \"The spin-glass phase-transition in the Hopfield model with p-spin interactions\",\n                \"Neural network models of list learning\",\n                \"Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning\",\n                \"Efficient Lifelong Learning with A-GEM\",\n                \"On Tiny Episodic Memories in Continual Learning\",\n                \"Reconstructing creative thoughts: Hopfield neural networks\",\n                \"On a Model of Associative Memory with Huge Storage Capacity\",\n                \"DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion\",\n                \"A ”Thermal” Perceptron Learning Rule\",\n                \"An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks\",\n                \"The organization of behavior; a neuropsychological theory\",\n                \"Introduction To The Theory Of Neural Computation\",\n                \"Neural networks and physical systems with emergent collective computational abilities.\",\n                \"Neurons with Graded Response Have Collective Computational Properties like Those of Two-State Neurons\",\n                \"Brain, neural networks, and computation\",\n                \"Parameter-Efficient Transfer Learning for NLP\",\n                \"Overcoming catastrophic forgetting in neural networks\",\n                \"Infinite-ranged models of spin-glasses\",\n                \"Associative Memory: A System-Theoretical Approach\",\n                \"Correlation Matrix Memories\",\n                \"Learning Multiple Layers of Features from Tiny Images\",\n                \"ImageNet Classification with Deep Convolutional Neural Networks\",\n                \"Dense Associative Memory Is Robust to Adversarial Inputs\",\n                \"Dense Associative Memory for Pattern Recognition\",\n                \"Learning to detect unseen object classes by between-class attribute transfer\",\n                \"MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges\",\n                \"RCV1: A New Benchmark Collection for Text Categorization Research.\",\n                \"Technical Report for ICCV 2021 Challenge SSLAD-Track3B: Transformers Are Better Continual Learners\",\n                \"Gradient Episodic Memory for Continual Learning\",\n                \"The psychology of belief\",\n                \"Networks of spiking neurons can emulate arbitrary Hopfield nets in temporal coding\",\n                \"Improved Robustness and Hyperparameter Selection in Modern Hopfield Networks\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]7 based on the context and research area of Sequential Learning in the Dense Associative Memory. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}