{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Contents\n\nPaper Abstract: \n\nPaper Section Text:\nIn Part I of this thesis, we focus on analyzing, predicting, and retrieving “emerging entities,”\ni.e., entities that are not (yet) part of the KB.\nThis falls into the domain of KB construction (KBC) or (cold start) knowledge base population (KBP), which encompasses the ambitious goal of creating a KB from raw input data (e.g., a collection of documents).\nKBP and KBC span many smaller subtasks, e.g., (new) entity mention detection, entity clustering, relation extraction, and slot filling  . We narrow our focus, and address the analysis, prediction, and representation of emerging entities in three successive chapters.\nMore specifically, in Chapter 3 we study whether there exist common temporal patterns in how entities emerge in online text streams,\nin Chapter 4 we propose a method that leverages EL to generate training data for predicting new mentions of emerging entities in social streams,\nand finally, in Chapter 5 we study a method for improving entity ranking by mining additional entity descriptions from different external sources, e.g., social media and search engine logs. In the first chapter, we analyze entities as they “emerge;” i.e., we study the time series of entity mentions in online text streams in the time span between their first mention (i.e., the entity surfaces) and the moment at which the entity is incorporated into the KB. Previous work on studying the expansion of Wikipedia through the addition of new pages studies the phenomenon from the perspective of Wikipedia itself, e.g., by analyzing how newly created articles fit in Wikipedia’s semantic network, studying the relation between activity on talk pages and the addition of new content to articles, or by studying controversy and disagreement on new content through “edit wars”    . Studying emerging entities has received considerable attention from the natural language processing and information retrieval communities.\nMost notably, different methods and systems for identifying and linking unknown or emerging entities have been proposed \n   .\nMore recently, Färber et al.  formalize and analyze the specific challenges and aspects that come with linking emerging entities, while Reinanda et al.  study the problem of identifying relevant documents for known and emerging as new information comes in. Addressing the task of detecting entities that are not in the KB, i.e., identifying emerging entities, can be traced back to the introduction of the “NIL clustering” subtask in the TAC KBP Entity Linking track in 2011 .\nThe task was a straight-forward EL task: participants were given a reference KB (derived from Wikipedia), a collection of documents (mostly news articles), and offsets for entity mentions that occur in the documents.\nThe task was to decide for each mention whether it refers to an entity that exists in the reference KB (and if yes, which), or whether it refers to a “NIL entity:” an entity that is not in the KB.\nIn the latter case, the system needs to assign a new ID (NIL ID) to the mention, and assign other mentions that refer to the same underlying real-world entity to the same NIL ID (i.e., cluster mentions to NIL IDs).\nSince common EL methods rely on ranking candidate entities to mentions, the inclusion of mentions for out-of-KB entities is typically approached by learning a threshold (for the EL system’s confidence or retrieval score) to determine whether or not the mention refers to an entity already described in the KB.\nThis approach was chosen, e.g., by   . In the TAC KBP Entity Linking track the entity mention detection step, shown to be the bottleneck in state-of-the-art EL systems , was not part of the task.\nHowever, in a realistic scenario, when entity mentions are not given, Wikipedia lexicons and/or lexical matching-based approaches cannot detect unseen entity mentions of new entities, which means different approaches need to be leveraged, e.g., named-entity recognition (NER).\nThe method for detecting emerging entities with ambiguous names, presented by Hoffart et al. , leverages lexical matching methods for detecting entity mentions.\nHowever, in this approach, only ambiguously named emerging entities can be detected, making it complementary to the method we present in Chapter 4, which focuses on detecting new (i.e., unseen) mentions of emerging entities.\nLin et al.  learn to identify entity mentions of absent entities by leveraging Google Books’ n-gram statistics.\nTheir method relies on part-of-speech tagging to identify noun phrases in documents, and deciding for each noun phrase that cannot be linked to a Wikipedia article whether or not it refers to an (absent) entity.\nAnother way to model the task of detecting emerging entities is entity set extraction, where given a seed list of entity mentions, the task is to retrieve more entities that are similar to the seed set .\nHowever, the task differs as it does not rely on a KB, and consequently, the rich structure and metadata that comes with it. The approach we propose in Chapter 4 revolves around automatically generating training data using an EL system.\nAutomatically generating training data for machine learning methods has been a longstanding and recurring theme in machine learning, dating back to the 1980s . The idea of using EL systems for generating pseudo-ground truth, or silver standard training data, is not new either.\nGuo et al.  used an EL system to collect additional context words for entities, to aid the disambiguation step.\nTheir mention detection step relies on the Wikipedia lexicon, meaning their system is unable to detect unseen entity mentions.\nSimilarly, Zhang et al.  apply EL to improve the disambiguation step in EL, by replacing unambiguous mentions in entity linked documents with ambiguous ones to generate artificial but realistic training data.\nZhou et al.  generate training data from Wikipedia to improve the disambiguation step as well.\nMore specifically, they consider Wikipedia links as positive examples, and generate additional negative samples by taking each other entity that may be referred to by the same anchor text. Attempts have been made to simulate human annotations for generating training data for named-entity recognition, too.\nKozareva  uses regular expressions for generating training data for NERC.\nNothman et al.  leverage the anchor text of links in between two same articles in different Wikipedia translations for training a multilingual NERC system.\nWu et al.  investigate generating training material for NERC from one domain and testing on another.\nBecker et al.  study the effects of sampling automatically generated data for training NER. What is new to the method for predicting mentions of emerging entities we propose in Chapter 4, is that it directly optimizes for the task at hand, i.e., we generate training data for a named-entity recognizer to detect a specific type of entity, i.e., entities that are worthy of being added to the KB. A task closely related to EL is entity ranking: in the end, most EL systems approach the disambiguation problem as an entity ranking problem.\nHowever, there is a huge difference between ranking entities for a candidate entity mention, and ranking entities for a user-issued query.\nIn particular, there may be a vocabulary gap between how users refer to entities, and how entities are described in knowledge bases.\nThe task of retrieving the correct entity for user-issued queries is of central importance in the general web search domain , but also in a discovery or exploratory search setting, where analysts may be interested in retrieving entities with partial names, or nicknames. As with many NLP and information extraction tasks, research into entity ranking too has flourished in recent years, largely driven by benchmarking campaigns, such as\nthe INEX Entity Ranking track [ref]57 [ref]58 , and\nthe TREC Entity track  .\nEntity ranking is commonly addressed — much like EL — by exploiting content and structure of knowledge bases, for example by\nincluding anchor texts and inter-entity links ,\ncategory structure  ,\nentity types 5, or\ninternal link structure . More recently, researchers have also started to focus on using “external” information for entity ranking, such as query logs, i.e., users’ past search behavior.\nBillerbeck et al.  use query logs of a web search engine to build click and session graphs, and walk this graph to answer entity queries.\nMottin et al.  use query logs to train an entity ranking system using different sets of features.\nHong et al. 3 follow a similar line of thought and enrich their knowledge base using linked web pages and queries from a query log. The entity ranking task is usually approached in a static setting, in which web pages and queries are added to the knowledge base before any ranking is performed.\nOne of the few initial attempts to bring in time in entity ranking is a position paper by Balog and Nørvåg , who propose temporally-aware entity retrieval, in which temporal information from knowledge bases is required.\nFinally, closing the loop, EL can be applied to queries to improve entity ranking 4.\nEntity ranking is inherently difficult due to the potential mismatch between the entity’s description in a knowledge base and the way people refer to the same entity when searching for it. One way to bridge this gap is to expand (textual) entity representations with terms that people use to refer to entities.\nCollecting terms associated with documents, and adding them to the document (document expansion) is in itself not a novel idea.\nPrevious work has shown that it improves retrieval effectiveness in a variety of retrieval tasks.\nSinghal and Pereira  are one of the first to use document expansion in a (speech) retrieval setting, motivated by the vocabulary mismatch introduced by errors made by automatic speech transcription.\nEver since, using external sources for expanding document representations has been a popular approach to improve retrieval effectiveness. In particular, it was shown that anchors can improve ad-hoc web search    .\nKemp and Ramamohanarao 8 state that document transformation using search history, i.e., adding queries that lead to the document to be clicked, brings documents closer to queries and hence improves retrieval effectiveness.\nSimilarly, Xue et al.  study the use of click-through data by adding queries to clicked document representations.\nIn this case, the click-through data includes a score that is derived from the number of clicks the query yields for a single document.\nGao et al.  follow a similar approach, but add smoothing to click-through data to counter sparsity issues.\nAmitay et al.  study the effectiveness of query reformulations for document expansion by appending all queries in a reformulation session to the top-k𝑘k returned documents for the last query.\nScholer et al.  propose a method to either add additional terms from associated queries to documents or replace the original content with these associated queries, all with the goal of providing more accurate document representations. Looking at other sources for expansion, Bao et al.  improve web search using social annotations (tags).\nThey use the annotations both as additional content and as a popularity measure.\nLee and Croft 7 explore the use of social anchors (i.e., content of social media posts linking to a document) to improve ad hoc search.\nNoll and Meinel  investigate a variety of “metadata” sources, including anchors, social annotations, and search queries.\nThey show that social annotations are concise references to entities and outperform anchors in several retrieval tasks.\nEfron et al.  show that document expansion can be beneficial when searching for very short documents (tweets). As an alternative to expanding entity representations, more recently, advances in neural nets and deep learning have given rise to (continuous) entity representations, i.e., entity embeddings  6 .\nOne of the drawback of these methods is that it is not trivial to employ them in an online, dynamic scenario, where representations may continually change. A common approach to incorporate different document expansions into a single document representation is to create fielded documents  .\nBased on fielded documents, a variety of retrieval methods have been proposed.\nRobertson et al.  introduce BM25F, the fielded version of BM25, which linearly combines query term frequencies over different field types.\nBroder et al.  propose an extension to BM25F, taking into account term dependencies.\nSvore and Burges  use a machine learning approach for learning BM25 over multiple fields, the original document fields (e.g., title and body) and so-called “popularity” fields (e.g., anchors, query-clicks).\nMacdonald et al.  compare the linear combination of term frequencies before computing retrieval scores to directly using retrieval scores in the learning to rank setting and show that it is hard to determine a clear winner.\nPérez-Agüera et al.  note how one of the challenges in structured IR is the fact that field importance differs among collections and that different collections means field importance needs to be optimized per collection.\n\nReference markers found: [57]\n\nContext windows for each [ref]:\n[ref]57: As with many NLP and information extraction tasks, research into entity ranking too has flourished in recent years, largely driven by benchmarking campaigns, such as\nthe INEX Entity Ranking track [ref]57 [ref]58 , and\nthe TREC Entity track  .\nEntity ranking is commonly addressed — much like EL — by exploiting content and structure of knowledge bases, for example by\nincluding anchor texts and\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 57,\n            \"titles\": [\n                \"인공지능 기반 공격 그래프 생성\",\n                \"Oxford dictionary\",\n                \"The learning behind gmail priority inbox\",\n                \"Evolutionary network analysis: A survey\",\n                \"Improving web search ranking by incorporating user behavior information\",\n                \"Oddball: Spotting anomalies in weighted graphs\",\n                \"Queries as anchors: selection by association\",\n                \"Recognition of user intentions for interface agents with variable order markov models\",\n                \"Modeling user search behavior\",\n                \"Topic pages: An alternative to the ten blue links\",\n                \"Finding experts and their details in e-mail corpora\",\n                \"On the use of semantic knowledge bases for temporally-aware entity retrieval\",\n                \"Category-based query modeling for entity search\",\n                \"Overview of the TREC 2009 entity track\",\n                \"Overview of the TREC 2010 entity track\",\n                \"Optimizing web search using social annotations\",\n                \"A critical review of 7 years of mobile device forensics\",\n                \"Finding and reminding: File organization from the desktop\",\n                \"Modelling long term goals\",\n                \"Making sense of microposts (#msm2013) concept extraction challenge\",\n                \"Semantic search on text and knowledge bases\",\n                \"Optimising selective sampling for bootstrapping named entity recognition\",\n                \"Automatic categorization of email into folders: Benchmark experiments on Enron and SRI corpora\",\n                \"Using dynamic time warping to find patterns in time series\",\n                \"Ranking entities using web search query logs\",\n                \"Linked data - the story so far\",\n                \"Making sense of social media streams through semantics: a survey\",\n                \"TwitIE: An open-source information extraction pipeline for microblog text\",\n                \"Learning structured embeddings of knowledge bases\",\n                \"Automatic information extraction\",\n                \"Prospective Memory: Theory And Applications\",\n                \"A taxonomy of web search\",\n                \"Exploiting site-level information to improve web search\",\n                \"Linking archives using document enrichment and term selection\",\n                \"Two-stage named-entity recognition using averaged perceptrons\",\n                \"Using encyclopedic knowledge for named entity disambiguation\",\n                \"ERD 2014: Entity recognition and disambiguation challenge\",\n                \"Rank by time or by relevance?: Revisiting email search\",\n                \"On the collective classification of email “speech acts”\",\n                \"Ranking users for intelligent message addressing\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]57 based on the context and research area of Contents. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}