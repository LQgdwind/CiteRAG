{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Analog fast Fourier transforms for scalable and efficient signal processing\n\nPaper Abstract: \n\nPaper Section Text:\nAnalog in-memory computing systems and digital processors are governed by fundamentally different scaling laws for their energy consumption. The energy associated with digital processing generally scales with the number of arithmetic operations such as multiplies and adds. Analog IMC systems follow a different scaling law, due to an essential characteristic of these architectures: though the matrix computations inside the memory array are extremely efficient, almost all the energy is consumed by peripheral circuits, particularly those involved in sensing and converting analog summed currents to digital outputs of 8-bit or higher resolution     . Therefore, the total required number of conversions by the analog-to-digital converter (ADC) is a useful measure that is expected to scale together with the total energy consumption, independent of the specific memory technology or circuit implementation. Beyond its importance for signal processing, the discrete Fourier transform epitomizes the different ways that the two hardware paradigms can scale. The DFT converts a one-dimensional signal 𝐱𝐱\\mathbf{x} with length N𝑁N to its frequency spectrum 𝐗𝐗\\mathbf{X}: This equation can be explicitly written as a complex-valued MVM: 𝐗=𝐖N​𝐱𝐗subscript𝐖𝑁𝐱\\mathbf{X}=\\mathbf{W}_{N}\\mathbf{x}, where 𝐖Nsubscript𝐖𝑁\\mathbf{W}_{N} is the DFT matrix of size N×N𝑁𝑁N\\times N. The element at the (n𝑛n, k𝑘k) position of this matrix has the value (ωN)n​ksuperscriptsubscript𝜔𝑁𝑛𝑘\\left(\\omega_{N}\\right)^{nk}, where ωN=e−i​2​π/Nsubscript𝜔𝑁superscript𝑒𝑖2𝜋𝑁\\omega_{N}=e^{-i2\\pi/N}. In digital hardware, the DFT is generally computed using an FFT algorithm. The FFT exploits divide-and-conquer techniques to compute the DFT using far fewer operations than by computing the above MVM directly. This fundamentally reduces the DFT’s computational complexity, and hence its energy scaling, from 𝒪​(N2)𝒪superscript𝑁2\\mathcal{O}(N^{2}) to 𝒪​(N​log2​N)𝒪𝑁subscriptlog2𝑁\\mathcal{O}(N\\text{log}_{2}N) . The reduction is possible by exploiting the symmetries of the DFT matrix. For matrices without symmetry or periodicity, such as matrices of deep neural network (DNN) weights, the number of operations is irreducible without the use of approximations. The conventional analog IMC approach to the DFT is to first expand the matrix 𝐖Nsubscript𝐖𝑁\\mathbf{W}_{N} into its real and imaginary parts, program the resulting matrix onto a resistive memory crossbar, then compute the DFT directly as an analog MVM    . When the DFT is computed directly using a single array, the energy scales simply with the number of outputs of a DFT; that is, 𝒪​(N)𝒪𝑁\\mathcal{O}(N). However, this result holds only for small N𝑁N. As N𝑁N becomes large, the matrix 𝐖Nsubscript𝐖𝑁\\mathbf{W}_{N} eventually exceeds the maximum size of a memory array. The upper bound on the physical array size can arise from many factors, including the maximum current supported by the peripheral circuitry , parasitic I​R𝐼𝑅IR voltage drops along the array interconnects , accumulation of memory device conductance errors and noise [ref]33, write disturb due to accumulated leakage currents , and process yield considerations. Extending the direct MVM approach to this regime requires a large DFT matrix to be split across many physical arrays, with each array producing partial results that must be digitized then added, as shown in Fig. 1a. In this case, the number of ADC conversions needed to produce a single DFT output scales as 𝒪​(N)𝒪𝑁\\mathcal{O}(N), leading to an overall energy scaling of 𝒪​(N2)𝒪superscript𝑁2\\mathcal{O}(N^{2}). The area scales either with the total number of ADCs (if peripheral circuits are larger) or the number of elements in the DFT matrix (if the array is larger); in either case, the area scaling is also 𝒪​(N2)𝒪superscript𝑁2\\mathcal{O}(N^{2}). The rapid, quadratic growth of both energy and area with N𝑁N limits the size and nature of problems that can be processed efficiently with this approach using analog hardware. We now show that the FFT algorithm can be implemented on analog IMC hardware, with some special considerations, to inherit its fundamental scalability benefits. We specifically use the mixed-radix formulation of the Cooley-Tukey FFT algorithm , considering a DFT size that is a composite number N=N1×N2𝑁subscript𝑁1subscript𝑁2N=N_{1}\\times N_{2}. The input 𝐱𝐱\\mathbf{x} and output 𝐗𝐗\\mathbf{X}, which are vectors of length N𝑁N, can be reshaped into matrices with dimensions N1×N2subscript𝑁1subscript𝑁2N_{1}\\times N_{2}: we call these 𝐱~~𝐱\\mathbf{\\tilde{x}} and 𝐗~~𝐗\\mathbf{\\tilde{X}}, respectively. The indices of the new matrices are related to those of the original vectors by: x~n1,n2=xn1+N1​n2subscript~𝑥subscript𝑛1subscript𝑛2subscript𝑥subscript𝑛1subscript𝑁1subscript𝑛2\\tilde{x}_{n_{1},n_{2}}=x_{n_{1}+N_{1}n_{2}} and X~k1,k2=XN2​k1+k2subscript~𝑋subscript𝑘1subscript𝑘2subscript𝑋subscript𝑁2subscript𝑘1subscript𝑘2\\tilde{X}_{k_{1},k_{2}}=X_{N_{2}k_{1}+k_{2}}. By exploiting the periodicity of 𝐖Nsubscript𝐖𝑁\\mathbf{W}_{N}, Equation 1 can be re-written as: To explicitly show the mapping onto analog hardware, this equation can be cast in matrix form as: where ⊙direct-product\\odot denotes the element-wise (or Hadamard) product and 𝐓𝐓\\mathbf{T} is an N1×N2subscript𝑁1subscript𝑁2N_{1}\\times N_{2} matrix of twiddle factors defined by 𝐓m​n=(ωN)m​nsubscript𝐓𝑚𝑛superscriptsubscript𝜔𝑁𝑚𝑛\\mathbf{T}_{mn}=\\left(\\omega_{N}\\right)^{mn}. Fig. 1b shows the sequence of steps that implement the analog FFT in Equation 3. The input vector 𝐱𝐱\\mathbf{x} is initially reshaped into the matrix 𝐱~~𝐱\\mathbf{\\tilde{x}}. In the first stage, an N2subscript𝑁2N_{2}-point DFT is computed for every row of the matrix 𝐱~~𝐱\\mathbf{\\tilde{x}} using a sequence of analog MVMs on a resistive memory array; this executes the matrix-matrix multiplication 𝐖N2​𝐱~subscript𝐖subscript𝑁2~𝐱\\mathbf{W}_{N_{2}}\\tilde{\\mathbf{x}}. The results are digitized and multiplied element-wise with the twiddle matrix 𝐓𝐓\\mathbf{T} using digital multipliers. Next, a second stage of N1subscript𝑁1N_{1}-point DFTs is executed to perform the multiplication with 𝐖N1subscript𝐖subscript𝑁1\\mathbf{W}_{N_{1}}, again via a sequence of analog MVMs. Finally, the result 𝐗~~𝐗\\mathbf{\\tilde{X}} is reshaped back into a vector to obtain 𝐗𝐗\\mathbf{X}. The above Cooley-Tukey decomposition enables a large N𝑁N-point DFT, which otherwise needs to be split across many arrays, to be computed with just two arrays that implement an N1subscript𝑁1N_{1}-point analog DFT and an N2subscript𝑁2N_{2}-point analog DFT, respectively. To scale to very large N𝑁N, or to enable small memory arrays to scale to large N𝑁N, the Cooley-Tukey factorization can be applied recursively to further reduce the elementary DFT sizes, or radices, as shown in Fig. 1c. The physical size of the memory array sets the upper bound on the size K𝐾K of an elementary DFT, though a smaller value of K𝐾K may be chosen to optimize accuracy. To support a K𝐾K-point elementary analog DFT within a single array, the array needs to have at least 2​K2𝐾2K rows and 4​K4𝐾4K columns . Fig. 1d compares how the number of required ADC conversions scales between the two analog DFT approaches. For the direct MVM method, we assumed an array size (K=256𝐾256K=256) that corresponds to the largest analog MVM that can be executed by the SONOS array used in this work, and also matches the array size in other large analog IMC demonstrations to date . The array’s dimensions set the critical DFT size where the scaling behavior of the direct MVM method transitions from O​(N)𝑂𝑁O(N) to O​(N2)𝑂superscript𝑁2O(N^{2}). For the analog FFT, we considered both a large array (K=256𝐾256K=256) and a small array (K=16𝐾16K=16). If the Cooley-Tukey decomposition is only applied once, i.e. N≤K2𝑁superscript𝐾2N\\leq K^{2}, the total number of ADC conversions in the analog FFT scales as 𝒪​(N)𝒪𝑁\\mathcal{O}(N), as do the number of digital twiddle multiplications and memory accesses. When the DFT is recursively factorized, all of these operations increase proportionally with the number of Cooley-Tukey decompositions, which scales as 𝒪​(logK​N)𝒪subscriptlog𝐾𝑁\\mathcal{O}(\\text{log}_{K}N), as explained in more detail in Supplementary Section S5. Combining these two trends, the energy of the analog FFT scales as 𝒪​(N​logK​N)𝒪𝑁subscriptlog𝐾𝑁\\mathcal{O}(N\\text{log}_{K}N) in the limit of large N𝑁N. This is superior to the 𝒪​(N2)𝒪superscript𝑁2\\mathcal{O}(N^{2}) scaling of the analog direct MVM approach, leading to orders-of-magnitude lower energy consumption for large DFT sizes. The overall 𝒪​(N​log​N)𝒪𝑁log𝑁\\mathcal{O}(N\\text{log}N) scaling of the analog FFT is similar to that of the digital FFT, but there are essential differences in the algorithmic structure. In digital FFT implementations, recursively decomposing the DFT down to the smallest possible elementary DFTs (i.e. radix-2 or radix-4) optimally reduces the complexity from 𝒪​(N2)𝒪superscript𝑁2\\mathcal{O}(N^{2}) to 𝒪​(N​log2​N)𝒪𝑁subscriptlog2𝑁\\mathcal{O}(N\\text{log}_{2}N) . Meanwhile, the need for recursive decomposition increases the energy complexity of the analog FFT from 𝒪​(N)𝒪𝑁\\mathcal{O}(N) to 𝒪​(N​logK​N)𝒪𝑁subscriptlog𝐾𝑁\\mathcal{O}(N\\text{log}_{K}N). For analog FFTs, it is more optimal from an energy standpoint to minimize the depth of the Cooley-Tukey factorization tree by terminating the decomposition as soon as the factored DFT is small enough to fit onto one memory array. Further factorization would decrease the total number of arithmetic operations, which is helpful for digital systems, but would increase the number of intermediate DFT results and hence the number of ADC conversions, which is harmful for analog systems. This explains why having a larger physical array size is more energy-efficient for large analog FFTs, as shown in Fig. 1d. Mathematically, because of the 𝒪​(N​logK​N)𝒪𝑁subscriptlog𝐾𝑁\\mathcal{O}(N\\text{log}_{K}N) scaling of the analog FFT, increasing the array size (through K𝐾K) logarithmically reduces the power consumption. This weak scaling with array size means that even small arrays (or small MVMs within large arrays) can scale to large FFTs without a significant energy penalty, but there is still an energy benefit for FFT applications to develop memory arrays that can support large yet accurate analog MVMs.\n\nReference markers found: [33]\n\nContext windows for each [ref]:\n[ref]33: including the maximum current supported by the peripheral circuitry , parasitic I​R𝐼𝑅IR voltage drops along the array interconnects , accumulation of memory device conductance errors and noise [ref]33, write disturb due to accumulated leakage currents , and process yield considerations. Extending the direct MVM approach to this regime requires a large DFT matrix to be split across many\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 33,\n            \"titles\": [\n                \"On the accuracy of analog neural network inference accelerators\",\n                \"인공지능 기반 공격 그래프 생성\",\n                \"Range-doppler imaging of rotating objects\",\n                \"SAR imaging of moving targets\",\n                \"Fundamentals of Radar Signal Processing\",\n                \"MIMO radar for advanced driver-assistance systems and autonomous driving: Advantages and challenges\",\n                \"Principles of digital dynamic-range compression\",\n                \"Hearing Aid Signal Processing\",\n                \"OFDM and its wireless applications: A survey\",\n                \"Modulation and Multiple Access for 5G Networks\",\n                \"Generalized frequency division multiplexing for 5th generation cellular networks\",\n                \"Fast Fourier transforms: A tutorial review and a state of the art\",\n                \"A new golden age for computer architecture\",\n                \"Neuromorphic computing using non-volatile memory\",\n                \"Analog architectures for neural network acceleration based on non-volatile memory\",\n                \"Memory devices and applications for in-memory computing\",\n                \"Long short-term memory networks in memristor crossbar arrays\",\n                \"Fully hardware-implemented memristor convolutional neural network\",\n                \"A CMOS-integrated compute-in-memory macro based on resistive random-access memory for AI edge devices\",\n                \"A compute-in-memory chip based on resistive random-access memory\",\n                \"Memristor-based hardware accelerators for artificial intelligence\",\n                \"An analog-AI chip for energy-efficient speech recognition and transcription\",\n                \"A 64-core mixed-signal in-memory compute chip based on phase-change memory for deep neural network inference\",\n                \"Analog matrix processor for edge AI real-time video analytics\",\n                \"Memristor-based discrete Fourier transform for improving performance and energy efficiency\",\n                \"Analogue signal and image processing with large memristor crossbars\",\n                \"Memristor-based analog computation and neural network classification with a dot product engine\",\n                \"Energy-efficient high-fidelity image reconstruction with memristor arrays for medical diagnosis\",\n                \"An algorithm for the machine calculation of complex Fourier series\",\n                \"HERMES-Core — a 1.59-TOPS/mm2 PCM on 14-nm CMOS in-memory compute core using 300-ps/LSB linearized CCO-based ADCs\",\n                \"Compute-in-memory chips for deep learning: Recent trends and prospects\",\n                \"Hardware implementation of memristor-based artificial neural networks\",\n                \"Programming memristor arrays with arbitrarily high precision for analog computing\",\n                \"Analysis and mitigation of parasitic resistance effects for analog in-memory neural network acceleration\",\n                \"Access devices for 3D crosspoint memory\",\n                \"In-memory computing array using 40nm multibit SONOS achieving 100 TOPS/W energy efficiency for deep neural network edge inference accelerators\",\n                \"An accurate, error-tolerant, and energy-efficient neural network inference engine based on SONOS analog memory\",\n                \"Subthreshold operation of SONOS analog memory to enable accurate low-power neural network inference\",\n                \"The sound spectrograph\",\n                \"Collecting image annotations using Amazon’s Mechanical Turk\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]33 based on the context and research area of Analog fast Fourier transforms for scalable and efficient signal processing. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}