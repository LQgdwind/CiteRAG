{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Can AI-Generated Text be Reliably Detected?\n\nPaper Abstract: \n\nPaper Section Text:\nArtificial Intelligence (AI) has made tremendous advances in recent years, from generative models in computer vision   to large language models (LLMs) in natural language processing (NLP)   .\nLLMs can now generate texts of supreme quality with the potential in many applications. For example, the recent model of ChatGPT  can generate human-like texts for various tasks such as writing codes for computer programs, lyrics for songs, completing documents, and question answering; its applications are endless. The trend in NLP shows that these LLMs will even get better with time. However, this comes with a significant challenge in terms of authenticity and regulations. AI tools have the potential to be misused by users for unethical purposes such as plagiarism, generating fake news, spamming, generating fake product reviews, and manipulating web content for social engineering in ways that can have negative impacts on society  . Some news articles rewritten by AI have led to many fundamental errors in them . Hence, there is a need to ensure the responsible use of these generative AI tools. In order to aid this, a lot of recent research focuses on detecting AI-generated texts. Several detection works study this problem as a binary classification problem [ref]3    .\nFor example, OpenAI fine-tunes RoBERTa-based  GPT-2 detector models to distinguish between non-AI generated and GPT-2 generated texts [ref]3. This requires such a detector to be fine-tuned with supervision on each new LLM for reliable detection.\nThese approaches that rely on a neural network for their detection, can be vulnerable to adversarial and poisoning attacks   0 1.\nAnother stream of work focuses on zero-shot AI text detection without any additional training overhead 2 3 4. These works evaluate the expected per-token log probability of texts and perform thresholding to detect AI-generated texts. Mitchell et¬†al.  observe that AI-generated passages tend to lie in negative curvature of log probability of texts. They propose DetectGPT, a zero-shot LLM text detection method, to leverage this observation.\nAnother line of work aims to watermark AI-generated texts to ease their detection 5 6 [ref]1 7. Watermarking eases the detection of LLM output text by imprinting specific patterns on them. Soft watermarking proposed in Kirchenbauer et¬†al. [ref]1 partitions tokens into green and red lists to help create these patterns. A watermarked LLM samples a token, with high probability, from the green list determined by its prefix token. These watermarks are often imperceptible to humans. However, watermarking might not be a useful tool to prevent LLM exploitation unless all the powerful LLMs are protected similarly. Krishna et¬†al. [ref]4 introduces an information retrieval-based detector by storing the outputs of the LLM in a database. For a candidate passage, their algorithm searches this database for semantically similar matches to make their detection robust to paraphrasing. However, storing user-LLM conversations might lead to serious privacy concerns. In this paper, through empirical and theoretical analysis, we show that these state-of-the-art AI-text detectors are unreliable in practical scenarios 8 9 . We study empirical attacks on soft watermarking [ref]1, and a wide range of zero-shot , retrieval-based [ref]4, and neural network-based detectors [ref]3. Paraphrasers: We show that a paraphrasing attack, where a lightweight neural network-based paraphraser is applied to the output text of the AI-generative model, can evade various types of detectors. Before highlighting the results, let us provide an intuition why this attack is successful. For a given sentence sùë†s, suppose P‚Äã(s)ùëÉùë†P(s) is the set of all paraphrased sentences that have similar meanings to the sentence sùë†s. Moreover, let L‚Äã(s)ùêøùë†L(s) be the set of sentences the source LLM can output with meanings similar to sùë†s. Suppose a user has generated sùë†s using an LLM and wants to evade detection. If |L‚Äã(s)|‚â™|P‚Äã(s)|much-less-thanùêøùë†ùëÉùë†|L(s)|\\ll|P(s)|, the user can randomly sample from P‚Äã(s)ùëÉùë†P(s) and avoid detection (if the detection model has a reasonably low false positive rate). Moreover, if |L‚Äã(s)|ùêøùë†|L(s)| is comparable to |P‚Äã(s)|ùëÉùë†|P(s)|, the detector cannot have low false positive and negative rates simultaneously. With this intuition in mind, in ¬ß2, we use light-weight neural network-based paraphrasers (2.3√ó2.3\\times and 5.5√ó5.5\\times smaller than the source LLM in terms of the number of parameters) to rephrase the source LLM‚Äôs output text. Our experiments show that this automated paraphrasing attack can drastically reduce the accuracy of various detectors, including those using soft watermarking. For example, a PEGASUS-based paraphraser  can drop the soft watermarking detector‚Äôs [ref]1 accuracy from 97%percent9797\\% to 80%percent8080\\% with just a degradation of 3.5 in the perplexity score. The area under the receiver operating characteristic (AUROC) curves of zero-shot detectors  drop from 96.5%percent96.596.5\\% to 25.2%percent25.225.2\\% using a T5-based paraphraser . We also observe that the performance of neural network-based trained detectors [ref]3 deteriorate significantly after our paraphrasing attack. For instance, the true positive rate of the RoBERTa-Large-Detector from OpenAI drops from 100%percent100100\\% to 60%percent6060\\% at a realistic low false positive rate of 1%percent11\\%. In addition, we show that retrieval-based detector by Krishna et¬†al. [ref]4 designed to evade paraphrase attacks are vulnerable to recursive paraphrasing. In fact, the accuracy of their detector falls from 100%percent100100\\% to 25%percent2525\\% with our recursive paraphrase attack. We also show that recursive paraphrasing can further deteriorate the performance of watermarking-based [ref]1 detectors (from 99%percent9999\\% to 15%percent1515\\% true positive rate at 1%percent11\\% false positive rate), zero-shot , and neural network-based detectors [ref]3. Impossibility of Detection: In ¬ß3, we present an impossibility result regarding the detection of AI-generated texts.\nAs language models advance, so does their ability to emulate human text. Indeed,\nthe problem of AI-text detection becomes more important and interesting in the presence of language models that are designed to mimic humans and evade detection111Evasion tools: https://goldpenguin.org/blog/make-ai-writing-undetectable/222Undetectable AI: https://undetectable.ai/333StealthGPT: https://www.stealthgpt.ai/ .\nWith new advances in LLMs, the distribution of AI-generated texts becomes increasingly similar to human-generated texts, making them harder to detect.\nThis similarity is reflected in the decreasing total variation distance between the distributions of human and AI-generated text sequences. Adversaries, by seeking to mimic the human-generated text distribution using AI models, implicitly reduce the total variation distance between the two distributions to evade detection. Theorem¬†1 shows that as the total variation between the two distributions decreases, the performance of even the best possible detector deteriorates.\nIt bounds the area under the receiver operating characteristic curve (AUROC) of the best possible detector Dùê∑D as where ùñ≥ùñµ‚Äã(‚Ñ≥,‚Ñã)ùñ≥ùñµ‚Ñ≥‚Ñã\\mathsf{TV}(\\mathcal{M},\\mathcal{H}) is the total variation distance between model-generated text distribution ‚Ñ≥‚Ñ≥\\mathcal{M} and human-generated text distribution ‚Ñã‚Ñã\\mathcal{H}.\nIt shows that as the total variation distance diminishes, the best-possible detection performance approaches 1/2121/2, which represents the AUROC corresponding to a classifier that randomly labels text as AI or human-generated. Our impossibility result does not imply that detection performance will necessarily become as bad as random, but that reliable detection may be unachievable.\nIn most real-world scenarios, a detector is considered good if it can achieve a high true positive rate, say 90%, while maintaining a low false positive rate, say 1%.\nThis is impossible to achieve when the two distributions overlap more than 11% (i.e., total variation <0.89absent0.89<0.89).\nThe aim of this analysis is to urge caution when dealing with detection systems that purport to detect text produced by an AI model.\nAny such system needs to be independently and rigorously evaluated for reliability and bias, ideally on language models designed to evade detection, before deployment in the real world.\n We complement our result with a tightness analysis, where we demonstrate that for a given human distribution ‚Ñã‚Ñã\\mathcal{H}, there exists a distribution ‚Ñ≥‚Ñ≥\\mathcal{M} and a detector Dùê∑D for which the above bound holds with equality.\nIn ¬ß4, we also present a method to empirically estimate the total variation distance and show that it decreases with an increase in model size. Generalizability: Although our analysis considers the text generated by all humans and general language models, it can also be applied to specific scenarios, such as particular writing styles, clever prompt engineering, or sentence paraphrasing, by defining ‚Ñ≥‚Ñ≥\\mathcal{M} and ‚Ñã‚Ñã\\mathcal{H} appropriately.\nOur impossibility result does not make any assumptions on the nature of the distributions, such as ‚Ñã‚Ñã\\mathcal{H} being a monolithic distribution shared by all humans, as claimed by Kirchenbauer et¬†al.  in more recent work.\nThe same result can be derived using ‚Ñã‚Ñã\\mathcal{H} to represent the text distribution produced by an individual human (e.g. Donald Trump or Barack Obama, to borrow the example used in¬†) and ‚Ñ≥‚Ñ≥\\mathcal{M} to represent the output distribution of an AI seeking to mimic the individual‚Äôs writing style.\n‚Ñ≥‚Ñ≥\\mathcal{M} could also represent the text distribution of a general-purpose language model like GPT-4 when prompted in a specific manner, e.g., ‚ÄòWrite a speech in the style of ‚Ä¶‚Äô, to induce a certain writing style.\n‚Ñã‚Ñã\\mathcal{H} and ‚Ñ≥‚Ñ≥\\mathcal{M} could also represent a collection of human distributions and a collection of model distributions (mimicking the human distributions) combined together using appropriate weights based on context. In Section¬†3.3, we extend our impossibility result to include the case where pseudorandom number generators are used in LLMs (e.g., sampling tokens) instead of true randomness.\nThis pseudorandomness could make the AI-generated text distribution very different from the human-generated text distribution¬†.\nThis is because the pseudorandom AI-generated distribution is a collection of Dirac delta function distributions and a human is exorbitantly unlikely to produce a sample corresponding to any of the delta functions.\nThis means that the total variation between the human and pseudorandom AI-generated distributions is almost one, implying that an almost perfect detector could exist in theory.\nHowever, the existence of such a detector does not imply that it could be computed efficiently.\nWe show that, for a computationally bounded detector, the bound in Theorem¬†1 could be modified with a small correction term œµitalic-œµ\\epsilon to account for the presence of pseudorandomness.\nFormally, for a polynomial-time computable detector Dùê∑D, we prove the following bound on its performance: where œµitalic-œµ\\epsilon is a negligible function 1/bt1superscriptùëèùë°1/b^{t} of the number of bits bùëèb used in the seed of the pseudorandom generator for a positive integer tùë°t. Our impossibility result could also be used to show that AI-generated text, even with an embedded watermark, can be made difficult to detect by simply passing it through a paraphrasing tool. For a sequence sùë†s generated by a language model, we set ‚Ñ≥‚Ñ≥\\mathcal{M} and ‚Ñã‚Ñã\\mathcal{H} to be the distributions of sequences of similar meaning to sùë†s produced by the paraphraser and humans. The goal of the paraphraser is to make its output distribution similar to the distribution of human-generated sequences with respect to the total variation distance.\nThe above result puts a constraint on the performance of the detector on the rephrased AI text. Spoofing Attacks: Finally, we discuss the possibility of spoofing attacks on text generative models in ¬ß5. In this setting, an attacker generates a non-AI text that is detected to be AI-generated. An adversary can potentially launch spoofing attacks to produce derogatory texts that are detected to be AI-generated to affect the reputation of the target LLM‚Äôs developers.\nThough the random seed used for generating watermarked text is private [ref]1, we develop an attack that smartly queries the target LLM multiple times to learn its watermarking scheme. An adversarial human can then use this information to compose texts that are detected to be watermarked.\nRetrieval-based detectors [ref]4 can also be easily spoofed by registering paraphrased versions of human essays in their database. An adversarial human with the knowledge of another person‚Äôs essay could prompt an LLM to paraphrase it. The output of the LLM (paraphrased version of the human essay) would be stored in its database. The original human essay will now be detected as AI-generated since retrieval-based detectors can be robust to simple paraphrasing (while they are not robust to recursive paraphrasing). For zero-shot and neural network-based detectors, we find that a na√Øve attack where different human texts are combined leads to them being classified as AI-generated text.\nFigure¬†1 illustrates vulnerabilities of existing AI-text detectors. Identifying AI-generated text is a critical problem to avoid its misuse by users for unethical purposes such as plagiarism, generating fake news and spamming. However, deploying unreliable detectors is not the right solution to tackle this issue as a detector with a high false positive rate will cause more harm than good in society.\nOur results highlight the sensitivities of a wide range of detectors to simple practical attacks such as paraphrasing attacks. More importantly, our results indicate the impossibility of developing reliable detectors in practical scenarios ‚Äî to maintain reliable detection performance, LLMs would have to trade off their performance. We hope that these findings can initiate an honest dialogue within the community concerning the ethical and dependable utilization of AI-generated text.\n\nReference markers found: [3]\n\nContext windows for each [ref]:\n[ref]3: of these generative AI tools. In order to aid this, a lot of recent research focuses on detecting AI-generated texts. Several detection works study this problem as a binary classification problem [ref]3    .\nFor example, OpenAI fine-tunes RoBERTa-based  GPT-2 detector models to distinguish between non-AI generated and GPT-2 generated texts [ref]3. This requires such a detector to be\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 3,\n            \"titles\": [\n                \"Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense\",\n                \"A watermark for large language models\",\n                \"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\",\n                \"Gpt-2: 1.5b release\",\n                \"High-Resolution Image Synthesis with Latent Diffusion Models\",\n                \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\",\n                \"Language Models are Few-Shot Learners\",\n                \"OPT: Open Pre-trained Transformer Language Models\",\n                \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\",\n                \"Chatgpt: Optimizing language models for dialogue\",\n                \"Generating sentiment-preserving fake online reviews using neural language models and their human-and machine-based detection\",\n                \"Deepfake bot submissions to federal public comment websites cannot be distinguished from human submissions\",\n                \"Cnet secretly used ai on articles that didn‚Äôt disclose that fact, staff say\",\n                \"Automatic Detection of Machine Generated Text: A Critical Survey\",\n                \"Real or Fake? Learning to Discriminate Machine from Human Generated Text\",\n                \"Tweepfake: About detecting deepfake tweets. arxiv\",\n                \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\",\n                \"Explaining and Harnessing Adversarial Examples\",\n                \"CUDA: Convolution-based Unlearnable Datasets\",\n                \"Certifying model accuracy under distribution shifts\",\n                \"Improved Certified Defenses against Data Poisoning with (Deterministic) Finite Aggregation\",\n                \"Release strategies and the social impacts of language models\",\n                \"Automatic Detection of Generated Text is Easiest when Humans are Fooled\",\n                \"Gltr: Statistical detection and visualization of generated text\",\n                \"Natural language watermarking: Design, analysis, and a proof-of-concept implementation\",\n                \"Linguistic steganography on twitter: hierarchical language modeling with manual interaction\",\n                \"Protecting Language Generation Models via Invisible Watermarking\",\n                \"Attacking Neural Text Detectors\",\n                \"My ai safety lecture for ut effective altruism\",\n                \"GPT detectors are biased against non-native English writers\",\n                \"Pegasus: Pre-training with extracted gap-sentences for abstractive summarization\",\n                \"Parrot: Paraphrase generation for nlu.\",\n                \"Large Language Models can be Guided to Evade AI-Generated Text Detection\",\n                \"On the Reliability of Watermarks for Large Language Models\",\n                \"Don‚Äôt Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization\",\n                \"Language models are unsupervised multitask learners\",\n                \"Comparison of two pseudo-random number generators\",\n                \"How to generate cryptographically strong sequences of pseudorandom bits\",\n                \"On the Use of ArXiv as a Dataset\",\n                \"GPT-4 Technical Report\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]3 based on the context and research area of Can AI-Generated Text be Reliably Detected?. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}