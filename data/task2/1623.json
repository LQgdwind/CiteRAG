{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Contents\n\nPaper Abstract: \n\nPaper Section Text:\nWith the rise of more complex queries, information needs, search tasks, and challenging domains, the lexical matching-based retrieval models described above may not suffice.\nSemantic Search, or ‚Äúsearch with meaning,‚Äù aims to move beyond the constraints of simple keyword matching, by incorporating additional ‚Äústructure‚Äù in the search process.\nThis can be in terms of semantic matching, e.g., by applying topic modeling methods to project documents and queries in some (lower dimensional) semantic space, where matching can happen without direct lexical overlap between query and document¬†. In this thesis, however, we focus on one particular form of additional ‚Äústructure:‚Äù real-world knowledge.\nBy incorporating knowledge from external sources, e.g., knowledge bases such as Wikipedia, one can enrich text and improve matching¬† .\nBut incorporating real-world knowledge also better supports the exploratory search process that is inherent to E-Discovery¬† 8.\nIn particular, the real-world entities that occur in documents are central to question answering from text (the questions being, e.g., the 5 W‚Äôs)¬†. Given a KB that describes real-world entities and concepts (e.g., Wikipedia), EL addresses the task of identifying and disambiguating occurrences of entities in text.\nOr, more specifically: Matching a textual entity mention, possibly identified by a named entity recognizer, to a knowledge base entry, such as a Wikipedia page that is a canonical entry for that entity¬†. EL is a key component in modern-day applications such as semantic search and advanced search interfaces, and plays a major role in accessing and populating the Web of Data¬†.\nIt can also help to improve NLP tasks¬†,\nor to ‚Äúanchor‚Äù a piece of text in background knowledge; authors or readers may find entity links to supply useful pointers¬†.\nAnother application can be found in search engines, where it is increasingly common to link queries to entities to present entity-specific overviews¬†,\nor to improve ad-hoc retrieval¬† . EL can be traced back to several related tasks that preceded it, such as record linkage¬† in the Database community, and coreference resolution¬† in the NLP community.\nHowever, in its current form EL has received a lot of attention from the IR community, as well as industry, since the Text Analysis Conference (TAC) Knowledge Base Population track introduced an EL task in 2009¬†. The EL task can formally be described as follows:\ngiven an entity mention mùëöm (a term or phrase) occurring in reference document dùëëd, identify the entity eùëíe from a knowledge base K‚ÄãBùêæùêµKB that is the most likely referent of mùëöm.\nIn this thesis, EL plays a central role in Part¬†I (i.e., in Chapters¬†3, 4, and 5).\nEL consists of two distinct steps:\n(i) recognizing mentions of entities in the knowledge base in text (entity mention detection), and subsequently\n(ii) linking them to their referent knowledge base entries (entity disambiguation).\nWe briefly explain the core challenges and methods for both steps. The first step, known as entity mention detection, aims to identify the word sequences that refer to entities (entity mentions).\nOne approach is to apply named-entity recognition (NER).\nNER has a rich history, and has been studied since the 1990s in the Message Understanding Conferences¬†.\nYears later NER gained more traction as part of the CoNLL shared tasks, where winning systems achieved around 90% accuracy of recognizing named-entities in news articles¬†.\nNER methods typically rely on learning patterns in sequences of words, by leveraging the structure of language,\ne.g., the functions of words and their surrounding words (entity mentions are typically proper names¬†),\nor features relating to their surface form (e.g., in many languages capitalized words are more likely to refer to entities). An alternative approach for entity mention detection is so-called lexical matching, or dictionary-based mention detection¬†.\nThis method leverages the rich metadata of a KB such as Wikipedia, by creating a lexicon or dictionary that maps entities (i.e., Wikipedia page IDs) to entity surface forms (i.e., entity mentions).\nThese surface forms are extracted from the rich variety of different ways used to refer to Wikipedia pages.\nFor example, a single entity on Wikipedia can be represented by its:\ntitle,\nanchor texts from hyperlinks on other Wikipedia pages (i.e., the text that is used to hyperlink from one Wikipedia page to another),\nand redirect pages (mostly manually-added links that cover (common) misspellings or alternative names of Wikipedia pages), e.g., the Wikipedia page for Kendrick Lamar has redirect pages such as K-Dot (Kendrick Lamar‚Äôs former stage name),\nand also misspellings, such as Kendrick Lamarr and Kendrick Llama. After extracting all these candidate entity mentions and generating the mappings of surface forms to entities, statistics on their usage can be further leveraged to estimate the probability that a (sequence of) word(s) is a reference to a KB entity (i.e., an entity mention).\nOne commonly used and intuitive method of estimating the probability that an nùëõn-gram is an entity mention is the so-called keyphraseness.\nIt boils down to the fraction of the number of times an nùëõn-gram is used to refer to an entity (i.e., the nùëõn-gram is used as an anchor text in Wikipedia), over the number of times the nùëõn-gram occurs in Wikipedia (including the number of times it is not part of a hyperlink)¬†.\nTo illustrate, the phrase Kendrick appears 5,037 times in Wikipedia articles,111Wikipedia dump dated June 2014 of which it is part of a hyperlink (only) 24 times, resulting in Kendrick‚Äôs keyphraseness score of 245,037=0.0052450370.005\\frac{24}{5,037}=0.005.\nIn contrast, the phrase Kendrick Lamar appears 698 times, of which 501 times as a hyperlink, yielding a keyphraseness score, i.e., the prior probability of the phrase Kendrick Lamar being an entity mention, of 501698=0.7185016980.718\\frac{501}{698}=0.718. Lexical matching-based EL approaches have shown to be successful in the challenging domain of social streams¬†, and have shown to be suitable for adaptation to new genres and languages¬†.\nAs an added advantage, these methods are independent of language-dependent linguistic annotation pipelines, which are prone to cascading errors¬†.\nTheir drawback, however, is that they are unable to identify entity mentions that are not in the lexicon, e.g., misspellings not covered by Wikipedia‚Äôs redirect pages. Given the detection of (candidate) entity mentions, the next step is to assign the referent entities from the KB to the mentions, or alternatively, determine that the mention refers to an entity that is not in the KB.\nAlso known as entity disambiguation, this task is commonly approached by one of the following two approaches:\n(i) local and\n(ii) global entity disambiguation methods. Local methods attempt to resolve a mention (i.e., assign the correct entity to a mention) by only considering properties of a (candidate) entity mention and the candidate KB entities.\nA common local method of disambiguating entity mentions is by employing the commonness-score¬†.\nCommonness is a simple fraction of the number of times the entity mention is used as an anchor for a specific entity eùëíe, over the number of times it is used as an anchor to any entity in the KB¬†.\nContinuing with the previous example, the entity mention ‚ÄúKendrick‚Äù is used as an anchor text of a hyperlink 24 times, of which in 8 cases, the hyperlink points to the Wikipedia page Kendrick, Idaho, i.e., the probability that the mention ‚ÄúKendrick‚Äù refers to Kendrick, Idaho is 824=0.3338240.333\\frac{8}{24}=0.333, whereas the probability that the mention refers to Kendrick_Lamar is 224=0.0832240.083\\frac{2}{24}=0.083. Commonness thus favors the most common target entity for a mention, which makes it work well in practice, particularly in domains that refer to popular entities by nature (e.g., news streams).\nBecause of the emphasis on single occurrences of entity mentions, local EL methods have been shown to be highly effective in domains in which context is limited and/or noisy, e.g., microblog posts¬†.\nAdditional methods have been proposed to expand context when it is limited¬† . There is an increasing interest in approaches that link multiple entities in a document simultaneously¬†.\nThese so-called global methods are based on a notion of ‚Äúcoherence‚Äù in the set of entities in a document.\nThere are several methods for defining the ‚Äúcoherence‚Äù of a set of entities.\nOne is to rely on the structural information of the KB, e.g., the link or category structure¬†, leveraging categories and contexts , or by using graph-based metrics¬†.\nThe TAC Knowledge Base Population (KBP) track has seen many systems that incorporate global approaches to EL¬†    .\nAlso, several publicly available EL frameworks that aim to resolve each entity in a document apply global methods, e.g., ‚ÄòDBpedia Spotlight‚Äô¬† and the ‚ÄòGLOW‚Äô framework [ref]217.\nThe most common global approach uses entity ‚Äúrelatedness‚Äù¬†.\nRelatedness is based on the overlap of the sets of ‚Äúrelated‚Äù entities of a pair of entities.\nRelated entities can be, e.g., all entities that are linked to or from the Wikipedia page that represents the entity of interest. A known challenge of global methods is that entity mentions in the document can themselves be ambiguous, i.e., the number of pairwise comparisons between candidate entities for multiple mentions is exponential in their number.\nThe problem of optimizing the global coherence function is NP-hard .\nA common strategy to deal with this problem is to decrease the search space, e.g., by only considering non-ambiguous entity mentions (i.e., mentions that are only associated with a single target entity)¬†.\nWhile this evidently reduces complexity, the presence of such entities cannot always be assumed.\nAnother approach is to resolve the ambiguous entity mentions to their most popular candidate¬†.\nCucerzan  introduces a method for joint disambiguation that does not consider all combinations of assigning one disambiguation per surface form at a time.\nInstead, all possible disambiguations of a surface form (candidates) are considered simultaneously.\nAnother method is to set a threshold on the number of entities to consider per mention (e.g., at most 20 candidate entities¬†[ref]217).\nRatinov et¬†al. [ref]217 train a local approach to resolve individual entity mentions, before applying the global approach for coherence. The domain of social streams brings additional challenges to the EL task.\nWhereas typical NER approaches perform in the upper 90% range in terms of accuracy on ‚Äúclean,‚Äù grammatically correct and correctly spelled news articles, the accuracy drops considerably when performing NER on misspelled, short, and noisy text, such as social media posts.\nThe noisy character and unreliable capitalization of social streams degrades the effectiveness of NER methods¬†[ref]61  .\nOne approach to addressing this is to tailor the linguistic annotation pipeline to Twitter, which relies heavily on large amounts of training data¬† .\nAnother approach to combat the lack of context, proposed by Cassidy et¬†al. , is to expand a social media post with additional posts of the author, and additional posts found by a clustering algorithm, before optimizing the EL system‚Äôs output for global coherence.\nFinally, incorporating spatiotemporal features¬† has been shown to improve EL effectiveness in social streams.\nThe method we follow in this thesis however, embraces the little context and ignores the notion of global coherence, opting instead for machine learning methods with features that focus on learning mappings between individual nùëõn-gram and entities¬†.\n\nReference markers found: [217]\n\nContext windows for each [ref]:\n[ref]217: global approaches to EL¬†    .\nAlso, several publicly available EL frameworks that aim to resolve each entity in a document apply global methods, e.g., ‚ÄòDBpedia Spotlight‚Äô¬† and the ‚ÄòGLOW‚Äô framework [ref]217.\nThe most common global approach uses entity ‚Äúrelatedness‚Äù¬†.\nRelatedness is based on the overlap of the sets of ‚Äúrelated‚Äù entities of a pair of entities.\nRelated entities can be, e.g., all\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 217,\n            \"titles\": [\n                \"Document Filtering for Long-tail Entities\",\n                \"Oxford dictionary\",\n                \"The learning behind gmail priority inbox\",\n                \"Evolutionary network analysis: A survey\",\n                \"Improving web search ranking by incorporating user behavior information\",\n                \"Oddball: Spotting anomalies in weighted graphs\",\n                \"Queries as anchors: selection by association\",\n                \"Recognition of user intentions for interface agents with variable order markov models\",\n                \"Modeling user search behavior\",\n                \"Topic pages: An alternative to the ten blue links\",\n                \"Finding experts and their details in e-mail corpora\",\n                \"On the use of semantic knowledge bases for temporally-aware entity retrieval\",\n                \"Category-based query modeling for entity search\",\n                \"Overview of the TREC 2009 entity track\",\n                \"Overview of the TREC 2010 entity track\",\n                \"Optimizing web search using social annotations\",\n                \"A critical review of 7 years of mobile device forensics\",\n                \"Finding and reminding: File organization from the desktop\",\n                \"Modelling long term goals\",\n                \"Making sense of microposts (#msm2013) concept extraction challenge\",\n                \"Semantic search on text and knowledge bases\",\n                \"Optimising selective sampling for bootstrapping named entity recognition\",\n                \"Automatic categorization of email into folders: Benchmark experiments on Enron and SRI corpora\",\n                \"Using dynamic time warping to find patterns in time series\",\n                \"Ranking entities using web search query logs\",\n                \"Linked data - the story so far\",\n                \"Making sense of social media streams through semantics: a survey\",\n                \"TwitIE: An open-source information extraction pipeline for microblog text\",\n                \"Learning structured embeddings of knowledge bases\",\n                \"Automatic information extraction\",\n                \"Prospective Memory: Theory And Applications\",\n                \"A taxonomy of web search\",\n                \"Exploiting site-level information to improve web search\",\n                \"Linking archives using document enrichment and term selection\",\n                \"Two-stage named-entity recognition using averaged perceptrons\",\n                \"Using encyclopedic knowledge for named entity disambiguation\",\n                \"ERD 2014: Entity recognition and disambiguation challenge\",\n                \"Rank by time or by relevance?: Revisiting email search\",\n                \"On the collective classification of email ‚Äúspeech acts‚Äù\",\n                \"Ranking users for intelligent message addressing\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]217 based on the context and research area of Contents. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}