{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Towards Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey\n\nPaper Abstract: \n\nPaper Section Text:\nUnlike the analysis of neurons, layers, or modules mentioned earlier, this section introduces works that utilize the entire model architecture to provide explanations. As is shown in Figure 5, we categorize these methods based on an intuitive perspective—the type of explanation output: uni-modal explanations, multi-modal explanations, and interactive explanations.\nThe methods summarized are presented in Table V. Feature Attribution.   These methods, primarily proposed in earlier years, were used as explanation techniques for CNNs or other models. Although they are not directly related to MLLMs, we include them here as background to provide readers with a more comprehensive understanding of the timeline in XAI development. We also hope these methods can inspire new explanation techniques for MLLMs.  [ref]284 [ref]285 explained the decision-making process of models by assigning contribution values to sparse features. LIME  trained simple linear models in local subspaces of the input space, approximating the behavior of complex models in those regions. Experiments show that LIME is effective for both expert and non-expert users, enhancing interpretability across various tasks, such as model comparison, trust assessment, improving unreliable models, and gaining insights into predictions. DeepLIFT [ref]284 propagated the contributions of each neuron in the network back to the input features to decompose the predicted output. It compares each neuron’s activation value with its “reference activation value” to allocate contribution scores based on the differences. SHAP [ref]285 unified six existing methods for explaining model predictions (including LIME and DeepLIFT) by introducing the concept of “additive feature attribution,” assigning contribution values to each feature to elucidate predictions for individual samples. Uni-modal Explanation.   Building on Class Activation Mapping (CAM) introduced in [ref]286, several follow-up methods    , collectively known as the CAM Family, identify critical regions in input images and display them as class activation maps (heatmaps). CAM [ref]286 originally demonstrated object localization in CNNs without bounding box annotations by highlighting important areas. Grad-CAM , a CAM variant, extends this to multiple CNNs architectures without modification. U-CAM  targets VQA tasks, generating visual attention maps through gradient-based estimates. Score-CAM  improved on CAM by using forward scores to compute activation weights, making it gradient-independent and effective in recognition and localization. Lastly, gScore-CAM  enhances CLIP interpretability, using gradient sampling groups to produce reliable attention maps while reducing complexity and avoiding distractions from text in images.   identify the most important image regions for the decision-making process by optimizing an objective function, typically aiming to minimize classification accuracy, and then use heatmaps or saliency maps to highlight significant areas of the image. I-GOS  determines the most important regions in an image by minimizing classification accuracy and employing integrated gradients instead of traditional gradients to calculate the descent direction. The paper  mentions two visualization techniques for CNNs: (1) using activation maximization to illustrate the category concepts captured by the CNN, and (2) calculating saliency maps for specific images and categories through backpropagated gradients.    are based on relevance propagation methods that define the correlation between features and classification outcomes and reverse this correlation back to the input image through various propagation strategies.   discusses two strategies: (1) Taylor decomposition, which linearly approximates the classification function by performing a Taylor expansion around a neutral data point (one that does not belong to any category) to identify each pixel’s contribution; (2) Layer-wise Relevance Propagation (LRP), which propagates the classification relevance from each layer to the previous one.  extends Deep Taylor Decomposition (DTD) to multilayer neural networks, generating heatmaps to assess the importance of individual pixels in classification. Multi-modal Explanation.        offer multimodal interpretability, such as image and text explanations, providing more detailed and systematic insights compared to single-modal methods.  introduces a general transformer explanation framework capable of interpreting (i) self-attention architectures, (ii) hybrid self-attention and cross-attention models, and (iii) encoder-decoder attention designs. DIME  enhances fine-grained interpretability by decoupling information flows of different modalities, clarifying how each modality contributes to the model’s decisions. CCM  improves answer explanations for VQA models with a collaborative correlation module that strengthens the link between answers and explanations and enhances the quality of visual and textual outputs.  integrates textual and visual explanations in a VQA system, presenting answers in a human-like style for better clarity and comprehension. VALE  combines SHAP for identifying influential image regions with SAM and pre-trained vision-language models (VLMs) to generate visual (e.g., heatmaps) and natural language explanations, offering a comprehensive view of the model’s reasoning. Interactive Explanation.   The field of explainable AI has recently grappled with the question of whether the decision-making process of deep neural networks (DNNs) can be interpreted in terms of a set of sparse symbolic concepts. A body of work has explored different types of interactions between a DNN’s input variables as a means of interpreting its inner workings. Sundararajan et al. , Janizek et al. , and Tsai et al.  have each proposed distinct approaches to modeling these input-level interactions. Building on this, Ren et al.  utilized the Harsanyi dividend to represent the AND-type interactions encoded by DNNs. Interestingly, their experimental findings suggest that DNNs tend to rely on a sparse set of such interactions between input variables. Further advancing this line of inquiry, Li et al.  revealed that low-order interactions exhibit higher transferability across diverse input samples in discriminative neural networks. Complementing this, Ren et al.  formally derived the common conditions under which the sparsity of interactions can be guaranteed. Additionally, Ren et al.  introduced a method to learn optimal masked states of input variables based on their interactions, mitigating the bias introduced by sub-optimal masking in Shapley value-based interpretations. Taking a broader view, Chen et al.  extracted common interaction patterns shared across different neural network architectures, suggesting that such generalizable interactions may underpin the networks’ inference mechanisms. Moreover, Cheng et al.  proposed an approach to extract interactions from a DNN’s intermediate layers, shedding light on how these inference patterns are gradually learned and forgotten during the forward propagation process. GANSpace  employed principal component analysis (PCA) to uncover the key directions within the latent space. By selectively perturbing the layers along these primary axes, they achieved a high degree of explainability and control over the generated images. The Harsanyi interaction theory  offers a compelling perspective on the representational capabilities of neural networks, providing insights into their behavior and learning processes. This can be detailed as follows. Wang et al.  established and mathematically verified an inverse relationship between the adversarial transferability of deep neural networks (DNNs) and the interactions present within adversarial perturbations. Ren et al.  highlighted that adversarial attacks predominantly target higher-order interactions rather than lower-order ones. Similarly, Zhou et al.  demonstrated that lower-order interactions exhibit superior generalization properties compared to their higher-order counterparts. Liu et al.  provided an explanation for the observation that DNNs are more adept at learning lower-order interactions than higher-order ones. Deng et al.  identified a surprising bottleneck: neural networks often fail to encode middle-order bivariate interactions effectively. Ren et al.  showed that Bayesian neural networks (BNNs) are less likely than standard neural networks to capture complex Harsanyi interactions. Additionally, Ren et al.  and Zhang et al.  uncovered a two-phase learning dynamic in neural networks’ interaction acquisition, a phenomenon consistently observed across various architectures and tasks. Finally, Deng et al.  unified a range of classical attribution methods by proving that their underlying mechanisms could be reformulated as distinct ways of redistributing interaction effects among input variables. Some explainability methods support interactive explanations, offering flexible, fine-grained analyses based on user preferences, such as focusing on specific image areas or neurons in MLLMs. These systems integrate various modalities, like text, images, and charts, to create comprehensive explanation frameworks. Their flexibility and depth provide users with broader insights, aiding informed decision-making in complex scenarios.    proposed the use of comprehensive explanation systems or agents to provide multimodal insights into model behavior.   combined multiple explainability tools to enhance the understanding of complex neural networks. Specifically,  unified various explainability techniques into a coherent syntax and created richer, more effective user interfaces. These interfaces, especially in visual tasks, help users better grasp the internal workings of neural networks.  introduced MAIA, which integrates and automates a series of explainability tools. MAIA addresses two key challenges: (1) reducing sensitivity to spurious features and (2) automatically detecting potentially misclassified inputs, offering deeper insights into complex neural models. LVLM-Interpret  focused on identifying the critical image patches influencing model outputs. It proposes a novel interactive application that enhances the explainability of these patches, helping users understand the internal mechanics of LVLMs. Together, these works emphasize the importance of combining multiple explainability tools and user interaction to achieve more comprehensive and accessible model explanations. Diffusion Explainer  is an interactive visualization tool for diffusion, designed to explain how stable diffusion transforms text prompts into images. By comparing the image generation results of different text prompts, users can identify how changes in keywords impact the generated images. Others. Additionally, some studies analyze model properties from an architectural perspective. Tran et al. compared recurrent (RNN) and non-recurrent (Transformer) structures in modeling hierarchical information, showing that recurrent structures are advantageous for capturing hierarchy and offering insights for interpretability research. Yang et al. and Ramesh et al. focus on analyzing MLLMs: Yang et al. propose the “Law of Vision Representation,” revealing a strong link between cross-modal alignment, vision consistency, and model performance, while Ramesh et al.  examined various interpretability methods (e.g., attention weights, gradient-based approaches), evaluating their strengths and limitations across multimodal tasks and providing recommendations for improvement.\n\nReference markers found: [284]\n\nContext windows for each [ref]:\n[ref]284: them here as background to provide readers with a more comprehensive understanding of the timeline in XAI development. We also hope these methods can inspire new explanation techniques for MLLMs.  [ref]284 [ref]285 explained the decision-making process of models by assigning contribution values to sparse features. LIME  trained simple linear models in local subspaces of the input space,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 284,\n            \"titles\": [\n                \"A Unified Approach to Interpreting Model Predictions\",\n                \"A Survey of Large Language Models\",\n                \"Deep learning for computer vision: A brief review\",\n                \"A Survey on Multimodal Large Language Models for Autonomous Driving\",\n                \"Efficient Multimodal Large Language Models: A Survey\",\n                \"The (R)Evolution of Multimodal Large Language Models: A Survey\",\n                \"MM-LLMs: Recent Advances in MultiModal Large Language Models\",\n                \"Visual Prompting in Multimodal Large Language Models: A Survey\",\n                \"Large Multimodal Agents: A Survey\",\n                \"Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection\",\n                \"Urbanclip: Learning text-enhanced urban region profiling with contrastive language-image pretraining from the web\",\n                \"Reefknot: A comprehensive benchmark for relation hallucination evaluation, analysis and mitigation in multimodal large language models\",\n                \"A Survey on Multimodal Large Language Models\",\n                \"Multimodal Large Language Models: A Survey\",\n                \"Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook\",\n                \"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multi-modal Large Language Model\",\n                \"Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models\",\n                \"Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality\",\n                \"Unifying Multimodal Transformer for Bi-directional Image and Text Generation\",\n                \"Generating Images with Multimodal Language Models\",\n                \"Instruct-Imagen: Image Generation with Multi-modal Instruction\",\n                \"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models\",\n                \"MMBench: Is Your Multi-modal Model an All-around Player?\",\n                \"MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\",\n                \"MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts\",\n                \"Unveiling the Tapestry of Consistency in Large Vision-Language Models\",\n                \"SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension\",\n                \"SEED-Bench-2: Benchmarking Multimodal Large Language Models\",\n                \"SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension\",\n                \"A Comprehensive Survey on Cross-modal Retrieval\",\n                \"Deep supervised cross-modal retrieval\",\n                \"Probabilistic Embeddings for Cross-Modal Retrieval\",\n                \"Multimodal video sentiment analysis using deep learning approaches, a survey\",\n                \"FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset\",\n                \"End-to-end Generative Pretraining for Multimodal Video Captioning\",\n                \"Videoagent: A memory-augmented multimodal agent for video understanding\",\n                \"Mdmmt: Multidomain multimodal transformer for video retrieval\",\n                \"End-to-End Referring Video Object Segmentation with Multimodal Transformers\",\n                \"UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation\",\n                \"Evolution and prospects of foundation models: From large language models to large multimodal models.\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]284 based on the context and research area of Towards Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}