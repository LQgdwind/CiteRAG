{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Supermasks in Superposition\n\nPaper Abstract: \n\nPaper Section Text:\nIn continual learning, a model aims to solve a number of tasks sequentially   without catastrophic forgetting   .\nAlthough numerous approaches have been proposed in the context of continual learning, there lacks a convention of scenarios in which methods are trained and evaluatedÂ . The key identifiers of scenarios include: 1) whether task identity is provided during training, 2) provided during inference, 3) whether class labels are shared during evaluation,\nand 4) whether the overall task space is discrete or continuous.\nThis results in an exhaustive set of 16 possibilities, many of which are invalid or uninteresting. For example,\nif task identity is never provided in training, providing it in inference is no longer helpful.\nTo that end, we highlight four applicable scenarios, each with a further breakdown of discrete vs. continuous, when applicable, as shown inÂ TableÂ 1. We decompose continual learning scenarios via a three-letter taxonomy that explicitly addresses the three most critical scenario variations.\nThe first two letters specify whether task identity is given during training (ğ–¦ğ–¦\\mathsf{G} if given, ğ–­ğ–­\\mathsf{N} if not) and during inference (ğ–¦ğ–¦\\mathsf{G} if given, ğ–­ğ–­\\mathsf{N} if not).\nThe third letter specifies a subtle but important distinction: whether labels are shared (ğ—Œğ—Œ\\mathsf{s}) across tasks or not (ğ—ğ—\\mathsf{u}).\nIn the unshared case, the model must predict both the correct task ID and the correct class within that task. In the shared case, the model need only predict the correct, shared label across tasks, so it need not represent or predict which task the data came from.\nFor example, when learning 5 permutations of MNIST in the ğ–¦ğ–­ğ–¦ğ–­\\mathsf{GN} scenario (task IDs given during train but not test), a shared label ğ–¦ğ–­ğ—Œğ–¦ğ–­ğ—Œ\\mathsf{GNs} scenario will evaluate the model on the correct predicted label across 10 possibilities, while in the unshared ğ–¦ğ–­ğ—ğ–¦ğ–­ğ—\\mathsf{GNu} case the model must predict across 50 possibilities, a more difficult problem. A full expansion of possibilities entails both ğ–¦ğ–¦ğ—Œğ–¦ğ–¦ğ—Œ\\mathsf{GGs} and ğ–¦ğ–¦ğ—ğ–¦ğ–¦ğ—\\mathsf{GGu}, but as ğ—Œğ—Œ\\mathsf{s} and ğ—ğ—\\mathsf{u} describe only model evaluation, any model capable of predicting shared labels can predict unshared equally well using the provided task ID at test time. Thus these cases are equivalent, and we designate both ğ–¦ğ–¦ğ–¦ğ–¦\\mathsf{GG}.\nMoreover,\nthe ğ–­ğ–­ğ—ğ–­ğ–­ğ—\\mathsf{NNu} scenario is invalid because unseen labels signal the presence of a new task (the â€œlabels trickâ€ in ), making the scenario actually ğ–¦ğ–­ğ—ğ–¦ğ–­ğ—\\mathsf{GNu}, and so we consider only the shared label case ğ–­ğ–­ğ—Œğ–­ğ–­ğ—Œ\\mathsf{NNs}. We leave out the discrete vs. continuous distinction as most research efforts operate within one framework or the other, and the taxonomy applies equivalently to discrete domains with integer â€œTask IDsâ€ as to continue domains with â€œTask Embeddingâ€ or â€œTask Contextâ€ vectors. The remainder of this paper follows the majority of extant literature in focusing on the case with discrete task boundaries (see e.g.  for progress in the continuous scenario).\nEquipped with this taxonomy, we review three existing approaches for continual learning. \n\n\n\nScenario\nDescription\n\n\n\nTask space discreet\n\nor continuous?\n\n\n\n\nExample methods /\n\ntask names used\n\n\n\n\nğ–¦ğ–¦ğ–¦ğ–¦\\mathsf{GG}\nTask Given during train and Given during inference\nEither\n\n\n\nPNNÂ , BatchEÂ [ref]51, PSPÂ , â€œTask learningâ€Â , â€œTask-ILâ€Â \n\n\nğ–¦ğ–­ğ—Œğ–¦ğ–­ğ—Œ\\mathsf{GNs}\nTask Given during train, Not inference; shared labels\nEither\nEWCÂ , SI [ref]54, â€œDomain learningâ€Â , â€œDomain-ILâ€Â \n\nğ–¦ğ–­ğ—ğ–¦ğ–­ğ—\\mathsf{GNu}\nTask Given during train, Not inference; unshared labels\nDiscrete only\nâ€œClass learningâ€Â , â€œClass-ILâ€Â \n\nğ–­ğ–­ğ—Œğ–­ğ–­ğ—Œ\\mathsf{NNs}\nTask Not given during train Nor inference; shared labels\nEither\nBGD, â€œContinuous/discrete task agnostic learningâ€Â \n\n (1) Regularization based methods \nMethods like Elastic Weight Consolidation (EWC)  and Synaptic Intelligence (SI) [ref]54 penalize the movement of parameters that are important for solving previous tasks\nin order to mitigate catastrophic forgetting.\nMeasures of parameter importance vary; e.g. EWC uses the Fisher Information matrix . These methods operate in the ğ–¦ğ–­ğ—Œğ–¦ğ–­ğ—Œ\\mathsf{GNs} scenario (TableÂ 1).\nRegularization approaches ameliorate but do not exactly eliminate catastrophic forgetting. (2) Using exemplars, replay, or generative models \nThese methods aim to explicitly or implicitly (with generative models) capture data from previous tasks. For instance,Â 0 performs classification based on the nearest-mean-of-examplars in a feature space. Additionally,   prevent the model from increasing loss on examples from previous tasks while 1 and 5 respectively use memory buffers and generative models to replay past data.\nExact replay of the entire dataset can trivially eliminate catastrophic forgetting but at great time and memory cost. Generative approaches can reduce catastrophic forgetting, but generators are also susceptible to forgetting. Recently,  successfully mitigate this obstacle by parameterizing a generator with a hypernetwork . (3) Task-specific model components \nInstead of modifying the learning objective or replaying data, various methodsÂ   1 0 2    [ref]51 use different model components for different tasks. In Progressive Neural Networks (PNN), Dynamically Expandable Networks (DEN), and Reinforced Continual Learning (RCL)   , the model is expanded for each new task. More efficiently, 2 fixes the network size and randomly assigns which nodes are active for a given task. In 1 , the weights of disjoint subnetworks are trained for each new task. Instead of learning the weights of the subnetwork, for each new task Mallya et al. 0 learn a binary mask that is applied to a network pretrained on ImageNet.\nRecently, Cheung et al.Â  superimpose many models into one by using different (and nearly orthogonal) contexts for each task. The task parameters can then be effectively retrieved using the correct task context.\nFinally, BatchEÂ [ref]51 learns a shared weight matrix on the first task and learn only a rank-one elementwise scaling matrix for each subsequent task. Our method falls into this final approach (3) as it introduces task-specific supermasks.\nHowever, while all other methods in this category are limited to the ğ–¦ğ–¦ğ–¦ğ–¦\\mathsf{GG} scenario, SupSup can be used to achieve compelling performance in all four scenarios.\nWe compare primarily with BatchEÂ [ref]51 and Parameter Superposition (abbreviated PSP)  as they are recent and performative. BatchE requires very few additional parameters for each new task while achieving comparable performance to PNN and scaling to SplitImagenet. Moreover, PSP outperforms regularization based approaches like SI [ref]54.\nHowever, both BatchE [ref]51 and PSP  require task identity to use task-specific weights, so they can only operate in the ğ–¦ğ–¦ğ–¦ğ–¦\\mathsf{GG} setting.\n\nReference markers found: [51]\n\nContext windows for each [ref]:\n[ref]51: Scenario\nDescription\n\n\n\nTask space discreet\n\nor continuous?\n\n\n\n\nExample methods /\n\ntask names used\n\n\n\n\nğ–¦ğ–¦ğ–¦ğ–¦\\mathsf{GG}\nTask Given during train and Given during inference\nEither\n\n\n\nPNNÂ , BatchEÂ [ref]51, PSPÂ , â€œTask learningâ€Â , â€œTask-ILâ€Â \n\n\nğ–¦ğ–­ğ—Œğ–¦ğ–­ğ—Œ\\mathsf{GNs}\nTask Given during train, Not inference; shared labels\nEither\nEWCÂ , SI [ref]54, â€œDomain learningâ€Â ,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 51,\n            \"titles\": [\n                \"Reinforced Continual Learning\",\n                \"Convex neural networks\",\n                \"Gender shades: Intersectional accuracy disparities in commercial gender classification\",\n                \"Efficient lifelong learning with a-gem\",\n                \"Superposition of many models into one\",\n                \"Imagenet: A large-scale hierarchical image database\",\n                \"Sparse Networks from Scratch: Faster Training without Losing Performance\",\n                \"An algorithm for quadratic programming\",\n                \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\",\n                \"Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs\",\n                \"Catastrophic forgetting in connectionist networks\",\n                \"Continual learning via neural pruning\",\n                \"An empirical investigation of catastrophic forgetting in gradient-based neural networks\",\n                \"Your classifier is secretly an energy based model and you should treat it like one\",\n                \"On Calibration of Modern Neural Networks\",\n                \"Hypernetworks\",\n                \"Momentum Contrast for Unsupervised Visual Representation Learning\",\n                \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks\",\n                \"Neural networks and physical systems with emergent collective computational abilities\",\n                \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\n                \"Adam: A method for stochastic optimization\",\n                \"Overcoming catastrophic forgetting in neural networks\",\n                \"Learning multiple layers of features from tiny images\",\n                \"Backpropagation applied to handwritten zip code recognition\",\n                \"Mnist handwritten digit database\",\n                \"Gradient Episodic Memory for Continual Learning\",\n                \"SGDR: Stochastic Gradient Descent with Warm Restarts\",\n                \"Proving the Lottery Ticket Hypothesis: Pruning is All You Need\",\n                \"Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights\",\n                \"PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\",\n                \"Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization\",\n                \"Catastrophic interference in connectionist networks: The sequential learning problem\",\n                \"Model Cards for Model Reporting\",\n                \"Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science\",\n                \"Revisiting natural gradient for deep networks\",\n                \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\",\n                \"Searching for Activation Functions\",\n                \"Whatâ€™s Hidden in a Randomly Weighted Neural Network?\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]51 based on the context and research area of Supermasks in Superposition. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}