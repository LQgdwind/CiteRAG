{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Model-based Offline Imitation Learning with Non-expert Data\n\nPaper Abstract: \n\nPaper Section Text:\nIn section 4 we discussed how the sample complexity of Algorithm 1 and 2 crucially depends on the uncertainty of Tlsubscriptğ‘‡ğ‘™T_{l} in the states induced by Ï€^^ğœ‹\\hat{\\pi} and Ï€âˆ—superscriptğœ‹\\pi^{*}. Instead of relying on the assumption on Cğ¶C, we decompose the modelâ€™s uncertainty into Uâ€‹(Ï€^)ğ‘ˆ^ğœ‹U(\\hat{\\pi}) and Uâ€‹(Ï€âˆ—)ğ‘ˆsuperscriptğœ‹U(\\pi^{*}), and argue that while Uâ€‹(Ï€âˆ—)ğ‘ˆsuperscriptğœ‹U(\\pi^{*}) is a fundamental property of the dataset, we can attempt to control the value of Uâ€‹(Ï€^)ğ‘ˆ^ğœ‹U(\\hat{\\pi}). In our experiments, we utilise uncertainty quantification techniques[ref]18  [ref]17  to estimate and control Uâ€‹(Ï€^)ğ‘ˆ^ğœ‹U(\\hat{\\pi}). Following works in the model based offline reinforcement learning literature, we train Tlsubscriptğ‘‡ğ‘™T_{l} as an ensemble of Kğ¾K probabilistic neural networks that each outputs a mean and a covariance matrix Î£Î£\\Sigma to estimate the transition from the behavior dataset, and use the discrepancies in their predictions to estimate uncertainty. While there are a myriad of possible techniques for this endeavor, for our main evaluations we use the one presented in [ref]18 and observe that it is sufficient to obtain good results. Specifically, we define the state-action wise estimated uncertainty U^â€‹(s,a):=maxi=1Kâ€‹â€–Î£iâ€‹(s,a)â€–assign^ğ‘ˆğ‘ ğ‘superscriptsubscriptğ‘–1ğ¾normsubscriptÎ£ğ‘–ğ‘ ğ‘\\hat{U}(s,a):=\\max_{i=1}^{K}||\\Sigma_{i}(s,a)|| and the uncertainty penalized discriminator fuâ€‹(s,a)=fâ€‹(s,a)âˆ’Î»â€‹U^â€‹(s,a)subscriptğ‘“ğ‘¢ğ‘ ğ‘ğ‘“ğ‘ ğ‘ğœ†^ğ‘ˆğ‘ ğ‘f_{u}(s,a)=f(s,a)-\\lambda\\hat{U}(s,a)\nwhere the objective of the policy then becomes(with our previously defined f^^ğ‘“\\hat{f}) In our experiments we attempt to answer the following questions: i) In the offline setting, does Algorithm 2 outperform BC in the low data regime? ii) Is Algorithm 2 competitive with Adversarial IL methods that are trained online? iii) To what extent does Uâ€‹(Ï€^)ğ‘ˆ^ğœ‹U(\\hat{\\pi}) affect the algorithmâ€™s performance and can we validate that we can indeed control Uâ€‹(Ï€^)ğ‘ˆ^ğœ‹U(\\hat{\\pi}) with the proposed methods? We answer the above questions with standard benchmark tasks from OpenAi gym with the Mujoco8 simulator. We collect both expert and behavior datasets using SAC[ref]8. All behavior datasets are collected for 1M time steps, after which the collection terminates. We then evaluate our algorithm along with BC and Adversarial IL from section 3. We implement our own Adversarial IL algorithm and practice fair comparison by training it on an identical codebase and hyperparameters(where they apply) as on Algorithm 2, except that it uses a true model Ttsubscriptğ‘‡ğ‘¡T_{t} to sample rollouts instead. BC also shares the same policy architecture with Adversarial IL and Algorithm 2. We first highlight several factors that are expected to affect the magnitude of Uâ€‹(Ï€^)ğ‘ˆ^ğœ‹U(\\hat{\\pi}). They include: \n1) Dataset Size: The number of state action pairs used to train the model\n2) Dataset Coverage: The degree of exploration or the number of policies used to collect the data.\n3) Uncertainty Penalization: This requires uncertainty quantification techniques.\n4) Starting State: As alluded before, the starting state of rollouts directly affects the modelâ€™s uncertainty. \n\nNote that 1) and 2) are both\ndetermined prior running the algorithm. Assuming that 1) and 2) are known, we can use this prior to determine the degree of 3). In order to evaluate the severity of 2), we collect two different behavior datasets to train the model. Dataset pwÏ€Î¼subscriptsuperscriptğ‘superscriptğœ‹ğœ‡ğ‘¤p^{\\pi^{\\mu}}_{w} is collected by a slow learning policy that upon termination, obtains medium level performance, or approximately half the returns of the Ï€âˆ—superscriptğœ‹\\pi^{*}. We deliberately induce slow learning so that the best performing policy in pwÏ€Î¼subscriptsuperscriptğ‘superscriptğœ‹ğœ‡ğ‘¤p^{\\pi^{\\mu}}_{w} is still suboptimal. Dataset pnÏ€Î¼subscriptsuperscriptğ‘superscriptğœ‹ğœ‡ğ‘›p^{\\pi^{\\mu}}_{n} is collected entirely by a random policy. Note that in the evaluated Mujoco Tasks, a random policy admits a much narrower dataset than a learning policy. Main experimental evaluations are shown in Table 1. Performance for Algororithm 2 was evaluated on the Tlsubscriptğ‘‡ğ‘™T_{l} trained with the pwÏ€Î¼subscriptsuperscriptğ‘superscriptğœ‹ğœ‡ğ‘¤p^{\\pi^{\\mu}}_{w}(Algorithm2_w on Table 1) and pnÏ€Î¼subscriptsuperscriptğ‘superscriptğœ‹ğœ‡ğ‘›p^{\\pi^{\\mu}}_{n}(Algorithm2_n on Table 1) dataset. For all tasks, we experiment with varying number of expert trajectories, namely 1,3,10 and 30. When training the model, we utilise only the prescribed expert dataset(s) in addition to the behavior dataset. We observe that Algorithm2_w always outperforms BC with large margin on the 1,3,10 Expert Traj settings, while they both become near optimal when provided 30 expert trajectories. We notice that BC is more susceptible to Mğ‘€M in both directions, and its performance admits a steeper slope as a function of Mğ‘€M. This is confirmatory evidence to our analysis in that the dependency on Mğ‘€M is much lower for Algorithm 2. For Aâ€‹lâ€‹gâ€‹oâ€‹râ€‹iâ€‹tâ€‹hâ€‹mâ€‹2nğ´ğ‘™ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘¡â„ğ‘šsubscript2ğ‘›Algorithm2_{n}, we obtain much lower performance than Aâ€‹lâ€‹gâ€‹oâ€‹râ€‹iâ€‹tâ€‹hâ€‹mâ€‹2wğ´ğ‘™ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘¡â„ğ‘šsubscript2ğ‘¤Algorithm2_{w}, which is expected due to lower coverage of pnÏ€Î¼subscriptsuperscriptğ‘superscriptğœ‹ğœ‡ğ‘›p^{\\pi^{\\mu}}_{n}. However, it still outperforms BC on most settings. We also note that BC is significantly more susceptible to high variance among trials, which could be detrimental for real world settings. We also point out that our implementation of Adversarial IL aligns with the literature      with respect to its performance, and also our analysis in that it is robust to low data regimes. Despite having to use a learned model instead, Algorithm 2 trained on pwÏ€Î¼subscriptsuperscriptğ‘superscriptğœ‹ğœ‡ğ‘¤p^{\\pi^{\\mu}}_{w} is almost as good as Adversarial IL.\n\n\nWe also empirically study the effect of different starting states from section 5. Results are presented in Table 2. We find that the performance heavily relies on where Ï€^^ğœ‹\\hat{\\pi} begin its rollouts on Tlsubscriptğ‘‡ğ‘™T_{l}. During experiments, we find that Tlsubscriptğ‘‡ğ‘™T_{l}â€™s low accuracy on states other than from dTtÏ€Î¼â€‹(s)subscriptsuperscriptğ‘‘superscriptğœ‹ğœ‡subscriptğ‘‡ğ‘¡ğ‘ d^{\\pi^{\\mu}}_{T_{t}}(s) leads to unstable policy learning. This result aligns with our theoretical findings, and it suggests that different starting states could potentially lead to stronger model-based offline RL results. Additional implementation details/evaluations are provided in the Appendix. Table 1: Mean and Std for performance of Algorithm 2 compared to BC and Adversarial IL evaluated on the last iteration, with varying number of provided expert trajectories. Scores are normalized such that a random policy scores 0.00 and the expert scores 1.00. All trials were averaged over different random seeds. For BC, the numbers presented are averaged over 20 different seeds due to a much shorter run time. Adversarial IL and Algorithm 2 were averaged over 3 seeds. For all 1 Expert Traj experiments, different trajectory was used each trial. Table 2: Algorithm2_w evaluated on different rollout starting states, all on the 3 Expert Traj settings. Scores are evaluated using the same procedure as in Table 1.\n\nReference markers found: [18]\n\nContext windows for each [ref]:\n[ref]18: is a fundamental property of the dataset, we can attempt to control the value of Uâ€‹(Ï€^)ğ‘ˆ^ğœ‹U(\\hat{\\pi}). In our experiments, we utilise uncertainty quantification techniques[ref]18  [ref]17  to estimate and control Uâ€‹(Ï€^)ğ‘ˆ^ğœ‹U(\\hat{\\pi}). Following works in the model based offline reinforcement learning literature, we train Tlsubscriptğ‘‡ğ‘™T_{l} as an ensemble of Kğ¾K\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 18,\n            \"titles\": [\n                \"MOPO: Model-based Offline Policy Optimization\",\n                \"Disagreement-regularized imitation learning\",\n                \"Openai gym\",\n                \"Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations\",\n                \"Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization\",\n                \"Learning robust rewards with adversarial inverse reinforce- ment learning\",\n                \"A Divergence Minimization Perspective on Imitation Learning Methods\",\n                \"Imitation Learning as f-Divergence Minimization\",\n                \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\n                \"Generative adversarial imitation learning\",\n                \"State Alignment-based Imitation Learning\",\n                \"Algorithms for inverse reinforcement learning\",\n                \"Efficient training of artificial neural networks for autonomous navigation\",\n                \"Sqil: imitation learning via regularized behavioral cloning\",\n                \"Efficient reductions for imitation learning\",\n                \"State aware imitation learning\",\n                \"Maximum entropy inverse reinforcement learning\",\n                \"COMBO: Conservative Offline Model-Based Policy Optimization\",\n                \"Error bounds of imitating policies and environments\",\n                \"Toward the Fundamental Limits of Imitation Learning\",\n                \"Provably good batch reinforce- ment learning without great exploration\",\n                \"Offline reinforcement learning: Tuto- rial, review, and perspectives on open problems\",\n                \"MOReL: Model- based offline reinforcement learning\",\n                \"When to trust your model: Model-based policy optimization\",\n                \"The importance of pessimism in fixed- dataset policy optimization\",\n                \"Behavioral Cloning from Noisy Demonstrations\",\n                \"Im- itation learning from imperfect demonstration\",\n                \"Mujoco: A physics engine for model-based control\",\n                \"Sample efficient imitation learning for continuous control\",\n                \"Robot learning from failed demonstrations\",\n                \"Provablyefficient imitation learning from observation alone\",\n                \"Generative adversarial nets\",\n                \"A game-theoretic approach to apprenticeship learning\",\n                \"Reinforcement Learning: Theory and Algorithms\",\n                \"Strictly Batch Imitation Learning by Energy-based Distribution Matching\",\n                \"Scalable Bayesian Inverse Reinforcement Learning\",\n                \"Imitation Learning via Off-Policy Distribution Matching\",\n                \"Primal Wassertein Imitation Learning\",\n                \"A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning\",\n                \"Model-based imitation learning by probabilistic trajectory matching\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]18 based on the context and research area of Model-based Offline Imitation Learning with Non-expert Data. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}