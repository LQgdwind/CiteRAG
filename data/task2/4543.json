{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Mixed-Precision Neural Networks: A Survey\n\nPaper Abstract: \n\nPaper Section Text:\nWhat is meant by deterministic quantization is a one-to-one mapping that exists between the \"real\" (hereon used to mean a floating, non-quantized value) and quantized values. One type of deterministic quantization is rounding. Works like ,  and  round the real value into a \"0\" or \"1\", i.e. utilizing binary representation. For example,  used the sign function as follows where q is the quantized value and r is the real value.\nMoreover, [ref]38 compiled the works done on binary neural networks, which utilize rounding techniques. In [ref]21, is a general form of the sâ€‹iâ€‹gâ€‹nğ‘ ğ‘–ğ‘”ğ‘›sign function, where the real value is quantized into multi-levels. To quantize the real value into its closest fixed point,  proposed a rounding scheme as follows. where r, i, f, and Ïµitalic-Ïµ\\epsilon denote the real value, number of integer bits, number of fractional bits, and the smallest number (positive) that one can represent in the fixed-point format in the equation. The real values âˆ‰[i,f]absentğ‘–ğ‘“\\not\\in[i,f] are normalized to either the upper bound or the lower bound of the fixed-point representation. Moreover, â€™s rounding scheme converts the real value into a k-bit integer. Vector quantization is yet another deterministic quantization technique where the real parameters are grouped into clusters and then quantization is applied to each cluster. Several extensions to vector quantization have been proposed as well.    . In [ref]60 the k-means clustering is used to form the weight clusters as follows. where for the weight matrix WâˆˆRuâ€‹xâ€‹vğ‘Šsuperscriptğ‘…ğ‘¢ğ‘¥ğ‘£W\\in R^{uxv}, the scalars wâ€‹eâ€‹iâ€‹gâ€‹hâ€‹tâˆˆR1â€‹xâ€‹uâˆ—vğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡superscriptğ‘…1ğ‘¥ğ‘¢ğ‘£weight\\in R^{1xu*v}. Each element in weight is, then after the clustering, given a cluster index and one can form a codebook with the cluster centers câ€‹eâ€‹nâ€‹tâ€‹râ€‹oâ€‹iâ€‹dğ‘ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘–ğ‘‘centroid which is a 1â€‹xâ€‹w1ğ‘¥ğ‘¤1xw row vector.\nThen, the centroid of each cluster is used to quantize the real weights in that cluster. In addition, deterministic quantization also comprises formulations whereby the quantization is regarded as an optimization problem whose purpose is to find good approximations of the real values like in   and .  formulated the loss function as part of an optimization problem in order to allocate a larger bitwidth for significant weights compared to non-significant weights. Their optimization formulation is as follows where nğ‘›n is the total number of parameters, Ntâ€‹oâ€‹tâ€‹aâ€‹l(.)N_{total}(.) is the total number of bits in the network, Nmâ€‹iâ€‹n(.)N_{min}(.) is the minimum number of bits used to represent a weight and U.B. is utilized an upper bound on the acceptable accuracy lost.  proposed to binarize the weights that do not increase the loss through the following optimization formulation. where Npâ€‹aâ€‹râ€‹aâ€‹mâ€‹ssubscriptğ‘ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ N_{params} and Nlâ€‹aâ€‹yâ€‹eâ€‹râ€‹ssubscriptğ‘ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘ N_{layers} are the total number of parameters in layer and the total number of layers respectively. In addition, codebook quantization, where the real value is quantized into a codebook, is listed as deterministic quantization. It is worth mentioning that the codebook can be either predefined or learned from the data. In , the predefined codebook {âˆ’1,1}11\\{-1,1\\} is used.  uses a 2-element codebook (binarized) but where the codebook values are scaled to some values. Similarly,  uses a predefined ternary codebook {âˆ’1,0,1}101\\{-1,0,1\\} while  uses a scaled ternary codebook. A powers-of-two predefined codebook {â€¦,âˆ’2(âˆ’2),âˆ’2(âˆ’1),âˆ’1,0,+1,+2(âˆ’1),2(âˆ’2),â€¦}â€¦superscript22superscript21101superscript21superscript22â€¦\\{...,-2^{(-2)},-2^{(-1)},-1,0,+1,+2^{(-1)},2^{(-2)},...\\} is used in . Examples of leaned codebooks can be found in  [ref]60 and .\n\nReference markers found: [38]\n\nContext windows for each [ref]:\n[ref]38: and  round the real value into a \"0\" or \"1\", i.e. utilizing binary representation. For example,  used the sign function as follows where q is the quantized value and r is the real value.\nMoreover, [ref]38 compiled the works done on binary neural networks, which utilize rounding techniques. In [ref]21, is a general form of the sâ€‹iâ€‹gâ€‹nğ‘ ğ‘–ğ‘”ğ‘›sign function, where the real value is quantized into\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\",\n                \"Deep neural networks for object detection\",\n                \"DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars\",\n                \"Toward collaborative inferencing of deep neural networks on internet-of-things devices\",\n                \"Cadd-spliceâ€”improving genome-wide variant effect prediction using deep learning-derived splice scores\",\n                \"Enabling Cognitive Smart Cities Using Big Data and Machine Learning: Approaches and Challenges\",\n                \"Importance Estimation for Neural Network Pruning\",\n                \"Pruning Filters for Efficient ConvNets\",\n                \"Learning both Weights and Connections for Efficient Neural Networks\",\n                \"Optimal brain damage\",\n                \"Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning\",\n                \"Exploring the Regularity of Sparse Structure in Convolutional Neural Networks\",\n                \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\",\n                \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\",\n                \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\",\n                \"Squeezenext: Hardware-aware neural network design\",\n                \"Efficient methods and hardware for deep learning\",\n                \"Searching for mobilenetv3\",\n                \"Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search\",\n                \"Distilling the Knowledge in a Neural Network\",\n                \"Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy\",\n                \"Model Compression via Distillation and Quantization\",\n                \"Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion\",\n                \"Experimental determination of precision requirements for back-propagation training of artificial neural networks\",\n                \"LSQ+: Improving low-bit quantization through learnable offsets and better initialization\",\n                \"One Weight Bitwidth to Rule Them All\",\n                \"Binarized neural networks\",\n                \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\",\n                \"XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\",\n                \"Lq-nets: Learned quantization for highly accurate and compact deep neural networks\",\n                \"Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights\",\n                \"Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks\",\n                \"Hawq: Hessian aware quantization of neural networks with mixed-precision\",\n                \"Bit-Mixer: Mixed-precision networks with runtime bit-width selection\",\n                \"Weight discretization paradigm for optical neural networks\",\n                \"ZeroQ: A Novel Zero Shot Quantization Framework\",\n                \"1.1 computingâ€™s energy problem (and what we can do about it)\",\n                \"Rethinking Differentiable Search for Mixed-Precision Neural Networks\",\n                \"Binary neural networks: A survey\",\n                \"Pact: Parameterized clipping activation for quantized neural networks\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]38 based on the context and research area of Mixed-Precision Neural Networks: A Survey. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}