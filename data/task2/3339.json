{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Contents\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section we introduce Mixed Systems and show that they extend and subsume in a unified framework: nondeterminism, probability spaces, and factor graphs. This section is inspired in part byÂ . \nxğ‘¥xyğ‘¦yQğ‘„QCğ¶C(Î©,Ï€)Î©ğœ‹\\left(\\Omega,\\pi\\right)\n The intuition is illustrated on FigureÂ 1, which will guide us for the different notions attached to Mixed Systems. A Mixed System will be a pair, consisting of a probability space (Î©,Ï€)Î©ğœ‹(\\Omega,\\pi) and a state space Qğ‘„Q collecting the configurations of a tuple of state variables (here: xğ‘¥x and yğ‘¦y), related by a relation Cğ¶C. The probability space is â€œprivateâ€, in that it is not directly exposed to any interaction with the environment. Interactions with the environment only occur through the state variables, thus seen as â€œvisibleâ€. This distinction private/visible is shown on FigureÂ 1 by the outgoing pins x,yğ‘¥ğ‘¦x,y, which contrast with the absence of outgoing pin for the probabilistic box. We are interested in understanding how Mixed Systems are â€œexecutedâ€ (we call this the sampling), and how state propertiesâ€”which are not by themselves randomâ€”can still get some kind of probabilistic evaluation. A Mixed System (or system for short) is a tuple S=(Î©,Ï€,X,C)ğ‘†Î©ğœ‹ğ‘‹ğ¶S=(\\Omega,\\pi,X,C), where: (Î©,Ï€)Î©ğœ‹(\\Omega,\\pi) is a probability space; Xğ‘‹X is a finite set of variables with domain Q=âˆxâˆˆXQxğ‘„subscriptproductğ‘¥ğ‘‹subscriptğ‘„ğ‘¥\\begin{array}[]{c}Q=\\prod_{x\\in{X}}Q_{x}\\end{array}; and CâŠ†Î©Ã—Qğ¶Î©ğ‘„C\\subseteq{\\Omega{\\times}{Q}} is a relation. In the sequel, we write to mean (Ï‰,q)âˆˆCğœ”ğ‘ğ¶(\\omega,q){\\in}C. Sğ‘†S is called consistent if Ï€â€‹(Î©ğ–¼)>0ğœ‹superscriptÎ©ğ–¼0\\pi(\\Omega^{\\sf c})>0, where\nÎ©ğ–¼=def{Ï‰âˆˆÎ©âˆ£âˆƒq:Ï‰â€‹Câ€‹q}subscriptdefsuperscriptÎ©ğ–¼conditional-setğœ”Î©:ğ‘ğœ”ğ¶ğ‘\\Omega^{\\sf c}\\,=_{\\rm def}\\,\\{\\omega{\\in}\\Omega\\mid\\exists{q}:\\omega C{q}\\}.\nIf Sğ‘†S is consistent, its sampling is well defined and consists in: sampling Ï‰âˆˆÎ©ğœ”Î©\\omega{\\in}\\Omega according to conditional probability Ï€ğ–¼superscriptğœ‹ğ–¼\\pi^{\\sf c}, where: and, then, nondeterministically selecting qâˆˆQğ‘ğ‘„q{\\in}{Q} such that Ï‰â€‹Câ€‹qğœ”ğ¶ğ‘\\omega C{q}. This two-step procedure is denoted by Sâ€‹â¤³â€‹qğ‘†â¤³ğ‘S\\,{\\leadsto}\\,q. If Sğ‘†S is consistent, its probabilistic semantics is defined as the pair Ï€Â¯,Ï€Â¯:2Qâ†’ [ref]1:Â¯ğœ‹Â¯ğœ‹â†’superscript2ğ‘„01\\overline{\\pi},\\underline{\\pi}:2^{Q}\\rightarrow [ref]1, where, for\nany state property AâŠ†Qğ´ğ‘„A\\subseteq{Q}: The following generalized likelihood â„“:2Qâ†’ [ref]1:â„“â†’superscript2ğ‘„01\\ell:2^{Q}\\rightarrow [ref]1 is also of interest: In the sequel, we shall denote by ğ•Šâ€‹(X)ğ•Šğ‘‹\\mathbb{S}(X) the class of all (possibly inconsistent) Mixed Systems having Xğ‘‹X as their set of variables. â–¡â–¡\\Box Ï€Â¯Â¯ğœ‹\\overline{\\pi} defined by formula (25) is not a probability on Qğ‘„Q, but only an outer probability111sometimes called also exterior or upper probability., i.e., a function Ï€Â¯:2Qâ†’ [ref]1:Â¯ğœ‹â†’superscript2ğ‘„01\\overline{\\pi}:2^{Q}\\rightarrow [ref]1 such that Ï€Â¯â€‹(âˆ…)=0Â¯ğœ‹0\\overline{\\pi}(\\emptyset)=0, Ï€Â¯â€‹(Q)=1Â¯ğœ‹ğ‘„1\\overline{\\pi}(Q)=1, and Ï€Â¯Â¯ğœ‹\\overline{\\pi} is sub-additive, meaning that it satisfies Note that (28) resembles (25) if we rewrite the latter as Ï€Â¯â€‹(A)=âˆ‘Ï‰âˆˆÎ©âˆƒAÏ€ğ–¼â€‹(Ï‰)Â¯ğœ‹ğ´subscriptğœ”subscriptÎ©ğ´superscriptğœ‹ğ–¼ğœ”\\overline{\\pi}(A)=\\sum_{\\omega\\in{\\Omega_{{\\exists}A}}}\\pi^{\\sf c}(\\omega). The same comments hold, mutatis mutandis, regarding the inner probability Ï€Â¯Â¯ğœ‹\\underline{\\pi}, which is super-additive. Note that, [Specializing to pure nondeterministic systems]\n A pure nondeterministic system is specified as a subset C^âŠ†Q^ğ¶ğ‘„\\widehat{C}\\subseteq{Q} of the state space. To reformulate it as a Mixed System, simply take (Î©,Ï€)Î©ğœ‹(\\Omega,\\pi) trivial, i.e., Î©={Ï‰}Î©ğœ”\\Omega=\\{\\omega\\}, a singleton, equipped with the trivial probability such that Ï€â€‹(Ï‰)=1ğœ‹ğœ”1\\pi(\\omega)=1, and define Ï‰â€‹Câ€‹qğœ”ğ¶ğ‘\\omega C{q} iff qâˆˆC^ğ‘^ğ¶q\\in\\widehat{C}. â–¡â–¡\\Box [Specializing to pure probabilistic systems]\n A pure probabilistic system is specified as a pair (Î©,Ï€)Î©ğœ‹(\\Omega,\\pi). To reformulate as a Mixed System, take Q=Î©ğ‘„Î©Q=\\Omega, and let Cğ¶C be the diagonal of Î©Ã—QÎ©ğ‘„\\Omega\\times{Q}; finally, let xğ‘¥x be the variable with domain Qğ‘„Q. â–¡â–¡\\Box To capture the blending of nondeterminism and probability, outer probabilities are directly used in the Dempster-Shafer theory of evidenceÂ   .222Outer and inner probabilities were called upper and lower in Â . Outer probabilities do not support key limit theorems for use in statistics, such as the law of large numbers, the central limit theorem, and more. Hence, whereas the theory of evidence comes with reasoning capabilities, it does not directly support learning or estimation. In formal methods for probabilistic systems (in the context of imperative programming), the blending of probability and nondeterminism was addressed by a number of authors, see, e.g.,Â       [ref]21 . Nondeterministic choice between alternatives is considered inÂ  and written PâŠ“Pâ€²square-intersectionğ‘ƒsuperscriptğ‘ƒâ€²P\\,{\\sqcap}\\,P^{\\prime}, whereas probabilistic choice is specified as PâŠ•aPâ€²P\\,{{}_{a}\\oplus}\\,P^{\\prime} (Pğ‘ƒP is selected with probability ağ‘a and Pâ€²superscriptğ‘ƒâ€²P^{\\prime} with probability 1âˆ’a1ğ‘1{-}a) or PâŠ•baPâ€²P\\,{{}_{a}\\oplus_{b}}\\,P^{\\prime} (Pğ‘ƒP is selected with probability at least ağ‘a and Pâ€²superscriptğ‘ƒâ€²P^{\\prime} with probability at least bğ‘b). The evaluation of formulas must specify how nondeterminism interplays with probabilities. A comprehensive approach was proposed inÂ [ref]21, where demonic and angelic nondeterminisms are seen as adversarial and beneficial, respectively. These notions mirror the outer and inner probabilities used in Dempster theory. Unfortunately, outer and inner probabilities do not bring limit theorems of probability theory (law of large numbers, etc.), which are the core of machine learning. Through formulas (25,26) in DefinitionÂ 1, the probabilistic semantics of Mixed Systems is defined as the associated outer and inner probabilities. Hence, Mixed Systems offer the calculus of the theory of evidence, and mirror the demonic and angelic types of nondeterminism.\nOn the other hand, since classical probability spaces are first class citizens of the model of Mixed Systems, this model also preserves the apparatus needed for machine learning. In AppendixÂ A.1, we develop a more detailed comparison of the semantics of Mixed Systems versus imperative probabilistic programming with demonic and angelic nondeterminism, followingÂ [ref]21. Finally, the generalized likelihood of formula (28) is the basis for inference, estimation, or machine learning, when multiple hypotheses or nuisance parameters are consideredÂ â€”we are not aware of any use of a mirror notion where â€œminâ€ would be substituted for â€œmaxâ€. â–¡â–¡\\Box [outer probabilities]\n Consider model S1â€‹â€–S2â€–â€‹S3subscriptğ‘†1normsubscriptğ‘†2subscriptğ‘†3S_{1}\\|S_{2}\\|S_{3} of SectionÂ 1.2. Pick an instant nğ‘›n and let S=defSâ€‹(n,xnâˆ’1,fnâˆ’1)subscriptdefğ‘†ğ‘†ğ‘›subscriptğ‘¥ğ‘›1subscriptğ‘“ğ‘›1S\\,=_{\\rm def}\\,S(n,x_{n-1},f_{n-1}) be the Mixed System defined by (5,6,10) for instant nğ‘›n and given values for xnâˆ’1,fnâˆ’1subscriptğ‘¥ğ‘›1subscriptğ‘“ğ‘›1x_{n-1},f_{n-1}. With reference to (11), we wish to evaluate the probability that xn>ymaxsubscriptğ‘¥ğ‘›subscriptğ‘¦x_{n}>y_{\\max} and ynâ‰¤ymaxsubscriptğ‘¦ğ‘›subscriptğ‘¦y_{n}\\leq y_{\\max} occurs under adversarial nondeterminism. Denoting by Qvsubscriptğ‘„ğ‘£Q_{v} the domain of vğ‘£v and by ğ”¹ğ”¹\\mathbb{B} the Boolean domain, the underlying probability space of Sğ‘†S is (Î©,Ï€)Î©ğœ‹(\\Omega,\\pi), where Î©=QvÃ—ğ”¹Î©subscriptğ‘„ğ‘£ğ”¹\\Omega=Q_{v}{\\times}\\mathbb{B} and Ï€=Î¼Ã—Î²ğœ‹ğœ‡ğ›½\\pi=\\mu{\\times}\\beta, where Î²ğ›½\\beta is Bernoulli(10âˆ’6)superscript106(10^{-6}). Domain Qğ‘„Q for the variables of Sğ‘†S is Q=QxÃ—QyÃ—QuÃ—QvÃ—ğ”¹Ã—ğ”¹ğ‘„subscriptğ‘„ğ‘¥subscriptğ‘„ğ‘¦subscriptğ‘„ğ‘¢subscriptğ‘„ğ‘£ğ”¹ğ”¹Q=Q_{x}{\\times}Q_{y}{\\times}Q_{u}{\\times}Q_{v}{\\times}\\mathbb{B}{\\times}\\mathbb{B}, and relation Cğ¶C is defined by the nonprobabilistic equations of Sğ‘†S, i.e., (5,10) in which we discard the statement ğ‘Ÿğ‘“nâˆ¼Î²similar-tosubscriptğ‘Ÿğ‘“ğ‘›ğ›½\\mathit{rf}_{n}\\sim\\beta. Finally, let Câ€‹(un)ğ¶subscriptğ‘¢ğ‘›C(u_{n}) denote the relation Cğ¶C in which the value of unsubscriptğ‘¢ğ‘›u_{n} is given (uğ‘¢u is observed). Then\n Inspecting (5,10) shows that the condition defining set Wğ‘ŠW rewrites as First, if Ï†â€‹(un,xnâˆ’1)â‰¤ymaxğœ‘subscriptğ‘¢ğ‘›subscriptğ‘¥ğ‘›1subscriptğ‘¦\\varphi(u_{n},x_{n-1})\\leq y_{\\max} holds, then W=âˆ…ğ‘ŠW=\\emptyset. We thus assume in the sequel Ï†â€‹(un,xnâˆ’1)>ymaxğœ‘subscriptğ‘¢ğ‘›subscriptğ‘¥ğ‘›1subscriptğ‘¦\\varphi(u_{n},x_{n-1})>y_{\\max}. Thus we need to evaluate with respect to Ï€Â¯Â¯ğœ‹\\overline{\\pi} the predicate Condition fn=Tsubscriptğ‘“ğ‘›Tf_{n}{=}\\mbox{\\tt T} is equivalent to the conjunction of the following two conditions: 1) ğ‘ğ‘˜n=Fsubscriptğ‘ğ‘˜ğ‘›F\\mathit{bk}_{n}{=}\\mbox{\\tt F} (backup sensor is not available), 2) fnâˆ’1=Tsubscriptğ‘“ğ‘›1Tf_{n-1}{=}\\mbox{\\tt T} or ğ‘Ÿğ‘“n=Tsubscriptğ‘Ÿğ‘“ğ‘›T\\mathit{rf}_{n}{=}\\mbox{\\tt T}. Recall that the value of fnâˆ’1subscriptğ‘“ğ‘›1f_{n-1} is given. We thus distinguish the following two cases: fnâˆ’1=Tsubscriptğ‘“ğ‘›1Tf_{n-1}{=}\\mbox{\\tt T}: then, fn=Tsubscriptğ‘“ğ‘›Tf_{n}{=}\\mbox{\\tt T} whatever the value of ğ‘ğ‘˜nsubscriptğ‘ğ‘˜ğ‘›\\mathit{bk}_{n} is, and, using (34): fnâˆ’1=Fsubscriptğ‘“ğ‘›1Ff_{n-1}{=}\\mbox{\\tt F}: then, fn=Tsubscriptğ‘“ğ‘›Tf_{n}{=}\\mbox{\\tt T} if and only if ğ‘Ÿğ‘“n=tsubscriptğ‘Ÿğ‘“ğ‘›t\\mathit{rf}_{n}{=}\\mbox{{t}} and ğ‘ğ‘˜n=Fsubscriptğ‘ğ‘˜ğ‘›F\\mathit{bk}_{n}{=}\\mbox{\\tt F}. Thus, chosing ğ‘ğ‘˜n=Fsubscriptğ‘ğ‘˜ğ‘›F\\mathit{bk}_{n}{=}\\mbox{\\tt F} ensures that: ğ‘Ÿğ‘“n=tsubscriptğ‘Ÿğ‘“ğ‘›t\\mathit{rf}_{n}{=}\\mbox{{t}} and Ïˆâ€‹(Ï†â€‹(un,xnâˆ’1),v)â‰¤ymaxğœ“ğœ‘subscriptğ‘¢ğ‘›subscriptğ‘¥ğ‘›1ğ‘£subscriptğ‘¦\\psi(\\varphi(u_{n},x_{n-1}),v)\\leq y_{\\max} together yield ynâ‰¤ymaxsubscriptğ‘¦ğ‘›subscriptğ‘¦y_{n}{\\leq}y_{\\max}. Alternatively, ğ‘ğ‘˜n=Tsubscriptğ‘ğ‘˜ğ‘›T\\mathit{bk}_{n}{=}\\mbox{\\tt T} prevents the condition ynâ‰¤ymaxsubscriptğ‘¦ğ‘›subscriptğ‘¦y_{n}{\\leq}y_{\\max} from occurring. By definition of the outer probability (25), we finally get, using (34): which corresponds to the probabilistic evaluation of the predicate â€œxn>ymaxsubscriptğ‘¥ğ‘›subscriptğ‘¦x_{n}{>}y_{\\max} and ynâ‰¤ymaxsubscriptğ‘¦ğ‘›subscriptğ‘¦y_{n}{\\leq}y_{\\max}â€ if the nondeterministic alternative ğ‘ğ‘˜n=F/Tsubscriptğ‘ğ‘˜ğ‘›FT\\mathit{bk}_{n}{=}\\mbox{\\tt F}/\\mbox{\\tt T} is interpreted as demonicÂ [ref]21. â–¡â–¡\\Box Conditioning is generally not considered in the field of probabilistic automata. It is, however, central in probabilistic programming, see, e.g.,Â   [ref]64 for studies in which conditioning is the main subject. The observe primitive, pervasive in all tools, is used to specify posterior distributions given constraints (as we do in DefinitionÂ 1). The litterature on probabilistic programming distinguishes between hard (also called deterministic) and soft (also called stochastic) conditioningÂ  [ref]64. In the basics of probability theory, however, the only notion is that of conditional expectationÂ , from which other notions are derived, e.g., conditional probability, transition probability or stochastic kernel, and disintegration (or regular version of conditional expectation). Deriving such notions is straightforward in our case, since we restrict ourselves to discrete probability spaces. We will discuss this further when extending Bayesian networks to Mixed Systems, in SectionÂ 2.2. Inconsistency formalizes self-contradiction, for Mixed Systems.\nThe condition â€œÏ€â€‹(Î©ğ–¼)>0ğœ‹superscriptÎ©ğ–¼0\\pi(\\Omega^{\\sf c})>0â€ in statementÂ 2 of DefinitionÂ 1 means that Î©ğ–¼superscriptÎ©ğ–¼\\Omega^{\\sf c} has non-empty intersection with the support of Ï€ğœ‹\\pi, defined as the set of Ï‰ğœ”\\omegaâ€™s of positive probability: Ï€â€‹(Ï‰)>0ğœ‹ğœ”0\\pi({\\omega})>0. This simple definition for the support, which is only valid for discrete probabilities, allows us to propose a simple definition for the notion of consistency. When continuous probability spaces are considered (like the Gaussian), the above definition for the support no longer holds. The right definition relies on topological properties. As a consequence, our elementary definition of consistency would no longer apply. This is next illustrated on our running example. [consistency] \nConsider model (18,22). StatementÂ 2 of DefinitionÂ 1 defines consistency as the existence of a state qğ‘q in relation through Cğ¶C with an Ï‰ğœ”\\omega belonging to the support of Ï€ğœ‹\\pi, which is fairly simple. Suppose, for a while, that un,xn,yn,vnsubscriptğ‘¢ğ‘›subscriptğ‘¥ğ‘›subscriptğ‘¦ğ‘›subscriptğ‘£ğ‘›u_{n},x_{n},y_{n},v_{n} possess real domain, Î¼â€‹(dâ€‹v)=Ï‡â€‹(v)â€‹dâ€‹vğœ‡ğ‘‘ğ‘£ğœ’ğ‘£ğ‘‘ğ‘£\\mu(dv)=\\chi(v)dv, where dâ€‹vğ‘‘ğ‘£dv denotes the Lebesgue measure, density Ï‡ğœ’\\chi is continuous and everywhere positive, and function vâ†¦Ïˆâ€‹(x,v)maps-toğ‘£ğœ“ğ‘¥ğ‘£v\\mapsto\\psi(x,v) is bijective and bicontinuous for every fixed xğ‘¥x. Then, fixing the value of ynsubscriptğ‘¦ğ‘›y_{n}, for a given pair (un,xnâˆ’1)subscriptğ‘¢ğ‘›subscriptğ‘¥ğ‘›1(u_{n},x_{n-1}), will fix the value of vnsubscriptğ‘£ğ‘›v_{n} if fn=Tsubscriptğ‘“ğ‘›Tf_{n}{=}\\mbox{\\tt T} in the equation defining ynsubscriptğ‘¦ğ‘›y_{n}. With reference to Exampleâ€‰3, the only difference is that a parallel composition with the statement observeâ€‹yobserveğ‘¦{\\texttt{{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}observe}}}\\;y was added. So, it still makes sense to consider the two casesâ€‰1 andâ€‰2 of Exampleâ€‰3. In caseâ€‰1, we get\nW={(ğ‘Ÿğ‘“,v)âˆ£Ïˆâ€‹(Ï†â€‹(un,xnâˆ’1),v)=yn}ğ‘Šconditional-setğ‘Ÿğ‘“ğ‘£ğœ“ğœ‘subscriptğ‘¢ğ‘›subscriptğ‘¥ğ‘›1ğ‘£subscriptğ‘¦ğ‘›W=\\left\\{(\\mathit{rf},v)\\mid\\psi(\\varphi(u_{n},x_{n-1}),v){=}y_{n}\\right\\}, whence (Î²Ã—Î¼)â€‹(W)=0ğ›½ğœ‡ğ‘Š0(\\beta{\\times}\\mu)(W){=}0. Deducing inconsistency would be nonsense, however, since the support of Î¼ğœ‡\\mu is â„â„\\mathbb{R}. This illustrates that our pedestrian definition of consistency no longer works if real variables and distributions having densities with respect to Lebesgue measure are considered.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: qâˆˆQğ‘ğ‘„q{\\in}{Q} such that Ï‰â€‹Câ€‹qğœ”ğ¶ğ‘\\omega C{q}. This two-step procedure is denoted by Sâ€‹â¤³â€‹qğ‘†â¤³ğ‘S\\,{\\leadsto}\\,q. If Sğ‘†S is consistent, its probabilistic semantics is defined as the pair Ï€Â¯,Ï€Â¯:2Qâ†’ [ref]1:Â¯ğœ‹Â¯ğœ‹â†’superscript2ğ‘„01\\overline{\\pi},\\underline{\\pi}:2^{Q}\\rightarrow [ref]1, where, for\nany state property AâŠ†Qğ´ğ‘„A\\subseteq{Q}: The following generalized likelihood â„“:2Qâ†’\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"Synchronization of Bernoulli sequences on shared letters\",\n                \"Markov two-components processes\",\n                \"Markovian dynamics of concurrent systems\",\n                \"Branching cells as local states for event structures and nets: Probabilistic applications\",\n                \"True-concurrency probabilistic models: Branching cells and distributed probabilities for event structures\",\n                \"True-concurrency probabilistic models: Markov nets and a law of large numbers\",\n                \"Concurrency, sigma-algebras, and probabilistic fairness\",\n                \"Relational reasoning via probabilistic coupling\",\n                \"Real-Time Probabilistic Programming\",\n                \"A markovian decision process\",\n                \"Building a hybrid systems modeler on synchronous languages principles\",\n                \"The synchronous languages 12 years later\",\n                \"A calculus of stochastic systems for the specification, simulation, and hidden state estimation of mixed stochastic/nonstochastic systems\",\n                \"Mixed Nondeterministic-Probabilistic Automata: Blending graphical probabilistic models with nondeterminism\",\n                \"On a class of probability spaces\",\n                \"Measure transformer semantics for bayesian machine learning\",\n                \"A lambda-calculus foundation for universal probabilistic programming\",\n                \"Concurrency and Probability: Removing Confusion, Compositionally\",\n                \"Bayesian network semantics for petri nets\",\n                \"Stan: A Probabilistic Programming Language\",\n                \"Algorithmic analysis of qualitative and quantitative termination problems for affine probabilistic programs\",\n                \"Semantics of higher-order probabilistic programs with conditioning\",\n                \"Bisimulations for non-deterministic labelled markov processes\",\n                \"Abstract probabilistic automata\",\n                \"Probabilities and potentials\",\n                \"Upper and lower probabilities induced by a multivalued mapping\",\n                \"A generalization of bayesian inference\",\n                \"Verifying probabilistic programs using a hoare like logic\",\n                \"Stochastic Nondeterminism and Effectivity Functions\",\n                \"Bisimulation metrics for continuous markov decision processes\",\n                \"Model-based testing of probabilistic systems\",\n                \"Equivalence notions and model minimization in markov decision processes\",\n                \"Church: a language for generative models\",\n                \"The Design and Implementation of Probabilistic Programming Languages\",\n                \"Probabilistic programming\",\n                \"Stochastic processes as concurrent constraint programs\",\n                \"Measure Theory\",\n                \"Probabilistic couplings for probabilistic reasoning\",\n                \"A provably correct sampler for probabilistic programs\",\n                \"A probabilistic powerdomain of evaluations\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of Contents. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}