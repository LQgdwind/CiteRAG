{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Optimization of FPGA-based CNN Accelerators Using Metaheuristics\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, we review the existing approaches targeting the optimization of CONV implementation on FPGA platforms. Although CNN structures perform well for their intended applications, they have the potential to be further optimized with minimal impact on accuracy. For example, the values of FMs and weight parameters are originally represented as 323232-bit floating-point numbers. However, it has been demonstrated that fewer bits can be used to represent these values without a noticeable accuracy dropÂ . This reduces the hardware requirements for CNN implementation as well as reduces the inference latency. The authors inÂ  proposed an accelerator for LeNet-5 architecture to perform handwritten digits classification. The proposed strategy is based on three major aspects; loop parallelization to utilize resources, fixed-point data optimization to find the minimum number of bits that maintains accuracy level, and finally implementing MAC approximate units through logic blocks such as look-up tables (LUTs) and flip-flops (FFs) rather than using high-precision digital signal processors (DSPs). With these optimizations, the authors achieve less memory usage and reduced network latency. However, the solution is problem specific and deals with a relatively small CNN. Large CNNs would require advanced techniques to achieve the full potential of FPGA resources. InÂ , the authors adopted the polyhedral-based data dependence analysisÂ  to optimize the computations and memory access operations in CONV layers. Specifically, they proposed an analytical design scheme to estimate the computational performance of a given CONV design. The loop transformations are employed to derive all possible CONV designs. The goal behind employing these techniques is to fully utilize the hardware resources provided by the target FPGA platform for effective acceleration. After enumerating all candidate solutions, the roofline performance modelÂ  is used to identify the optimal design for each layer. Here, we must emphasize that different CONV layers have different structures, and therefore varying optimal loop unrolling and loop tiling factors. Considering AlexNet architecture discussed in FigureÂ 1, the optimal unrolling factors âŸ¨Tn,TmâŸ©subscriptğ‘‡ğ‘›subscriptğ‘‡ğ‘š\\langle T_{n},T_{m}\\rangle for the second and third CONV layers are âŸ¨24,20âŸ©2420\\langle 24,20\\rangle and âŸ¨5,96âŸ©596\\langle 5,96\\rangle, respectively, where Tnsubscriptğ‘‡ğ‘›T_{n} and Tmsubscriptğ‘‡ğ‘šT_{m} are the unrolling factors for the input FMs and output FMs, respectively. Hence, designing a CLP to accelerate these differently structured layers requires complex hardware implementation to reconfigure the computational units and their interconnects. To overcome this issue, the authors inÂ  designed the CLP based on the uniform cross-layer unrolling factors. More precisely, they found the unified unrolling factors âŸ¨Tnâ€²,Tmâ€²âŸ©subscriptsuperscriptğ‘‡â€²ğ‘›subscriptsuperscriptğ‘‡â€²ğ‘š\\langle T^{{}^{\\prime}}_{n},T^{{}^{\\prime}}_{m}\\rangle that maximize the overall performance. Accordingly, the computational engine of the CLP is implemented as Tmâ€²subscriptsuperscriptğ‘‡â€²ğ‘šT^{{}^{\\prime}}_{m} duplicated tree-shaped poly structures. These structures receive identical Tnâ€²subscriptsuperscriptğ‘‡â€²ğ‘›T^{{}^{\\prime}}_{n} values from the input FMs and each structure receives Tnâ€²subscriptsuperscriptğ‘‡â€²ğ‘›T^{{}^{\\prime}}_{n} weights from one of the Tmâ€²subscriptsuperscriptğ‘‡â€²ğ‘šT^{{}^{\\prime}}_{m} kernels. Each structure multiplies its data using Tnâ€²subscriptsuperscriptğ‘‡â€²ğ‘›T^{{}^{\\prime}}_{n} digital multipliers. Then, an adder tree is used to accumulate the multipliers outputs as well as the previous partial result. Furthermore, they used the double-buffering technique to overlap the off-chip memory and CLP buffers data transfers with computation. Suda et al.Â [ref]17 proposed an OpenCL-based framework for accelerating CNN inference. Focusing on CONV layers, the authors reformulated the CONV operation into a matrix multiplication operation. Specifically, they organized CONV kernels as a 222-D matrix of size MÃ—(NÃ—KÃ—K)ğ‘€ğ‘ğ¾ğ¾M\\times(N\\times K\\times K). Similarly, the input FMs were flattened and rearranged as a 222-D matrix of (NÃ—KÃ—K)ğ‘ğ¾ğ¾(N\\times K\\times K) rows and (RÃ—C)ğ‘…ğ¶(R\\times C) columns. In this way, the output of CONV layer is calculated by multiplying these two matrices. To speed up CNN operations, the authors followed the same strategy inÂ  and modeled each type of CNN layer using unified loop unrolling factors. Precisely, they unrolled the output matrix of CONV operation in both dimensions by the factor Toâ€‹uâ€‹tsubscriptğ‘‡ğ‘œğ‘¢ğ‘¡T_{out}. Thus, they designed a CLP consisting of Toâ€‹uâ€‹tÃ—Toâ€‹uâ€‹tsubscriptğ‘‡ğ‘œğ‘¢ğ‘¡subscriptğ‘‡ğ‘œğ‘¢ğ‘¡T_{out}\\times T_{out} structures, each of which computes an output feature. Accordingly, kernel weights and input FMs matrices are tiled into blocks of size Toâ€‹uâ€‹tÃ—Toâ€‹uâ€‹tsubscriptğ‘‡ğ‘œğ‘¢ğ‘¡subscriptğ‘‡ğ‘œğ‘¢ğ‘¡T_{out}\\times T_{out}. On each iteration, a tile from the kernel weight matrix and a tile from the input FMs matrix are fetched into the on-chip memory. Then, each CLP structure performs Toâ€‹uâ€‹tsubscriptğ‘‡ğ‘œğ‘¢ğ‘¡T_{out} MAC operations on a row of kernel weights tile and a column of input FMs tile. To further improve the throughput of CONV layer, the computations on the inputs to CLP structures were also unrolled using the factor Tiâ€‹nsubscriptğ‘‡ğ‘–ğ‘›T_{in} so as to perform Tiâ€‹nsubscriptğ‘‡ğ‘–ğ‘›T_{in} MAC operations, out of the Toâ€‹uâ€‹tsubscriptğ‘‡ğ‘œğ‘¢ğ‘¡T_{out}, in parallel.\nTo find the optimal unrolling configuration, the authors model the execution time of each layer as a function of the unrolling factors. Then, the standard genetic algorithm was used to explore the design space for the minimum overall execution time considering FPGA resource constraints. Hence, this approach is referred to as GA-based Single-CLP. It is noteworthy that the high-level synthesis tool employed to compile their OpenCL codes to hardware restricted Tiâ€‹nsubscriptğ‘‡ğ‘–ğ‘›T_{in} to be from the set {1,2,4,8,16}124816\\{1,2,4,8,16\\} and Toâ€‹uâ€‹tsubscriptğ‘‡ğ‘œğ‘¢ğ‘¡T_{out} to be integer multiplicative of Tiâ€‹nsubscriptğ‘‡ğ‘–ğ‘›T_{in}. The FPGA-based CNN accelerators discussed earlier employed a single globally-optimized CLP design to maximize the overall throughput. However, using a CNN accelerator with uniform unrolling factors leads to sub-optimal performance for some CONV layers due to their significantly varying dimensions, which affects the overall performance. For example, the cross-layer optimization for AlexNet layers inÂ  increases the total execution cycles of the layer-based optimization by 44,4424444244,442 cycles. Moreover, following the methodology inÂ  to derive the optimal Single-CLP design for SqueezeNetÂ 1.11.11.1 implementation on Virtex-7 690T FPGA, the results show that the dynamic utilization of CLPâ€™s MAC units is less than 777777%Â . To improve the throughput of the Single-CLP design, Shen et al.Â  proposed to partition the available hardware resources between multiple CLPs. In doing so, they introduced a two-step iterative algorithm that searches for candidate partitions in the first step and then optimizes the tiling factors of each CONV layer in the second step. Specifically, the algorithm starts with a predefined target performance aiming to find a design with such a performance. During the first phase, it generates several candidate partitions of computational resources. For each candidate design, the number of CLPs is set to the partition size. Then, the methodology inÂ  is adopted to compute âŸ¨Tn,TmâŸ©subscriptğ‘‡ğ‘›subscriptğ‘‡ğ‘š\\langle T_{n},T_{m}\\rangle for each CLP. The last process in this phase distributes CNN layers on the adopted CLPs. To facilitate this process, the authors proposed to order memory-bounded layers based on their need for computation and communication, whereas computational-bounded layers are ordered based on the difference between the number of input FMs and output FMs. With such ordering, the authors assume that similar behavioral layers will be neighboring. Thus, they constrain the algorithm to only assign neighboring layers to the same CLP. At the end of the first phase, the performance of candidate designs is evaluated. If no one meets the target performance, the algorithm slightly reduces the target performance and repeats the first phase. When a valid design is found, the algorithm moves on to the second phase and computes âŸ¨Tr,TcâŸ©subscriptğ‘‡ğ‘Ÿsubscriptğ‘‡ğ‘\\langle T_{r},T_{c}\\rangle based on the methodology inÂ . The parameters for the design with the minimum bandwidth requirement is then used to configure a generic CLP template in high-level synthesis. In this work, we improve the previously discussed works by efficiently partitioning hardware resources between multiple CLPs. In doing so, we employ intelligent metaheuristics to help explore the intractable design space for multiple CLPs design that is more optimized than that achieved with conventional iterative algorithms. This results in better utilization of FPGA compute resources, improving CNN throughput. Furthermore, unlike the work inÂ [ref]17 which constraints the unrolling factors to be from a predefined set, this work allows the unrolling factors to be whatever value that yields the highest performance. Last, but not least, the authors inÂ  have specified the number of CLPs in multiple CLPs design to be 666 at most. This is because they want to keep the optimization time of their algorithm within an acceptable amount of time. This paper proposes a metaheuristic-based optimization framework that is efficient in exploring design space and takes less than a minute to provide an optimal Multi-CLP design on a single CPU.\n\nReference markers found: [17]\n\nContext windows for each [ref]:\n[ref]17: outputs as well as the previous partial result. Furthermore, they used the double-buffering technique to overlap the off-chip memory and CLP buffers data transfers with computation. Suda et al.Â [ref]17 proposed an OpenCL-based framework for accelerating CNN inference. Focusing on CONV layers, the authors reformulated the CONV operation into a matrix multiplication operation. Specifically,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 17,\n            \"titles\": [\n                \"Maximizing CNN Accelerator Efficiency Through Resource Partitioning\",\n                \"Mandarin speech recognition using convolution neural network with augmented tone features\",\n                \"A convolutional neural network approach for face verification\",\n                \"Multi-view Face Detection Using Deep Convolutional Neural Networks\",\n                \"CNN Based Vehicle Counting with Virtual Coil in Traffic Surveillance Video\",\n                \"A pedestrian and vehicle rapid identification model based on convolutional neural network\",\n                \"Malaysia traffic sign recognition with convolutional neural network\",\n                \"Imagenet classification with deep convolutional neural networks\",\n                \"FPGA-Based Accelerators of Deep Learning Networks for Learning and Classification: A Review\",\n                \"Computer vision algorithms and hardware implementations: A survey\",\n                \"A Survey on Efficient Convolutional Neural Networks and Hardware Acceleration\",\n                \"Minimizing computation in convolutional neural networks\",\n                \"Mobilenets: Efficient convolutional neural networks for mobile vision applications\",\n                \"The Smart Image Recognition Mechanism for Crop Harvesting System in Intelligent Agriculture\",\n                \"IoT-based agriculture: Deep learning in detecting apple fruit diseases\",\n                \"A high performance FPGA-based accelerator for large-scale convolutional neural networks\",\n                \"Optimizing FPGA-based accelerator design for deep convolutional neural networks\",\n                \"Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks\",\n                \"FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks\",\n                \"Meta-heuristics: an overview\",\n                \"Simulated annealing algorithm for deep learning\",\n                \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\",\n                \"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\n                \"Going deeper with convolutions\",\n                \"FxP-QNet: A Post-Training Quantizer for the Design of Mixed Low-Precision DNNs with Dynamic Fixed-Point Representation\",\n                \"FPGA-Based Convolutional Neural Network Accelerator with Resource-Optimized Approximate Multiply - Accumulate Unit\",\n                \"Polyhedral-based data reuse optimization for configurable computing\",\n                \"Roofline: an insightful visual performance model for multicore architectures\",\n                \"Vivado Design Suite Product Guide: Floating-Point Operator v7.1 [Online]\",\n                \"User Guide: 7 Series FPGAs Memory Resources [Online]\",\n                \"Iterative Computer Algorithms with Applications in Engineering: Solving Combinatorial Optimization Problems\",\n                \"VLSI physical design automation: theory and practice\",\n                \"Optimization by simulated annealing\",\n                \"Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm\",\n                \"Equation of state calculations by fast computing machines\",\n                \"Evolutionary algorithms, simulated annealing and tabu search: a comparative study\",\n                \"Tabu searchâ€”part I\",\n                \"Tabu searchâ€”part II\",\n                \"A userâ€™s guide to tabu search\",\n                \"Tabu search\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]17 based on the context and research area of Optimization of FPGA-based CNN Accelerators Using Metaheuristics. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}