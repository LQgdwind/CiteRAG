{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Contents\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, we extend the algorithm of Section 2 and combine it with the graph shattering technique  , which has been used extensively in recent years to get sub-logarithmic-time distributed algorithms for a large number of graph problems (mostly in the LOCAL model). By using this technique in our setting, we prove the following theorem. Theorem 1.2 (restated).\nThere is a randomized CONGEST algorithm that d2-colors a graph with Δ2+1superscriptΔ21\\Delta^{2}+1 colors in O​(log⁡Δ)+𝑁𝐷2​(log⁡n)⋅poly⁡log⁡log⁡n𝑂Δ⋅subscript𝑁𝐷2𝑛poly𝑛O(\\log\\Delta)+\\mathit{ND}_{2}(\\log n)\\cdot\\operatorname{poly}\\log\\log n rounds, with high probability. Here 𝑁𝐷2​(log⁡n)subscript𝑁𝐷2𝑛\\mathit{ND}_{2}(\\log n) is the sum of d⋅c⋅x⋅𝑑𝑐𝑥d\\cdot c\\cdot x and the time to compute a distance-222 x𝑥x-CONGEST-routable network decomposition with weak cluster diameter d𝑑d and c𝑐c cluster colors on subgraphs of size poly⁡log⁡npoly𝑛\\operatorname{poly}\\log n with node identifiers from a space of size poly⁡npoly𝑛\\operatorname{poly}n (cf. Definition 3.16 for a formal definition). The current state of the art for 𝑁𝐷2​(log⁡n)subscript𝑁𝐷2𝑛\\mathit{ND}_{2}(\\log n) is 2O​(log⁡log⁡n)superscript2𝑂𝑛2^{O(\\sqrt{\\log\\log n})} [ref]22. However, the complexity for distance-111 network decompositions that can deal with a large identifier space was improved subsequent to the submission of this manuscript to poly⁡log⁡log⁡npoly𝑛\\operatorname{poly}\\log\\log n rounds . Before the publication of  the complexity in Theorem 1.2 for distance-222 coloring matched the state of the art for distance-111 (Δ+1)Δ1(\\Delta+1)-coloring [ref]19. As the achievements of  improve the complexity for distance-111 coloring from O​(log⁡Δ)+2O​(log⁡log⁡n)𝑂Δsuperscript2𝑂𝑛O(\\log\\Delta)+2^{O(\\sqrt{\\log\\log n})} to O​(log⁡Δ)+poly⁡log⁡log⁡n𝑂Δpoly𝑛O(\\log\\Delta)+\\operatorname{poly}\\log\\log n there currently is a gap between the complexities of distance-111 and distance-222 coloring. If  (or an alternative approach) extends to distance-222 decompositions, and such an extension is very likely, it will match again. In the remaining part of the writeup we use the best known upper bound of 𝑁𝐷2​(log⁡n)=2O​(log⁡log⁡n)subscript𝑁𝐷2𝑛superscript2𝑂𝑛\\mathit{ND}_{2}(\\log n)=2^{O(\\sqrt{\\log\\log n})}. From a very high-level point of view, the rough idea of graph shattering applied to our problem is as follows. The algorithm of Section 2 consists of O​(log⁡n)𝑂𝑛O(\\log n) individual O​(1)𝑂1O(1)-round steps, where in each step, each live node gets colored with constant probability. Thus, very roughly, if we just run the algorithm for O​(log⁡Δ)𝑂ΔO(\\log\\Delta) steps, each node remains uncolored with probability at most 1/poly⁡(Δ)1polyΔ1/\\operatorname{poly}(\\Delta). Further, if nodes succeeded sufficiently independently, after O​(log⁡Δ)𝑂ΔO(\\log\\Delta) rounds, each node would only have O​(log⁡n)𝑂𝑛O(\\log n) uncolored neighbors. By combining these two properties, one can hope that after O​(log⁡Δ)𝑂ΔO(\\log\\Delta) rounds, all the remaining live nodes induce components (in G2superscript𝐺2G^{2}) of size at most polylog⁡npolylog𝑛\\operatorname{polylog}n. By adapting techniques developed in  to our G2superscript𝐺2G^{2}-coloring algorithm, we will show that this indeed (almost) is the case. We call this part of the algorithm, where we reduce the original problem to a problem on components of polylog⁡npolylog𝑛\\operatorname{polylog}n size, the preshattering phase of our algorithm. The remaining problem that we need to solve on the components of size polylog⁡npolylog𝑛\\operatorname{polylog}n is a list coloring problem. Because these problems for each component are on much smaller graphs, they can be solved efficiently by using the best known deterministic algorithm. For the specific setting, where we have small components, but each node still has an ID from the original large ID space, the best known deterministic CONGEST algorithm (that can tolerate such a large ID space and works for G2superscript𝐺2G^{2}) can be obtained by combining a network decomposition algorithm of Ghaffari and Portmann [ref]22 with a recent deterministic CONGEST coloring algorithm of Bamberger, Kuhn, and Maus . It requires 2O​(log⁡N)=2O​(log⁡log⁡n)superscript2𝑂𝑁superscript2𝑂𝑛2^{O(\\sqrt{\\log N})}=2^{O(\\sqrt{\\log\\log n})} time, where N=polylog⁡n𝑁polylog𝑛N=\\operatorname{polylog}n is the maximum component size. We call this second phase of solving the remaining list coloring instances on the components the postshattering phase. While the general outline of the algorithm is relatively standard and largely follows the ideas of the distance-1 coloring algorithm for the LOCAL model in , there are various challenges that we have to cope with in order to apply the idea in the CONGEST model and to the d2-coloring problem.\nIn  [ref]19,\nthe algorithm for the preshattering phase is very simple: In each step of the algorithm, every live node tries a uniformly random color from its current list of available colors. As we have seen in Sec. 2, we cannot run this algorithm in the d2-coloring setting as it is not possible for a live node to learn its list of available colors (i.e., learn the colors already chosen by its 2-neighbors). We would therefore like to show that the much more involved randomized algorithm of Sec. 2 also has the same shattering properties as the basic “choose-a-random-available-color” algorithm. Unfortunately, this is not obvious and we use a multi-stage algorithm to prove what we need. Greatly simplified, we do the following. We first show that O​(log⁡Δ)𝑂ΔO(\\log\\Delta) rounds of an adaptation of the algorithm of Sec. 2 suffice to (essentially) reduce the maximum degree of the subgraph of G2superscript𝐺2G^{2} induced by the live nodes to O​(log⁡n)𝑂𝑛O(\\log n). At this point, it is possible for each live node to learn a sufficiently large list of available colors in O​(log⁡Δ)𝑂ΔO(\\log\\Delta) rounds and we can now indeed run the basic preshattering algorithm of  to reduce the problem to a problem on polylog⁡npolylog𝑛\\operatorname{polylog}n-size components. For the postshattering phase, while we only have components of poly⁡log⁡npoly𝑛\\operatorname{poly}\\log n size, the input to the problem is still large because each node still has an ID of size O​(log⁡n)𝑂𝑛O(\\log n) bits and because each node has a color list consisting of up to O​(log⁡n)𝑂𝑛O(\\log n) colors from a range of size O​(Δ2)𝑂superscriptΔ2O(\\Delta^{2}). In order to have an efficient CONGEST algorithm for the problem, we have to reduce both the ID space and the color space of the remaining components. It is sufficient to obtain new node IDs that are unique up to distance poly⁡log⁡log⁡npoly𝑛\\operatorname{poly}\\log\\log n. We can obtain such IDs with O​(log⁡log⁡n)𝑂𝑛O(\\log\\log n) bits by first applying the network decomposition algorithm of [ref]22 and then assigning unique labels in each cluster. For reducing the color space, we show that in each cluster of the network decomposition, we can efficiently (and deterministically) find a renaming of the colors such that for every node v𝑣v, all colors in v𝑣v’s list are mapped to distinct new colors and such that the colors are from a space of size poly⁡log⁡npoly𝑛\\operatorname{poly}\\log n. For each of the steps, the implementation in G2superscript𝐺2G^{2} rather than in G𝐺G adds some additional complications. In the following, we give a detailed overview over all the steps of our algorithm. Before going into the details of the algorithm, let us note that some regimes of ΔΔ\\Delta in relation to n𝑛n greatly simplify the problem. If log⁡n=O​(log⁡Δ)𝑛𝑂Δ\\log n=O(\\log\\Delta), using the O​(log⁡n)𝑂𝑛O(\\log n)-time algorithm of Section 2 already yields the claimed time complexity. The problem is also simpler when Δ≤log⁡n⋅poly⁡log⁡log⁡nΔ⋅𝑛poly𝑛\\Delta\\leq\\log n\\cdot\\operatorname{poly}\\log\\log n, as we can then essentially simulate the preshattering algorithm of  for G𝐺G on G2superscript𝐺2G^{2} and combine it with our postshattering algorithm from Section 3.2 (for details, see Appendix C). From now on, in Section 3.1, we assume to be out of those simpler regimes, i.e., we assume throughout that Δ=2o​(log⁡n)∩Ω~​(log⁡n)Δsuperscript2𝑜𝑛~Ω𝑛\\Delta=2^{o(\\log n)}\\cap\\tilde{\\Omega}(\\log n) holds.\n\nReference markers found: [22]\n\nContext windows for each [ref]:\n[ref]22: (cf. Definition 3.16 for a formal definition). The current state of the art for 𝑁𝐷2​(log⁡n)subscript𝑁𝐷2𝑛\\mathit{ND}_{2}(\\log n) is 2O​(log⁡log⁡n)superscript2𝑂𝑛2^{O(\\sqrt{\\log\\log n})} [ref]22. However, the complexity for distance-111 network decompositions that can deal with a large identifier space was improved subsequent to the submission of this manuscript to\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 22,\n            \"titles\": [\n                \"Distance-2 Coloring in the CONGEST Model\",\n                \"A fast and simple randomized parallel algorithm for the maximal independent set problem\",\n                \"Sublinear algorithms for (Δ+1) vertex coloring\",\n                \"Network decomposition and locality in distributed computation\",\n                \"Efficient deterministic distributed coloring with small bandwidth\",\n                \"Distributed approximation on power graphs\",\n                \"Deterministic (Δ + 1)-coloring in sublinear (in Δ) time in static, dynamic and faulty networks\",\n                \"Deterministic distributed vertex coloring in polylogarithmic time\",\n                \"Distributed Graph Coloring: Fundamentals and Recent Developments\",\n                \"Locally-Iterative Distributed (Δ+1)-Coloring below Szegedy-Vishwanathan Barrier, and Applications to Self-Stabilization and to Restricted-Bandwidth Models\",\n                \"Distributed (Δ+1)-coloring in linear (in Δ) time\",\n                \"The locality of distributed symmetry breaking\",\n                \"An algorithmic approach to the Lovaśz local lemma\",\n                \"Derandomizing local distributed algorithms under bandwidth restrictions\",\n                \"An optimal distributed (Δ+1)-coloring algorithm?\",\n                \"A spatial-reuse TDMA/FDMA for mobile multi-hop radio networks\",\n                \"Deterministic Distributed Dominating Set Approximation in the CONGEST Model\",\n                \"(2​Δ−1)-edge-coloring is much easier than maximal matching in the distributed setting\",\n                \"Distributed testing of distance-k colorings\",\n                \"Distributed maximal independent set using small messages\",\n                \"Improved Deterministic Network Decomposition\",\n                \"On derandomizing local distributed algorithms\",\n                \"Improved Network Decompositions using Small Messages with Applications on MIS, Neighborhood Covers, and Beyond\",\n                \"Distributed (Δ+1)-coloring in sublogarithmic rounds\",\n                \"Simple distributed Δ+1-coloring of graphs\",\n                \"Faster deterministic distributed coloring through recursive list coloring\",\n                \"On the complexity of distributed graph coloring\",\n                \"Locality in distributed graph algorithms\",\n                \"A simple parallel algorithm for the maximal independent set problem\",\n                \"MAC layer and coloring\",\n                \"Distributed Computing: A Locality-Sensitive Approach\",\n                \"Polylogarithmic-Time Deterministic Network Decomposition and Distributed Derandomization\",\n                \"A new technique for distributed symmetry breaking\",\n                \"Generated Reference Title 33 for [ref]22\",\n                \"Generated Reference Title 34 for [ref]22\",\n                \"Generated Reference Title 35 for [ref]22\",\n                \"Generated Reference Title 36 for [ref]22\",\n                \"Generated Reference Title 37 for [ref]22\",\n                \"Generated Reference Title 38 for [ref]22\",\n                \"Generated Reference Title 39 for [ref]22\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]22 based on the context and research area of Contents. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}