{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training\n\nPaper Abstract: \n\nPaper Section Text:\nThe following subsections detail FS-DETRâ€™s architecture and main components. Template encoding: Let ğ“i,jâˆˆâ„HpÃ—WpÃ—3,iâˆˆ{1,â€¦,m},jâˆˆ{1,â€¦,k}formulae-sequencesubscriptğ“ğ‘–ğ‘—superscriptâ„subscriptğ»ğ‘subscriptğ‘Šğ‘3formulae-sequenceğ‘–1â€¦ğ‘šğ‘—1â€¦ğ‘˜\\mathbf{T}_{i,j}\\in\\mathbb{R}^{H_{p}\\times W_{p}\\times 3},i\\in\\{1,\\dots,m\\},j\\in\\{1,\\dots,k\\}\nbe the template images of the available classes (sampled from Cbâ€‹aâ€‹sâ€‹esubscriptğ¶ğ‘ğ‘ğ‘ ğ‘’C_{base} during training) where mğ‘šm is the number of classes at the current training iteration (mğ‘šm can vary), and kğ‘˜k is the number of examples per class (i.e. k-shot detection; kğ‘˜k can also vary). A CNN backbone (e.g. ResNet-50) generates template features ğ—=CNNâ€‹(ğ“),ğ—âˆˆâ„mâ€‹kÃ—dformulae-sequenceğ—CNNğ“ğ—superscriptâ„ğ‘šğ‘˜ğ‘‘\\mathbf{X}=\\texttt{CNN}(\\mathbf{T}),\\;\\mathbf{X}\\in\\mathbb{R}^{mk\\times d} using either average or attention pooling (see Sec.Â 5). Pseudo-class embeddings: We propose to dynamically and randomly associate, at each training iteration, the kğ‘˜k template prompts in ğ—ğ—\\mathbf{X} belonging to the i-th class (for that iteration) with a pseudo-class represented by a pseudo-class embedding ğœğ¢ğ¬âˆˆâ„dsubscriptsuperscriptğœğ¬ğ¢superscriptâ„ğ‘‘\\mathbf{c^{s}_{i}}\\in\\mathbb{R}^{d}, which are added to the templates as follows: where ğ‚sâˆˆâ„mâ€‹kÃ—dsuperscriptğ‚ğ‘ superscriptâ„ğ‘šğ‘˜ğ‘‘\\mathbf{C}^{s}\\in\\mathbb{R}^{mk\\times d} contains the pseudo-class embeddings for all templates at the current iteration. The pseudo-class embeddings are initialised from a normal distribution and learned during training. They are not determined by the ground-truth categories and are class-agnostic. During each inference step, we arbitrarily associate to a template prompt (belonging to some class) the iâ€‹-thğ‘–-thi\\textrm{-th} embedding as described by Eq.Â 1. The goal is to retrieve the pseudo-class iğ‘–i. Note that the actual class information is not used. As the assigned embedding changes at every iteration, there is no correlation between the actual classes and the learned embeddings. See also Fig.Â 1 that exemplifies this process.\nIn the proposed FS-DETR, each decoded object query ğ¨isubscriptğ¨ğ‘–\\mathbf{o}_{i} in ğğ\\mathbf{O} will attempt to predict a pseudo-class using a classifier.\nPseudo-class embeddings\nadd a signature to each visual prompt allowing the network to track the template within and dissociate it from the rest of the templates belonging to a different class. As transformers are permutation invariant, this vectors are required in order to track the visual prompt within the model. Templates as visual prompts: We propose to provide the templates ğ—ssuperscriptğ—ğ‘ \\mathbf{X}^{s} as visual prompts to the system by prepending them to the sequence of object queries fed to the decoder: As shown below, the templates will induce pseudo-class related information into the object queries via attention. This can be interpreted as a new form of training-aware soft-promptingÂ [ref]30. FS-DETRÂ  encoder: Given a target image ğˆâˆˆâ„Hâ€²Ã—Wâ€²Ã—3ğˆsuperscriptâ„superscriptğ»â€²superscriptğ‘Šâ€²3\\mathbf{I}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times 3}, the same CNN backbone used for template feature extraction\nfirst\ngenerates image features ğ™=CNNâ€‹(ğˆ),ğ™âˆˆâ„SÃ—d,S=HÃ—Wformulae-sequenceğ™CNNğˆformulae-sequenceğ™superscriptâ„ğ‘†ğ‘‘ğ‘†ğ»ğ‘Š\\mathbf{Z}=\\texttt{CNN}(\\mathbf{I}),\\;\\mathbf{Z}\\in\\mathbb{R}^{S\\times d},\\;S=H\\times W, which are enriched with positional information through positional encodings ğ™â†ğ™+ğs,ğsâˆˆâ„SÃ—dformulae-sequenceâ†ğ™ğ™subscriptğğ‘ subscriptğğ‘ superscriptâ„ğ‘†ğ‘‘\\mathbf{Z}\\leftarrow\\mathbf{Z}+\\mathbf{P}_{s},\\;\\mathbf{P}_{s}\\in\\mathbb{R}^{S\\times d}. The features ğ™ğ™\\mathbf{Z} are then processed by FS-DETRâ€™s encoder layers in order to be enriched with global contextual information.\nThe lâˆ’limit-fromğ‘™l-th encoding layer processes the output features of the previous layer ğ™lâˆ’1superscriptğ™ğ‘™1\\mathbf{Z}^{l-1} using a series of Multi-Head Self-Attention (MHSA), Layer Normalization (LN), and MLP layers (typical inÂ  andÂ [ref]3), as well as a newly proposed Multi-Head Cross-Attention (MHCA) layer as followsÂ 222\nWe follow DETRâ€™s notation\nwhere ğâ€²superscriptğâ€²\\mathbf{O^{\\prime}} is added to Lâ€‹Nâ€‹(ğ•lâˆ’1)ğ¿ğ‘superscriptğ•ğ‘™1LN(\\mathbf{V}^{l-1}) and then projected to form the query Q and key K for self-attention.\nHere, the first layer ğ•lâˆ’1superscriptğ•ğ‘™1\\mathbf{V}^{l-1} is initialised as [Xs,zâ€‹eâ€‹râ€‹oâ€‹s]subscriptğ‘‹ğ‘ ğ‘§ğ‘’ğ‘Ÿğ‘œğ‘ [X_{s},zeros] while in DETR with zâ€‹eâ€‹râ€‹oâ€‹sğ‘§ğ‘’ğ‘Ÿğ‘œğ‘ zeros.\n: The purpose of the MHCA layer above is to filter and highlight early on, before decoding, the image tokens of interest. We have found that such a layer noticeably increases few-shot accuracy (see also SectionÂ 5).\nFS-DETRâ€™s encoder is implemented by stacking L=6ğ¿6L=6 blocks, each following Eq.Â (3)-(5). As image tokens are permutation invariant, we followedÂ [ref]3 and used a fixed positional encoding. For the templates, pseudo-class embeddings serve as positional encodings. FS-DETRÂ  decoder: FS-DETRâ€™s decoder accepts as input the concatenated templates and learnable object queries ğâ€²superscriptğâ€²\\mathbf{O}^{\\prime} which are transformed\nby the decoderâ€™s layers through self-attention and cross-attention layers in order to be eventually used for pseudo-class prediction and bounding box regression. The lâˆ’limit-fromğ‘™l-th decoding layer processes the output features of the previous layer ğ•lâˆ’1superscriptğ•ğ‘™1\\mathbf{V}^{l-1} as follows: where ğ•0=[ğ—sâ€‹zerosâ€‹(N,d)]superscriptğ•0delimited-[]superscriptğ—ğ‘ zerosğ‘ğ‘‘\\mathbf{V}^{0}=\\large[\\mathbf{X}^{s}\\;\\;\\texttt{zeros}(N,d)\\large].\nNotably, different MLPs are used to process the decoderâ€™s features ğ•=[ğ•ğ—sâ€‹ğ•ğ]ğ•delimited-[]subscriptğ•superscriptğ—ğ‘ subscriptğ•ğ\\mathbf{V}=[\\mathbf{V}_{\\mathbf{X}^{s}}\\;\\mathbf{V}_{\\mathbf{O}}] corresponding to the templates ğ•ğ—ssubscriptğ•superscriptğ—ğ‘ \\mathbf{V}_{\\mathbf{X}^{s}} and the object queries ğ•ğsubscriptğ•ğ\\mathbf{V}_{\\mathbf{O}}: FS-DETRâ€™s decoder consists of L=6ğ¿6L=6 layers implemented using Eqs.Â (6)-(9). FS-DETRÂ  training and loss functions: For each base class that exists in the target image, we create a template for that class by randomly sampling and cropping an object from that category using a different image (containing an object of the same class) from the train set. After applying image augmentation, the cropped object/template is passed through the CNN backbone of FS-DETR. For each target image and template iğ‘–i (depicted in that image), the ground truth is yi=(cis,bi)subscriptğ‘¦ğ‘–subscriptsuperscriptğ‘ğ‘ ğ‘–subscriptğ‘ğ‘–y_{i}=(c^{s}_{i},b_{i}), where cissubscriptsuperscriptğ‘ğ‘ ğ‘–c^{s}_{i} is the target pseudo-class label (up to mğ‘šm classes in total) and biâˆˆ[ref]0 subscriptğ‘ğ‘–superscript014b_{i}\\in[ref]0 ^{4} are the normalised bounding box coordinates. To calculate the loss for training FS-DETR, only the Nğ‘N transformed object queries ğ•ğLâˆˆâ„›NÃ—dsubscriptsuperscriptğ•ğ¿ğsuperscriptâ„›ğ‘ğ‘‘\\mathbf{V}^{L}_{\\mathbf{O}}\\in\\mathcal{R}^{N\\times d} from the output of the last decoding layer are used for pseudo-class classification and bounding box regression (i.e. ğ•ğ—sLsubscriptsuperscriptğ•ğ¿superscriptğ—ğ‘ \\mathbf{V}^{L}_{\\mathbf{X}^{s}} is not used). To this end, pseudo-class and bounding box prediction heads are used to produce a set of Nğ‘N predictions {y^i}i=1Nsuperscriptsubscriptsubscript^ğ‘¦ğ‘–ğ‘–1ğ‘\\{\\hat{y}_{i}\\}_{i=1}^{N} consisting of the pseudo-class probabilities p^iâ€‹(cs)subscript^ğ‘ğ‘–superscriptğ‘ğ‘ \\hat{p}_{i}(c^{s}) and the bounding box coordinates b^isubscript^ğ‘ğ‘–\\hat{b}_{i}. The heads are implemented using a 3-layer MLP and ReLU activations. Similarly toÂ [ref]3, we used an additional special pseudo-class âˆ…\\varnothing to denote tokens without valid object predictions.\nNote that as the training is done in a class-agnostic way via mapping of the base class templates to pseudo-classes (the actual class information is discarded) the model is capable to generalise to the unseen novel categories. FollowingÂ [ref]3,\nbipartite matching is used to find an optimal permutation {y^Ïƒâ€‹i}i=1Nsuperscriptsubscriptsubscript^ğ‘¦ğœğ‘–ğ‘–1ğ‘\\{\\hat{y}_{\\sigma{i}}\\}_{i=1}^{N}. Finally, the loss is: where IoU is the GIoU loss ofÂ  and Î»isubscriptğœ†ğ‘–\\lambda_{i} are the loss weights. Pre-training: Transformers are generally more data hungry than CNNs due to their lack of inductive biasÂ . Therefore,\nbuilding representations that generalise well to unseen data, and prevent overfitting within the DETR framework, requires larger amounts of data. To this end, we used images from ImageNet-100Â  and to some extent MSCOCO, for unsupervised pre-training where the classes and the bounding boxes are generated on-the-fly using an object proposal system, without using any labels. Note, that unlike all prior works, we make use of neg. templates as prompts, training the network using our proposed loss. See also the appendix.\n\nReference markers found: [30]\n\nContext windows for each [ref]:\n[ref]30: the decoder: As shown below, the templates will induce pseudo-class related information into the object queries via attention. This can be interpreted as a new form of training-aware soft-promptingÂ [ref]30. FS-DETRÂ  encoder: Given a target image ğˆâˆˆâ„Hâ€²Ã—Wâ€²Ã—3ğˆsuperscriptâ„superscriptğ»â€²superscriptğ‘Šâ€²3\\mathbf{I}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times 3}, the same CNN backbone used for\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 30,\n            \"titles\": [\n                \"SSD: Single Shot MultiBox Detector\",\n                \"DETReg: Unsupervised Pretraining with Region Priors for Object Detection\",\n                \"Cascade R-CNN: Delving into High Quality Object Detection\",\n                \"End-to-end object detection with transformers\",\n                \"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\",\n                \"Up-DETR: Unsupervised pre-training for object detection with transformers\",\n                \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n                \"The Pascal Visual Object Classes Challenge: A Retrospective\",\n                \"The pascal visual object classes (VOC) challenge\",\n                \"Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector\",\n                \"Few-shot object detection with model calibration\",\n                \"Generalized few-shot object detection without forgetting\",\n                \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\",\n                \"Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning\",\n                \"Query Adaptive Few-Shot Object Detection with Heterogeneous Graph Convolutional Networks\",\n                \"Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment\",\n                \"Few-Shot Object Detection with Fully Cross-Transformer\",\n                \"Mask R-CNN\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"One-Shot Object Detection with Co-Attention and Co-Excitation\",\n                \"Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection\",\n                \"Visual Prompt Tuning\",\n                \"Few-shot Object Detection via Feature Reweighting\",\n                \"tSF: Transformer-based Semantic Filter for Few-Shot Learning\",\n                \"CornerNet: Detecting Objects as Paired Keypoints\",\n                \"Transformation invariant few-shot object detection\",\n                \"AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration\",\n                \"Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection\",\n                \"Focal Loss for Dense Object Detection\",\n                \"Microsoft COCO: Common Objects in Context\",\n                \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\",\n                \"Swin transformer: Hierarchical vision transformer using shifted windows\",\n                \"Decoupled Weight Decay Regularization\",\n                \"Conditional DETR for Fast Training Convergence\",\n                \"On the momentum term in gradient descent learning algorithms\",\n                \"DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection\",\n                \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\",\n                \"You Only Look Once: Unified, Real-Time Object Detection\",\n                \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\n                \"Generalized intersection over union: A metric and a loss for bounding box regression\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]30 based on the context and research area of FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}