{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Work-efficient Batch-incremental Minimum Spanning Trees with Applications to the Sliding Window Model\n\nPaper Abstract: \n\nPaper Section Text:\nMSTs have a long and interesting history. The problem of dynamically maintaining the MST under modifications to the underlying graph has been well studied. Spira and Pan  were the first to tackle the dynamic problem, and give an Oâ€‹(n)ğ‘‚ğ‘›O(n) sequential algorithm for vertex insertion that is based on Boruvkaâ€™s algorithm. The first sublinear time algorithm for edge updates was given by Frederickson , who gave an Oâ€‹(m)ğ‘‚ğ‘šO(\\sqrt{m}) algorithm. A well-celebrated improvement to Fredericksonâ€™s algorithm was given by Eppstein et. al , who introduced the sparsification technique to reduce the cost to Oâ€‹(n)ğ‘‚ğ‘›O(\\sqrt{n}). A great number of subsequent dynamic algorithms, including parallel ones, take advantage of Eppsteinâ€™s sparsification. The sequential incremental MST problem, i.e., the problem of maintaining the MST subject to new edge insertions but no deletions, can be solved in Oâ€‹(lgâ¡(n))ğ‘‚lgğ‘›O(\\lg(n)) time per update using dynamic trees   to find the heaviest weight edge on the cycle induced by the new edge and remove it. Holm et. al. gave the first polylogarithmic time algorithm for fully dynamic MSTÂ [ref]31, supporting updates in Oâ€‹(lg4â¡(n))ğ‘‚superscriptlg4ğ‘›O(\\lg^{4}(n)) amortized time per operation, and later improved by a lgâ¡lgâ¡nlglgğ‘›\\lg\\lg n factorÂ , still amortized and also in expectation. No worst case polylogarithmic time algorithm is known for the fully dynamic case. This paper is concerned with algorithms for MSTs that are both parallel and dynamic. We review a range of existing such algorithms below. Parallel single-update algorithms. Pawagi and Ramakrishnan  give a parallel algorithm for vertex insertion (with an arbitrary number of adjacent edges) and edge-weight updates in Oâ€‹(lgâ¡(n))ğ‘‚lgğ‘›O(\\lg(n)) span but Oâ€‹(n2â€‹lgâ¡(n))ğ‘‚superscriptğ‘›2lgğ‘›O(n^{2}\\lg(n)) work. Varman and Doshi 2 3 improve this to Oâ€‹(nâ€‹lgâ¡(n))ğ‘‚ğ‘›lgğ‘›O(n\\lg(n)) work. Jung and Mehlhorn  give an algorithm for vertex insertion in Oâ€‹(lgâ¡(n))ğ‘‚lgğ‘›O(\\lg(n)) span, and Oâ€‹(n)ğ‘‚ğ‘›O(n) work. While this bound is optimal for dense insertions, i.e.Â inserting a vertex adjacent to Î˜â€‹(n)Î˜ğ‘›\\Theta(n) edges, it is inefficient for sparse graphs. Tsin 1 show how to extend the work of Pawagi and Ramakrishnan  to handle vertex deletions in the same time bounds, thus giving a fully-vertex-dynamic algorithm parallel algorithm in Oâ€‹(lgâ¡(n))ğ‘‚lgğ‘›O(\\lg(n)) span and Oâ€‹(n2â€‹lgâ¡(n))ğ‘‚superscriptğ‘›2lgğ‘›O(n^{2}\\lg(n)) work. Das and Ferragina  give algorithms for inserting and deleting edges in Oâ€‹(lgâ¡(m/n)â€‹lgâ¡(n))ğ‘‚lgğ‘šğ‘›lgğ‘›O(\\lg(m/n)\\lg(n)) span and Oâ€‹(n2/3â€‹lgâ¡(m/n))ğ‘‚superscriptğ‘›23lgğ‘šğ‘›O(n^{2/3}\\lg(m/n)) work. Subsequent improvements by Ferragina  , and Das and Ferragina  improve the span bound to Oâ€‹(lgâ¡(n))ğ‘‚lgğ‘›O(\\lg(n)) with the same work bound. Parallel batch-dynamic algorithms. The above are all algorithms for single vertex or edge updates. To take better advantage of parallelism, some algorithms that process batch updates have been developed.\nPawagi  gives an algorithm for batch vertex insertion that inserts â„“â„“\\ell vertices in Oâ€‹(lgâ¡(n)â€‹lgâ¡(â„“))ğ‘‚lgğ‘›lgâ„“O(\\lg(n)\\lg(\\ell)) span and Oâ€‹(nâ€‹â„“â€‹lgâ¡(n)â€‹lgâ¡(â„“))ğ‘‚ğ‘›â„“lgğ‘›lgâ„“O(n\\ell\\lg(n)\\lg(\\ell)) work. Johnson and Metaxas  give an algorithm for the same problem with Oâ€‹(lgâ¡(n)â€‹lgâ¡(â„“))ğ‘‚lgğ‘›lgâ„“O(\\lg(n)\\lg(\\ell)) span and Oâ€‹(nâ€‹â„“)ğ‘‚ğ‘›â„“O(n\\ell) work. Pawagi and Kaser  were the first to give parallel batch-dynamic algorithms for fully-dynamic MSTs. For inserting â„“â„“\\ell independent vertices, inserting â„“â„“\\ell edges, or decreasing the cost of â„“â„“\\ell edges, their algorithms takes Oâ€‹(lgâ¡(n)â€‹lgâ¡(â„“))ğ‘‚lgğ‘›lgâ„“O(\\lg(n)\\lg(\\ell)) span and Oâ€‹(nâ€‹â„“)ğ‘‚ğ‘›â„“O(n\\ell) work. Their algorithms for increasing the cost of or deleting â„“â„“\\ell edges, or deleting a set of vertices with total degree â„“â„“\\ell take Oâ€‹(lgâ¡(n)+lg2â¡(â„“))ğ‘‚lgğ‘›superscriptlg2â„“O(\\lg(n)+\\lg^{2}(\\ell)) span and Oâ€‹(n2â€‹(1+lg2â¡(â„“)lgâ¡(n)))ğ‘‚superscriptğ‘›21superscriptlg2â„“lgğ‘›O(n^{2}(1+\\frac{\\lg^{2}(\\ell)}{\\lg(n)})) work. Shen and Liang  give an algorithm that can insert â„“â„“\\ell edges, modify â„“â„“\\ell edges, or delete a vertex of degree â„“â„“\\ell in Oâ€‹(lgâ¡(n)â€‹lgâ¡(â„“))ğ‘‚lgğ‘›lgâ„“O(\\lg(n)\\lg(\\ell)) span and Oâ€‹(n2)ğ‘‚superscriptğ‘›2O(n^{2}) work. Ferragina and Luccio   give algorithms for handling â„“=Oâ€‹(n)â„“ğ‘‚ğ‘›\\ell=O(n) edge insertions in Oâ€‹(lgâ¡(n))ğ‘‚lgğ‘›O(\\lg(n)) span and Oâ€‹(nâ€‹lgâ¡lgâ¡lgâ¡(n)â€‹lgâ¡(m/n))ğ‘‚ğ‘›lglglgğ‘›lgğ‘šğ‘›O(n\\lg\\lg\\lg(n)\\lg(m/n)) work, and â„“â„“\\ell edge updates in Oâ€‹(lgâ¡(n)â€‹lgâ¡(m/n))ğ‘‚lgğ‘›lgğ‘šğ‘›O(\\lg(n)\\lg(m/n)) span and Oâ€‹(â„“â€‹nâ€‹lgâ¡lgâ¡lgâ¡(n)â€‹lgâ¡(m/n))ğ‘‚â„“ğ‘›lglglgğ‘›lgğ‘šğ‘›O(\\ell n\\lg\\lg\\lg(n)\\lg(m/n)) work. Lastly, Das and Ferraginaâ€™s algorithm  can be extended to the batch case to handle â„“â„“\\ell edge insertions in O((â„“+lg(m/n)lg(n))O((\\ell+\\lg(m/n)\\lg(n)) span and Oâ€‹(n2/3â€‹(â„“+lgâ¡(m/n)))ğ‘‚superscriptğ‘›23â„“lgğ‘šğ‘›O(n^{2/3}(\\ell+\\lg(m/n))) work. For a thorough and well written survey on the techniques used in many of the above algorithms, see Das and FerraginaÂ . Sliding window dynamic graphs. Dynamic graphs in the sliding window model were studied by Crouch et. alÂ . In\nthe sliding window model, graphs consist of an infinite stream of edges âŸ¨e1,e2,â€¦âŸ©subscriptğ‘’1subscriptğ‘’2â€¦\\langle e_{1},e_{2},...\\rangle, and the goal of queries is to compute\nsome property of the graph over the edges âŸ¨etâˆ’L+1,etâˆ’L+2,â€¦,etâŸ©subscriptğ‘’ğ‘¡ğ¿1subscriptğ‘’ğ‘¡ğ¿2â€¦subscriptğ‘’ğ‘¡\\langle e_{t-L+1},e_{t-L+2},\\dots,e_{t}\\rangle, where tğ‘¡t is the current time and and Lğ¿L is the fixed length\nof the window. Crouch et. al showed that several problems, including\nkğ‘˜k-connectivity, bipartiteness, sparsifiers, spanners, MSFs, and matchings, can be efficiently computed in this model. In particular, several of these results used a data structure for incremental MSF as a key ingredient.\nAll of these results assumed a sequential model of computation.\n\nReference markers found: [31]\n\nContext windows for each [ref]:\n[ref]31: per update using dynamic trees   to find the heaviest weight edge on the cycle induced by the new edge and remove it. Holm et. al. gave the first polylogarithmic time algorithm for fully dynamic MSTÂ [ref]31, supporting updates in Oâ€‹(lg4â¡(n))ğ‘‚superscriptlg4ğ‘›O(\\lg^{4}(n)) amortized time per operation, and later improved by a lgâ¡lgâ¡nlglgğ‘›\\lg\\lg n factorÂ , still amortized and also in expectation. No\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 31,\n            \"titles\": [\n                \"Faster Fully-Dynamic Minimum Spanning Forest\",\n                \"Parallel batch-dynamic graph connectivity\",\n                \"Batch-dynamic algorithms via parallel change propagation and applications to dynamic trees\",\n                \"An experimental analysis of change propagation in dynamic trees\",\n                \"Analyzing graph structure via linear measurements\",\n                \"Maintaining information in fully dynamic trees with top trees\",\n                \"Approximating s-t minimum cuts in Ã•(n22{}^{\\\\mbox{2}}) time\",\n                \"Programming parallel algorithms\",\n                \"Just join for parallel ordered sets\",\n                \"Fast set operations using treaps\",\n                \"O jistÃ©m problÃ©mu minimÃ¡lnÃ­m\",\n                \"Approximating the minimum spanning tree weight in sublinear time\",\n                \"Finding minimum spanning forests in logarithmic time and linear work using random sampling\",\n                \"Dynamic graphs in the sliding-window model\",\n                \"An o(n) work EREW parallel algorithm for updating MST\",\n                \"Parallel Dynamic Algorithms for Minimum Spanning Trees\",\n                \"An EREW PRAM algorithm for updating minimum spanning trees\",\n                \"Maintaining stream statistics over sliding windows\",\n                \"A note on two problems in connexion with graphs\",\n                \"Sparsificationâ€”a technique for speeding up dynamic graph algorithms\",\n                \"An EREW PRAM fully-dynamic algorithm for MST\",\n                \"A technique to speed up parallel fully dynamic algorithms for mst\",\n                \"Batch dynamic algorithms for two graph problems\",\n                \"Three techniques for parallel maintenance of a minimum spanning tree under batch of updates\",\n                \"Data structures for on-line updating of minimum spanning trees, with applications\",\n                \"A general framework for graph sparsification\",\n                \"An optimal randomized parallel algorithm for finding connected components in a graph\",\n                \"Parallel minimum cuts in near-linear work and low depth\",\n                \"Faster algorithms for edge connectivity via random 2-out contractions\",\n                \"Single pass sparsification in the streaming model with edge deletions\",\n                \"Incremental Exact Min-Cut in Poly-logarithmic Amortized Update Time\",\n                \"Poly-logarithmic deterministic fully-dynamic algorithms for connectivity, minimum spanning tree, 2-edge, and biconnectivity\",\n                \"An introduction to parallel algorithms, volumeÂ 17\",\n                \"Optimal algorithms for the vertex updating problem of a minimum spanning tree\",\n                \"Parallel algorithms for computing maximal independent sets in trees and for updating minimum spanning trees\",\n                \"A randomized linear-time algorithm to find minimum spanning trees\",\n                \"Improved Worst-Case Deterministic Parallel Dynamic Minimum Spanning Forest\",\n                \"On the shortest spanning subtree of a graph and the traveling salesman problem\",\n                \"Parallel tree contraction part 1: Fundamentals\",\n                \"A parallel algorithm for multiple updates of minimum spanning trees\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]31 based on the context and research area of Work-efficient Batch-incremental Minimum Spanning Trees with Applications to the Sliding Window Model. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}