{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: High Bandwidth Memory on FPGAs: A Data Analytics Perspective Kaan Kara12, Christoph Hagleitner2, Dionysios Diamantopoulos2, Dimitris Syrivelis2, Gustavo Alonso1\n\nPaper Abstract: \n\nPaper Section Text:\nPerforming machine learning (ML) within a DBMS is becoming increasingly important, with all major vendors now offering this functionalityÂ   [ref]32.\nSince ML has different requirements than traditional analytics, recent work has focused on providing accelerators to perform in-database ML efficientlyÂ   . In this paper, we focus on training generalized linear models (e.g., linear/logistic regression) with SGD. Recent work shows the advantage of using FPGAsÂ : The SGD accelerator can be designed as a fully pipelined dataflow architecture, saturating the memory bandwidth (6.5Â GB/s) on their target platform (Xeon+FPGAÂ ). HBM has therefore a large potential for improving SGD with a scale-out architecture on the FPGA and provide an even more pronounced advantage over general purpose processors. SGD is an optimization algorithm that minimizes problems of the following form: where (ğš1,b1),â€¦,(ğšm,bm)âˆˆ([âˆ’1,1]nÃ—â„)subscriptğš1subscriptğ‘1â€¦subscriptğšğ‘šsubscriptğ‘ğ‘šsuperscript11ğ‘›â„(\\mathbf{a}_{1},b_{1}),...,(\\mathbf{a}_{m},b_{m})\\in([-1,1]^{n}\\times\\mathbb{R}) is a set of samples and J:â„nÃ—â„â†’[0,âˆ):ğ½â†’superscriptâ„ğ‘›â„0J:\\mathbb{R}^{n}\\times\\mathbb{R}\\rightarrow[0,\\infty) is a non-negative convex loss function.\nSGD is shown in AlgorithmÂ 3. FPGA Implementation. \nWe implement FPGA-based SGD following a similar architecture as related workÂ , shown in FigureÂ 9.\nThe dataset is scanned Nğ‘N times (number of epochs) and forwarded to the compute modules.\nThese modules (Dot, ScalarEngine, and Update) work in a parallel dataflow fashion, as illustrated in FigureÂ 9, ensuring full utilization. Unlike Kara et al.Â  we do not allow stale updates to the model that can happen due to the latency of the entire pipeline and read-after-write dependencies (between lines 4 and 7).\nWe thus ensure high quality convergence, but have to accept a lower processing rate for low-dimensional datasets and lower minibatch sizes, since the entire pipeline as shown in FigureÂ 9 cannot be filled in those cases.\nNevertheless, considering the processing rate of only one engine, even in the worst case we match Kara et al.Â  (6.5 GB/s) and exceed it by 1.7x in the best case, thanks to the higher bandwidth we get from one HBM-shim port.\nWe use one bitstream with 14 SGD engines in the system architecture (FigureÂ 3); all engines capable of performing SGD in parallel, reading data from the HBM and writing the trained model back. Evaluation. \nWe evaluate the SGD implementation using the datasets listed in TableÂ II, considering the popular hyperparameter search use case occurring frequently in real world scenariosÂ : Multiple models are trained on the same dataset, however with differing hyperparameters, to find the best set that achieves the highest test score.\nThis problem is trivially parallel, but requires many processors and is usually performed using entire clusters.\nOur goal is to show that 1 FPGA with an HBM can replace multiple CPUs for performing this work, in an in-database ML setting.\nSGD is an iterative algorithm: data is read multiple times; so the initial copy cost from the CPU to the FPGA is negligible (<<1% of total runtime). Evaluation: Processing rate. FigureÂ 10(a) shows how the processing rate scales with the number of parallel jobs, for training the last layer of an InceptionV3 neural networkÂ [ref]36 for a binary classification problem, using the IM dataset. There are 28 hyperparameter configurations to search, resulting in 28 training jobs, each can be performed in parallel.\nThe plotted processing rate is calculated by dividing the total size of the consumed data (number of epochs Ã—\\times dataset size) by the end-to-end runtime (including copying of the trained model to CPUâ€™s memory).\nWe observe that the FPGA scales until 14, since there are 14 engines, reaching 156Â GB/s processing rate at the peak. When using 28 parallel threads, the XeonE5 can reach maximum 34Â GB/s while POWER9 reaches 49Â GB/s. As in the microbenchmarks (SectionÂ II), how the data is accessed is incredibly important for HBM performance: If there is only one copy of the dataset, it creates a bottleneck when accessing the HBM from 14 engines in parallel, diminishing the HBM advantage (FigureÂ 10(a)): FPGA-nonreplicated rate stays at 12.8Â GB/s and does not scale with the number of engines used.\nTo utilize the HBM, we have to replicate the dataset such that each engine can access its own physical memory channel without utilizing the crossbar.\nThe replication will not be possible if the dataset is larger than 512Â MiB (the size of 2 physical memory channels on the HBM, exposed as an AXI3 port by the HBM-shim in SectionÂ III).\nHowever, in that case a blockwise scan approach can be followedÂ :\nA block of the dataset that fits 512Â MiB is read for multiple epochs and then gets exchanged for the next block, and so on. This reduces the IO complexity between the CPU and the FPGA while replication leads to high bandwidth from HBM. Evaluation: Effect of dimensionality. In FigureÂ 10(b) we show how the processing rates across platforms and datasets differ. Lower dimensional datasets (AEA) lead to lower processing rate per engine because the SGD pipeline (FigureÂ 9) cannot be kept at 100% utilization. The reason is that we keep the read-after-write (RAW) dependency (lines 4 and 7 in AlgorithmÂ 3) in SGD, causing bubbles during the processing if the dataset is low dimensional. In certain cases, the RAW dependency can be ignored in FPGA-based implementationsÂ , since SGD tends to be tolerant against constant stale updates.\nHowever, the tolerance is not guaranteed especially if the samples are ordered in a certain fashion, so we choose to respect the RAW dependency to get high quality convergence. Evaluation: Minibatch size. FigureÂ 11 shows the convergence over time for minibatch sizes used during SGD on the FPGA.\nLarger minibatch sizes increase pipeline utilization, leading to higher processing rate and therefore faster convergence.\nThe goal here is to show that we can converge to the same loss, when increasing the minibatch size from 1 to 16, while achieving significant speedup.\nAccordingly, we use 16 in all SGD experiments both on the CPU and the FPGA (FigureÂ 9).\n\nReference markers found: [32]\n\nContext windows for each [ref]:\n[ref]32: Performing machine learning (ML) within a DBMS is becoming increasingly important, with all major vendors now offering this functionalityÂ   [ref]32.\nSince ML has different requirements than traditional analytics, recent work has focused on providing accelerators to perform in-database ML efficientlyÂ   . In this paper, we focus on\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 32,\n            \"titles\": [\n                \"In-RDBMS Hardware Acceleration of Advanced Analytics\",\n                \"In-Datacenter Performance Analysis of a Tensor Processing Unit\",\n                \"A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services\",\n                \"Serving DNNs in Real Time at Datacenter Scale with Project Brainwave\",\n                \"IBM POWER9 Opens Up a New Era of Acceleration Enablement: OpenCAPI\",\n                \"AWS F1 Instances\",\n                \"Baidu FPGA Instances\",\n                \"A Configurable Cloud-Scale DNN Processor for Real-Time AI\",\n                \"FINN: A Framework for Fast, Scalable Binarized Neural Network Inference\",\n                \"FPGA-accelerated Dense Linear Machine Learning: A Precision-Convergence Trade-Off\",\n                \"Hardware Acceleration of Database Operations\",\n                \"FPGA-based Data Partitioning\",\n                \"Consensus in a Box: Inexpensive Coordination in Hardware\",\n                \"Accelerating Generalized Linear Models with MLWeaving: A One-Size-Fits-All System for Any-Precision Learning\",\n                \"Scalable Inference of Decision Tree Ensembles: Flexible Design for CPU-FPGA Platforms\",\n                \"Accelerating Pattern Matching Queries in Hybrid CPU-FPGA Architectures\",\n                \"Xilinx Ultrascale+ Devices\",\n                \"Xilinx VCU1525\",\n                \"A Reconfigurable Computing System Based on a Cache-Coherent Fabric\",\n                \"Main-memory hash joins on multi-core cpus: Tuning to the underlying hardware\",\n                \"Convex Optimization: Algorithms and Complexity\",\n                \"AXI HBM IP Documentation by Xilinx\",\n                \"Alpha Data ADM-PCIE-9H7\",\n                \"Intel Xeon Processor E5-2690 v4\",\n                \"MonetDB: Two Decades of Research in Column-Oriented Database Architectures\",\n                \"doppiodb 1.0: Machine learning inside a relational engine\",\n                \"doppiodb 2.0: hardware techniques for improved integration of machine learning into databases\",\n                \"GPU Join Processing Revisited\",\n                \"Database architecture optimized for the new bottleneck: Memory access\",\n                \"Tpc-h analyzed: Hidden messages and lessons learned from an influential benchmark\",\n                \"IBM DB2 Machine Learning\",\n                \"Oracle Data Mining\",\n                \"Predictive Analysis with SAP: The Comprehensive Guide\",\n                \"Columnml: column-store machine learning with on-the-fly data transformation\",\n                \"Random search for hyper-parameter optimization\",\n                \"Rethinking the Inception Architecture for Computer Vision\",\n                \"Communication-Efficient Distributed Dual Coordinate Ascent\",\n                \"Achronix Speedster7t\",\n                \"Design space exploration for 3D-stacked DRAMs\",\n                \"Knights Landing: Second-Generation Intel Xeon Phi Product\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]32 based on the context and research area of High Bandwidth Memory on FPGAs: A Data Analytics Perspective Kaan Kara12, Christoph Hagleitner2, Dionysios Diamantopoulos2, Dimitris Syrivelis2, Gustavo Alonso1. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}