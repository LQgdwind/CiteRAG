{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives\n\nPaper Abstract: \n\nPaper Section Text:\nOn the other hand,\nas the community continues to explore the technical advancements of these large models, there is a growing recognition of their potential limitations, fueling concerns and driving further investigation. Given the sheer volume of the data used to train the large pretrained models,\nit will be hard to test the models’ robustness performances\nagainst traditionally defined out-of-domain data:\nsince the models might have already seen the domains during training. However, in certain application fields, where the data are of a different nature to the daily language and images,\nthese large models are still suffering a fairly large performance drop,\nalthough still better than the previous standalone models. For example, the community has shown limited performances of SAM over medical images 0 ,\nof GPT over medical texts [ref]445 ,\nof Stable Diffusion  over medical images and text .\nOther domains, such as legal texts \nand financial texts  are also facing challenges to directly applying the generic large models.\nThe challenges of applying pre-trained models to specific domains have become significant, and there are surveys dedicated to the discussions . With these specific challenges of the model’s understanding over the knowledge in specific domains, it seems fine-tuning the pretrained models on these specific domains is a natural solution.\nHowever, it seems a direct fine-tuning of the pretrained models\non certain datasets only appear to solve the problems\nbut will reveal more weakness when more sophisticated testing are performed  ,\nwhich is potentially due to the\ncatastrophic forgetting issue of large models ,\nor in general, machine learning models . It is widely observed that machine learning models frequently generate outputs that perpetuate stereotypes and discrimination against marginalized groups.\nThese models not only reflect, but sometimes exacerbate the biases present in the dataset . When it comes to these large pre-trained models, these issues are seemingly getting worse, as the construction of larger models typically requires a larger volume of data, which can result in reduced data quality. The issues exist in both discriminative models and generative models.\nA classifier may erroneously exploit demographic information for prediction   ,\nand a text generation model could directly express prejudiced views against\ncertain demographic groups   . For instance, bias pertaining to race, gender, and other demographic attributes has been discovered in models such as GPT-2  and BERT .  identified persistent bias towards Muslim people in GPT-3.  discovered gender bias in earlier models like ELMO 5, and  identified gender stereotypes in more recent models like GPT-3. Although the RLHF technique improves the model’s sentiment towards all social groups, it does not appear to reduce the disparities among them . Furthermore, biases have been assessed in pre-trained models across different languages  51, and systematic evaluations have identified bias pertaining to nine demographic attributes in masked language models  . These studies reveal the significant and widespread nature of bias in pre-trained language models . The papers discussing such issues are more than what our survey can cover. For a more comprehensive overview, one can refer to other surveys dedicated to discussions of the critical need for fairness in pre-trained language models 4  .\nSome studies   [ref]282 focused on improving fairness in PLMs.\nFor instance,  eliminated gender bias using instruction text.\n alleviated the transfer of bias from PLMs to student models during the distillation process.\n[ref]282 employed the approach of PEFT to eliminate biases from the model.\nMeanwhile, several studies such as  have focused on enhancing bias quantification metrics.\nIt has been discovered that current metrics inherently incorporate bias  and are occasionally unreliable . Another challenge for large pre-trained models is hallucination.\nIn general, it refers to the\nstate that the machine learning models,\nin particular, natural language generation models,\ncan generate texts that are plausibly correct,\nbut wrong . While  associated hallucination with the inability of the model to understand domain-specific knowledge,\nthe broader scope of the community tend to consider it more generic.\nFor example,\nit can happen in many generic text-generation task\nsuch as machine translation \nand\nimage captioning  2. More formally, hallucination is defined as “the generated content that is\nnonsensical or unfaithful to the provided source content”  .\nThere are also different definitions available 13   .\nIn summary, we believe the key differences of these definitions lie in the agreement on where the factuality should be aligned to,\nwhich again aligns with our general theme of trustworthy machine learning in this paper: the complete definition must be specified by the stakeholders.\n\nReference markers found: [445]\n\nContext windows for each [ref]:\n[ref]445: large performance drop,\nalthough still better than the previous standalone models. For example, the community has shown limited performances of SAM over medical images 0 ,\nof GPT over medical texts [ref]445 ,\nof Stable Diffusion  over medical images and text .\nOther domains, such as legal texts \nand financial texts  are also facing challenges to directly applying the generic large models.\nThe\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 445,\n            \"titles\": [\n                \"Evaluating the visualization of what a Deep Neural Network has learned\",\n                \"Explaining individual predictions when features are dependent: More accurate approximations to shapley values\",\n                \"Deep Learning with Differential Privacy\",\n                \"Counterfactual vision and language learning\",\n                \"Gpt-3-driven pedagogical agents to train children’s curious question-asking skills\",\n                \"Persistent anti-muslim bias in large language models\",\n                \"Sanity checks for saliency maps\",\n                \"One-network adversarial fairness\",\n                \"Bias-resilient neural network\",\n                \"Learning optimal and fair decision trees for non-discriminative decision-making\",\n                \"Intrinsic dimensionality explains the effectiveness of language model fine-tuning\",\n                \"Vqa: Visual question answering\",\n                \"Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization\",\n                \"Domain-Adversarial Neural Networks\",\n                \"Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey\",\n                \"CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models\",\n                \"Adversarial invariant feature learning with accuracy constraint for domain generalization\",\n                \"What learning algorithm is in-context learning? Investigations with linear models\",\n                \"Flamingo: a visual language model for few-shot learning\",\n                \"Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning\",\n                \"Advanced econometrics\",\n                \"Uncovering and mitigating algorithmic bias through learned latent structure\",\n                \"Estimation of the parameters of a single equation in a complete system of stochastic equations\",\n                \"Square Attack: a query-efficient black-box adversarial attack via random search\",\n                \"Exploring Length Generalization in Large Language Models\",\n                \"PaLM 2 Technical Report\",\n                \"Invariant risk minimization\",\n                \"Logic-guided data augmentation and regularization for consistent question answering\",\n                \"A General Language Assistant as a Laboratory for Alignment\",\n                \"Craft: A benchmark for causal reasoning about forces and interactions\",\n                \"Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\",\n                \"Understanding confounding and mediation\",\n                \"On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation\",\n                \"An Actor-Critic Algorithm for Sequence Prediction\",\n                \"Learning to Understand Goal Specifications by Modelling Reward\",\n                \"Learning de-biased representations with biased representations\",\n                \"Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022\",\n                \"Instrumental variable methods for causal inference\",\n                \"Your fairness may vary: Pretrained language model fairness in toxic text classification\",\n                \"Adversarial transformation networks: Learning to generate adversarial examples\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]445 based on the context and research area of Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}