{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Locally Sampleable Uniform Symmetric Distributions\n\nPaper Abstract: \n\nPaper Section Text:\nNow we show how to handle a general uniform symmetric distribution and obtain our classification result.\nFor convenience, we use a non-empty set Ψ⊆{0,1,…,n}Ψ01…𝑛\\Psi\\subseteq\\left\\{0,1,\\ldots,n\\right\\} to denote the acceptable Hamming weights and use 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} to denote the uniform distribution over strings of Hamming weights in ΨΨ\\Psi.\nThen our goal is to show that local functions cannot approximate 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} unless ΨΨ\\Psi is {0}0\\left\\{0\\right\\} (the point distribution on 0nsuperscript0𝑛0^{n}), {n}𝑛\\left\\{n\\right\\} (the point distribution on 1nsuperscript1𝑛1^{n}), {0,n}0𝑛\\left\\{0,n\\right\\} (uniform over {0n,1n}superscript0𝑛superscript1𝑛\\left\\{0^{n},1^{n}\\right\\}), {0,2,4,…}024…\\left\\{0,2,4,\\ldots\\right\\} (uniform over strings with even parity), {1,3,5,…}135…\\left\\{1,3,5,\\ldots\\right\\} (uniform over strings with odd parity), or {0,1,…,n}01…𝑛\\left\\{0,1,\\ldots,n\\right\\} (uniform over all strings). We will often refer to the corresponding 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} as the six special distributions. Let s∈Ψ𝑠Ψs\\in\\Psi be an element closest to the middle weight n/2𝑛2n/2. Note the majority of the mass of 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} is supported on strings roughly as close to n/2𝑛2n/2 as s𝑠s is.\nInformally, we view 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} as either 𝒟ssubscript𝒟𝑠\\mathcal{D}_{s} (uniform over the Hamming slice of weight s𝑠s) or 12​𝒟s+12​𝒟n−s12subscript𝒟𝑠12subscript𝒟𝑛𝑠\\frac{1}{2}\\mathcal{D}_{s}+\\frac{1}{2}\\mathcal{D}_{n-s} (uniform over the Hamming slices of weight s𝑠s and n−s𝑛𝑠n-s).\nThen the above six locally sampleable distributions can be classified by s𝑠s: either it is the endpoint (i.e., s𝑠s equals 00 or n𝑛n) or it is the middle point (i.e., s𝑠s roughly equals n/2𝑛2n/2).\nOur proof follows this intuition. If s=o​(n)𝑠𝑜𝑛s=o(n) or s=(1−o​(1))​n𝑠1𝑜1𝑛s=(1-o(1))n, we will show that it must be the case of s∈{0,n}𝑠0𝑛s\\in\\left\\{0,n\\right\\}.\nOtherwise o​(n)≪s≪(1−o​(1))​nmuch-less-than𝑜𝑛𝑠much-less-than1𝑜1𝑛o(n)\\ll s\\ll(1-o(1))n, and we will show that it must be the case of s≈n/2𝑠𝑛2s\\approx n/2 and ΨΨ\\Psi is effectively all-even, all-odd, or everything. The s=o​(n)𝑠𝑜𝑛s=o(n) or s=(1−o​(1))​n𝑠1𝑜1𝑛s=(1-o(1))n case is essentially handled by . For completeness, we give a simple (albeit quantitatively weaker) argument that suffices for our purposes. Our treatment (Theorem 4.3) is similar to  and is presented in Appendix B. Now we turn to the more interesting case where o​(n)≪s≪(1−o​(1))​nmuch-less-than𝑜𝑛𝑠much-less-than1𝑜1𝑛o(n)\\ll s\\ll(1-o(1))n and aim to show that 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} is essentially uniform over even parities, odd parities, or everything.\nIn this scenario, since s𝑠s is relatively far from the boundaries, the framework explained in Subsection 2.1 is now applicable. However, we need a number of new ideas since ΨΨ\\Psi is unstructured in general.\nFor illustration and to introduce these new ingredients, we start with some concrete examples. The first illustrating example is when ΨΨ\\Psi contains opposite Hamming slices, both of which are significant.\nFor concreteness, consider Ψ={n/3,2​n/3}Ψ𝑛32𝑛3\\Psi=\\left\\{n/3,2n/3\\right\\}.\nTo rule out this possibility, we follow the previous outline to reduce the local function f𝑓f to more structured local functions where one has many small neighborhoods that are not connected.\nAssume after this graph elimination process, we obtained a local function g𝑔g with many non-connected neighborhoods of small sizes.\nSince the marginal distribution of coordinates of 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} is an unbiased coin, one may attempt to classify these neighborhoods by their distance to unbiased distributions as in Subsection 2.1; then use their independence to derive a 1−e−Ωd​(n)1superscript𝑒subscriptΩ𝑑𝑛1-e^{-\\Omega_{d}(n)} bound when they are far from being unbiased,\nand use anticoncentration inequalities to show that the output of g𝑔g rarely hits the desired Hamming weights when they are almost unbiased. The second case indeed works. The first case, however, unfortunately fails, the reason for which is that g𝑔g being far from unbiased does not necessarily mean that f𝑓f cannot sample from 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi}.\nFor example, f𝑓f could use one uniform bit to choose to sample from 𝒟n/3subscript𝒟𝑛3\\mathcal{D}_{n/3} or 𝒟2​n/3subscript𝒟2𝑛3\\mathcal{D}_{2n/3}, where g𝑔g only needs to handle the former without the need to approximate 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} itself.\nThough this example is artificial since   have ruled out local sampling schemes for 𝒟n/3subscript𝒟𝑛3\\mathcal{D}_{n/3}, the argument is oblivious to this and cannot work for more contrived examples that are similar in spirit to the above one. To get around this, we observe that though the individual marginals of 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} are unbiased, their joint distribution is far from any biased distribution (in fact, any product distribution, but focusing only on biased ones is sufficient for us).\nHence we alter the criteria of our two cases for handling the behavior on marginals. We call a neighborhood Type-1 if it is not close to any biased distribution, and a neighborhood Type-2 if it is close to some biased distribution. (Note that these definitions are slightly different from those used in .)\nAt this point, if g𝑔g has many small non-connected Type-1 neighborhoods, then it is (1−e−Ωd​(n))1superscript𝑒subscriptΩ𝑑𝑛(1-e^{-\\Omega_{d}(n)})-far from both 𝒟n/3subscript𝒟𝑛3\\mathcal{D}_{n/3} and 𝒟2​n/3subscript𝒟2𝑛3\\mathcal{D}_{2n/3}, which, by a union bound, means it is also (1−e−Ωd​(n))1superscript𝑒subscriptΩ𝑑𝑛(1-e^{-\\Omega_{d}(n)})-far from any mixture of 𝒟n/3,𝒟2​n/3subscript𝒟𝑛3subscript𝒟2𝑛3\\mathcal{D}_{n/3},\\mathcal{D}_{2n/3} and, particularly, 𝒟Ψ=𝒟{n/3,2​n/3}subscript𝒟Ψsubscript𝒟𝑛32𝑛3\\mathcal{D}_{\\Psi}=\\mathcal{D}_{\\left\\{n/3,2n/3\\right\\}}.\nOn the other hand, we can similarly argue that the Hamming weight of a Type-2 neighborhood is a random variable with noticeable variance, since the Hamming weight of the biased distribution that it implicitly approximates has such a property.\nTherefore, if g𝑔g has many small non-connected Type-2 neighborhoods, the previous argument using anticoncentration inequalities still works. We remark that technically, the Hamming weight of a biased distribution has noticeable variance only if the bias itself is not extremely close to 00 or 111.\nFortunately we can ignore those (Claim 4.10) by truncating the tail of ΨΨ\\Psi. Since o​(n)≪s≪(1−o​(1))​nmuch-less-than𝑜𝑛𝑠much-less-than1𝑜1𝑛o(n)\\ll s\\ll(1-o(1))n, this does not incur much error. In fact, this is the only place where we actually use the assumption that o​(n)≪s≪(1−o​(1))​nmuch-less-than𝑜𝑛𝑠much-less-than1𝑜1𝑛o(n)\\ll s\\ll(1-o(1))n. The second interesting example is the majority case where 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} is uniform over strings that have more ones than zeros, i.e., Ψ={n/2+1,n/2+2,…,n}Ψ𝑛21𝑛22…𝑛\\Psi=\\left\\{n/2+1,n/2+2,\\ldots,n\\right\\}.\nNote that this distribution is not close to any of the six special distributions, and thus, if we believe that the classification result is indeed true, it cannot be approximated by local distributions.\nResults are known on distinguishing this distribution with local functions [ref]17  ; however, their arguments do not seem to generalize. To prove that any local function’s output is far from 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi}, we still follow the previous outline to obtain a d𝑑d-local function g𝑔g that has n/Cd𝑛subscript𝐶𝑑n/C_{d} non-connected neighborhoods of size Cdsubscript𝐶𝑑C_{d}, where Cd≫1much-greater-thansubscript𝐶𝑑1C_{d}\\gg 1 is a large constant depending only on d𝑑d from the graph elimination results.\nAs before, if many of these neighborhoods are Type-1, then we readily obtain (Lemma 4.12) a 1−e−Ωd​(n)1superscript𝑒subscriptΩ𝑑𝑛1-e^{-\\Omega_{d}(n)} distance bound by their independence.\nTo handle the case where most of these neighborhoods are Type-2, we recall that past work   used anticoncentration inequalities\nto show that the local distribution cannot hit the support of the desired distribution with as much mass as it is supposed to. However this simply does not work, since ΨΨ\\Psi covers a consecutively wide range of Hamming weights that significantly exceeds the amount of independence we could use in g𝑔g.\nMore precisely, g𝑔g only has n/Cd𝑛subscript𝐶𝑑n/C_{d} independent small neighborhoods, hence their Hamming weight sum is essentially a discrete Gaussian of variance Ωd​(n)subscriptΩ𝑑𝑛\\Omega_{d}(n).\nDue to the random shift from other neighborhoods that we cannot control, the support of this distribution can, in the worst case, completely lie in ΨΨ\\Psi which has range n/2𝑛2n/2.\nTherefore, we cannot hope to establish a similar result saying with high probability the output of g𝑔g does not hit the support of 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi}. To circumvent this, we explore the sharp threshold phenomenon in ΨΨ\\Psi. Observe that the range of ΨΨ\\Psi, though wide, has a sharp cutoff at Hamming weight n/2𝑛2n/2. This should not happen in a distribution that behaves like a Gaussian, unless the cutoff appears at the tail.\nIn other words, we expect to see that in the probability density function of the Hamming weight of the output of g𝑔g, the mass on the right boundary of n/2𝑛2n/2 does not differ from the mass on the left boundary of n/2𝑛2n/2, in contrast to 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi}.\nTo be more precise, we consider the potential function where Cd′superscriptsubscript𝐶𝑑′C_{d}^{\\prime} is another large constant depending on d𝑑d.\nLet ϕ1subscriptitalic-ϕ1\\phi_{1} be ϕitalic-ϕ\\phi when X∼𝒟Ψsimilar-to𝑋subscript𝒟ΨX\\sim\\mathcal{D}_{\\Psi} and ϕ2subscriptitalic-ϕ2\\phi_{2} be ϕitalic-ϕ\\phi when X𝑋X is the output of g𝑔g.\nThen obviously ϕ1subscriptitalic-ϕ1\\phi_{1} is roughly 111.\nHowever ϕ2subscriptitalic-ϕ2\\phi_{2} should be od​(1)subscript𝑜𝑑1o_{d}(1): since g𝑔g has many small non-connected Type-2 neighborhoods, the Hamming weight distribution of g𝑔g can be shown to be a Gaussian-like distribution with variance n/Cd𝑛subscript𝐶𝑑n/C_{d}.\nIf the mean of the distribution is ≪n/2+Θd​(n)much-less-thanabsent𝑛2subscriptΘ𝑑𝑛\\ll n/2+\\Theta_{d}(\\sqrt{n}), then ϕ2subscriptitalic-ϕ2\\phi_{2} is negative since Cd′superscriptsubscript𝐶𝑑′C_{d}^{\\prime} is sufficiently large and the two intervals in the definition are consecutive.\nOn the other hand if the mean is ≫n/2+Θd​(n)much-greater-thanabsent𝑛2subscriptΘ𝑑𝑛\\gg n/2+\\Theta_{d}(\\sqrt{n}), then ϕ2subscriptitalic-ϕ2\\phi_{2} is od​(1)subscript𝑜𝑑1o_{d}(1) by standard concentration for the left-hand side of ϕ2subscriptitalic-ϕ2\\phi_{2}.\nIn summary, this shows that ϕ1−ϕ2subscriptitalic-ϕ1subscriptitalic-ϕ2\\phi_{1}-\\phi_{2} is always at least 1−od​(1)1subscript𝑜𝑑11-o_{d}(1), which implies a 1/Cd′1superscriptsubscript𝐶𝑑′1/C_{d}^{\\prime} distance bound since ϕitalic-ϕ\\phi has range bounded by Cd′superscriptsubscript𝐶𝑑′C_{d}^{\\prime}. Later it is put together with the Type-1 analysis by a union bound. Let 𝒫Ψsubscript𝒫Ψ\\mathcal{P}_{\\Psi} be the distribution over Hamming weights of strings drawn from 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi}.\nTo extend the analysis of the above majority distribution to a general ΨΨ\\Psi, the key point is to discover the threshold phenomenon in ΨΨ\\Psi and to construct the corresponding potential function to separate 𝒫Ψsubscript𝒫Ψ\\mathcal{P}_{\\Psi} from a discrete Gaussian 𝒫𝒫\\mathcal{P} with variance Θ​(n)Θ𝑛\\Theta(n) and an unknown mean.\nTo this end, we follow the same approach to select some Hamming weights S⊆Ψ𝑆ΨS\\subseteq\\Psi and T⊆{0,1,…,n}∖Ψ𝑇01…𝑛ΨT\\subseteq\\left\\{0,1,\\ldots,n\\right\\}\\setminus\\Psi, then define To ensure ϕitalic-ϕ\\phi is large under 𝒫Ψsubscript𝒫Ψ\\mathcal{P}_{\\Psi}, S𝑆S needs to take a significant amount of mass in 𝒫Ψsubscript𝒫Ψ\\mathcal{P}_{\\Psi}.\nTo ensure ϕitalic-ϕ\\phi is small under 𝒫𝒫\\mathcal{P}, S𝑆S and T𝑇T should not be too far apart (Lemma 3.9), i.e., elements in S𝑆S should be paired with distinct elements in T𝑇T such that the distance of each pair is not too large.\nGuided by this intuition, we construct S𝑆S and T𝑇T in a greedy way: we iteratively pick out s∈Ψ,t∉Ψformulae-sequence𝑠Ψ𝑡Ψs\\in\\Psi,t\\notin\\Psi to update S,T𝑆𝑇S,T, where the selected pair needs to minimize the distance |s−t|𝑠𝑡|s-t|.\nIf we select K𝐾K pairs in total, then the absolute distance between each pair is O​(K)𝑂𝐾O(K) by a simple induction argument and the minimality of the distance of each pair upon selection.\nTherefore, we only need to make sure K𝐾K is small while ϕitalic-ϕ\\phi is large, say at least δ𝛿\\delta, under 𝒫Ψsubscript𝒫Ψ\\mathcal{P}_{\\Psi}. For this purpose, we truncate the δ𝛿\\delta-tail and only look for (s,t)𝑠𝑡(s,t) pairs thereinto.\nFormally, let 0≤ℓ≤r≤n0ℓ𝑟𝑛0\\leq\\ell\\leq r\\leq n be such that Hamming weights in [ℓ,r]ℓ𝑟[\\ell,r] cover 1−δ1𝛿1-\\delta fraction of 2nsuperscript2𝑛2^{n}.\nThen we iteratively pick out s∈Ψ∩[ℓ,r]𝑠Ψℓ𝑟s\\in\\Psi\\cap[\\ell,r] and t∈[ℓ,r]∖Ψ𝑡ℓ𝑟Ψt\\in[\\ell,r]\\setminus\\Psi while minimizing their distance |s−t|𝑠𝑡|s-t| until ϕ≫δmuch-greater-thanitalic-ϕ𝛿\\phi\\gg\\delta under 𝒫Ψsubscript𝒫Ψ\\mathcal{P}_{\\Psi}.\nSimilarly, if we selected K𝐾K pairs, the distance of each pair is at most O​(K)𝑂𝐾O(K).\nIn addition, since we have removed the δ𝛿\\delta-tail, we can show (Claim 4.17) that every Hamming slice in Ψ∩[ℓ,r]Ψℓ𝑟\\Psi\\cap[\\ell,r] has probability mass ≫δ​log⁡(1/δ)nmuch-greater-thanabsent𝛿1𝛿𝑛\\gg\\frac{\\delta\\sqrt{\\log(1/\\delta)}}{\\sqrt{n}} under the uniform distribution (and thus under 𝒫Ψsubscript𝒫Ψ\\mathcal{P}_{\\Psi} by its definition).\nTherefore K≪n/log⁡(1/δ)much-less-than𝐾𝑛1𝛿K\\ll\\sqrt{n/\\log(1/\\delta)}.\nCrucially, this additional log factor helps us in upper bounding ϕitalic-ϕ\\phi under the discrete Gaussian 𝒫𝒫\\mathcal{P}.\nLet Δ=s−n/2Δ𝑠𝑛2\\Delta=s-n/2. Then Thus if Δ≫n​log⁡(1/δ)much-greater-thanΔ𝑛1𝛿\\Delta\\gg\\sqrt{n\\log(1/\\delta)}, then 𝒫​(s)−Cd′⋅𝒫​(t)≤𝒫​(s)≪δ/n𝒫𝑠⋅superscriptsubscript𝐶𝑑′𝒫𝑡𝒫𝑠much-less-than𝛿𝑛\\mathcal{P}(s)-C_{d}^{\\prime}\\cdot\\mathcal{P}(t)\\leq\\mathcal{P}(s)\\ll\\delta/\\sqrt{n}, otherwise Δ≪n​log⁡(1/δ)much-less-thanΔ𝑛1𝛿\\Delta\\ll\\sqrt{n\\log(1/\\delta)} and 𝒫​(s)−Θ​(1)⋅𝒫​(t)≈(2−Δ2/n−Θ​(1)⋅2−(Δ+K)2/n)/n<0𝒫𝑠⋅Θ1𝒫𝑡superscript2superscriptΔ2𝑛⋅Θ1superscript2superscriptΔ𝐾2𝑛𝑛0\\mathcal{P}(s)-\\Theta(1)\\cdot\\mathcal{P}(t)\\approx\\left(2^{-\\Delta^{2}/n}-\\Theta(1)\\cdot 2^{-(\\Delta+K)^{2}/n}\\right)/\\sqrt{n}<0, which implies that ϕ≪K⋅δ/n≪δmuch-less-thanitalic-ϕ⋅𝐾𝛿𝑛much-less-than𝛿\\phi\\ll K\\cdot\\delta/\\sqrt{n}\\ll\\delta under 𝒫𝒫\\mathcal{P}. One caveat here is that the actual Hamming weight distribution of the sum of small non-connected Type-2 neighborhoods may embed a periodic pattern.\nFor example, 𝒫𝒫\\mathcal{P} may be a discrete Gaussian on even numbers, which is exactly the case in the locally sampleable distribution 𝒟{0,2,4,…}subscript𝒟024…\\mathcal{D}_{\\left\\{0,2,4,\\ldots\\right\\}}.\nLuckily, by the argument for ruling out periodic Hamming slices in , periods other than two are also ruled out for 𝒫𝒫\\mathcal{P} (Lemma 4.13).\nTherefore, we only need to additionally make sure the selected pairs (s,t)𝑠𝑡(s,t) have an even distance (Lemma 4.16), since otherwise we may have 𝒫​(s)≈1/n𝒫𝑠1𝑛\\mathcal{P}(s)\\approx 1/\\sqrt{n} but 𝒫​(t)=0𝒫𝑡0\\mathcal{P}(t)=0 when s𝑠s and t𝑡t have different parities. Combining the above arguments, we can prove a weaker classification result: if the local distribution 𝒫𝒫\\mathcal{P} is ε𝜀\\varepsilon-close to 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} where there exists some s∈Ψ𝑠Ψs\\in\\Psi such that o​(n)≪s≪(1−o​(1))​nmuch-less-than𝑜𝑛𝑠much-less-than1𝑜1𝑛o(n)\\ll s\\ll(1-o(1))n, then it is (Od​(ε)+e−Θd​(n))subscript𝑂𝑑𝜀superscript𝑒subscriptΘ𝑑𝑛\\left(O_{d}(\\varepsilon)+e^{-\\Theta_{d}(n)}\\right)-close to the uniform distribution or the uniform distribution over even / odd Hamming weights.\nThe additional exponentially small factor comes from analyzing Type-2 neighborhoods (Lemma 4.13), where an additive e−Θd​(n)superscript𝑒subscriptΘ𝑑𝑛e^{-\\Theta_{d}(n)} is used to rule out the atypical case that the Hamming weight distribution is not Gaussian-like due to correlations outside those non-connected Type-2 neighborhoods.\nWhile this seems inevitable in our current analysis framework, this factor is minor when ε𝜀\\varepsilon is not extremely small.\nTo further shave it, we explore the structure of d𝑑d-local functions from different angles. Let ε≪e−Θd​(n)much-less-than𝜀superscript𝑒subscriptΘ𝑑𝑛\\varepsilon\\ll e^{-\\Theta_{d}(n)}.\nAssume our distribution 𝒫𝒫\\mathcal{P} is ε𝜀\\varepsilon-close to 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} and is (Od​(ε)+e−Θd​(n))subscript𝑂𝑑𝜀superscript𝑒subscriptΘ𝑑𝑛\\left(O_{d}(\\varepsilon)+e^{-\\Theta_{d}(n)}\\right)-close to the uniform distribution.\nLet δ≪Od​(ε)+e−Θd​(n)much-less-than𝛿subscript𝑂𝑑𝜀superscript𝑒subscriptΘ𝑑𝑛\\delta\\ll O_{d}(\\varepsilon)+e^{-\\Theta_{d}(n)} be the distance between 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} and the uniform distribution.\nThen we will use the variance V𝑉V of the Hamming weight of 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} to show (Theorem 4.9) that δ𝛿\\delta is in fact Od​(ε)subscript𝑂𝑑𝜀O_{d}(\\varepsilon).\nOn the one hand, since 𝒫𝒫\\mathcal{P} is extremely close to the uniform, its (pairwise) marginals should be perfectly unbiased by granularity (Lemma 4.21).\nHence V≥n/4−n2⋅ε𝑉𝑛4⋅superscript𝑛2𝜀V\\geq n/4-n^{2}\\cdot\\varepsilon where n/4𝑛4n/4 is the variance of the Hamming weight of 𝒫𝒫\\mathcal{P}.\nOn the other hand, since 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} is already e−Θd​(n)superscript𝑒subscriptΘ𝑑𝑛e^{-\\Theta_{d}(n)}-close to the uniform distribution, it shall not miss central Hamming weights.\nIn fact, every m∉Ψ𝑚Ψm\\notin\\Psi must be ≫Θd​(n)much-greater-thanabsentsubscriptΘ𝑑𝑛\\gg\\Theta_{d}(n).\nTherefore we can upper bound V≤(n/4−Θd​(n2)⋅δ)/(1−δ)𝑉𝑛4⋅subscriptΘ𝑑superscript𝑛2𝛿1𝛿V\\leq\\left(n/4-\\Theta_{d}(n^{2})\\cdot\\delta\\right)/(1-\\delta) by comparing the variance calculation of 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} and the uniform distribution.\nThen δ≤Od​(ε)𝛿subscript𝑂𝑑𝜀\\delta\\leq O_{d}(\\varepsilon) follows from rearranging. Now we turn to the case that our distribution is (Od​(ε)+e−Θd​(n))subscript𝑂𝑑𝜀superscript𝑒subscriptΘ𝑑𝑛\\left(O_{d}(\\varepsilon)+e^{-\\Theta_{d}(n)}\\right)-close to the uniform distribution over strings of even (or odd) Hamming weights.\nThen the above argument still works if ΨΨ\\Psi contains only even numbers. Otherwise ΨΨ\\Psi may include some odd Hamming weights and thus make the upper bound on V𝑉V larger than n/4𝑛4n/4.\nTo amend this, we show (Lemma 4.22) that 𝒫𝒫\\mathcal{P} supports on strings with even Hamming weights and thus we can discard odd weights in ΨΨ\\Psi without increasing the distance ε𝜀\\varepsilon. Then the previous argument carries over.\nTo this end, we notice that the Hamming weight of 𝒫𝒫\\mathcal{P} has degree d𝑑d over 𝔽2subscript𝔽2\\mathbb{F}_{2} because f𝑓f is d𝑑d-local.\nHence by granularity, the expectation of the Hamming weight parity is an integer multiple of 2−dsuperscript2𝑑2^{-d}, which has to be zero as the distance ε≪2−dmuch-less-than𝜀superscript2𝑑\\varepsilon\\ll 2^{-d}. Combining the above new ideas, we now give a streamlined proof outline for proving the classification result in the regime where there exists some s∈Ψ𝑠Ψs\\in\\Psi such that o​(n)≪s≪(1−o​(1))​nmuch-less-than𝑜𝑛𝑠much-less-than1𝑜1𝑛o(n)\\ll s\\ll(1-o(1))n.\nAssume the output of a d𝑑d-local function f𝑓f is close to 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi}. First we apply the graph elimination results (Proposition 4.6) to reduce the d𝑑d-local function f𝑓f to a number of more structured functions g𝑔g, where each g𝑔g is d𝑑d-local and has Ωd​(n)subscriptΩ𝑑𝑛\\Omega_{d}(n) non-connected neighborhoods of size Od​(1)subscript𝑂𝑑1O_{d}(1). Then we classify each such neighborhood as Type-1 or Type-2, depending on whether its output distribution is far from any biased distribution. If g𝑔g has Ωd​(n)subscriptΩ𝑑𝑛\\Omega_{d}(n) Type-1 non-connected small neighborhoods, then we show (Lemma 4.12) that the output of g𝑔g is extremely far from 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} by their independence. Otherwise we have Ωd​(n)subscriptΩ𝑑𝑛\\Omega_{d}(n) Type-2 non-connected small neighborhoods.\nUnless 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} is close to the uniform distribution, uniform over even strings, or uniform over odd strings, we can construct (Lemma 4.16) a potential function ϕitalic-ϕ\\phi such that its expectation is large under the Hamming weight distribution of 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} but is small under any Gaussian-like distribution on even or on odd numbers.\nThen we show (Lemma 4.13) that the Hamming weight distribution of g𝑔g’s output is typically a sum of independent integral random variables that mimics a discrete Gaussian over odd or even numbers, which means ϕitalic-ϕ\\phi is indeed small under this distribution. Combining the large distance bound from the Type-1 case and the noticeable deviation of the expected value of ϕitalic-ϕ\\phi from the Type-2 case, we show (Theorem 4.8) that the output of f𝑓f is relatively far from 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi}. Since this contradicts the starting assumption, we must have that 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi} is close to one of the three special distributions, implying that the output of f𝑓f is also close to the very distribution. Finally we use additional treatment (Theorem 4.9) sketched above to further sharpen the bound.\n\nReference markers found: [17]\n\nContext windows for each [ref]:\n[ref]17: and thus, if we believe that the classification result is indeed true, it cannot be approximated by local distributions.\nResults are known on distinguishing this distribution with local functions [ref]17  ; however, their arguments do not seem to generalize. To prove that any local function’s output is far from 𝒟Ψsubscript𝒟Ψ\\mathcal{D}_{\\Psi}, we still follow the previous outline to obtain a\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 17,\n            \"titles\": [\n                \"Range Avoidance for Constant-Depth Circuits: Hardness and Algorithms\",\n                \"Improved bounds for the sunflower lemma\",\n                \"Random oracles separate PSPACE from the polynomial-time hierarchy\",\n                \"Note on Sunflowers\",\n                \"Verifying proofs in constant depth\",\n                \"Quantum advantage with shallow circuits\",\n                \"Quantum advantage with noisy shallow circuits\",\n                \"Large deviation bounds for decision trees and sampling lower bounds for AC0-circuits\",\n                \"One-way functions and circuit complexity\",\n                \"Concentration inequalities: A nonasymptotic theory of independence\",\n                \"Estimates for Bezout coefficients\",\n                \"The space complexity of sampling\",\n                \"Extractors for near logarithmic min-entropy\",\n                \"Elements of information theory\",\n                \"Explicit two-source extractors and resilient functions\",\n                \"Extractors and lower bounds for locally samplable sources\",\n                \"Sampling and Certifying Symmetric Functions\",\n                \"Verifying and decoding in constant depth\",\n                \"Range avoidance for low-depth circuits and connections to pseudorandomness\",\n                \"A lower bound for sampling disjoint sets\",\n                \"Fast parallel generation of random permutations\",\n                \"Computational limitations for small depth circuits\",\n                \"Probability inequalities for sums of bounded random variables\",\n                \"Random generation of combinatorial structures from a uniform distribution\",\n                \"Small depth proof systems\",\n                \"Locality Bounds for Sampling Hamming Slices\",\n                \"Sum of the first k binomial coefficients for fixed n𝑛n\",\n                \"Bounded-depth circuits cannot sample good codes\",\n                \"Converting high probability into nearly-constant time—with applications to parallel hashing\",\n                \"Sums of independent random variables, volume 82\",\n                \"Coding for sunflowers\",\n                \"The monotone complexity of k-clique on random graphs\",\n                \"On the range avoidance problem for circuits\",\n                \"Explicit codes for poly-size circuits and functions that are hard to sample on low entropy distributions\",\n                \"Classical and almost sure local limit theorems\",\n                \"Upper estimates of maximum probability for sums of independent random vectors\",\n                \"Bit-probe lower bounds for succinct data structures\",\n                \"The complexity of distributions\",\n                \"Extractors for Turing-machine sources\",\n                \"Extractors for circuit sources\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]17 based on the context and research area of Locally Sampleable Uniform Symmetric Distributions. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}