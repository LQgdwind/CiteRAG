{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval\n\nPaper Abstract: \n\nPaper Section Text:\nFrequency Baseline. Following prior works [ref]14 [ref]8, we first discretize the video-length normalized start-end points, then use moments with most frequent start-end points as predictions. For video retrieval, we randomly sample videos from the dataset. The results of this baseline is presented in Table 5. We observe this baseline has slightly better performance than chance, we hypothesize it is mainly caused by the fact that the annotators tend to annotate the first few seconds of the video [ref]14, as we have shown in Fig. 13 (Right). Models Trained with TEF. It is shown in [ref]14 [ref]8 that adding Temporal Endpoint Feature (TEF) [ref]14 improves models’ performance in moment retrieval tasks. In Table 5, we compare models trained with TEF.\nIn most cases, adding TEF increases models’ performance, which suggests there exists a certain degree of bias in the proposed dataset.\nThis phenomenon is also observed by recent works [ref]14 [ref]8 in various moment retrieval datasets, i.e., DiDeMo [ref]14, CharadesSTA [ref]9 and ActivityNet Captions .\nWe attribute this phenomenon into two aspects: (1)moment distribution bias - the moments are not evenly distributed over the video, e.g., in TVR and DiDeMo [ref]14, there are more moments appear at the beginning of the video. (2)language timestamp correlation bias - some query words are highly indicative of the potential temporal location of the queries, e.g., temporal connectives like ‘first’ strongly indicate the associated query might be located around the beginning of the video and pronouns like ‘He’ may suggest this query should not be placed at the beginning of the video as people would usually not use pronouns when they first mention someone.\nThe second bias commonly exists in datasets that are built by converting paragraphs into separate sentences, i.e., CharadesSTA [ref]9, TACoS  and ActivityNet Captions .\nTVR avoids this bias by explicitly ask annotators to write queries as individual sentences without requiring the context of a paragraph. XML with Sliding Windows.\nIn Sec. 5.3, we compared XML variants with different proposal generation strategies.\nIn Table 5, we further compare XML (sw, sliding window) with MCN/CAL models. For details of this variant, see Sec. 5.3.\nCompared to the best baseline (MEE+CAL), using the same set of sliding window proposals, we observe XML (sw) still perform much better (3.82 vs. 0.97, R@1 IoU=0.7).\nWe hypothesize that the lower performance of MCN/CAL models compared to XML is mainly caused by the difficulties of training and ranking with a large pool of proposal candidates (1.5M proposals for TVR train). Both MCN and CAL are trained with a ranking objective, which relies on informative negatives to learn effectively. However, effective negative sampling in such a large pool of candidates can be challenging. In comparison, XML breaks the video corpus level moment retrieval problem into two sub-problems: video-level and moment-level retrieval. At video-level retrieval, XML performs ranking within a small set of videos (17.4K), which eases the aforementioned issue. At moment-level, XML (sliding window) utilizes Binary Cross Entropy to maximize the similarity scores of each ground-truth clip, eliminating the need for manually designing a negative sampling strategy. Model Architecture.\nTable 6 presents a model architecture ablation.\nWe first compare with different self-encoder architectures, replacing our transformer style encoder with a bidirectional LSTM encoder  or a CNN encoder  .\nWe observe worse performance after the change and attribute this performance drop to the ineffectiveness of LSTMs and CNNs to capture long-term dependencies  .\nNext, we compare XML with a variant that uses a single max-pooled query instead of two modularized queries. Across all metrics, XML performs better than the variant without modular queries, showing the importance of considering different query representations in matching the context from different modalities. Feature Ablation. We tested XML model with different visual features, the results are shown in Table 7.\nThe model that uses both static appearance features (ResNet ) and action features (I3D ) outperforms models using only one of the features, demonstrating the importance of recognizing both the objects and the actions in the VCMR task. Retrieval Efficiency in 1M Videos. We consider Video Corpus Moment Retrieval in a video corpus containing 1M videos with 100 queries. Following [ref]8, we conduct this experiment in a simulated setting with each video containing 20 clips with max moment length of 14 clips. Each query containing 15 words. We report the following metrics: (1) feature encoding time (feat time) - measures the time for encoding the context (video and subtitle) features offline. (2) encoded feature size (feat size) - measures the disk space needed to store the encoded context features. (3) retrieval time (retrieval time) - measures the time needed to retrieve relevant moments for 100 new queries. It includes time for encoding the queries and performing approximate nearest neighbor search  or matrix multiplication. The time spent on data loading, pre-processing, feature extraction on backend models (i.e., ResNet-152, I3D, RoBERTa) are not considered as they should be similar if not the same for all the methods. Note that the retrieval time here is different from the runtime in Table 5, which additional includes feat time. We do not report feat time and feat size for ExCL  as it does not have the ability to pre-encode the features - its context encoding depends on the input queries. This experiment was conducted on an RTX 2080Ti GPU and an Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz ×\\times 40, with PyTorch  and FAISS . The results are shown in Table 8. Our XML model is more efficient than all the baselines. Compared to the best baseline methods MEE+MCN, XML is 18×18\\times faster in retrieval, 4.5×4.5\\times faster in feature encoding and needs 77% less disk space to store the encoded features. Besides, it also has 7.7×7.7\\times higher performance (3.25 vs. 0.42, IoU=0.7, R@1, on TVR test-public set). Note that MEE+ExCL has very poor retrieval time performance (287×287\\times slower than XML), as it requires early fusion of context and query features. In comparison, the other 3 methods are able to pre-encode the context features and only perform lightweight query encoding and highly optimized nearest neighbor search or matrix multiplication to obtain the moment predictions. Impact of #Retrieved Videos. In previous experiments, we fix the number of videos retrieved by XML to be 100 for corpus level moment retrieval experiments. To study the impact of this hyperparameter, we perform experiments when #videos ∈  0 00absent1050100200\\in  0 00, the results are shown in Table 9. Overall, we notice XML is not sensitive to the number of retrieved videos in terms of R@1, R@5 and R@10 (IoU=0.5, 0.7) in the tested range. When we focus on R@100, IoU=0.5, we find that using more videos helps improve the retrieval performance.\n\nReference markers found: [14]\n\nContext windows for each [ref]:\n[ref]14: Frequency Baseline. Following prior works [ref]14 [ref]8, we first discretize the video-length normalized start-end points, then use moments with most frequent start-end points as predictions. For video retrieval, we randomly sample videos\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 14,\n            \"titles\": [\n                \"Localizing Moments in Video with Temporal Language\",\n                \"Layer normalization\",\n                \"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\",\n                \"Reading Wikipedia to Answer Open-Domain Questions\",\n                \"Temporally grounding natural sentence in video\",\n                \"Imagenet: A large-scale hierarchical image database\",\n                \"Meteor universal: Language specific translation evaluation for any target language\",\n                \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n                \"Temporal localization of moments in video collections with natural language\",\n                \"TALL: Temporal Activity Localization via Language Query\",\n                \"MAC: Mining Activity Concepts for Language-based Temporal Localization\",\n                \"ExCL: Extractive Clip Localization Using Natural Language Descriptions\",\n                \"Deep sparse rectifier neural networks\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Localizing Moments in Video with Natural Language\",\n                \"Gradient flow in recurrent nets: the difficulty of learning long-term dependencies\",\n                \"Long short-term memory\",\n                \"Billion-scale similarity search with GPUs\",\n                \"The Kinetics Human Action Video Dataset\",\n                \"Referitgame: Referring to objects in photographs of natural scenes\",\n                \"Deepstory: Video story qa by deep embedded memory networks\",\n                \"Dense-Captioning Events in Videos\",\n                \"MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning\",\n                \"TVQA: Localized, Compositional Video Question Answering\",\n                \"TVQA+: Spatio-Temporal Grounding for Video Question Answering\",\n                \"Rouge: A package for automatic evaluation of summaries\",\n                \"BSN: Boundary Sensitive Network for Temporal Action Proposal Generation\",\n                \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\",\n                \"Learning a Text-Video Embedding from Incomplete and Heterogeneous Data\",\n                \"Bleu: a method for automatic evaluation of machine translation\",\n                \"Automatic differentiation in PyTorch\",\n                \"Grounding action descriptions in videos\",\n                \"The watershed transform: Definitions, algorithms and parallelization strategies\",\n                \"Coherent Multi-Sentence Video Description with Variable Level of Detail\",\n                \"Movie description\",\n                \"Bi-Directional Attention Flow for Machine Comprehension\",\n                \"Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding\",\n                \"Computer vision: algorithms and applications\",\n                \"Movieqa: Understanding stories in movies through question-answering\",\n                \"Attention Is All You Need\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]14 based on the context and research area of TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}