{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Unsupervised Continual Learning in Streaming Environments\n\nPaper Abstract: \n\nPaper Section Text:\nExperimental Setup:\nhere, all consolidated algorithms are required to classify streaming unlabeled samples where there is only one task n​T=1𝑛𝑇1nT=1. The data arrive batch-by-batch where the batch size is N=1000𝑁1000N=1000. Since the prequential procedure is employed here, the accuracy is calculated per batch and the final result is the average accuracy of all batches, namely prequential accuracy (preq. acc.). It reflects the performance of an algorithm in handling concept changes [ref]2. In the pre-training phase, we simply consider Ni​n​i​t={1​K, 5​K}subscript𝑁𝑖𝑛𝑖𝑡1𝐾5𝐾N_{init}=\\{1K,\\>5K\\} and n​E=50𝑛𝐸50nE=50. At the end of the first batch, a limited amount of labeled data per class (Nm=500subscript𝑁𝑚500N_{m}=500) is introduced to evaluate the prequential accuracy. The labeled data remain unchanged until the streaming phase is completed. This simulates a real-world scenario where the operators only release the labels once and let the algorithms autonomously adapt to any incoming concept changes . The reported results are the average over 5 runs. In each run, the same unlabeled data stream is handled. In other words, it tests the consistency of ADCN in dealing with a given problem. These protocols are also applied to all baselines. Benchmark Datasets:\nwe use synthetic and real-world datasets to demonstrate the efficacy of ADCN. Four of them are in the form of unstructured data, i.e., MNIST [ref]25, Kuzushiji-MNIST (KMNIST) , Fashion-MNIST (FMNIST)  and CIFAR10 . To induce the covariate drift to those data, the image data are rotated to arbitrary angles in the range of [a,b]𝑎𝑏[a,b]. That is, the images are rotated 4 times  ⟶[ref]6 ⟶ ⟶ ⟶05610⟶1115⟶015 \\longrightarrow[ref]6 \\longrightarrow \\longrightarrow  which leads to two drift types: abrupt and recurring. ADCN performance is also tested on the structured data, i.e., SEA , Hyperplane  and Credit Card Default  datasets. In addition to testing ADCN performance in handling non-stationary data, the use of structured data also tests the generalizability of the proposed framework, since instead of CNN, the MLP network is employed as the feature extractor. The datasets properties are summarized in Table I. Characteristics of the data (Char.): real-world data (R), synthetic data (syn), unstructured data (U), structured data (S). Algorithms and Parameters:\ntogether with ADCN, we also trained Deep Clustering Network (DCN) , Autoencoder followed by K-means (AE+KMEANS) and SCARGC . DCN is a two-stage approach which optimizes its network and clusters via joint optimization problem in (14) yet it does not incorporate evolving mechanism. AE+KMEANS is similar to DCN yet it optimizes its network and clusters alternately. SCARGC  is also considered as the baseline. It is a non-DNN method specifically designed for the infinite delay problem and considered as a state-of-the-art algorithm. Two versions of SCARGC built upon SVM (SCARGC+SVM) and KNN (SCARGC+KNN) classifiers are utilized here. On MNIST-based problems and CIFAR10 dataset, ADCN employs CNN as the feature extractor. The encoder part consists of two convolutional layers with 16 and 4 filters and max-pooling layers in between, whereas the decoder part is constructed by two transposed convolution operators with 4 and 16 filters. On structured datasets, ADCN put forward a two-layered MLP as a feature extractor where the number of nodes in each layer is set as 4×u4𝑢4\\times u. A tied weight constraint is applied to construct the decoder part. The initial node of SAE R1subscript𝑅1R_{1} is simply set as 96 and 2×u2𝑢2\\times u for unstructured and structured data cases, respectively. Finally, the ReLU activation function is utilized to induce nonlinear transformation in the intermediate layers, whereas the decoder output applies the Sigmoid activation function obtaining the normalized reconstructed input. The learning rate and momentum coefficient of SGD are set as 0.010.010.01 and 0.950.950.95. The weight decay strength is set at 5×10−55superscript1055\\times 10^{-5} attempting to avoid overfitting. ADCN parameters {αx,αd,αw}subscript𝛼𝑥subscript𝛼𝑑subscript𝛼𝑤\\{\\alpha_{x},\\alpha_{d},\\alpha_{w}\\} and α𝛼\\alpha are set to {0.001,0.001,0.005}0.0010.0010.005\\{0.001,0.001,0.005\\} and 0.01 which controls the drift rate and the amount of update from clustering loss. Note that all of these hyperparameters remain unchanged in all experiments to demonstrate the non-ad-hoc nature of ADCN. The learning batch size is selected as 16, whereas on CIFAR10, it is simply set as 128. To certify a fair comparison, all baselines are re-implemented in the same simulation scenario and procedure. For DNN-based methods, the hyperparameters and network structures are the same as those applied in ADCN, whereas the hyperparameters of non-DNN methods are re-tuned. It aims to obtain better performance, thereby providing a more competitive experimental setting for testing out ADCN. Results:\nfrom Table II, ADCN delivered up to 76%percent7676\\% performance improvement over consolidated baselines in terms of accuracy. It achieved the best predictive performance on 5 of 6 problems. Further, the t-test confirmed that this result is statistically significant (P << 0.05) in almost all cases. In addition to demonstrating the efficacy of ADCN learning policy, this also signifies that the evolving mechanisms of ADCN can generate appropriate network and cluster complexity for a given unsupervised data stream problem. ADCN predictive performance was comparable to DCN and AE+KMEANS on the SEA and Hyperplane problems. The number of initial clusters is believed to become the main cause. Note that the number of clusters of those two methods was 100 from the beginning of the training process, whereas ADCN started the process with only 2 clusters. As a result, those baselines could accommodate more knowledge enabling to deliver better predictive performance. NoC: Number of clusters, NoF: Number of latent features for clustering,\n×: Indicates that the numerical results of the respected baseline and ADCN are significantly different. One can see that ADCN outperforms SCARGC by a large margin in all cases in terms of accuracy. This demonstrates the advantage of utilizing the self-clustering mechanism in deep embedding space. We notice that the performance of SCARGC in these experiments was unsatisfactory, probably because it performs a direct input-space clustering method which may experience a saturating performance on high input dimension datasets since the Euclidean distance is ineffective to deal with high dimension problems . Further, the direct input-space clustering is less likely to obtain a better solution since the characteristic of input features are usually less stable. Note that the clustering works better on top of stable features . Another important finding is also shown in Table II where ADCN predictive performance is better than DCN and AE+KMEANS in almost all cases. This finding highlights the benefit of the evolving mechanism embraced by ADCN. The autonomous SAE is capable of increasing or decreasing the network capacity on demand, whereas the self-clustering evolution aims to accommodate uncovered latent space representations crafting a new knowledge. The comparison against AE+KMEANS also suggests that the combination of structural evolution and joint optimization in (14) is able to work hand-in-hand to find a solution for a given problem. The performance and network evolution of ADCN is pictorially exhibited in Fig. S-3 of supplemental document illustrating the Credit Card Default problem. The flexible nature of ADCN is demonstrated where the growing and pruning mechanisms are implemented. Furthermore, the evolution of accuracy is shown to rapidly increase and stable over time highlighting the efficacy of ADCN learning policy. The accuracy decreases in the case of drift but timely responded with the introduction of new nodes and new layers. From the figure, the evolving nature of the self-clustering mechanism is also demonstrated where the number of clusters can be evolved whenever there are uncovered latent features.\n\nReference markers found: [2]\n\nContext windows for each [ref]:\n[ref]2: is calculated per batch and the final result is the average accuracy of all batches, namely prequential accuracy (preq. acc.). It reflects the performance of an algorithm in handling concept changes [ref]2. In the pre-training phase, we simply consider Ni​n​i​t={1​K, 5​K}subscript𝑁𝑖𝑛𝑖𝑡1𝐾5𝐾N_{init}=\\{1K,\\>5K\\} and n​E=50𝑛𝐸50nE=50. At the end of the first batch, a limited amount of labeled data\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 2,\n            \"titles\": [\n                \"Learning without Forgetting\",\n                \"Extracting and composing robust features with denoising autoencoders\",\n                \"Knowledge Discovery from Data Streams\",\n                \"A generalized growing and pruning rbf (ggap-rbf) neural network for function approximation\",\n                \"Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning\",\n                \"Online tool condition monitoring based on parsimonious ensemble+\",\n                \"Continuous Learning in Single-Incremental-Task Scenarios\",\n                \"Online Deep Learning: Learning Deep Neural Networks on the Fly\",\n                \"Online incremental feature learning with denoising autoencoders\",\n                \"The Power of Depth for Feedforward Neural Networks\",\n                \"Autonomous Deep Learning: Continual Learning Approach for Dynamic Environments\",\n                \"Automatic construction of multi-layer perceptron network from streaming examples\",\n                \"Palm: An incremental construction of hyperplanes for data stream regression\",\n                \"Data stream classification guided by clustering on nonstationary environments and extreme verification latency\",\n                \"Continual multiview task learning via deep matrix factorization\",\n                \"Semi-supervised hashing for scalable image retrieval\",\n                \"Devdan: Deep evolving denoising autoencoder\",\n                \"Compose: A semisupervised learning framework for initially labeled nonstationary streaming data\",\n                \"A self-evolving mutually-operative recurrent network-based model for online tool condition monitoring in delay scenario\",\n                \"Weakly supervised deep learning approach in streaming environments\",\n                \"Unsupervised continual learning and self-taught associative memory hierarchies\",\n                \"Toward integrating feature selection algorithms for classification and clustering\",\n                \"Billion-scale similarity search with GPUs\",\n                \"Deep clustering for unsupervised learning of visual features\",\n                \"MNIST handwritten digit database\",\n                \"Deep Learning for Classical Japanese Literature\",\n                \"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms\",\n                \"Progress & Compress: A scalable framework for continual learning\",\n                \"A streaming ensemble algorithm (sea) for large-scale classification\",\n                \"Moa: Massive online analysis\",\n                \"The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients\",\n                \"On the surprising behavior of distance metrics in high dimensional space\",\n                \"Gradient Episodic Memory for Continual Learning\",\n                \"Overcoming catastrophic forgetting in neural networks\",\n                \"Lifelong Learning with Dynamically Expandable Networks\",\n                \"Learning multiple layers of features from tiny images\",\n                \"t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data\",\n                \"Continual learning through synaptic intelligence\",\n                \"Catastrophic forgetting in connectionist networks\",\n                \"The nature of statistical learning theory\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]2 based on the context and research area of Unsupervised Continual Learning in Streaming Environments. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}