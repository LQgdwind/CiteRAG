{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Deterministic computation of the characteristic polynomial in the time of matrix multiplication\n\nPaper Abstract: \n\nPaper Section Text:\nWhen outlining the approach of our determinant algorithm via the identity in\nEq.Â 1, we implicitly assumed that the\nmatrices used as input in recursive calls, i.e.Â ğ€1subscriptğ€1\\mathbf{{A}}_{1} and ğŠ2subscriptğŠ2\\mathbf{{K}}_{2}\nand ğğ\\mathbf{{B}}, do satisfy the input requirement of row reducedness: this is\nnot necessarily the case, even if starting from a reduced matrix ğ€ğ€\\mathbf{{A}}. Concerning ğ€1subscriptğ€1\\mathbf{{A}}_{1}, one may locate such a reduced submatrix of ğ€ğ€\\mathbf{{A}}\nand then permute rows and columns of ğ€ğ€\\mathbf{{A}} (which only affects the sign of\ndet(ğ€)ğ€\\det(\\mathbf{{A}})) to make this submatrix become the leading\nprincipal submatrix ğ€1subscriptğ€1\\mathbf{{A}}_{1}. This is a classical operation on reduced\nmatrices which suggests using a form slightly stronger than reduced form called\nweak Popov form [ref]36 (see SectionÂ 2.4). Assuming\nthat ğ€ğ€\\mathbf{{A}} has this form ensures that its leading principal submatrix\nğ€1subscriptğ€1\\mathbf{{A}}_{1} has it as well. This assumption is acceptable in terms of\ncomplexity since one can transform a reduced ğ€ğ€\\mathbf{{A}} into a weak Popov\nğğ\\mathbf{{P}} by means of fast linear algebra in a cost negligible compared to\nour target [42, Sec.â€‰3]; note that ğ€ğ€\\mathbf{{A}} and ğğ\\mathbf{{P}} have\nthe same determinant up to an easily found constant (see\nAlgorithmÂ 1). Next, the cases of ğŠ2subscriptğŠ2\\mathbf{{K}}_{2} and ğğ\\mathbf{{B}} are strongly linked. First, we\nwill not discuss ğŠ2subscriptğŠ2\\mathbf{{K}}_{2} but the whole kernel basis [ğŠ1â€‹ğŠ2]delimited-[]subscriptğŠ1subscriptğŠ2[\\mathbf{{K}}_{1}\\;\\;\\mathbf{{K}}_{2}]. The fastest known algorithm for computing such a basis is that of\n, and for best efficiency it outputs a matrix in shifted\nreduced form, which is a generalization of reducedness involving degree weights\ngiven by a tuple ğ’”âˆˆâ„¤mğ’”superscriptâ„¤ğ‘š\\boldsymbol{s}\\in\\mathbb{Z}^{m} called a shift (see\nSectionsÂ 2.3 andÂ 2.4 for definitions); the non-shifted\ncase is for ğ’”=ğŸğ’”0\\boldsymbol{s}=\\mathbf{0}. As in the determinant algorithm of\n[ref]32, here the shift for [ğŠ1â€‹ğŠ2]delimited-[]subscriptğŠ1subscriptğŠ2[\\mathbf{{K}}_{1}\\;\\;\\mathbf{{K}}_{2}] is taken as\nthe list of row degrees of ğ€ğ€\\mathbf{{A}}, denoted by ğ’”=rdegâ€‹(ğ€)ğ’”rdegğ€\\boldsymbol{s}=\\mathrm{rdeg}(\\mathbf{{A}}); for\nthe characteristic matrix one has ğ’”=(1,â€¦,1)ğ’”1â€¦1\\boldsymbol{s}=(1,\\ldots,1) but non-uniform\nshifts may arise in recursive calls. We want [ğŠ1â€‹ğŠ2]delimited-[]subscriptğŠ1subscriptğŠ2[\\mathbf{{K}}_{1}\\;\\;\\mathbf{{K}}_{2}] to\nbe not only ğ’”ğ’”\\boldsymbol{s}-reduced, but in ğ’”ğ’”\\boldsymbol{s}-weak Popov form: a direct\nconsequence is that ğğ\\mathbf{{B}} is in weak Popov form, and is thus suitable\ninput for a recursive call. To obtain [ğŠ1â€‹ğŠ2]delimited-[]subscriptğŠ1subscriptğŠ2[\\mathbf{{K}}_{1}\\;\\;\\mathbf{{K}}_{2}] we use the kernel basis algorithm of\n and transform its output into ğ’”ğ’”\\boldsymbol{s}-weak Popov form. A\nminor issue is that the fastest known algorithm for such transformations was\nwritten in [42, Sec.â€‰3] for non-shifted forms; yet it easily\nextends to shifted forms as we show in SectionÂ 4,\nobtaining the next result. There is an algorithmÂ ReducedToWeakPopov which takes as input a\nmatrix ğ€âˆˆğ•‚â€‹[x]mÃ—nğ€ğ•‚superscriptdelimited-[]ğ‘¥ğ‘šğ‘›\\mathbf{{A}}\\in\\mathbb{K}[x]^{m\\times n} with mâ‰¤nğ‘šğ‘›m\\leq n and a shift\nğ¬âˆˆâ„¤nğ¬superscriptâ„¤ğ‘›\\boldsymbol{s}\\in\\mathbb{Z}^{n} such that ğ€ğ€\\mathbf{{A}} is in ğ¬ğ¬\\boldsymbol{s}-reduced form,\nand returns an ğ¬ğ¬\\boldsymbol{s}-weak Popov form of ğ€ğ€\\mathbf{{A}} using\nOâ€‹(mÏ‰âˆ’2â€‹nâ€‹D+mÏ‰âˆ’1â€‹n)ğ‘‚superscriptğ‘šğœ”2ğ‘›ğ·superscriptğ‘šğœ”1ğ‘›\\mathchoice{O\\left(m^{\\omega-2}nD+m^{\\omega-1}n\\right)}{O(m^{\\omega-2}nD+m^{\\omega-1}n)}{O(m^{\\omega-2}nD+m^{\\omega-1}n)}{O(m^{\\omega-2}nD+m^{\\omega-1}n)} operations\nin ğ•‚ğ•‚\\mathbb{K}, where D=|rdegğ¬â€‹(ğ€)|âˆ’mâ‹…minâ¡(ğ¬)ğ·subscriptrdegğ¬ğ€â‹…ğ‘šğ¬D=|\\mathrm{rdeg}_{{\\boldsymbol{s}}}(\\mathbf{{A}})|-m\\cdot\\min(\\boldsymbol{s}). Here, following usual notation recalled in\nSectionÂ 2.1, |rdegğ’”â€‹(ğ€)|subscriptrdegğ’”ğ€|\\mathrm{rdeg}_{{\\boldsymbol{s}}}(\\mathbf{{A}})| is the sum of\nthe ğ’”ğ’”\\boldsymbol{s}-degrees of the rows of ğ€ğ€\\mathbf{{A}}. This result extends\n[42, Thm.â€‰13] since for ğ’”=ğŸğ’”0\\boldsymbol{s}=\\mathbf{0} the quantity\nDğ·D is the sum of the row degrees of ğ€ğ€\\mathbf{{A}} and in particular\nDâ‰¤mâ€‹degâ¡(ğ€)ğ·ğ‘šdegreeğ€D\\leq m\\deg(\\mathbf{{A}}), leading to the cost bound\nOâ€‹(mÏ‰âˆ’1â€‹nâ€‹degâ¡(ğ€))ğ‘‚superscriptğ‘šğœ”1ğ‘›degreeğ€\\mathchoice{O\\left(m^{\\omega-1}n\\deg(\\mathbf{{A}})\\right)}{O(m^{\\omega-1}n\\deg(\\mathbf{{A}}))}{O(m^{\\omega-1}n\\deg(\\mathbf{{A}}))}{O(m^{\\omega-1}n\\deg(\\mathbf{{A}}))}. To summarize, at this stage we have outlined how to ensure, without exceeding\nour target cost bound, that ğ€1subscriptğ€1\\mathbf{{A}}_{1} and ğğ\\mathbf{{B}} are valid input for\nrecursive calls, i.e.Â are in weak Popov form. Having det(ğ€1)subscriptğ€1\\det(\\mathbf{{A}}_{1}) and\ndet(ğ)ğ\\det(\\mathbf{{B}}), it remains to find det(ğŠ2)subscriptğŠ2\\det(\\mathbf{{K}}_{2}) and then the sought\ndet(ğ€)ğ€\\det(\\mathbf{{A}}) follows. We noted that, to ensure the form of ğğ\\mathbf{{B}} but\nalso for efficiency reasons, the kernel basis [ğŠ1â€‹ğŠ2]delimited-[]subscriptğŠ1subscriptğŠ2[\\mathbf{{K}}_{1}\\;\\;\\mathbf{{K}}_{2}] is\ncomputed in ğ’”ğ’”\\boldsymbol{s}-weak Popov form for the shift ğ’”=rdegâ€‹(ğ€)ğ’”rdegğ€\\boldsymbol{s}=\\mathrm{rdeg}(\\mathbf{{A}}).\nThis causes the main difficulty related to our modification of the determinant\nalgorithm of [ref]32: ğŠ2subscriptğŠ2\\mathbf{{K}}_{2} is not valid input for a recursive\ncall since it is in ğ’—ğ’—\\boldsymbol{v}-weak Popov form for some shift ğ’—ğ’—\\boldsymbol{v},\na subtuple of ğ’”ğ’”\\boldsymbol{s} which is possibly nonzero. A first idea is to extend our approach to the shifted case, allowing recursive\ncalls with such a ğ’—ğ’—\\boldsymbol{v}-reduced matrix: this is straightforward but\ngives an inefficient algorithm. Indeed, along the recursion the shift drifts\naway from its initial value and becomes arbitrarily large and unbalanced with\nrespect to the degrees of the input matrices of recursive calls. For example,\nas mentioned above the sum of row degrees of the initial non-shifted\nmÃ—mğ‘šğ‘šm\\times m matrix ğ€ğ€\\mathbf{{A}} is D=degâ¡(det(ğ€))ğ·degreeğ€D=\\deg(\\det(\\mathbf{{A}})), whereas\nfor the ğ’—ğ’—\\boldsymbol{v}-shifted (m/2)Ã—(m/2)ğ‘š2ğ‘š2(m/2)\\times(m/2) matrix ğŠ2subscriptğŠ2\\mathbf{{K}}_{2}\nwe only have the same bound Dğ·D instead of one related to\ndegâ¡(det(ğŠ2))degreesubscriptğŠ2\\deg(\\det(\\mathbf{{K}}_{2})) itself, which is known to be at most D/2ğ·2D/2 in\nour algorithm. This gap, here between Dğ·D and D/2ğ·2D/2, will only\ngrow as the algorithm goes down the tree of recursive calls, meaning that\ndegrees in matrices handled recursively are not sufficiently well controlled. Another idea is to compute a ğŸ0\\mathbf{0}-reduced matrix which has the same\ndeterminant as ğŠ2subscriptğŠ2\\mathbf{{K}}_{2}. Finding a ğŸ0\\mathbf{0}-reduced form of\nğŠ2subscriptğŠ2\\mathbf{{K}}_{2} within our target cost seems to be a difficult problem. The best\nknown algorithms for general ğŸ0\\mathbf{0}-reduction involve logâ¡(m)ğ‘š\\log(m)\nfactors, either explicitly  or implicitly  (in\nthe latter approach one starts by using the above-discussed triangularization\nprocedure of [ref]32 which we are modifying here to avoid\nlogâ¡(m)ğ‘š\\log(m) factors). More specific algorithms exploit the form of\nğŠ2subscriptğŠ2\\mathbf{{K}}_{2}, interpreting the problem as a change of shift from ğ’—ğ’—\\boldsymbol{v}\nto ğŸ0\\mathbf{0}; yet at the time of writing efficient changes of shifts have\nonly been achieved when the target shift is larger than the origin shift\n[27, Sec.â€‰5], a fact that offers degree control for the\ntransformation between the two matrices. Another possibility is to compute the\nso-called ğ’—ğ’—\\boldsymbol{v}-Popov form ğğ\\mathbf{{P}} of ğŠ2subscriptğŠ2\\mathbf{{K}}_{2}, since its\ntranspose ğğ–³superscriptğğ–³\\mathbf{{P}}^{\\mathsf{T}} is ğŸ0\\mathbf{0}-reduced by definition (see\nSectionÂ 2.4), and det(ğğ–³)=det(ğ)superscriptğğ–³ğ\\det(\\mathbf{{P}}^{\\mathsf{T}})=\\det(\\mathbf{{P}}) is\ndet(ğŠ2)subscriptğŠ2\\det(\\mathbf{{K}}_{2}) up to a constant. However this suffers from the same issue,\nas computing ğğ\\mathbf{{P}} is essentially the same as changing the shift\nğ’—ğ’—\\boldsymbol{v} into the nonpositive shift âˆ’ğœ¹ğœ¹-\\boldsymbol{\\delta}, where ğœ¹ğœ¹\\boldsymbol{\\delta} is\nthe list of diagonal degrees of ğŠ2subscriptğŠ2\\mathbf{{K}}_{2}  [ref]26. To circumvent these issues, we use the property that the transpose\nğŠ2ğ–³superscriptsubscriptğŠ2ğ–³\\mathbf{{K}}_{2}^{\\mathsf{T}} of a ğ’—ğ’—\\boldsymbol{v}-reduced matrix is in\nâˆ’ğ’…ğ’…-\\boldsymbol{d}-reduced form where ğ’…=rdegğ’—â€‹(ğŠ2)ğ’…subscriptrdegğ’—subscriptğŠ2\\boldsymbol{d}=\\mathrm{rdeg}_{{\\boldsymbol{v}}}(\\mathbf{{K}}_{2}).\nThis fact naturally comes up here since det(ğŠ2)=det(ğŠ2ğ–³)subscriptğŠ2superscriptsubscriptğŠ2ğ–³\\det(\\mathbf{{K}}_{2})=\\det(\\mathbf{{K}}_{2}^{\\mathsf{T}}), but seems otherwise rarely exploited in polynomial\nmatrix algorithms: in fact we did not find a previous occurrence of it apart\nfrom related degree considerations in [56, Lem.â€‰2.2]. Transposing the above two approaches using ğŠ2ğ–³superscriptsubscriptğŠ2ğ–³\\mathbf{{K}}_{2}^{\\mathsf{T}} instead of\nğŠ2subscriptğŠ2\\mathbf{{K}}_{2}, we observe that computing a ğŸ0\\mathbf{0}-reduced form of\nğŠ2ğ–³superscriptsubscriptğŠ2ğ–³\\mathbf{{K}}_{2}^{\\mathsf{T}} is a change of shift from âˆ’ğ’…ğ’…-\\boldsymbol{d} to\nğŸ0\\boldsymbol{0}, and computing the âˆ’ğ’…ğ’…-\\boldsymbol{d}-Popov form ğğ\\mathbf{{P}} of\nğŠ2ğ–³superscriptsubscriptğŠ2ğ–³\\mathbf{{K}}_{2}^{\\mathsf{T}} is essentially a change of shift from âˆ’ğ’…ğ’…-\\boldsymbol{d} to\nâˆ’ğœ¹ğœ¹-\\boldsymbol{\\delta}. In both cases the target shift is larger than the origin shift,\nimplying that the kernel-based change of shift of [27, Sec.â€‰5]\ninvolves matrices of well-controlled degrees. Still, this is not enough to make\nthis change of shift efficient as such, the difficulty being now that the\naverage row degree of ğŠ2ğ–³superscriptsubscriptğŠ2ğ–³\\mathbf{{K}}_{2}^{\\mathsf{T}} may not be small: only its average\ncolumn degree, which corresponds to the average row degree of ğŠ2subscriptğŠ2\\mathbf{{K}}_{2}, is\ncontrolled. Our solution uses the second approach, computing the âˆ’ğ’…ğ’…-\\boldsymbol{d}-Popov form\nğğ\\mathbf{{P}}, because it offers the a priori knowledge that the column degrees\nof ğğ\\mathbf{{P}} are exactly ğœ¹ğœ¹\\boldsymbol{\\delta}. We exploit this degree knowledge to\ncarry out partial linearization techniques, originally designed for approximant\nbases  , which we extend here to kernel bases.\nThese techniques allow us to reduce our problem to a kernel basis computation\nwhere the matrix entries have uniformly small degrees, implying that it can be\nefficiently handled via the minimal approximant basis algorithm\nPM-Basis from . The next result summarizes the new\nalgorithmic tool developed in SectionÂ 5 for finding\nğğ\\mathbf{{P}}. Let ğ¬âˆˆâ„¤mğ¬superscriptâ„¤ğ‘š\\boldsymbol{s}\\in\\mathbb{Z}^{m}, let ğ€âˆˆğ•‚â€‹[x]mÃ—mğ€ğ•‚superscriptdelimited-[]ğ‘¥ğ‘šğ‘š\\mathbf{{A}}\\in\\mathbb{K}[x]^{m\\times m} be in\nâˆ’ğ¬ğ¬-\\boldsymbol{s}-weak Popov form, let ğ›…âˆˆâ„¤â‰¥0mğ›…superscriptsubscriptâ„¤absent0ğ‘š\\boldsymbol{\\delta}\\in\\mathbb{Z}_{\\geq 0}^{m} be the\nâˆ’ğ¬ğ¬-\\boldsymbol{s}-pivot degree of ğ€ğ€\\mathbf{{A}}, and assume that ğ¬â‰¥ğ›…ğ¬ğ›…\\boldsymbol{s}\\geq\\boldsymbol{\\delta}. There is an algorithmÂ WeakPopovToPopov which takes as\ninput (ğ€,ğ¬)ğ€ğ¬(\\mathbf{{A}},\\boldsymbol{s}) and computes the âˆ’ğ¬ğ¬-\\boldsymbol{s}-Popov form of\nğ€ğ€\\mathbf{{A}} by performing PM-Basis at order less than\n|ğ’”|/m+4ğ’”ğ‘š4|\\boldsymbol{s}|/m+4 on an input matrix of row dimension at\nmost 6â€‹m6ğ‘š6m and column dimension at most 3â€‹m3ğ‘š3m, multiplying the inverse of a matrix in ğ•‚mÃ—msuperscriptğ•‚ğ‘šğ‘š\\mathbb{K}^{m\\times m} by a\nmatrix in ğ•‚â€‹[x]mÃ—mğ•‚superscriptdelimited-[]ğ‘¥ğ‘šğ‘š\\mathbb{K}[x]^{m\\times m} of column degree ğœ¹ğœ¹\\boldsymbol{\\delta}, and performing Oâ€‹(m2)ğ‘‚superscriptğ‘š2\\mathchoice{O\\left(m^{2}\\right)}{O(m^{2})}{O(m^{2})}{O(m^{2})} extra operations in ğ•‚ğ•‚\\mathbb{K}. Thus, computing the âˆ’ğ¬ğ¬-\\boldsymbol{s}-Popov form of ğ€ğ€\\mathbf{{A}} can be done in\nOâ€‹(mÏ‰â€‹ğ–¬â€²â¡(|ğ¬|/m))ğ‘‚superscriptğ‘šğœ”superscriptğ–¬â€²ğ¬ğ‘š\\mathchoice{O\\left(m^{\\omega}\\mathchoice{\\operatorname{\\mathsf{M}}^{\\prime}\\!\\left(|\\boldsymbol{s}|/m\\right)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)}\\right)}{O(m^{\\omega}\\mathchoice{\\operatorname{\\mathsf{M}}^{\\prime}\\!\\left(|\\boldsymbol{s}|/m\\right)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)})}{O(m^{\\omega}\\mathchoice{\\operatorname{\\mathsf{M}}^{\\prime}\\!\\left(|\\boldsymbol{s}|/m\\right)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)})}{O(m^{\\omega}\\mathchoice{\\operatorname{\\mathsf{M}}^{\\prime}\\!\\left(|\\boldsymbol{s}|/m\\right)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)}{\\operatorname{\\mathsf{M}}^{\\prime}(|\\boldsymbol{s}|/m)})} operations in\nğ•‚ğ•‚\\mathbb{K}. This theorem is a generalization of [42, Sec.â€‰4] to shifted forms,\nfor shifts âˆ’ğ’”ğ’”-\\boldsymbol{s} that satisfy the assumption ğ’”â‰¥ğœ¹ğ’”ğœ¹\\boldsymbol{s}\\geq\\boldsymbol{\\delta}.\nIndeed, if ğ€ğ€\\mathbf{{A}} is ğŸ0\\mathbf{0}-weak Popov, then one recovers\n[42, Thm.â€‰20] by taking\nğ’”=(degâ¡(ğ€),â€¦,degâ¡(ğ€))ğ’”degreeğ€â€¦degreeğ€\\boldsymbol{s}=(\\deg(\\mathbf{{A}}),\\ldots,\\deg(\\mathbf{{A}})) in the above theorem. For\ncomparison, the naive generalization of [42, Sec.â€‰4] to shifted\nforms\nruns in Oâ€‹(mÏ‰â€‹ğ–¬â€²â¡(maxâ¡(ğ’”)))ğ‘‚superscriptğ‘šğœ”superscriptğ–¬â€²ğ’”\\mathchoice{O\\left(m^{\\omega}\\mathchoice{\\operatorname{\\mathsf{M}}^{\\prime}\\!\\left(\\max(\\boldsymbol{s})\\right)}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))}\\right)}{O(m^{\\omega}\\mathchoice{\\operatorname{\\mathsf{M}}^{\\prime}\\!\\left(\\max(\\boldsymbol{s})\\right)}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))})}{O(m^{\\omega}\\mathchoice{\\operatorname{\\mathsf{M}}^{\\prime}\\!\\left(\\max(\\boldsymbol{s})\\right)}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))})}{O(m^{\\omega}\\mathchoice{\\operatorname{\\mathsf{M}}^{\\prime}\\!\\left(\\max(\\boldsymbol{s})\\right)}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))}{\\operatorname{\\mathsf{M}}^{\\prime}(\\max(\\boldsymbol{s}))})}, which exceeds our\ntarget complexity as soon as maxâ¡(ğ’”)â‰«|ğ’”|/mmuch-greater-thanğ’”ğ’”ğ‘š\\max(\\boldsymbol{s})\\gg|\\boldsymbol{s}|/m.\nHence the use of partial linearization techniques, which were not needed in the\nnon-shifted case featuring maxâ¡(ğ’”)=|ğ’”|/m=degâ¡(ğ€)ğ’”ğ’”ğ‘šdegreeğ€\\max(\\boldsymbol{s})=|\\boldsymbol{s}|/m=\\deg(\\mathbf{{A}}). As mentioned above, our AlgorithmÂ WeakPopovToPopov is based on the\ncomputation of a kernel basis with a priori knowledge of the degree profile of\nthe output. This kernel problem is very close to the one handled in\n[56, Sec.â€‰4], except that in this reference one only has upper\nbounds on the output degrees, implying a certain numberâ€”possibly logarithmic\nin mğ‘šmâ€”of calls to PM-Basis to recover the output and its\nactual degrees. In the same spirit but in the context of approximant bases, [28, Sec.â€‰5]\nuses partial linearization techniques to reduce an arbitrary input with known\noutput degrees to essentially one call to PM-Basis, whereas\n[55, Algo.â€‰2] assumes weaker output degree information\nand makes a potentially logarithmic number of calls to PM-Basis.\n\nReference markers found: [36]\n\nContext windows for each [ref]:\n[ref]36: leading\nprincipal submatrix ğ€1subscriptğ€1\\mathbf{{A}}_{1}. This is a classical operation on reduced\nmatrices which suggests using a form slightly stronger than reduced form called\nweak Popov form [ref]36 (see SectionÂ 2.4). Assuming\nthat ğ€ğ€\\mathbf{{A}} has this form ensures that its leading principal submatrix\nğ€1subscriptğ€1\\mathbf{{A}}_{1} has it as well. This assumption is acceptable in terms\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 36,\n            \"titles\": [\n                \"Computing Popov and Hermite Forms of Rectangular Polynomial Matrices\",\n                \"Efficient algorithms for computing the characteristic polynomial in a domain\",\n                \"On the complexity of partial derivatives\",\n                \"A uniform approach for the fast computation of matrix-type PadÃ© approximants\",\n                \"Shifted normal forms of polynomial matrices\",\n                \"Normal forms for general polynomial matrices\",\n                \"On computing the determinant in small parallel time using a small number of processors\",\n                \"Triangular factorization and inversion by fast matrix multiplication\",\n                \"Algebraic Complexity Theory\",\n                \"On fast multiplication of polynomials over arbitrary algebras\",\n                \"On the minimum computation time of functions\",\n                \"Fast parallel matrix inversion algorithms\",\n                \"The numerical solution of the secular equation\",\n                \"Fast Computation of the Rank Profile Matrix and the Generalized Bruhat Decomposition\",\n                \"Efficient computation of the characteristic polynomial\",\n                \"Abstract Algebra\",\n                \"Collected Problems in Higher Algebra, Problem nÂ°979\",\n                \"Minimal Bases of Rational Vector Spaces, with Applications to Multivariable Linear Systems\",\n                \"A simple recurrent formula for inverting a matrix (abstract)\",\n                \"Modern Computer Algebra (third edition)\",\n                \"Nearly optimal algorithms for canonical matrix forms\",\n                \"On the complexity of polynomial matrix computations\",\n                \"Certification of Minimal Approximant Bases\",\n                \"Triangular xğ‘¥x-basis decompositions and derandomization of linear algebra algorithms over Kâ€‹[x]ğ¾delimited-[]ğ‘¥{K}[x]\",\n                \"Faster polynomial multiplication over finite fields\",\n                \"A generalization of the fast LUP matrix decomposition algorithm and applications\",\n                \"Fast Computation of Minimal Interpolation Bases in Popov Form for Arbitrary Shifts\",\n                \"Computing minimal interpolation bases\",\n                \"Fast computation of approximant bases in canonical form\",\n                \"Linear Systems\",\n                \"On the complexity of computing determinants\",\n                \"Fast algorithms for the characteristic polynomial\",\n                \"Fast, deterministic computation of the Hermite normal form and determinant of a polynomial matrix\",\n                \"Powers of Tensors and Fast Matrix Multiplication\",\n                \"Sur les variations sÃ©culaires des Ã©lÃ©ments elliptiques des sept plantÃ¨tes principales\",\n                \"Bruhat canonical form for linear systems\",\n                \"On lattice reduction for polynomial matrices\",\n                \"Computing Canonical Bases of Modules of Univariate Relations\",\n                \"Faster Algorithms for the Characteristic Polynomial\",\n                \"Invariant description of linear, time-invariant controllable systems\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]36 based on the context and research area of Deterministic computation of the characteristic polynomial in the time of matrix multiplication. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}