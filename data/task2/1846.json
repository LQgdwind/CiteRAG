{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Residual Gaussian Process: A Tractable Nonparametric Bayesian Emulator for Multi-fidelity Simulations\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section we introduce a novel tractable and scalable structure to model multi-fidelity simulation data.\nRather than imposing a concatenating structure as in NARGP, which forgoes the tractable nature of a GP , we decompose the GP defined over the high-fidelity output into a sum of GPs relating to the differences between successive fidelities.\nSpecifically, we impose the following residual structure where 𝐫f​(𝝃)=𝐲f​(𝝃)−𝐲f−1​(𝝃)superscript𝐫𝑓𝝃superscript𝐲𝑓𝝃superscript𝐲𝑓1𝝃{\\bf r}^{f}(\\bm{\\xi})={\\bf y}^{f}(\\bm{\\xi})-{\\bf y}^{f-1}(\\bm{\\xi}), f=2,…,F𝑓2…𝐹f=2,\\ldots,F, are the residual functions between fidelities. The function 𝐫1​(𝝃)superscript𝐫1𝝃{\\bf r}^{1}(\\bm{\\xi}), on the other hand, is defined as 𝐫1​(𝝃)=𝐲1​(𝝃)superscript𝐫1𝝃superscript𝐲1𝝃{\\bf r}^{1}(\\bm{\\xi})={\\bf y}^{1}(\\bm{\\xi}). Note that we present the model for the multivariate case. The univariate case can be obtained from the formulae presented below in an obvious manner. The unknown functions 𝐫f​(𝝃)superscript𝐫𝑓𝝃{\\bf r}^{f}(\\bm{\\xi}) are then treated as random processes that can be approximated using any probabilistic data-driven model.\nWe place an independent, zero-mean GP prior over each residual function 𝐫f​(𝝃)superscript𝐫𝑓𝝃{\\bf r}^{f}(\\bm{\\xi}), i.e., where 𝛀f∈ℝd×dsuperscript𝛀𝑓superscriptℝ𝑑𝑑\\mathbf{\\Omega}^{f}\\in\\mathbb{R}^{d\\times d} is an unknown coregionalization matrix corresponding to the correlations between the components of 𝐫f​(𝝃)superscript𝐫𝑓𝝃{\\bf r}^{f}(\\bm{\\xi}), ⊗tensor-product\\otimes is the Kronecker product and δ​(𝝃,𝝃′)𝛿𝝃superscript𝝃′\\delta(\\bm{\\xi},\\bm{\\xi}^{\\prime}) is the Kronecker-delta function. Without loss of generality, the mean is assumed to be identically zero by centering the observations. We note that this is the multivariate GP model of Conti and O’Hagan , with a separable covariance structure. That is, the covariance matrices assume the forms 𝐊f⊗𝛀ftensor-productsuperscript𝐊𝑓superscript𝛀𝑓{\\bf K}^{f}\\otimes\\mathbf{\\Omega}^{f}, in which [𝐊f]n​m=kf​(𝝃n,𝝃m|𝜽f)subscriptdelimited-[]superscript𝐊𝑓𝑛𝑚superscript𝑘𝑓subscript𝝃𝑛conditionalsubscript𝝃𝑚superscript𝜽𝑓[{\\bf K}^{f}]_{nm}=k^{f}(\\bm{\\xi}_{n},\\bm{\\xi}_{m}|\\bm{\\theta}^{f}), 𝝃n,𝝃m∈𝒳fsubscript𝝃𝑛subscript𝝃𝑚superscript𝒳𝑓\\bm{\\xi}_{n},\\bm{\\xi}_{m}\\in\\mathcal{X}^{f}, captures the correlations between values of 𝐫f​(𝝃)superscript𝐫𝑓𝝃{\\bf r}^{f}(\\bm{\\xi}) at different inputs 𝝃𝝃\\bm{\\xi} and 𝛀fsuperscript𝛀𝑓\\mathbf{\\Omega}^{f} captures the spatial correlations, i.e., between different components of 𝐫f​(𝝃)superscript𝐫𝑓𝝃{\\bf r}^{f}(\\bm{\\xi}). The covariance functions kf​(𝝃,𝝃|𝜽)superscript𝑘𝑓𝝃conditional𝝃𝜽k^{f}(\\bm{\\xi},\\bm{\\xi}|\\bm{\\theta}) contain unknown hyperparameters 𝜽fsuperscript𝜽𝑓\\bm{\\theta}^{f}. The terms δ​(𝝃,𝝃′)​τf​𝐈𝛿𝝃superscript𝝃′superscript𝜏𝑓𝐈\\delta(\\bm{\\xi},\\bm{\\xi}^{\\prime})\\tau^{f}{\\bf I} are included to account for zero-mean, i.i.d. measurement error (across inputs and spatial coordinates), or, equivalently, as regularization terms to prevent ill-conditioning during training. The τfsuperscript𝜏𝑓\\tau^{f} are treated as hyperparameters or (optionally) fixed to some small values in the regularization interpretation . The distribution (3) is conditioned on the full set of hyperparameters {𝜽f,Ωf,τf}superscript𝜽𝑓superscriptΩ𝑓superscript𝜏𝑓\\{\\bm{\\theta}^{f},\\Omega^{f},\\tau^{f}\\} but to avoid notational clutter, conditioning on hyperparameters and inputs or observations is not explicitly indicated. In the measurement error interpretation, the targets, i.e., given values of 𝐫f​(𝝃)superscript𝐫𝑓𝝃{\\bf r}^{f}(\\bm{\\xi}), are considered to be values of a latent function 𝐫lf​(𝝃)superscriptsubscript𝐫𝑙𝑓𝝃{\\bf r}_{l}^{f}(\\bm{\\xi}) corrupted by the i.i.d. noise ϵfsuperscriptbold-italic-ϵ𝑓\\boldsymbol{\\epsilon}^{f} with priors 𝐫lf(𝝃)∼𝒢𝒫(𝐫lf(𝝃)∣𝟎,kf(𝝃,𝝃′|𝜽f)⊗𝛀f){\\bf r}_{l}^{f}(\\bm{\\xi})\\sim\\mathcal{GP}({\\bf r}_{l}^{f}(\\bm{\\xi})\\mid{\\bf 0},k^{f}(\\bm{\\xi},\\bm{\\xi}^{\\prime}|\\bm{\\theta}^{f})\\otimes\\mathbf{\\Omega}^{f}) and ϵf∼𝒢​𝒫​(ϵf∣𝟎,δ​(𝝃,𝝃′)​τf​𝐈)similar-tosuperscriptbold-italic-ϵ𝑓𝒢𝒫conditionalsuperscriptbold-italic-ϵ𝑓0𝛿𝝃superscript𝝃′superscript𝜏𝑓𝐈\\boldsymbol{\\epsilon}^{f}\\sim\\mathcal{GP}(\\boldsymbol{\\epsilon}^{f}\\mid{\\bf 0},\\delta(\\bm{\\xi},\\bm{\\xi}^{\\prime})\\tau^{f}{\\bf I}). Choosing the right kernel function for a specific application is non-trivial.\nWhen there is no prior knowledge to guide the choice, the automatic relevance determinant (ARD) kernel  with 𝜽f=(θ0f,…,θ1f)Tsuperscript𝜽𝑓superscriptsuperscriptsubscript𝜃0𝑓…superscriptsubscript𝜃1𝑓𝑇\\bm{\\theta}^{f}=(\\theta_{0}^{f},\\ldots,\\theta_{1}^{f})^{T} is often used. The ARD kernel can freely capture the influence of each individual input (coordinate of 𝝃𝝃\\bm{\\xi}) on the output.\nThe hyperparameters {τf,𝜽f}superscript𝜏𝑓superscript𝜽𝑓\\{\\tau^{f},\\bm{\\theta}^{f}\\} can be estimated by maximizing the log-marginal likelihood (see section 3.1). The high-fidelity prior in this model can be written as follows by virtue of the independence assumption. Again, this is the GP over the noisy observations with cumulative i.i.d. noise variance ∑fτfsubscript𝑓subscript𝜏𝑓\\sum_{f}\\tau_{f}, and underlying latent function 𝐲lF(𝝃)∼𝒢𝒫(𝐲lF(𝝃)∣𝟎,∑f=1Fkf(𝝃,𝝃′|𝜽f)⊗𝛀f){\\bf y}_{l}^{F}(\\bm{\\xi})\\sim\\mathcal{GP}({\\bf y}^{F}_{l}(\\bm{\\xi})\\mid{\\bf 0},\\sum_{f=1}^{F}k^{f}(\\bm{\\xi},\\bm{\\xi}^{\\prime}|\\bm{\\theta}^{f})\\otimes\\mathbf{\\Omega}^{f}). Beginning with the lowest fidelity level f=1𝑓1f=1, we assume the prior (3) and use inputs 𝒳1superscript𝒳1\\mathcal{X}^{1} together with outputs 𝐘1superscript𝐘1{\\bf Y}^{1} to learn {τ1,𝜽1}superscript𝜏1superscript𝜽1\\{\\tau^{1},\\bm{\\theta}^{1}\\} and find the predictive posterior 𝐫l1​(𝝃)∼𝒩​(𝐫l1​(𝝃)∣𝝁r1​(𝝃),𝐕r1​(𝝃))similar-tosubscriptsuperscript𝐫1𝑙𝝃𝒩conditionalsubscriptsuperscript𝐫1𝑙𝝃subscriptsuperscript𝝁1𝑟𝝃subscriptsuperscript𝐕1𝑟𝝃{\\bf r}^{1}_{l}(\\bm{\\xi})\\sim\\mathcal{N}\\left({\\bf r}^{1}_{l}(\\bm{\\xi})\\mid\\bm{\\mu}^{1}_{r}(\\bm{\\xi}),{\\bf V}^{1}_{r}(\\bm{\\xi})\\right). At the next step we use inputs 𝒳2superscript𝒳2\\mathcal{X}^{2} and outputs 𝐑2:=𝐘2−𝐘𝐞21assignsuperscript𝐑2superscript𝐘2subscriptsuperscript𝐘1subscript𝐞2{\\bf R}^{2}:={\\bf Y}^{2}-{\\bf Y}^{1}_{{\\bf e}_{2}} in the same procedure to learn {τ2,𝜽2}superscript𝜏2superscript𝜽2\\{\\tau^{2},\\bm{\\theta}^{2}\\}, and obtain the posterior for 𝐫l2​(𝝃)∼𝒩​(𝐫l2​(𝝃)∣𝝁r2​(𝝃),𝐕r2​(𝝃))similar-tosuperscriptsubscript𝐫𝑙2𝝃𝒩conditionalsubscriptsuperscript𝐫2𝑙𝝃subscriptsuperscript𝝁2𝑟𝝃subscriptsuperscript𝐕2𝑟𝝃{\\bf r}_{l}^{2}(\\bm{\\xi})\\sim\\mathcal{N}\\left({\\bf r}^{2}_{l}(\\bm{\\xi})\\mid\\bm{\\mu}^{2}_{r}(\\bm{\\xi}),{\\bf V}^{2}_{r}(\\bm{\\xi})\\right). Here we define a matrix of residuals 𝐑2superscript𝐑2{\\bf R}^{2} using the notation introduced in section 2. This procedure is repeated up to fidelity level f=F𝑓𝐹f=F, i.e., independently learning the hyperparameters associated with each 𝐫f​(𝝃)superscript𝐫𝑓𝝃{\\bf r}^{f}(\\bm{\\xi}), given observations {𝐑f=𝐘f−𝐘𝐞ff−1}f=2Fsuperscriptsubscriptsuperscript𝐑𝑓superscript𝐘𝑓subscriptsuperscript𝐘𝑓1subscript𝐞𝑓𝑓2𝐹\\{{\\bf R}^{f}={\\bf Y}^{f}-{\\bf Y}^{f-1}_{{\\bf e}_{f}}\\}_{f=2}^{F}. The predictive posteriors over 𝐫lf​(𝝃)superscriptsubscript𝐫𝑙𝑓𝝃{\\bf r}_{l}^{f}(\\bm{\\xi}) are derived using standard Gaussian conditioning rules , and the posterior for the high-fidelity latent function 𝐲lF​(𝝃)superscriptsubscript𝐲𝑙𝐹𝝃{\\bf y}_{l}^{F}(\\bm{\\xi}) can be written compactly as the following sum of GPs where vec​(⋅)vec⋅{\\rm vec}{(\\cdot)} denotes vectorization and 𝐤f​(𝝃)=(kf​(𝝃,𝝃1|𝜽f),…,kf​(𝝃,𝝃Nf|𝜽f))Tsuperscript𝐤𝑓𝝃superscriptsuperscript𝑘𝑓𝝃conditionalsubscript𝝃1superscript𝜽𝑓…superscript𝑘𝑓𝝃conditionalsubscript𝝃subscript𝑁𝑓superscript𝜽𝑓𝑇{\\bf k}^{f}(\\bm{\\xi})=(k^{f}(\\bm{\\xi},\\bm{\\xi}_{1}|\\bm{\\theta}^{f}),\\ldots,k^{f}(\\bm{\\xi},\\bm{\\xi}_{N_{f}}|\\bm{\\theta}^{f}))^{T} is the vector of covariances between the latent function values 𝐫lf​(⋅)superscriptsubscript𝐫𝑙𝑓⋅{\\bf r}_{l}^{f}(\\cdot) at 𝝃𝝃\\bm{\\xi} and points in 𝒳fsuperscript𝒳𝑓\\mathcal{X}^{f}. We note that since the posteriors over each 𝐫f​(𝝃)superscript𝐫𝑓𝝃{\\bf r}^{f}(\\bm{\\xi}) are learned independently, the training procedure is parallelizable if implemented without active learning (discussed below). The formulation (7) considers a general case in which noise terms τf​𝐈superscript𝜏𝑓𝐈\\tau^{f}{\\bf I} are included.\nA computational model (of any fidelity) can, however, be treated as deterministic function without random noise . Thus, we can consider the residual information to be deterministic and model it using GPs without the noise terms, in which case 𝐲lF​(𝝃)=𝐲F​(𝝃)superscriptsubscript𝐲𝑙𝐹𝝃superscript𝐲𝐹𝝃{\\bf y}_{l}^{F}(\\bm{\\xi})={\\bf y}^{F}(\\bm{\\xi}) and the predictive posterior reduces to Note that the coregionalization matrices 𝛀fsuperscript𝛀𝑓\\mathbf{\\Omega}^{f} cancel out for the expectation predictions (of any fidelity). This is consistent with autokrigeability , which is utilized by Xing et al.  to deal with high-dimensional, single-fidelity mechanical design simulations. Noiseless data is the usual assumption for simulations (the so-called ground truth approximation), but one could attempt to incorporate systematic errors arising from the model formulation (model inadequacy), from the numerical approximation (numerical errors), or from parameter uncertainty. In the same spirit as Xing et al. , we now simplify the model by setting 𝛀f=𝐈superscript𝛀𝑓𝐈\\mathbf{\\Omega}^{f}={\\bf I} for f=1,…,F𝑓1…𝐹f=1,\\dots,F. This assumes that the components of 𝐲F​(𝝃)superscript𝐲𝐹𝝃{\\bf y}^{F}(\\bm{\\xi}) are mutually independent given {𝜽f}superscript𝜽𝑓\\{\\bm{\\theta}^{f}\\}. Note that the variance of the prediction then ignores the spatial correlations, which will affect the uncertainty estimation, but the mean prediction is unaffected.\nWe will discuss this issue further in Section 3.2, in which we see that the active learning process is not affected. Retaining the noise terms and coregionalisation matrices, i.e., retaining Eq. (7), requires an additional set of hyperparameters, namely the τfsuperscript𝜏𝑓\\tau^{f} and the entries of each 𝛀fsuperscript𝛀𝑓\\mathbf{\\Omega}^{f}. We may even use a richer covariance structure, such as the linear model of coregionalisation [ref]25 with linear combinations of separable covariances. In low-dimensional output spaces, this could be of benefit in terms of accuracy and variance capture. In very high-dimensional spaces, on the other hand, these approaches lead to problems in terms of maximizing the likelihood (or evidence lower bound) due to the higher number of inputs in the optimization problem [ref]25 (see section 3.3). If modelling the noise is deemed important for a particular problem (perhaps because the solutions are known to be corrupted), a compromise could be achieved by retaining the {τf}superscript𝜏𝑓\\{\\tau^{f}\\} terms and setting 𝛀f=𝐈superscript𝛀𝑓𝐈\\mathbf{\\Omega}^{f}={\\bf I}. We discuss this formulation later. We point out, however, that in the examples in Section 4, the noise terms are not included. For most data sets, Nfsubscript𝑁𝑓N_{f} decreases in size with f𝑓f, and is typically small for large values of f𝑓f. The uncertainty in the predictions for 𝐫lfsuperscriptsubscript𝐫𝑙𝑓{\\bf r}_{l}^{f} (or 𝐫fsuperscript𝐫𝑓{\\bf r}^{f}) therefore increases with f𝑓f. We have to keep in mind, however, that that uncertainty is also bounded by the scales of the residuals between different fidelities. For high values of f𝑓f, the simulations for adjacent fidelities would tend to be similar, and therefore the residuals would tend to be small. Thus, the added uncertainties for large f𝑓f will be small. For small f𝑓f, the residuals between adjacent fidelities are expected to be larger but the uncertainties would be smaller by virtue of the greater number of samples. The additive structure can, therefore, potentially ensure that the final uncertainty reflects the true model uncertainty. We discuss this further in the first example in section 4.2.\n\nReference markers found: [25]\n\nContext windows for each [ref]:\n[ref]25: namely the τfsuperscript𝜏𝑓\\tau^{f} and the entries of each 𝛀fsuperscript𝛀𝑓\\mathbf{\\Omega}^{f}. We may even use a richer covariance structure, such as the linear model of coregionalisation [ref]25 with linear combinations of separable covariances. In low-dimensional output spaces, this could be of benefit in terms of accuracy and variance capture. In very high-dimensional spaces, on\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 25,\n            \"titles\": [\n                \"A General Framework for Multi-fidelity Bayesian Optimization with Gaussian Processes\",\n                \"Survey of multifidelity methods in uncertainty propagation, inference, and optimization\",\n                \"Predicting the output from a complex computer code when fast approximations are available\",\n                \"Special section on multidisciplinary design optimization: Metamodeling in multidisciplinary design optimization: How far have we really come?\",\n                \"High-performance integrated control of water quality and quantity in urban water reservoirs\",\n                \"The Design and Analysis of Computer Experiments\",\n                \"Comparison of surrogate-based uncertainty quantification methods for computationally expensive simulators\",\n                \"Bayesian emulation of complex multi-output and dynamic computer models\",\n                \"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations\",\n                \"Construction of response surface approximations for design optimization\",\n                \"Reduced-order modeling of time-dependent PDEs with multiple parameters in the boundary data\",\n                \"Review of multi-fidelity models\",\n                \"Aerodynamic shape optimization by variable-fidelity computational fluid dynamics models: A review of recent progress\",\n                \"Multi-fidelity Gaussian process regression for computer experiments\",\n                \"Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling\",\n                \"Multi-fidelity Gaussian process regression for prediction of random fields\",\n                \"Deep Gaussian processes\",\n                \"A stochastic collocation algorithm with multifidelity models\",\n                \"Greedy nonlinear autoregression for multifidelity computer models at different scales\",\n                \"Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control\",\n                \"Deep Gaussian processes for multi-fidelity modeling\",\n                \"Gaussian Processes for Machine Learning\",\n                \"Bayesian calibration of computer models\",\n                \"Shared-GP: Learning interpretable shared hidden structure across data spaces for design space analysis and exploration\",\n                \"Kernels for vector-valued functions: A review\",\n                \"A new multi-task learning framework for fuel cell model outputs in high-dimensional spaces\",\n                \"Doubly stochastic variational inference for deep gaussian processes\",\n                \"Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences\",\n                \"Improved error bounds for scattered data interpolation by radial basis functions\",\n                \"Improving the sample complexity using global data\",\n                \"Information-theoretic regret bounds for Gaussian process optimization in the bandit setting\",\n                \"On Kernelized Multi-armed Bandits\",\n                \"Exponential regret bounds for Gaussian process bandits with deterministic observations\",\n                \"On prediction properties of kriging: Uniform error bounds and robustness\",\n                \"Uniform Error and Posterior Variance Bounds for Gaussian Process Regression with Application to Safe Control\",\n                \"Random fields and geometry\",\n                \"Convergence rate of the data-independent p-greedy algorithm in kernel-based approximation\",\n                \"Emulation of physical processes with Emukit\",\n                \"Computational materials science: An introduction\",\n                \"Generated Reference Title 39 for [ref]25\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]25 based on the context and research area of Residual Gaussian Process: A Tractable Nonparametric Bayesian Emulator for Multi-fidelity Simulations. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}