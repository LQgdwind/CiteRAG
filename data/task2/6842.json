{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Adversarial Illusions in Multi-Modal Embeddings\n\nPaper Abstract: \n\nPaper Section Text:\nMachine learning pipelines. \nA machine learning model takes input x∈𝒳𝑥𝒳x\\in\\mathcal{X} and produces output y∈𝒴𝑦𝒴y\\in\\mathcal{Y} that can be a class label for x𝑥x, a generated image, or a text response by a chatbot. To create a model for some task 𝒳→𝒴→𝒳𝒴\\mathcal{X}\\rightarrow\\mathcal{Y}, an ML engineer needs to build a training pipeline for that task. Training models from scratch requires a lot of resources and data. An encoder θ𝜃\\theta that maps inputs into representations can help improve performance on many tasks 2.\nHowever, training an encoder is expensive, so many ML pipelines rely on open-sourced or commercial models for high-quality embeddings. ML engineers can then focus on training a downstream model ϕitalic-ϕ\\phi that operates on the embeddings. Many ML pipelines thus have two stages: Multi-modal embeddings. \nMulti-modal encoders map inputs x(m)superscript𝑥𝑚x^{(m)} from modalities m∈ℳ𝑚ℳm\\in\\mathcal{M} into a single embedding space. Each modality m𝑚m has its own encoder θ(m)superscript𝜃𝑚\\theta^{(m)}. Encoders are trained on multi-modal tuples (x(m),y(m¯))superscript𝑥𝑚superscript𝑦¯𝑚(x^{(m)},y^{(\\bar{m})}) that are semantically aligned (e.g., a picture of wolves and the text “Wolves,” as shown in Figure 5). Encoders are trained using contrastive learning  that pushes representations of aligned inputs closer in the embedding space and away from the representations of semantically different inputs. Given a bi-modal dataset 𝒟=(X,Y)∈m×m¯𝒟𝑋𝑌𝑚¯𝑚\\mathcal{D}=(X,Y)\\in m\\times\\bar{m}, we\ndefine an encoder’s alignment on 𝒟𝒟\\mathcal{D} as the mean cosine similarity between the embeddings of the elements in each (x,y)∈𝒟𝑥𝑦𝒟(x,y)\\in\\mathcal{D}: In this paper, we focus on images, sounds, and texts. As of this writing, ImageBind [ref]10 and AudioCLIP [ref]13 are the only open-source multi-modal embeddings that support these three modalities. Multi-modal embeddings exhibit “emergent” alignment between modalities. Semantically similar images and sounds (e.g., a picture of a dog and an audio recording of a dog barking) have similar embeddings, even though the training data does not include image-audio tuples. The distinction between natural (present in the training data) and emergent alignment is important for multi-modal encoders, but attacks in this paper are agnostic to the target modality and the source of its alignment. We generically define an encoder’s organic alignment on datasets 𝒟𝒟\\mathcal{D} where for all (x,y)∈𝒟𝑥𝑦𝒟(x,y)\\in\\mathcal{D}, x𝑥x and y𝑦y are semantically related, regardless of whether it is learned from aligned training pairs or “emerged” from other, aligned modalities. Downstream models. \nMulti-modal embeddings align semantically similar inputs across modalities. Thus, downstream models do not need to be trained on multiple modalities and can operate directly on the embeddings. Zero-shot classification matches the embedding of an input to the closest class embedding (mean embedding of known class representatives). With multi-modal embeddings, zero-shot classification can classify inputs to classes in a different modality, e.g., label an audio with an image class. Retrieval, similar to zero-classification, matches a query to an item that’s closest to it in the embedding space. Image generation takes an embedding and performs conditional generation using a diffusion model. ImageBind’s image encoder is initialized from the CLIP visual encoder model 2. Therefore, diffusion models that operate on CLIP embeddings (e.g., unCLIP ) can also operate on ImageBind embeddings. Text generation can use multi-modal embeddings as inputs to instruction-following language models, e.g., PandaGPT . Adversarial perturbations. \nAn adversarial perturbation δ𝛿\\delta added to input x𝑥x causes a model ϕitalic-ϕ\\phi to produce an incorrect output (as in untargeted adversarial examples ), or even a specific output ytsubscript𝑦𝑡y_{t} chosen by the adversary . Adversarial examples for text were first studied in the context of toxicity classification  and reading comprehension . Recently, jailbreaking attacks on dialog systems, i.e., chatbots, were demonstrated to bypass internal guardrails on hate speech . All of these attacks target a specific downstream task. Several recent papers 8   deploy adversarial perturbations against encoders that use contrastive learning. These attacks work by increasing contrastive loss and are necessarily untargeted: they prevent perturbed inputs from being embedded near the original input but the adversary has no control over the placement or alignment of the perturbed input. In other recent work  1 , adversarial perturbations are used for jailbreaking and prompt injection in multi-modal chatbots (a specific downstream task). In 8, adversarial perturbations against CLIP image embeddings are shown to affect downstream tasks. The attack is not cross-modal, the adversary does not have an arbitrary choice of inputs, and perturbations are very large and visible. Dong et al.  present an untargeted attack on image embeddings and a targeted attack on text descriptions in a specific language model (rather than multi-modal embeddings). The attack causes the model to incorrectly predict the main object in an image. There is no analysis of how an embedding attack transfers to multiple downstream tasks, modalities beyond images and text, or the robustness of defenses. Zhao et al. 9 present two transfer-based and one query-based attack against visual chatbots. The attacks are evaluated only via embedding alignment (CLIP Score) between the adversarial image xδsubscript𝑥𝛿x_{\\delta} and target ytsubscript𝑦𝑡y_{t}. This is insufficient to show transferability to downstream tasks. For example, to attack a generation model ϕitalic-ϕ\\phi based on a multi-modal embedding θ𝜃\\theta, it is not enough to show that θ​(xδ)𝜃subscript𝑥𝛿\\theta(x_{\\delta}) and θ​(yt)𝜃subscript𝑦𝑡\\theta(y_{t}) are aligned. The attack succeeds only if ϕ​(θ​(xδ))italic-ϕ𝜃subscript𝑥𝛿\\phi(\\theta(x_{\\delta}))—i.e., the image generated from the embedding, rather than the image used to produce the embedding—is classified to ytsubscript𝑦𝑡y_{t}.\nThere are no modalities beyond images and text,\ntransferability across encoders is limited to CLIP as the target, and there is no analysis of defenses, even though adversarial images are foiled by simple countermeasures such as applying JPEG at evaluation time (see Section 6.1). Our paper demonstrates that adversarial perturbations enable cross-modal, downstream task-agnostic adversarial alignment of arbitrary inputs in multi-modal embeddings. We present a black-box, query-based version of the attack, analyze transferability across different embeddings, and demonstrate that the attack can evade several defenses. Other attacks. \nPrevious work considered collisions in NLP models 1, but—unlike this\npapers—it targets specific downstream tasks in a single modality (text), and the adversary does not have an arbitrary choice of colliding inputs. Poisoning and backdoor attacks   compromise embeddings at training time. By contrast, this paper focuses on evaluation-time attacks against “clean,” unmodified models.\n\nReference markers found: [10]\n\nContext windows for each [ref]:\n[ref]10: as the mean cosine similarity between the embeddings of the elements in each (x,y)∈𝒟𝑥𝑦𝒟(x,y)\\in\\mathcal{D}: In this paper, we focus on images, sounds, and texts. As of this writing, ImageBind [ref]10 and AudioCLIP [ref]13 are the only open-source multi-modal embeddings that support these three modalities. Multi-modal embeddings exhibit “emergent” alignment between modalities. Semantically\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 10,\n            \"titles\": [\n                \"Explaining and Harnessing Adversarial Examples\",\n                \"Square Attack: a query-efficient black-box adversarial attack via random search\",\n                \"Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs\",\n                \"Are aligned neural networks adversarially aligned?\",\n                \"Audio Adversarial Examples: Targeted Attacks on Speech-to-Text\",\n                \"Reproducible scaling laws for contrastive language-image learning\",\n                \"How Robust is Google’s Bard to Adversarial Image Attacks?\",\n                \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n                \"Audio Set: An ontology and human-labeled dataset for audio events\",\n                \"R-LPIPS: An Adversarially Robust Perceptual Similarity Metric\",\n                \"ImageBind: One embedding space to bind them all\",\n                \"On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models\",\n                \"AudioCLIP: Extending CLIP to image, text and audio\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Deceiving Google’s Perspective API Built for Detecting Toxic Comments\",\n                \"Adversarial examples are not bugs, they are features\",\n                \"Label Poisoning is All You Need\",\n                \"BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning\",\n                \"Adversarial Examples for Evaluating Reading Comprehension Systems\",\n                \"Certified Robustness to Adversarial Word Substitutions\",\n                \"Audiocaps: Generating captions for audios in the wild\",\n                \"Adversarial Self-Supervised Contrastive Learning\",\n                \"Adversarial examples in the physical world\",\n                \"Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning\",\n                \"BindDiffusion: One diffusion model to bind them all\",\n                \"Delving into Transferable Adversarial Examples and Black-box Attacks\",\n                \"Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial Examples\",\n                \"Towards Deep Learning Models Resistant to Adversarial Attacks\",\n                \"Adversarial Training Methods for Semi-Supervised Text Classification\",\n                \"Representation Learning with Contrastive Predictive Coding\",\n                \"Visual adversarial examples jailbreak aligned large language models\",\n                \"Learning Transferable Visual Models From Natural Language Supervision\",\n                \"Certified Defenses against Adversarial Examples\",\n                \"Hierarchical Text-Conditional Image Generation with CLIP Latents\",\n                \"ImageNet Large Scale Visual Recognition Challenge\",\n                \"Defense-GAN: Protecting classifiers against adversarial attacks using generative models\",\n                \"Adversarial training for free!\",\n                \"Plug and pray: Exploiting off-the-shelf components of multi-modal models\",\n                \"JPEG-resistant adversarial images\",\n                \"Demystifying the Adversarial Robustness of Random Transformation Defenses\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]10 based on the context and research area of Adversarial Illusions in Multi-Modal Embeddings. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}