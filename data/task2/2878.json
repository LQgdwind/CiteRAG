{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, we provide new numerical results and detailed information about our experiments in the main paper. Decision boundary plots in Figure 1:  In order to visualize the capabilities of our convex training approach, we perform an experiment on the spiral dataset which is known to be challenging for 2-layer networks while 3-layer networks can readily interpolate the training data (i.e. exactly fit the training labels) . As the baselines of our analysis, we include the two-layer convex training approach in  and recently introduced three-layer convex training approach (with weight decay regularization and several architectural and parametric assumptions) in . As our experimental setup, we consider a binary classification task with 𝐲∈{+1,−1}n𝐲superscript11𝑛\\mathbf{y}\\in\\{+1,-1\\}^{n} and squared loss. We choose (n,m1,m2,P1,P2,β)=(30,5,1,11,11,1​e−4)𝑛subscript𝑚1subscript𝑚2subscript𝑃1subscript𝑃2𝛽305111111𝑒4(n,m_{1},m_{2},P_{1},P_{2},\\beta)=(30,5,1,11,11,1e-4) and for  we use P=50𝑃50P=50 neurons/hyperplane arrangements. We also use CVPXY with MOSEK solver  [ref]53  to globally solve the convex programs. As demonstrated in Figure 1, baselines methods, especially , fit a function that is significantly different than the underlying spiral data distribution. This clearly shows that\nsince  is restricted two-layer networks and  have multiple assumptions, i.e., unit Frobenius norm constraints on the layer weights (‖𝐖l​k‖F≤1,∀l∈[L−2]formulae-sequencesubscriptnormsubscript𝐖𝑙𝑘𝐹1for-all𝑙delimited-[]𝐿2\\|\\mathbf{W}_{lk}\\|_{F}\\leq 1,\\forall l\\in[L-2]) and last hidden layer weights cannot be matrices (𝐰(L−1)​k∈ℝmL−2subscript𝐰𝐿1𝑘superscriptℝsubscript𝑚𝐿2\\mathbf{w}_{(L-1)k}\\in\\mathbb{R}^{m_{L-2}}), both baseline approaches fail to reflect true expressive power of deep networks (L>2𝐿2L>2). On the other hand, our convex training approach for path regularized networks fits a model that successfully describes the underlying data distribution for this challenging task. Additional experiments:  We also conduct experiments on several datasets available in UCI Machine Learning Repository , where we particularly selected the datasets from  such that n≤500𝑛500n\\leq 500. For these datasets, we consider a conventional binary classification framework with (m1,m2,K,β)=(100,1,40,0.5)subscript𝑚1subscript𝑚2𝐾𝛽1001400.5(m_{1},m_{2},K,\\beta)=(100,1,40,0.5) and compare the test accuracies of non-convex architectures trained with SGD and Adam with their convex counter parts in (8). For these experiments, we use the 80%−20%percent80percent2080\\%-20\\% splitting ratio for the training and test sets. Furthermore, we train each algorithm long enough to reach training accuracy one. As shown in Table 3, our convex approach achieves higher or the same test accuracy compared to the standard non-convex training approach for most of the datasets (precisely 202020 and 191919 out of 212121 datasets for SGD and Adam, respectively). We also note that for this experiment, we used the unconstrained form in (11) with the approximate version in Remark 3.3 of . Details for the experiments in Table 2 and Figure 5: We first note that for the experiments in Table 2, we use CVX/CVPXY  [ref]53  to globally solve the proposed convex program in (8). For these experiments, we use a laptop with i7 processor and 16GB of RAM. In order to tune the learning rate of SGD/GD, we first perform training with a bunch of learning different learning rates and select the one with the best performance on the validation datasets, which is 0.0050.0050.005 in these experiments. For comparatively larger scale image classification experiments in Figure 5, we utilize a cluster GPU with 50GB of memory. However since the equivalent convex program in (8) has constraint which are challenging to handle for these datasets, we propose the following unconstrained convex problem which has the same global minima with the constrained version as discussed in [ref]48 where λ>0𝜆0\\lambda>0 coefficient to penalize the violated constraints and h𝒞​(𝐳)subscriptℎ𝒞𝐳h_{\\mathcal{C}}(\\mathbf{z}) is a function to sum the absolute value of all constraint violations defined as Thus, we obtain an unconstrained version of convex optimization problem (11), where one can use commonly employed first-order gradient based optimizers, e.g., SGD and Adam, available in deep learning libraries such PyTorch and Tensorflow. For both CIFAR-10 and Fashion-MNIST, we use the same training and test splits in the original datasets. We again perform a grid search to tune the learning rate, where the best performance is achieved by the following choices and as the learning rates for CIFAR-10 and Fashion-MNIST, respectively. We choose the momentum coefficient of the SGD optimizer as 0.90.90.9. In addition to this, we set P1​P2=Ksubscript𝑃1subscript𝑃2𝐾P_{1}P_{2}=K and λ=1​e−5𝜆1𝑒5\\lambda=1e-5. More importantly, we remark that these experiments are performed by using a small sampled subset of hyperplane arrangements rather than sampling all possible arrangements as detailed in Remark 3.3 of . In particular, we first generate random weight matrices from a multivariate standard normal distribution and then solve the convex program using only the arrangements of the sampled weight matrices. Details for the experiments in Figure 6: Layerwise training with shallow neural networks was proven to work remarkably well. Particularly,  shows that one can train three-layer neural networks sequentially to build deep networks that outperform end-to-end training with SOTA architectures. In Figure 6, we apply this layerwise training procedure with our convex training approach. In particular, each stage in this figure is our three-layer convex formulation in Theorem 1. Here, we use the same experimental setting in the previous section. We observe that making the network deeper by stacking convex layers resulted in significant performance improvements. Specificaly, at the fifth stage, we achieved almost 85%percent8585\\% accuracy for CIFAR-10 unlike below 60%percent6060\\% accuracies in Figure 5.\n\nReference markers found: [53]\n\nContext windows for each [ref]:\n[ref]53: and for  we use P=50𝑃50P=50 neurons/hyperplane arrangements. We also use CVPXY with MOSEK solver  [ref]53  to globally solve the convex programs. As demonstrated in Figure 1, baselines methods, especially , fit a function that is significantly different than the underlying spiral data\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 53,\n            \"titles\": [\n                \"A Rewriting System for Convex Optimization Problems\",\n                \"Norm-based capacity control in neural networks\",\n                \"Path-sgd: Path-normalized optimization in deep neural networks\",\n                \"Convex geometry of two-layer relu networks: Implicit autoencoding and interpretable models\",\n                \"Convex geometry and duality of over-parameterized neural networks\",\n                \"Convex optimization for shallow neural networks\",\n                \"Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-layer Networks\",\n                \"Failures of gradient-based deep learning\",\n                \"Deep learning, volume 1\",\n                \"Learning One-hidden-layer Neural Networks with Landscape Design\",\n                \"Spurious Local Minima are Common in Two-Layer ReLU Neural Networks\",\n                \"Efficient approaches for escaping higher order saddle points in non-convex optimization\",\n                \"On the complexity of training neural networks with continuous activation functions\",\n                \"Training a 3-node neural network is np-complete\",\n                \"Hardness results for neural network approximation problems\",\n                \"SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data\",\n                \"On the Power of Over-parametrization in Neural Networks with Quadratic Activation\",\n                \"On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization\",\n                \"Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks\",\n                \"Parallel Deep Neural Networks Have Zero Duality Gap\",\n                \"Revealing the Structure of Deep Neural Networks via Convex Duality\",\n                \"Global Optimality Beyond Two Layers: Training Deep ReLU Networks via Convex Programs\",\n                \"Deep neural networks with multi-branch architectures are intrinsically less non-convex\",\n                \"Global optimality in neural network training\",\n                \"Wide Residual Networks\",\n                \"Residual Networks Behave Like Ensembles of Relatively Shallow Networks\",\n                \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\",\n                \"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\",\n                \"Xception: Deep Learning with Depthwise Separable Convolutions\",\n                \"Aggregated Residual Transformations for Deep Neural Networks\",\n                \"Understanding Deep Neural Networks with Rectified Linear Units\",\n                \"Tight Hardness Results for Training Depth-2 ReLU Networks\",\n                \"The Computational Complexity of ReLU Network Training Parameterized by Data Dimensionality\",\n                \"Deep Residual Learning for Image Recognition\",\n                \"Representation costs of linear neural networks: Analysis and design\",\n                \"L1 regularization in infinite dimensional feature spaces\",\n                \"Convex optimization\",\n                \"Breaking the curse of dimensionality with convex neural networks\",\n                \"Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers\",\n                \"Global convergence of three-layer neural networks in the mean field regime\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]53 based on the context and research area of Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}