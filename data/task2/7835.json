{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Logical Characterizations of Recurrent Graph Neural Networks with Reals and Floats\n\nPaper Abstract: \n\nPaper Section Text:\nGraph Neural Networks (GNNs)\n  \nhave proven to be highly useful for processing graph data in numerous applications that span\na remarkable range of disciplines\nincluding bioinformatics¬†, recommender systems , traffic forecasting ,\nand a multitude of others. The success of GNNs in applications has\nstimulated lively research into their theoretical properties\nsuch as expressive power. A landmark result is due to\nBarcel√≥ et al. [ref]5 which was among\nthe first to characterize the expressive power of GNNs in terms of\nlogic, see     and references therein for related results.\nMore precisely, Barcel√≥ et al.¬†show that a basic GNN model with a\nconstant number of iterations has exactly the same expressive power\nas graded modal logic GMLGML\\mathrm{GML} in restriction to properties definable in first-order logic FO. In this article, we advance the analysis of the expressive\npower of GNNs in two directions. First, we study the relation between GNN models\nbased on real numbers, as mostly studied in theory, and GNN models\nbased on floating point numbers, as mostly used in practice. And\nsecond, we focus on a family of basic recurrent GNNs while\nprevious research has mainly considered GNNs with a constant number\nof iterations and, in one case, recurrent GNNs with two termination conditions .\nThe GNNs studied in the current paper have a simple and natural termination condition: termination\nis signaled via designated feature vectors and thus the GNN ‚Äúdecides‚Äù\nby itself about when to terminate; this also means that termination can\nbe learned in the training phase. This termination condition is enough\nto capture, e.g., termination after a pre-determined number of rounds\nas in¬†.\nWe remark that some of our results also apply to constant iteration GNNs and to recurrent GNNs with a\ntermination condition based on fixed points, as used in the inaugural work on GNNs ;\nsee the conclusion for further details on the latter. We provide three main results. The first one is that recurrent GNNs with floats, or GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s, have the same expressive power as the graded modal substitution calculus GMSCGMSC\\mathrm{GMSC} [ref]21 [ref]1 . This is a rule-based modal logic\nthat extends the modal substitution calculus MSCMSC\\mathrm{MSC} [ref]21 with counting modalities. MSCMSC\\mathrm{MSC} has been shown to precisely correspond to distributed computation models based on automata [ref]21 and Boolean circuits [ref]1. It is related to the graded modal Œºùúá\\mu-calculus, but orthogonal in expressive power.\nThe\ncorrespondence between GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s and GMSCGMSC\\mathrm{GMSC} is as follows. Theorem 3.2.\nThe following have the same expressive power: GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s, GMSCGMSC\\mathrm{GMSC}, and R-simple aggregate-combine GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s. Here R-simple aggregate-combine GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s\nmean GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s that use basic aggregate-combine functions as specified by Barcel√≥ et al.¬†[ref]5 and the truncated ReLUReLU\\mathrm{ReLU} as the non-linearity function, see Section 2.1 for the formalities.\nThe theorem shows that an R-simple model of GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s suffices, and in fact GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s with a more complex architecture can be turned into equivalent R-simple ones. We emphasize that the characterization provided by Theorem¬†3.2 is absolute, that is, not relative to a background logic. In contrast, the characterization by Barcel√≥ et al.¬†[ref]5 is relative to first-order logic. Our characterization does rely, however, on an assumption about floating point arithmetics. We believe that this assumption is entirely natural as it reflects practical implementations of floats. Our second result shows that recurrent GNNs with reals, or GNN‚Äã[‚Ñù]GNNdelimited-[]‚Ñù\\mathrm{GNN}[\\mathbb{R}]s, have the same expressive power as the infinitary modal logic œâ‚Äã-‚ÄãGMLùúî-GML\\omega\\text{-}\\mathrm{GML} that consists of infinite disjunctions of GMLGML\\mathrm{GML}-formulas. Theorem 3.4. GNN‚Äã[‚Ñù]GNNdelimited-[]‚Ñù\\mathrm{GNN}[\\mathbb{R}]s have the same expressive power as œâ‚Äã-‚ÄãGMLùúî-GML\\omega\\text{-}\\mathrm{GML}. Again, this result is absolute.\nAs we assume no restrictions on the arithmetics used in GNN‚Äã[‚Ñù]GNNdelimited-[]‚Ñù\\mathrm{GNN}[\\mathbb{R}]s, they are very powerful:\nwith the infinitary logic œâ‚Äã-‚ÄãGMLùúî-GML\\omega\\text{-}\\mathrm{GML}\nit is easy to define even undecidable graph properties.\nWe regard the theorem as an interesting theoretical upper bound on the expressivity of GNNs operating in an unrestricted, recurrent message passing scenario with messages flowing to neighbouring nodes.\nWe note that GNN‚Äã[‚Ñù]GNNdelimited-[]‚Ñù\\mathrm{GNN}[\\mathbb{R}]s can\neasily be shown more expressive\nthan GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s. Our third result considers GNN‚Äã[‚Ñù]GNNdelimited-[]‚Ñù\\mathrm{GNN}[\\mathbb{R}]s and GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s relative to a very expressive background logic, probably the most natural choice in the recurrent GNN context: monadic second-order logic MSOMSO\\mathrm{MSO}. Theorem 4.3.\nLet PùëÉP be a property definable in MSOMSO\\mathrm{MSO}. Then PùëÉP is expressible as a GNN‚Äã[‚Ñù]GNNdelimited-[]‚Ñù\\mathrm{GNN}[\\mathbb{R}] if and only if it is expressible as a GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}. This result says that, remarkably, for the very significant and large class of MSOMSO\\mathrm{MSO}-definable properties, using actual reals with unrestricted arithmetic gives no more expressive power than using floats, by Theorem¬†3.2 even in the R-simple aggregate-combine setting. Thus, for this class of properties, the theoretical analyses\nfrom the literature do not diverge from the practical implementation!\nTaken together, the above results also imply that in restriction to MSOMSO\\mathrm{MSO}-definable properties, GNN‚Äã[‚Ñù]GNNdelimited-[]‚Ñù\\mathrm{GNN}[\\mathbb{R}]s are equivalent to the (finitary!) graded modal substitution calculus GMSCGMSC\\mathrm{GMSC}. In fact, all of the following have the same expressive power relative to MSOMSO\\mathrm{MSO}: GNN‚Äã[‚Ñù]GNNdelimited-[]‚Ñù\\mathrm{GNN}[\\mathbb{R}]s, GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s, R-simple aggregate-combine GNN‚Äã[F]GNNdelimited-[]F\\mathrm{GNN[F]}s,\ngraded modal substitution calculus GMSCGMSC\\mathrm{GMSC}, and œâ‚Äã-‚ÄãGMLùúî-GML\\omega\\text{-}\\mathrm{GML}. In passing, we also develop characterizations of GNNs in terms of distributed automata, which are also crucial tools in our proofs, building links between\nGNNs and distributed computing. Potential future research as well as the limitations of the results presented in this article are discussed in the conclusion section. Related Work.\nBarcel√≥ et al. [ref]5 study aggregate-combine GNNs with a constant number of iterations. They characterize these‚Äîin restriction to properties definable in first-order logic‚Äîin terms of graded modal logic GMLGML\\mathrm{GML} and show that a global readout mechanism leads to a model equivalent to the two-variable fragment of first-order logic with counting quantifiers FOC2superscriptFOC2\\mathrm{FOC}^{2}. Our work extends the former\nresult to include\nrecurrence in a natural way while we leave studying global readouts as future work.\nGrohe  connects the guarded fragment of first-order logic with counting GFO+CGFOC\\mathrm{GFO}+\\mathrm{C} and polynomial-size bounded-depth circuits,\nlinking (non-recurrent) GNNs to the circuit complexity class TC0superscriptTC0\\mathrm{TC}^{0}.\nBenedikt et al.¬† use logics with a generalized form of counting via Presburger quantifiers to obtain characterizations for (non-recurrent) GNNs.\nIt is worth mentioning that the expressive power of (basic) recurrent graph neural network models is invariant under the Weifeiler-Leman test, which has recently been studied in numerous works also in relation to GNNs  3  [ref]5. Pfluger et al.¬† investigate recurrent GNNs with two kinds of termination conditions: one based on reaching a fixed point when iteratively generating\nfeature vectors, and one by terminating after a number of rounds determined by the size of the input graph. Their result is relative to a certain logic LocMMFP, introduced specifically\nfor this purpose. LocMMFP extends first-order logic with a least fixed-point operator over unary monotone formulas, and their characterization is in\nterms of the two-way graded Œºùúá\\mu-calculus. We remark that MSOMSO\\mathrm{MSO} significantly\ngeneralizes LocMMFP and that the graded two-way Œºùúá\\mu-calculus is\nincomparable in expressive power to our GMSCGMSC\\mathrm{GMSC}.\nPfluger et al.¬†do not consider R-simple aggregate-combine GNNs. We view our work as complementing and being in the spirit\nof both Barcel√≥ et al. [ref]5 and¬†. GNNs are essentially distributed systems, and logical characterizations for distributed systems have been studied widely. A related research direction begins with Hella et al. , Kuusisto [ref]21 and Hella et al.  by results linking distributed computation models to modal logics. The articles  and  give characterizations of constant-iteration scenarios with modal logics, and [ref]21 lifts the approach to recurrent message-passing algorithms via showing that the modal substitution calculus MSCMSC\\mathrm{MSC} captures the expressivity of finite distributed message passing automata. Building on the work on MSCMSC\\mathrm{MSC}, Reiter showed in  that the Œºùúá\\mu-fragment of the modal Œºùúá\\mu-calculus captures the expressivity of finite message passing automata in the asynchronous scenario.\nAhvonen et al. [ref]1 recently established a match between MSCMSC\\mathrm{MSC} and distributed systems based on circuits.\n\nReference markers found: [5]\n\nContext windows for each [ref]:\n[ref]5: ,\nand a multitude of others. The success of GNNs in applications has\nstimulated lively research into their theoretical properties\nsuch as expressive power. A landmark result is due to\nBarcel√≥ et al. [ref]5 which was among\nthe first to characterize the expressive power of GNNs in terms of\nlogic, see     and references therein for related results.\nMore precisely, Barcel√≥ et al.¬†show that a basic\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 5,\n            \"titles\": [\n                \"Decidability of Graph Neural Networks via Logical Characterizations\",\n                \"Descriptive complexity for distributed computing with circuits\",\n                \"Descriptive complexity for neural networks via Boolean networks\",\n                \"A new n-ary existential quantifier in description logics\",\n                \"The logical expressiveness of graph neural networks\",\n                \"Handbook of Modal Logic, volume¬†3 of Studies in logic and practical reasoning\",\n                \"On the Correspondence Between Monotonic Max-Sum GNNs and Datalog\",\n                \"Where first-order and monadic second-order logic coincide\",\n                \"A new model for learning in graph domains\",\n                \"The logic of graph neural networks\",\n                \"The Descriptive Complexity of Graph Neural Networks\",\n                \"Inductive Representation Learning on Large Graphs\",\n                \"Weak models of distributed computing, with connections to modal logic\",\n                \"The accuracy of floating point summation\",\n                \"Accuracy and stability of numerical algorithms, Second Edition\",\n                \"On the expressive completeness of the propositional mu-calculus with respect to monadic second order logic\",\n                \"Graph Neural Network for Traffic Forecasting: A Survey\",\n                \"The complexity of the graded Œºùúá\\\\mathrm{\\\\mu}-calculus\",\n                \"Modal Logic and Distributed Message Passing Automata\",\n                \"Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks\",\n                \"Recurrent graph neural networks and their connections to bisimulation and logic\",\n                \"Asynchronous distributed automata: A characterization of the modal mu-fragment\",\n                \"Best \\\"ordering\\\" for floating-point addition\",\n                \"The graph neural network model\",\n                \"RUN-CSP: unsupervised learning of message passing networks for binary constraint satisfaction problems\",\n                \"Monadic second order logic on tree-like structures\",\n                \"Rounding errors in algebraic processes\",\n                \"Graph Neural Networks in Recommender Systems: A Survey\",\n                \"A Comprehensive Survey on Graph Neural Networks\",\n                \"Graph neural networks and their current applications in bioinformatics\",\n                \"How Powerful are Graph Neural Networks?\",\n                \"Generated Reference Title 31 for [ref]5\",\n                \"Generated Reference Title 32 for [ref]5\",\n                \"Generated Reference Title 33 for [ref]5\",\n                \"Generated Reference Title 34 for [ref]5\",\n                \"Generated Reference Title 35 for [ref]5\",\n                \"Generated Reference Title 36 for [ref]5\",\n                \"Generated Reference Title 37 for [ref]5\",\n                \"Generated Reference Title 38 for [ref]5\",\n                \"Generated Reference Title 39 for [ref]5\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]5 based on the context and research area of Logical Characterizations of Recurrent Graph Neural Networks with Reals and Floats. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}