{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: First-order Policy Optimization for Robust Markov Decision Process This research was partially supported by NSF DMS-2134037 and AFOSR FA9550-22-1-0447.\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, we discuss an online method of estimating the robust state-action value function Qrπsubscriptsuperscript𝑄𝜋𝑟Q^{\\pi}_{r} for a given policy π𝜋\\pi, by using samples ξ𝜉\\xi collected during the interaction with the nominal environment ℳNsubscriptℳN\\mathcal{M}_{\\mathrm{N}}.\nBy incorporating this online estimation method into the previously discussed SRPMD methods, we are able to learn a robust policy without the need of training policy within its worst-case environment.\nConsequently, we will also establish the sample complexity of the SRPMD methods with different stepsize schemes discussed in Section 4. To facilitate our presentation, let us define operator F:ℝ|𝒮|​|𝒜|→ℝ|𝒮|​|𝒜|:𝐹→superscriptℝ𝒮𝒜superscriptℝ𝒮𝒜F:\\mathbb{R}^{\\lvert{\\mathcal{S}}\\rvert\\lvert\\mathcal{A}\\rvert}\\to\\mathbb{R}^{\\lvert{\\mathcal{S}}\\rvert\\lvert\\mathcal{A}\\rvert} by where νπsuperscript𝜈𝜋\\nu^{\\pi} denotes the stationary state-action pair distribution induced by policy π𝜋\\pi within the nominal environment ℳNsubscriptℳN\\mathcal{M}_{\\mathrm{N}} , and operator 𝒯π:ℝ|𝒮|​|𝒜|→ℝ|𝒮|​|𝒜|:superscript𝒯𝜋→superscriptℝ𝒮𝒜superscriptℝ𝒮𝒜{\\mathcal{T}}^{\\pi}:\\mathbb{R}^{\\lvert{\\mathcal{S}}\\rvert\\lvert\\mathcal{A}\\rvert}\\to\\mathbb{R}^{\\lvert{\\mathcal{S}}\\rvert\\lvert\\mathcal{A}\\rvert} is defined by where in the second equality we denote ℙuπ​(s′,a′|s,a)=ℙu​(s′|s,a)​π​(a′|s′)superscriptsubscriptℙ𝑢𝜋superscript𝑠′conditionalsuperscript𝑎′𝑠𝑎subscriptℙ𝑢conditionalsuperscript𝑠′𝑠𝑎𝜋conditionalsuperscript𝑎′superscript𝑠′\\mathbb{P}_{u}^{\\pi}(s^{\\prime},a^{\\prime}|s,a)=\\mathbb{P}_{u}(s^{\\prime}|s,a)\\pi(a^{\\prime}|s^{\\prime}).\nWe will also write the previous definition in matrix form as Clearly, given the rectangularity of uncertainty set in (1.2), we have the following equivalent definition of 𝒯πsuperscript𝒯𝜋{\\mathcal{T}}^{\\pi}, where M​(π,x)∈ℝ|𝒮|𝑀𝜋𝑥superscriptℝ𝒮M(\\pi,x)\\in\\mathbb{R}^{\\lvert{\\mathcal{S}}\\rvert} is defined as [M​(π,x)]​(s)=∑a∈𝒜π​(a|s)​x​(s,a)delimited-[]𝑀𝜋𝑥𝑠subscript𝑎𝒜𝜋conditional𝑎𝑠𝑥𝑠𝑎[M(\\pi,x)](s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)x(s,a),\nand σXsubscript𝜎𝑋\\sigma_{X} denotes the support function of set X𝑋X. Given (2.4) in Proposition 2.2, it should be clear that the robust state-action value function Qrπsubscriptsuperscript𝑄𝜋𝑟Q^{\\pi}_{r} is a fixed point of operator F𝐹F.\nOn the other hand, since 𝒯πsuperscript𝒯𝜋{\\mathcal{T}}^{\\pi} is a γ𝛾\\gamma-contraction in ∥⋅∥∞subscriptdelimited-∥∥⋅\\lVert\\cdot\\rVert_{\\infty}-norm.\nThus whenever min⁡(νπ)>0superscript𝜈𝜋0\\min(\\nu^{\\pi})>0,\nQrπsubscriptsuperscript𝑄𝜋𝑟Q^{\\pi}_{r} is the unique fixed-point of F𝐹F. We propose the robust temporal difference (RTD) method (Algorithm 3) and establish its sample complexity for finding a stochastic estimate of Qrπsubscriptsuperscript𝑄𝜋𝑟Q^{\\pi}_{r}.\nFor a given policy, an initial state s0∈𝒮subscript𝑠0𝒮s_{0}\\in{\\mathcal{S}}, and initial action a0∼π(⋅|s0)a_{0}\\sim\\pi(\\cdot|s_{0}), the\nRTD method, at any iteration t≥0𝑡0t\\geq 0,\n(1) Given (st,at)subscript𝑠𝑡subscript𝑎𝑡(s_{t},a_{t}), collects st+1∼ℙN(⋅|st,at)s_{t+1}\\sim\\mathbb{P}_{\\mathrm{N}}(\\cdot|s_{t},a_{t}),\nand make actions at+1∼π(⋅|st+1)a_{t+1}\\sim\\pi(\\cdot|s_{t+1});\n(2) Constructs ζt=(st,at,st+1,at+1)subscript𝜁𝑡subscript𝑠𝑡subscript𝑎𝑡subscript𝑠𝑡1subscript𝑎𝑡1\\zeta_{t}=(s_{t},a_{t},s_{t+1},a_{t+1}), and performs the following Unlike our previous discussions where generic convergence properties of SRPMD are established in agnostic to 𝒰𝒰\\mathcal{U}, the construction of operator f​(⋅;⋅)𝑓⋅⋅f(\\cdot;\\cdot) depends on the available information on the uncertainty set 𝒰𝒰\\mathcal{U}, which we discuss below. Evaluation with Known 𝒰𝒰\\mathcal{U}.\nWhen offline data is available, one can often take the nominal kernel ℙℕsubscriptℙℕ\\mathbb{P}_{\\mathbb{N}} as the empirical kernel estimated from the data and construct the uncertainty set 𝒰𝒰\\mathcal{U} by employing statistical deviation theory .\nNote that the henceforth proposed method does not require us to compute and store ℙℕsubscriptℙℕ\\mathbb{P}_{\\mathbb{N}} and 𝒰𝒰\\mathcal{U} beforehand, instead one can form ℙℕ(⋅|s,a)\\mathbb{P}_{\\mathbb{N}}(\\cdot|s,a) and 𝒰s,asubscript𝒰𝑠𝑎\\mathcal{U}_{s,a} whenever these quantities are needed.\nWe consider a stochastic operator f​(x;ζ):ℝ|𝒮|​|𝒜|→ℝ|𝒮|​|𝒜|:𝑓𝑥𝜁→superscriptℝ𝒮𝒜superscriptℝ𝒮𝒜f(x;\\zeta):\\mathbb{R}^{\\lvert{\\mathcal{S}}\\rvert\\lvert\\mathcal{A}\\rvert}\\to\\mathbb{R}^{\\lvert{\\mathcal{S}}\\rvert\\lvert\\mathcal{A}\\rvert}, where\nζ=(s,a,s′,a′)𝜁𝑠𝑎superscript𝑠′superscript𝑎′\\zeta=(s,a,s^{\\prime},a^{\\prime}) denotes a random quadruple sampled from a Markov chain defined over 𝒵=(𝒮×𝒜)2𝒵superscript𝒮𝒜2\\mathcal{Z}=({\\mathcal{S}}\\times\\mathcal{A})^{2}.\nSpecifically, the operator takes the form of Let {ζt}subscript𝜁𝑡\\left\\{\\zeta_{t}\\right\\} be the Markov chain of state-action pairs generated by policy π𝜋\\pi within ℳℕsubscriptℳℕ\\mathcal{M}_{\\mathbb{N}}, then given (5.1) and (5.2), by letting νπsuperscript𝜈𝜋\\nu^{\\pi} denotes the stationary distribution of {ζt}subscript𝜁𝑡\\left\\{\\zeta_{t}\\right\\}, we have\n𝔼ζ∼νπ​f​(x;ζ)=F​(x).subscript𝔼similar-to𝜁superscript𝜈𝜋𝑓𝑥𝜁𝐹𝑥\\mathbb{E}_{\\zeta\\sim\\nu^{\\pi}}f(x;\\zeta)=F(x). Evaluation with Unknown 𝒰𝒰\\mathcal{U}.\nIn contrast to training with offline data, another typical application of robust MDP involves hedging against the environment changes when the deployed environment of the learned policy is different from the training environment ℙℕsubscriptℙℕ\\mathbb{P}_{\\mathbb{N}}.\nIn this case both the training environment ℙℕsubscriptℙℕ\\mathbb{P}_{\\mathbb{N}} and the uncertainty set 𝒰𝒰\\mathcal{U} can be unknown.\nWe consider the notion of ϵitalic-ϵ\\epsilon-contamination model  when modeling the ambiguity set, namely, The ambiguity set 𝒫s,asubscript𝒫𝑠𝑎\\mathcal{P}_{s,a} defined in (1.3) is therefore the convex combination of the transition kernel of the training environment and another set of pre-specified probability distributions 𝒬s,asubscript𝒬𝑠𝑎\\mathcal{Q}_{s,a} over 𝒮𝒮{\\mathcal{S}}: Clearly, users can adjust the tunable parameter ϵ∈[ref]0 [ref]1italic-ϵ01\\epsilon\\in[ref]0 [ref]1 based on their robustness preference.\nThe choice of 𝒬s,asubscript𝒬𝑠𝑎\\mathcal{Q}_{s,a} also allows encoding prior knowledge on the transition kernel of the potential deployed environment into planning.\nIn particular, setting 𝒬s,a=Δ𝒮subscript𝒬𝑠𝑎subscriptΔ𝒮\\mathcal{Q}_{s,a}=\\Delta_{{\\mathcal{S}}} corresponds to the R-contamination model considered in .\nFor the ambiguity set considered in (5.5), we consider stochastic operator f​(x;ζ)𝑓𝑥𝜁f(x;\\zeta) defined as Notably, the stochastic operator (5.6) does not require any information on the unknown uncertainty set 𝒰𝒰\\mathcal{U} (5.5).\nOne can also immediately verify that 𝔼ζ∼νπ​f​(x;ζ)=F​(x),subscript𝔼similar-to𝜁superscript𝜈𝜋𝑓𝑥𝜁𝐹𝑥\\mathbb{E}_{\\zeta\\sim\\nu^{\\pi}}f(x;\\zeta)=F(x),\nwhere νπsuperscript𝜈𝜋\\nu^{\\pi} denotes the stationary distribution of {ζt}subscript𝜁𝑡\\left\\{\\zeta_{t}\\right\\}.\nIt is also possible to define alternative uncertainty sets other than (5.5),\nas long as one can construct f​(x;ζ)𝑓𝑥𝜁f(x;\\zeta) as an unbiased estimator of F​(x)𝐹𝑥F(x) [ref]26. Through out the rest of our discussions, we make the following assumption on the to-be-evaluated policy and the nominal environment ℳNsubscriptℳN\\mathcal{M}_{\\mathrm{N}}, which is commonly assumed in the literature of reinforcement learning. The policy π𝜋\\pi satisfies mins∈𝒮,a∈𝒜⁡π​(a|s)>0subscriptformulae-sequence𝑠𝒮𝑎𝒜𝜋conditional𝑎𝑠0\\min_{s\\in{\\mathcal{S}},a\\in\\mathcal{A}}\\pi(a|s)>0, and the Markov chain {st}subscript𝑠𝑡\\left\\{s_{t}\\right\\} induced by π𝜋\\pi within the nominal MDP ℳNsubscriptℳN\\mathcal{M}_{\\mathrm{N}} is aperiodic and irreducible. Combining Assumption 1 and the finiteness of the state space 𝒮𝒮{\\mathcal{S}}, the Markov chain {st}subscript𝑠𝑡\\left\\{s_{t}\\right\\} satisfies geometric-mixing property .\nIn addition, the stationary distribution of {st}subscript𝑠𝑡\\left\\{s_{t}\\right\\}, denoted by μπsuperscript𝜇𝜋\\mu^{\\pi}, satisfies μπ​(s)>0superscript𝜇𝜋𝑠0\\mu^{\\pi}(s)>0 for all s∈𝒮𝑠𝒮s\\in{\\mathcal{S}}.\nConsequently, we also have νmin=mins∈𝒮,a∈𝒜⁡νπ​(s,a)>0subscript𝜈subscriptformulae-sequence𝑠𝒮𝑎𝒜superscript𝜈𝜋𝑠𝑎0\\nu_{\\min}=\\min_{s\\in{\\mathcal{S}},a\\in\\mathcal{A}}\\nu^{\\pi}(s,a)>0. The following lemma characterizes the sample complexity of the RTD method for obtaining a stochastic estimation of Qrπsubscriptsuperscript𝑄𝜋𝑟Q^{\\pi}_{r}, which utilizes the machinery of stochastic approximation applied to contraction operators developed in . Under Assumption 1, for any ϵ>0italic-ϵ0\\epsilon>0,\nlet αt=αsubscript𝛼𝑡𝛼\\alpha_{t}=\\alpha for some properly chosen α𝛼\\alpha,\nthen RTD method finds an estimate θTsubscript𝜃𝑇\\theta_{T} satisfying\n𝔼ξ​∥θT−Qπ∥∞≤ϵ,subscript𝔼𝜉subscriptdelimited-∥∥subscript𝜃𝑇superscript𝑄𝜋italic-ϵ\\mathbb{E}_{\\xi}\\lVert\\theta_{T}-Q^{\\pi}\\rVert_{\\infty}\\leq\\epsilon, in at most iterations,\nwhere ξ={ζt}t=0T𝜉superscriptsubscriptsubscript𝜁𝑡𝑡0𝑇\\xi=\\left\\{\\zeta_{t}\\right\\}_{t=0}^{T} denotes the trajectory collected by the RTD method,\nand 𝒪~​(⋅)~𝒪⋅\\widetilde{\\mathcal{O}}(\\cdot) ignores polylogarithmic terms. We begin by establishing several properties of the operator F𝐹F defined in (5.1), the stochastic operator f𝑓f defined in (5.4),\nand the Markov chain {ζt}subscript𝜁𝑡\\left\\{\\zeta_{t}\\right\\}.\nFor operator F𝐹F, note that where the inequality uses (5.3).\nHence F𝐹F is a (1−min⁡(νπ))1superscript𝜈𝜋\\left(1-\\min(\\nu^{\\pi})\\right)-contraction in ∥⋅∥∞subscriptdelimited-∥∥⋅\\lVert\\cdot\\rVert_{\\infty} norm.\nConsequently, Qrπsubscriptsuperscript𝑄𝜋𝑟Q^{\\pi}_{r} is the unique fixed point of F𝐹F.\nThe rest of the proof focuses on stochastic operator defined in (5.4), while the argument for the stochastic operator defined in (5.6) follows similar lines. For operator f𝑓f defined in (5.1), we have for any ζ𝜁\\zeta, Now by defining zx=M​(π,x)subscript𝑧𝑥𝑀𝜋𝑥z_{x}=M(\\pi,x) for any x𝑥x, we know\n|zx​(s)−zy​(s)|=|∑a∈𝒜π​(a|s)​(x​(s,a)−y​(s,a))|≤∥x−y∥∞subscript𝑧𝑥𝑠subscript𝑧𝑦𝑠subscript𝑎𝒜𝜋conditional𝑎𝑠𝑥𝑠𝑎𝑦𝑠𝑎subscriptdelimited-∥∥𝑥𝑦\\lvert z_{x}(s)-z_{y}(s)\\rvert=\\lvert\\textstyle\\sum\\nolimits_{a\\in\\mathcal{A}}\\pi(a|s)\\left(x(s,a)-y(s,a)\\right)\\rvert\\leq\\lVert x-y\\rVert_{\\infty} holds for any s∈𝒮𝑠𝒮s\\in{\\mathcal{S}}.\nHence we have where inequality (a)𝑎(a) uses the fact that ∥u(⋅|s,a)∥1=∥ℙu(⋅|s,a)−ℙN(⋅|s,a)∥1≤2\\lVert u(\\cdot|s,a)\\rVert_{1}=\\lVert\\mathbb{P}_{u}(\\cdot|s,a)-\\mathbb{P}_{\\mathrm{N}}(\\cdot|s,a)\\rVert_{1}\\leq 2.\nThus we obtain Additionally, one can readily verify that Lastly, for the Markov chain {ζt}subscript𝜁𝑡\\left\\{\\zeta_{t}\\right\\}, we proceed to establish its fast-mixing property under Assumption 1.\nNote that the stationary distribution of {ζt}subscript𝜁𝑡\\left\\{\\zeta_{t}\\right\\}, denoted by νπsuperscript𝜈𝜋\\nu^{\\pi}, is given by\nνπ​(s,a,s′,a′)=μπ​(s)​π​(a|s)​ℙN​(s′|s,a)​π​(a′|s′)superscript𝜈𝜋𝑠𝑎superscript𝑠′superscript𝑎′superscript𝜇𝜋𝑠𝜋conditional𝑎𝑠subscriptℙNconditionalsuperscript𝑠′𝑠𝑎𝜋conditionalsuperscript𝑎′superscript𝑠′\\nu^{\\pi}(s,a,s^{\\prime},a^{\\prime})=\\mu^{\\pi}(s)\\pi(a|s)\\mathbb{P}_{\\mathrm{N}}(s^{\\prime}|s,a)\\pi(a^{\\prime}|s^{\\prime}).\nLet us denote the transition kernel of {ζt}subscript𝜁𝑡\\left\\{\\zeta_{t}\\right\\} by ℙζsubscriptℙ𝜁\\mathbb{P}_{{\\bf\\zeta}},\nand accordingly denote the transition kernel of {st}subscript𝑠𝑡\\left\\{s_{t}\\right\\} by ℙSsubscriptℙS\\mathbb{P}_{\\mathrm{S}}.\nThen for any ζ∈𝒵𝜁𝒵\\zeta\\in\\mathcal{Z}, for some κ∈(0,1)𝜅01\\kappa\\in(0,1) and C>0𝐶0C>0, where the last inequality follows from the geometric-mixing property of {st}subscript𝑠𝑡\\left\\{s_{t}\\right\\} given Assumption 1,\nand equality (a)𝑎(a) follows from the Markov property.\nThus we obtain that Consequently, let us define Tα≔min{k:k≥0,maxζ∈𝒵∥ℙζk(ζ,⋅)−νπ(⋅)∥TV≤α}T_{\\alpha}\\coloneqq\\min\\left\\{k:k\\geq 0,\\max_{\\zeta\\in\\mathcal{Z}}\\lVert\\mathbb{P}^{k}_{\\zeta}(\\zeta,\\cdot)-\\nu^{\\pi}(\\cdot)\\rVert_{\\mathrm{TV}}\\leq\\alpha\\right\\}. From (5.8), (5.9), (5.10), and (5.11), it should be clear that the operators F,f𝐹𝑓F,f and the stochastic process {ζt}subscript𝜁𝑡\\left\\{\\zeta_{t}\\right\\} satisfy Assumption 2.1 - 2.3 in .\nNow for any ϵ>0italic-ϵ0\\epsilon>0, consider taking αt=α>0subscript𝛼𝑡𝛼0\\alpha_{t}=\\alpha>0 in RTD method, where\nα=𝒪​(ϵ2​νmin2​(1−γ)4log⁡(1/ϵ)​log⁡(|𝒮|​|𝒜|))𝛼𝒪superscriptitalic-ϵ2superscriptsubscript𝜈min2superscript1𝛾41italic-ϵ𝒮𝒜\\alpha=\\mathcal{O}\\left(\\frac{\\epsilon^{2}\\nu_{\\mathrm{min}}^{2}(1-\\gamma)^{4}}{\\log(1/\\epsilon)\\log(\\lvert{\\mathcal{S}}\\rvert\\lvert\\mathcal{A}\\rvert)}\\right)\nand α​Tα=𝒪​((1−γ)4​νmin2|𝒜|2​log⁡(|𝒮|​|𝒜|))𝛼subscript𝑇𝛼𝒪superscript1𝛾4superscriptsubscript𝜈min2superscript𝒜2𝒮𝒜\\alpha T_{\\alpha}=\\mathcal{O}\\left(\\frac{(1-\\gamma)^{4}\\nu_{\\mathrm{min}}^{2}}{\\lvert\\mathcal{A}\\rvert^{2}\\log(\\lvert{\\mathcal{S}}\\rvert\\lvert\\mathcal{A}\\rvert)}\\right),\nthe rest of the proof then follows the same lines as in Corollary 3.1.1 therein.\n∎ Given Lemma 5.1, we proceed to establish the sample complexity of the SRPMD method with different stepsize schemes discussed in Section 4.\nWe begin with the stepsize scheme (4.10) considered in Theorem 4.1, which demonstrates linear convergence up to policy evaluation error. Suppose the stepsizes {ηk}subscript𝜂𝑘\\left\\{\\eta_{k}\\right\\} in SRPMD satisfy\nηk≥ηk−1​(1−1−γM)−1​M′subscript𝜂𝑘subscript𝜂𝑘1superscript11𝛾𝑀1superscript𝑀′\\eta_{k}\\geq\\eta_{k-1}\\left(1-\\tfrac{1-\\gamma}{M}\\right)^{-1}M^{\\prime}\nfor all k≥1𝑘1k\\geq 1,\nM′superscript𝑀′M^{\\prime} is defined as in Theorem 3.1.\nFurthermore, for any ϵ>0italic-ϵ0\\epsilon>0, suppose 𝔼ξk​∥Qrπk,ξk−Qπk∥∞≤esubscript𝔼subscript𝜉𝑘subscriptdelimited-∥∥subscriptsuperscript𝑄subscript𝜋𝑘subscript𝜉𝑘𝑟superscript𝑄subscript𝜋𝑘𝑒\\mathbb{E}_{\\xi_{k}}\\lVert Q^{\\pi_{k},\\xi_{k}}_{r}-Q^{\\pi_{k}}\\rVert_{\\infty}\\leq e with 4​M​e/(1−γ)2≤ϵ/24𝑀𝑒superscript1𝛾2italic-ϵ24Me/(1-\\gamma)^{2}\\leq\\epsilon/2.\nThen\nSRPMD outputs a policy πksubscript𝜋𝑘\\pi_{k}\nwith 𝔼​[fρ​(πk)−fρ​(π∗)]≤ϵ𝔼delimited-[]subscript𝑓𝜌subscript𝜋𝑘subscript𝑓𝜌superscript𝜋italic-ϵ\\mathbb{E}\\left[f_{\\rho}(\\pi_{k})-f_{\\rho}(\\pi^{*})\\right]\\leq\\epsilon in iterations,\nwhere M𝑀M is defined as in Lemma 3.2, and Δ0=fρ​(π0)−fρ​(π∗)subscriptΔ0subscript𝑓𝜌subscript𝜋0subscript𝑓𝜌superscript𝜋\\Delta_{0}=f_{\\rho}(\\pi_{0})-f_{\\rho}(\\pi^{*}).\nIn addition, the total number of samples required by SRPMD can be bounded by The bound on the total number of iterations k𝑘k can be readily obtained from (4.11) in Theorem 4.1, if 4​M​e/(1−γ)2≤ϵ/24𝑀𝑒superscript1𝛾2italic-ϵ24Me/(1-\\gamma)^{2}\\leq\\epsilon/2.\nTo satisfy this condition, suppose one needs to run RTD for T𝑇T iterations when evaluating for each Qrπksubscriptsuperscript𝑄subscript𝜋𝑘𝑟Q^{\\pi_{k}}_{r},\nthen from (5.7) in Lemma 5.1, one can bound T𝑇T by The bound on the total number of samples follows immediately by combining the previous two observations.\n∎ Given Proposition 5.1, we remark that the sample complexity of applying SRPMD to solving the robust MDP with (𝐬,𝐚)𝐬𝐚(\\mathbf{s},\\mathbf{a})-rectangular uncertainty sets is comparable to that of solving standard MDPs with linearly converging policy mirror descent methods, in terms of its dependence on the optimality gap , and is slightly worse in terms of its dependence on the effective horizon (1−γ)−1superscript1𝛾1(1-\\gamma)^{-1}.\nTo the best of our knowledge, this is the first sample complexity result for first-order policy-based method that is optimal in terms of the dependence on the optimality gap.\nA closer look to the analysis shows that this worse dependence on the effective horizon comes from the current convergence characterization of the RTD method, which exhibits a worse dependence on the effective horizon compared to the CTD method considered in  for evaluating policy in standard MDPs.\nSee Section 6 for more detailed discussions. Finally, we establish the sample complexity of SRPMD when using the Euclidean divergence, which allows a constant-stepsize scheme and attains sublinear convergence. Let w​(⋅)=∥⋅∥22𝑤⋅superscriptsubscriptdelimited-∥∥⋅22w(\\cdot)=\\lVert\\cdot\\rVert_{2}^{2} be the distance-generating function, and ηk=ηsubscript𝜂𝑘𝜂\\eta_{k}=\\eta for all k≥0𝑘0k\\geq 0.\nFurthermore, for any ϵ>0italic-ϵ0\\epsilon>0, suppose 𝔼ξk​∥Qrπk,ξk−Qπk∥∞≤esubscript𝔼subscript𝜉𝑘subscriptdelimited-∥∥subscriptsuperscript𝑄subscript𝜋𝑘subscript𝜉𝑘𝑟superscript𝑄subscript𝜋𝑘𝑒\\mathbb{E}_{\\xi_{k}}\\lVert Q^{\\pi_{k},\\xi_{k}}_{r}-Q^{\\pi_{k}}\\rVert_{\\infty}\\leq e with 4​M​e/(1−γ)2≤ϵ/24𝑀𝑒superscript1𝛾2italic-ϵ24Me/(1-\\gamma)^{2}\\leq\\epsilon/2.\nThen\nby taking η=72​|𝒮|2/((1−γ)2​M​ϵ)𝜂72superscript𝒮2superscript1𝛾2𝑀italic-ϵ\\eta=72\\lvert{\\mathcal{S}}\\rvert^{2}/\\left((1-\\gamma)^{2}M\\epsilon\\right),\nSRPMD outputs a policy πRsubscript𝜋𝑅\\pi_{R}\nwith 𝔼​[fρ​(πR)−fρ​(π∗)]≤ϵ𝔼delimited-[]subscript𝑓𝜌subscript𝜋𝑅subscript𝑓𝜌superscript𝜋italic-ϵ\\mathbb{E}\\left[f_{\\rho}(\\pi_{R})-f_{\\rho}(\\pi^{*})\\right]\\leq\\epsilon, where R∼Unif​({1​…​k})similar-to𝑅Unif1…𝑘R\\sim\\mathrm{Unif}(\\left\\{1\\ldots k\\right\\}), in iterations,\nwhere M𝑀M is defined as in Lemma 3.2, and Δ0=fρ​(π0)−fρ​(π∗)subscriptΔ0subscript𝑓𝜌subscript𝜋0subscript𝑓𝜌superscript𝜋\\Delta_{0}=f_{\\rho}(\\pi_{0})-f_{\\rho}(\\pi^{*}).\nIn addition, the total number of samples required by SRPMD can be bounded by The bound on the total number of iterations k𝑘k can be readily obtained from (4.14) in Theorem 4.2, provided To satisfy the first condition above, suppose one needs to run RTD for T𝑇T iterations when evaluating for each Qrπksubscriptsuperscript𝑄subscript𝜋𝑘𝑟Q^{\\pi_{k}}_{r},\nthen from (5.7) in Lemma 5.1, one can bound T𝑇T by To satisfies the second and third conditions, it suffices to have\nη≥max⁡{72​|𝒮|2k​(1−γ)3​ϵ2,36​|𝒮|2​e(1−γ)3​ϵ2},𝜂72superscript𝒮2𝑘superscript1𝛾3superscriptitalic-ϵ236superscript𝒮2𝑒superscript1𝛾3superscriptitalic-ϵ2\\eta\\geq\\max\\left\\{\\tfrac{72\\lvert{\\mathcal{S}}\\rvert^{2}}{k(1-\\gamma)^{3}\\epsilon^{2}},\\tfrac{36\\lvert{\\mathcal{S}}\\rvert^{2}e}{(1-\\gamma)^{3}\\epsilon^{2}}\\right\\},\nwhich can be readily satisfied by η=72​|𝒮|2/((1−γ)2​M​ϵ)𝜂72superscript𝒮2superscript1𝛾2𝑀italic-ϵ\\eta=72\\lvert{\\mathcal{S}}\\rvert^{2}/\\left((1-\\gamma)^{2}M\\epsilon\\right) given the bound on k𝑘k and e𝑒e.\nThe bound on the total number of samples T​k𝑇𝑘Tk then follows immediately by combining the previous observations.\n∎ To the best of our knowledge, all the obtained sample complexities in this section appear to be new in the literature of first-order methods applied to the robust MDP problem.\nThe best sample complexity for PGM applied to this problem in the existing literature is at the order of 𝒪​(1/ϵ7)𝒪1superscriptitalic-ϵ7\\mathcal{O}(1/\\epsilon^{7}) , which focuses on the Euclidean Bregman divergence and a special subclass of uncertainty set defined in (5.5).\nIn comparison, as shown in Proposition 5.1 and 5.2, SRPMD with the same divergence improves this sample complexity by orders of magnitude, and applies to a much more general class of uncertainty sets.\n\nReference markers found: [0]\n\nContext windows for each [ref]:\n[ref]0: of the training environment and another set of pre-specified probability distributions 𝒬s,asubscript𝒬𝑠𝑎\\mathcal{Q}_{s,a} over 𝒮𝒮{\\mathcal{S}}: Clearly, users can adjust the tunable parameter ϵ∈[ref]0 [ref]1italic-ϵ01\\epsilon\\in[ref]0 [ref]1 based on their robustness preference.\nThe choice of 𝒬s,asubscript𝒬𝑠𝑎\\mathcal{Q}_{s,a} also allows encoding prior knowledge on the transition kernel of\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 0,\n            \"titles\": [\n                \"On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift\",\n                \"Robust Reinforcement Learning using Least Squares Policy Iteration with Provable Performance Guarantees\",\n                \"Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization\",\n                \"A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants\",\n                \"The theory of max-min and its application to weapons allocation problems\",\n                \"Twice regularized MDPs and the equivalence between robustness and regularization\",\n                \"A first-order approach to accelerated value iteration\",\n                \"Robust markov decision processes: Beyond rectangularity\",\n                \"Scalable First-Order Methods for Robust MDPs\",\n                \"Partial policy iteration for l1-robust markov decision processes\",\n                \"Robust estimation of a location parameter\",\n                \"Robust dynamic programming\",\n                \"Approximately optimal approximate reinforcement learning\",\n                \"A natural policy gradient\",\n                \"Robust modified policy iteration\",\n                \"On the linear convergence of natural policy gradient algorithm\",\n                \"Risk-Averse Learning by Temporal Difference Methods\",\n                \"On fréchet subdifferentials\",\n                \"Efficient Policy Iteration for Robust Markov Decision Processes via Regularization\",\n                \"First-order and stochastic optimization methods for machine learning\",\n                \"Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes\",\n                \"Markov chains and mixing times, volume 107\",\n                \"Policy mirror descent inherently explores action space\",\n                \"Implicit bias of gradient descent based adversarial training on separable data\",\n                \"Homotopic policy mirror descent: Policy convergence, implicit regularization, and improved sample complexity\",\n                \"Distributionally robust q𝑞q-learning\",\n                \"Distributionally Robust Offline Reinforcement Learning with Linear Function Approximation\",\n                \"Overcoming Exploration in Reinforcement Learning with Demonstrations\",\n                \"Introductory lectures on convex optimization: A basic course, volume 87\",\n                \"Robust control of markov decision processes with uncertain transition matrices\",\n                \"Sample complexity of robust reinforcement learning with a generative model\",\n                \"Curiosity-driven Exploration by Self-supervised Prediction\",\n                \"Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703\",\n                \"Markov decision processes: discrete stochastic dynamic programming\",\n                \"Convex analysis, volume 18\",\n                \"Reinforcement Learning under Model Mismatch\",\n                \"Risk-averse dynamic programming for markov decision processes\",\n                \"Adaptive Trust Region Policy Optimization: Global Convergence and Faster Rates for Regularized MDPs\",\n                \"Deep Reinforcement Learning with Robust and Smooth Policy\",\n                \"Lectures on Geometric Measure Theory, volume 3\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]0 based on the context and research area of First-order Policy Optimization for Robust Markov Decision Process This research was partially supported by NSF DMS-2134037 and AFOSR FA9550-22-1-0447.. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}