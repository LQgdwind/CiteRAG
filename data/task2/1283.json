{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Learning to Reduce Defocus Blur by Realistically Modeling Dual-Pixel Data\n\nPaper Abstract: \n\nPaper Section Text:\nWe evaluate our proposed RDPD and other existing defocus deblurring methods: the DP deblurring network (DPDNet)¬†, the edge-based defocus blur (EBDB)¬†, the defocus map estimation network (DMENet)¬†[ref]24, and the just noticeable blur (JNB)¬† estimation. DPDNet¬† is the only method that utilizes DP data for deblurring, and the others¬† [ref]24  use only a single image as input (i.e., IlsubscriptIùëô\\mathrm{I}_{l}) and estimate the defocus map in order to feed it to an off-the-shelf deconvolution method (i.e.,¬† ). EBDB¬† and JNB¬† are not learning-based methods; thus, we can test them directly. For the learning-based DMENet method, we cannot retrain it with the Canon data¬†, as it does not provide the ground truth defocus map. However, with our data generator we are able to generate defocus maps, which allows us to retrain DMENet with our synthetically generated data. Settings to generate DP data.¬†For our DP data generator, we define five camera parameter sets ‚Äì namely, {4,5,6},{5,8,6},{7,5,8},{10,13,12},{22,10,30}456586758101312221030\\{4,5,6\\},\\{5,8,6\\},\\{7,5,8\\},\\{10,13,12\\},\\{22,10,30\\} ‚Äì such that each set represents focal length, aperture size, and focus distance. Given the depth range found in the SYNTHIA dataset¬†6, these camera sets cover a wide range of front- as well as back-focus CoC sizes. For each image sequence in the SYNTHIA dataset, we generate five sequences based on the predefined camera sets. The radial distortion coefficients are set accordingly for each camera set. For the DP PSFs, we generate many representative PSF shapes by varying the parameters in the given ranges n‚àà{3,6,9}ùëõ369n\\in\\{3,6,9\\}, Œ±‚àà{0.4,0.6,0.8,1}ùõº0.40.60.81\\alpha\\in\\{0.4,0.6,0.8,1\\}, Œ≤‚àà{0.1,0.2,0.3,0.4}ùõΩ0.10.20.30.4\\beta\\in\\{0.1,0.2,0.3,0.4\\}, and Œ∫=0.14ùúÖ0.14\\kappa=0.14. Image noise layer strength is chosen randomly, where œÉ‚àà{5‚Äãe‚àí2,5.5‚Äãe‚àí2,‚ãØ,5‚Äãe‚àí1}ùúé5superscriptùëí25.5superscriptùëí2‚ãØ5superscriptùëí1\\sigma\\in\\{5e^{-2},5.5e^{-2},\\cdots,5e^{-1}\\}. These parameters are set empirically to model real camera hardware. More detail is provided in the supplemental material. We divide the SYNTHIA dataset¬†6 into training and testing sequences. We generate five sets of blurred images for each image sequence. In total, we synthesize 2023 training and 201 testing blurred DP views. Though our synthetic DP data generator enables an unlimited number of images to be generated, we found this number of images sufficient for training. In addition to our synthetically generated DP data, we use the DP ground truth data from¬† with 300300300 training, 747474 validation, and 767676 testing pairs of blurred images (with DP views) and corresponding sharp images. RDPD settings and training procedure.¬†We set the size of the convLSTM to 512512512 units. For patch-wise training, we fix the size of input and output layers to 512√ó512√ó75125127512\\times 512\\times 7 and 512√ó512√ó35125123512\\times 512\\times 3, respectively. We initialize the weights of the convolutional layers using He‚Äôs initialization¬†5 and use the Adam optimizer¬† to train the model. The initial learning rate is 5√ó10‚àí55superscript1055\\times 10^{-5}, which is decreased by half every 40 epochs. For domain generalization from synthetic to real data¬†2 [ref]41, we train our model iteratively using mini-batches of real (i.e., single image) as well as synthetic data (i.e., image sequence), where the patches are randomly cropped at each iteration. This type of iterative image/image sequence training becomes feasible since our recurrent model RDPD allows training and testing with any number of frames, and it does not need to be preset beforehand. We set the mini-batch size for the real data iteration to eight batches, because the dataset of real data has only single-image examples (i.e., no image sequences). For the synthetic data iteration, we set the mini-batch size to two sequences each of size four frames. We define three scales for our edge loss ‚Äì namely, m‚àà{3,7,11}ùëö3711m\\in\\{3,7,11\\}. The ŒªùúÜ\\lambda terms are found to be effective at Œªx=0.03subscriptùúÜùë•0.03\\lambda_{x}=0.03 and Œªy=0.02subscriptùúÜùë¶0.02\\lambda_{y}=0.02. To avoid overfitting, the dropout layer in the convLSTM is set to 0.40.40.4. Our model converges after 140 epochs. Although we train on image patches, our RDPD (with convLSTM) is fully convolutional and enables testing on full-resolution inputs. To demonstrate the effectiveness of each component in our model, an ablation study of different training settings is provided in the supplemental material. Single image results.¬†We evaluate our proposed RDPD against existing defocus deblurring methods for single-image inputs. For methods that utilize DP views for the input image (i.e., RDPD and DPDNet¬†), we introduce variations on the training data used for more comprehensive evaluations. The variations are RDPD+ and DPDNet+ that are trained on both Canon DP data from¬† combined with synthetic DP data generated by the process described in Sec.¬†2. The RDPD without the ++ sign is our baseline trained with synthetically generated DP data only. The DPDNet without the ++ is trained on Canon data only. In Table¬†1, we report quantitative results on real Canon DP data from¬† using standard metrics ‚Äì namely, MAE, PSNR, SSIM, and time. We also report the Naturalness Image Quality (NIQE) metric of the output deblurred images with respect to a reference model derived from the DP GT images. In general, our RDPD+ has the best overall PSNR compared to other methods. Particularly, RDPD+ achieves the best PSNR and MAE for both indoor and combined categories, and all with our lighter-weight network that enabled the fastest inference time. For the Outdoor dataset, the PSNR of RDPD+ is slightly lower (i.e., 0.080.080.08dB) due to the fact that the Outdoor dataset is imperfect as a result of the capturing process (see Fig.¬†3). DP cameras do not enable simultaneous capture of the DP images and corresponding ground truth sharp image (i.e., the image pairs can be captured only in succession at different times). As a result, the Outdoor ground-truth is imperfect with small local motion and illumination variations. The Indoor ground-truth is captured in more controlled conditions and has fewer imperfections. The slightly better performance of DPDNet for outdoor scenes is because DPDNet is learning to compensate for imperfections in the Outdoor dataset. A key strength of our work is the ability to synthetically generate DP data that is not impeded by imperfections of manual capture. RDPD+ is trained on the Outdoor dataset as well as the synthetically generated data (without such imperfections) debiasing the result from the imperfect ground-truth. The consequence is reduced fidelity to the imperfect ground-truth (in terms of PSNR/SSIM), but better defocus deblur performance overall. Similar behavior is observed when DPDNet is trained with Canon data and our synthetically generated data (i.e., DPDNet+). In Fig.¬†7, we also provide qualitative results of RDPD compared to other methods on data captured by Canon DSLR and Pixel 4 cameras. In general, RDPD+ is able to recover more details from the input deblurred image. Additionally, Fig.¬†7 demonstrates that the baseline RDPD achieves good deblurring results on Canon and Pixel 4 data despite being trained with synthetic data only. This result demonstrates the accuracy of the proposed framework for synthetic DP data generation and the ability of the recurrent model to generalize to different cameras. It can also be seen that DPDNet+ has improved results compared to DPDNet, demonstrating the benefit gained by DPDNet+ through the addition of synthetic DP data on training. The supplemental material contains more quantitative results, visual comparisons, and animated deblurring examples for both Canon and Pixel 4 cameras. Image sequence results.¬†Our RDPD is designed to handle input image sequences. Here, we investigate the improvement gained by training with image sequences vs. single frames. For this comparison, we introduce the RDPD+ variant sRDPD+, which is trained with single-frame inputs. As previously mentioned, there is no camera that enables access to DP views for video data. Nevertheless, we mimic the same capturing procedure in¬† in order to capture a sequence of images. We performed four captures of the same scene with small camera motion introduced between the captures. Each image has its own DP views and is captured at narrow and wide apertures. Fig.¬†8 presents the results on the sequence of images. The effectiveness of training with image sequences with RDPD+ can be seen from the average PSNR gain (i.e., +0.40.4+0.4dB) compared to sRDPD+ trained using single-image inputs. Table¬†2 shows the quantitative results on our synthetically generated DP image sequences. Our method RDPD+ (trained on multiple frames) achieves the best results as it utilizes the convLSTM architecture to better model the temporal dependencies in an image sequence. Recall that our RDPD network is lighter and has a much lower number of weights compared to DPDNet.\n\nReference markers found: [24]\n\nContext windows for each [ref]:\n[ref]24: We evaluate our proposed RDPD and other existing defocus deblurring methods: the DP deblurring network (DPDNet)¬†, the edge-based defocus blur (EBDB)¬†, the defocus map estimation network (DMENet)¬†[ref]24, and the just noticeable blur (JNB)¬† estimation. DPDNet¬† is the only method that utilizes DP data for deblurring, and the others¬† [ref]24  use only a single image as input (i.e.,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 24,\n            \"titles\": [\n                \"Iterative Filter Adaptive Network for Single Image Defocus Deblurring\",\n                \"Defocus Deblurring Using Dual-Pixel Data\",\n                \"Online lens motion smoothing for video autofocus\",\n                \"Revisiting autofocus for smartphone cameras\",\n                \"NTIRE 2021 challenge for defocus deblurring using dual-pixel images: Methods and results\",\n                \"Automatic radial distortion estimation from a single image\",\n                \"Non-parametric blur map regression for depth of field extension\",\n                \"Blind deconvolution by means of the richardson‚Äìlucy algorithm\",\n                \"Simultaneous linear estimation of multiple view geometry and lens distortion\",\n                \"Practical poissonian-gaussian noise modeling and fitting for single-image raw-data\",\n                \"Learning Single Camera Depth Estimation using Dual-Pixels\",\n                \"Deep Video Deblurring: The Devil is in the Details\",\n                \"Learning Monocular Depth by Distilling Cross-domain Stereo Networks\",\n                \"Parameter-free radial distortion correction with center of distortion estimation\",\n                \"Deep depth from focus\",\n                \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification\",\n                \"Slanted Stixels: Representing San Francisco‚Äôs Steepest Streets\",\n                \"Gradient flow in recurrent nets: the difficulty of learning long-term dependencies\",\n                \"Long short-term memory\",\n                \"Sensor-based auto-focusing system using multi-scale feature extraction and phase correlation matching\",\n                \"Psf estimation using sharp edge prediction\",\n                \"Edge-based defocus blur estimation with adaptive scale selection\",\n                \"Adam: A method for stochastic optimization\",\n                \"Fast image deconvolution using hyper-laplacian priors\",\n                \"Deep defocus map estimation using domain adaptation\",\n                \"Understanding blind deconvolution algorithms\",\n                \"Automatic estimation and removal of noise from a single image\",\n                \"Single Image Super Resolution based on a Modified U-net with Mixed Gradient Loss\",\n                \"Blur calibration for depth from defocus\",\n                \"Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections\",\n                \"Focus on defocus: bridging the synthetic to real domain gap for depth estimation\",\n                \"Dual pixel exploration: Simultaneous depth estimation and image restoration\",\n                \"A Unified Approach of Multi-scale Deep and Hand-crafted Features for Defocus Estimation\",\n                \"A lens and aperture camera model for synthetic image generation\",\n                \"Line-based correction of radial lens distortion\",\n                \"Modeling defocus-disparity in dual-pixel sensors\",\n                \"Reflection removal using a dual-pixel sensor\",\n                \"U-Net: Convolutional Networks for Biomedical Image Segmentation\",\n                \"Discriminative blur detection features\",\n                \"Just noticeable defocus blur detection and estimation\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]24 based on the context and research area of Learning to Reduce Defocus Blur by Realistically Modeling Dual-Pixel Data. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}