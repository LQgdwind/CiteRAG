{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese\n\nPaper Abstract: \n\nPaper Section Text:\nTo evaluate the quality of TTS-Portuguese Corpus in practice, we explored the speech synthesis using models of prominence in the literature. We chose the models: DCTTS [ref]4 and Tacotron 2 . Here, we compare the models DCTTS and Tacotron 2. To maintain results reproducible, we used open source implementations and tried to replicate related works as faithfully as possible. In the cases where hyper-parameters were not specified, we empirically optimized those for our dataset. We have used the following implementations: DCTTS provided by  and\nTacotron 2 provided by . For all experiments, to speed up training, we initialized the model using the weights of the pre-trained model on English, using the LJ Speech dataset and we also use RTISI-LA  as a vocoder, which is a variation of the Griffin-Lim  vocoder. Although the acquisition avoided external noise as best as possible, the audio files were not recorded in a studio setting. Therefore, some noise may be present in part of the files. To reduce the interference with our analysis, we applied RNNoise [ref]44 in all audio files. RNNoise is based on Recurrent Neural Networks; more specifically Gated Recurrent Unit , and demonstrated good performance for noise suppression. We report two experiments: Experiment 1: replicates the implementation of the DCTTS model, training the model for Portuguese with the TTS-Portuguese Corpus. For this experiment, as reported in the DCTTS article, the model receives the text directly as input, so no phonetic transcription is used. As previously mentioned, the original DCTTS paper does not describe any normalization, so for the model to converge we tested different normalization options and decided to use, in all layers, 5% dropout and layer normalization . We did not use a fixed learning rate as described in the original article. Instead, we used a starting learning rate of 0.001 decaying using Noam’s learning rate decay scheme [ref]47. Experiment 2: this experiment explores Tacotron 2  model, for that we use the Mozilla TTS implementation . This model receives phonetic transcription as input instead of text directly. To perform phonetic transcription we use the Phonemizer444https://github.com/bootphon/phonemizer library that supports 121 languages and accents. In experiment 1, two parts of the model are trained separately. The first part of the model, called Text2Mel, is responsible for generating a Mel spectrogram from the input text and this part of the model was induced using the composition of the functions: binary cross-entropy, L1  and guided attention loss [ref]4. The second part, called SSRN, is responsible for the transformation of a mel spectrogram into the complete STFT spectrogram and applies super-resolution in the process and the loss function is composed of the functions L1 and binary cross-entropy. In experiment 2, no guided attention is used, therefore, the loss function did not include the cost of attention. Since the network is trained end-to-end, the loss depends on the output of two network modules. The first module converts text into Mel spectrogram. The second module is a SSRN-like module called CBHG (1-D Convolution Bank Highway Network Bidirectional Gated Recurrent Unit). Table 2 shows the hardware specifications of the equipment used for model training. Experiment 1 was trained on computer 2, while experiment 2 were performed using computer 1. Table 3 presents the training data from the experiments. The metrics presented in the table are: number of training steps, and the time required for training. It is important to note that experiment 1 is trained in two phases, both reported in the table: Text2Mel and SSRN.\n\nReference markers found: [4]\n\nContext windows for each [ref]:\n[ref]4: To evaluate the quality of TTS-Portuguese Corpus in practice, we explored the speech synthesis using models of prominence in the literature. We chose the models: DCTTS [ref]4 and Tacotron 2 . Here, we compare the models DCTTS and Tacotron 2. To maintain results reproducible, we used open source implementations and tried to replicate related works as faithfully as\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 4,\n            \"titles\": [\n                \"Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks With Guided Attention\",\n                \"“” alexa is my new bff” social roles\",\n                \"alexa is my new bff social roles, user satisfaction, and personification of the amazon echo\",\n                \"The teardown: Google home personal assistant\",\n                \"Siri, a virtual personal assistant-bringing intelligence to the interface\",\n                \"Statistical parametric speech synthesis using deep neural networks\",\n                \"Prediction of fujisaki model’s phrase commands\",\n                \"Software for a cascade/parallel formant synthesizer\",\n                \"Diphone synthesis using an overlap-add technique for speech waveforms concatenation\",\n                \"Speech parameter generation algorithms for hmm-based speech synthesis\",\n                \"Template-warping based speech driven head motion synthesis\",\n                \"Statistical parametric speech synthesis: A review\",\n                \"Automatic detection of unnatural word-level segments in unit-selection speech synthesis\",\n                \"Survey on various methods of text to speech synthesis\",\n                \"Deep Learning\",\n                \"Char2wav: End-to-end speech synthesis\",\n                \"Tacotron: A fully end-to-end text-to-speech synthesis model\",\n                \"Natural tts synthesis by conditioning wavenet on mel spectrogram predictions\",\n                \"Deep voice 3: 2000-speaker neural text-to-speech\",\n                \"Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search\",\n                \"Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis\",\n                \"Phonetic events from the labeling the european portuguese database for speech synthesis, feup/ipbdb\",\n                \"Lsf and lpc-derived features for large vocabulary distributed continuous speech recognition in brazilian portuguese\",\n                \"An open-source end-to-end asr system for brazilian portuguese using dnns built from newly assembled corpora\",\n                \"MLS: A Large-Scale Multilingual Dataset for Speech Research\",\n                \"The lj speech dataset\",\n                \"Deep voice: Real-time neural text-to-speech\",\n                \"Deep Voice 2: Multi-Speaker Neural Text-to-Speech\",\n                \"Signal estimation from modified short-time fourier transform\",\n                \"World: a vocoder-based high-quality speech synthesis system for real-time applications\",\n                \"WaveNet: A Generative Model for Raw Audio\",\n                \"Speech enhancement in the STFT domain\",\n                \"Multi-Scale Context Aggregation by Dilated Convolutions\",\n                \"Neural Machine Translation in Linear Time\",\n                \"Real-time signal estimation from modified short-time fourier transform magnitude spectra\",\n                \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n                \"Training very deep networks\",\n                \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\",\n                \"Speaker-dependent wavenet vocoder\",\n                \"Evaluation of deep learning approaches to text-to-speech systems for european portuguese\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]4 based on the context and research area of TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}