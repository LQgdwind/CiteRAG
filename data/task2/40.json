{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Towards a Framework for Certification of Reliable Autonomous Systems\n\nPaper Abstract: \n\nPaper Section Text:\nIn Section 2.2 we observed that the most solid and reliable way to certify autonomous software systems would be the adoption of formal methods. Formal methods are mathematically rigorous techniques for design, specification, validation, and verification of a wide variety of systems. They contribute to certification in a number of ways, from requirements design, to checking that all requirements can be true of the same system at the same time (before the system is built) , to verifying that early (or even partial) designs always satisfy requirements, to generating test cases, to checking during system runtime that the system is still meeting requirements. In order to verify that a system operates correctly, we need to know how the system works. If we do not have\nsufficient information on both how the system operates and what it means for the\nsystem to operate safely, then it is impossible to verify that these two\nbehaviour spaces match up with each other. Knowing how the system works includes\nknowing sufficient implementation details to be able to certify correctness. Though each method works differently, intuitively all formal methods verify that the system – or a model of the system – does what we expect it to do and nothing else.\nThis capability is required for certification, e.g., via safety cases per the standards of the aerospace industry, where certification requires making a case, backed by evidence, that the system meets standards for safety and reliability. In some formal methods, such as model checking and theorem proving,\nboth the system under certification and the properties to be verified\nare modelled using a rigorous, mathematical or logical language. We\nindicate these methods as formal at the property & system\nlevel. The system model itself, however, is necessarily an\nabstraction of the real system and hence it is incomplete: any results\nfrom methods that operate, albeit in a rigorous way, on abstractions\nof real systems, should be validated against the actual environment. (Note that many modern model checkers and theorem provers are now capable of generating software from their proven models that can be used as a basis for, or as the entire, real system implementation thus enabling straightforward validation.) Other methods model the properties to check using rigorous formalisms and languages, and check the property against the real system under certification. We define these methods, which include some static analysis approaches and runtime verification, as formal at the property level.\nSemi-formal methods specify either the system or the properties that they should meet using languages with informal or semi-formal semantics. UML  and Agent-UML  are examples of semi-formal specification languages, as well as test cases.\nInformal methods are based on specifications provided in natural language and are out of the scope of our investigation. In this section we review five verification methods: model checking,\ntheorem proving, static analysis, runtime verification, and systematic\ntesting. The first four of these are usually categorised as formal or semi-formal\nmethods. Software testing is not a formal\nmethod, but it is one of the most widely adopted verification approaches in\nindustry, often associated with quality assurance. The ‘Software\nTesting Services Market by Product, End-users, and Geography – Global\nForecast and Analysis 2019-2023’  foresees that the\nsoftware testing services market will grow at a compounded average\ngrowth rate of over 12% during the period 2019-2023. Software\ntesting – once automated and seamlessly integrated in a development\nmethod, as in the ‘continuous integration model’ –\nmay indeed\nrepresent\na useful complement to formal methods. Testing can be measurably improved by using artefacts from formal verification for test-case generation. However, as is well-known, testing is not exhaustive, which limits its utility. Figure 8 positions the five methods in a space characterised by the formality, exhaustivennes, and static vs. dynamic dimensions. Figure 9 details the formal methods and their relationships. A method is exhaustive when it verifies the entire behaviour space, over all possible\ninputs, including for an infinite input space. Exhaustive verification includes proving both the existence of ‘good’ or required behaviours and the absence of ‘bad’ or requirements-violating behaviours, something that can never be demonstrated through testing or simulation. Being exhaustive refers to the capability of exhaustively exploring the state space generated by the system model, as it is not currently possible, except for a small class of systems with specific characteristics, to be exhaustive with respect to a system of realistic industrial size. A method is static when it is applied on the system’s code, or on its abstract/simplified model, without needing the real system to be executed. A method is dynamic when it operates while the system runs. This method performs an exhaustive check that a system, given in a logical language, always satisfies its requirements.\nIntuitively, model checking explores all possible executions of the system,\nchecking the system model cannot reach states that violate the provided verification properties. Inputs: (1) a logical description of the system (either a model in a\nlogical language such as SMV  or AIGER , or the system itself, e.g., for software systems); (2) a requirement in a temporal logic such as LTL. See  for a survey with examples. Outputs: (1) an automated (push-button) proof (usually by assertion,\nwithout the full proof argument) that the system always satisfies the\nrequirement; or (2) a counterexample, which is an execution trace of the system\nfrom system initialisation that shows a step-by-step path to a system state\nwhere the system has operated faithfully to its definition yet still violated\nthe requirement. Strengths: Automated. Counterexamples are excellent maps for debugging and are generated without guidance from the user. Exhaustive.  Verification reasons over the entire state space. Verification of Absence of Bad Behaviours. Model checking both gives the\nexistence proof that a system always does what we expect given the inputs\nand that the system never does something we don’t expect for any input.\nIn other words, given an appropriate specification, it verifies the absence of behaviours that we don’t want, whether\nthey are triggered by an input or no input at all. Incremental. Model checking can efficiently analyse partial models/partial systems;\ncan be used from the earliest stages of the design process to save time pursuing\ndesigns that cannot satisfy their requirements based on the partial\ndesign-so-far. Weaknesses: Model checking is garbage-in, garbage-out; the results\nare only as accurate as the input system/model and the input requirement. One approach for validating the results of model checking against the actual environment by recognising assumption violations at runtime has been proposed in  , but the example addressed there is simpler than any real autonomous system.\nModelling a system in a logical language and specifying a requirement in a\ntemporal logic are the biggest bottlenecks to the use of model checking\n; both tasks are difficult to complete and to validate. It is easy to create models that are too large to analyse with current technology, such as large pieces of software or models over large numbers of variables. Not all logical systems can be specified in languages that can be model-checked. Applicability: model checking is widely used for both hardware and software,\nsuch as that seen in the Rules Layer    . System protocals are particularly amenable to model checking; it has successfully verified real-life, full-scale communication protocols  , air traffic control systems   , wheel braking systems , and more. Model checking\nhas been used for the Reactions Layer, but quickly reduces to either\nhybrid model checking or the verification of coarse\nabstractions    . Hybrid\nmodel checking can involve a wide array of\nnumerical techniques aimed at solving, for example, differential\nequations concerning control or environmental models. For the\nPrinciples Layer, model-checking has been used for verifying BDI agents 5 6 7,\nepistemic and temporal properties of multiagent systems   0, the agents knowledge, strategies and games , and general,\nhigh-level decision making 2 that has been extended\nto ethical principles  3. We note one important variation, program\nmodel-checking 3. Per its name, program model checking replaces the abstract model with the actual code/software system. It uses the same model-checking algorithms,\nbut on symbolic executions of the system. The advantage of such an approach is that\nit simplifies model validation (e.g., proving the link between the model and the real\nprogram/system) since we directly check the real\nprogram. However, a disadvantage is that the use of techniques such\nas symbolic execution can make the checking process slow and\ncomplex. Finally, note that the Principles Layer model-checking of\nhigh-level, and ethical, decision-making described\nabove 2 3  actually utilise\nprogram model checking over rational agents 4 1. A user creates a proof that a system satisfies its requirements that is checked by a known-to-be-correct proof assistant. Some automation of standard logical deductions is included in the proof assistant. Inputs: A theorem asserting a requirement holds over the\nsystem; the same large proof includes relevant definitions of the system and (sub-)\nrequirements and any necessary axioms. Outputs: A computer-checked proof that the theorem holds. If no such\nproof can exist (e.g., because the theorem does not hold), the user might gain the necessary insight into why this is the case from the way in which the proof fails. Strengths: Theorem proving can provide strong type-checking and\ngenerate proof obligations that provide unique insight into system operation.\nThe proofs generated are re-playable: they can be re-run by any future user at\nany time. If the input theorem changes (e.g., to perform a proof over the next version of the system) the old proof can be replayed (in some contexts) and only prompt the user for input where small changes are needed to update the proof to the new theorem. Proofs can be built up in extensible libraries and hierarchically inherit from/build upon each other. Large swaths of mathematics important to assertions over safety-critical systems have been proved in free libraries associated with the most popular theorem proving tools. Theorem proving can, and has, checked very large systems, even beyond what humans can inspect, such as the four-colour theorem . Weaknesses: This formal method requires the most experience from\nusers and has a comparatively high learning curve relative to other forms of verification. Setting up a proof correctly is a difficult activity even for trained experts. The user is the sole source of insight into why proofs fail; there is no automated counterexample like with model checking. Applicability: Theorem-proving has been widely used for\nsoftware seen in the Rules Layer, and techniques such\nas Hoare Logic  and tools such as\nIsabelle  are often used there. Theorem-proving has\nbeen used for the Reactions Layer, most notably verification of NASA’s aerospace systems via PVS  3  , verification of, e.g., automotive protocols through the KeyMaera system 6 5, and the use of theorem-proving tools to prove control\nsystem stability . See  for an introduction to PVS specification for a Boeing 737 autopilot. Theorem-proving for the Principles Layer is less common, though it has been tackled . During system design time, an automated tool examines a program or code fragment without executing it, to check for common bugs. Static analysis can be combined with techniques for expanding the types of bugs found, including symbolic execution (analysing a program’s control flow using symbolic values for variables to create a map of program execution with provable constraints), abstract interpretation (creating an abstraction of the program’s semantics that enables proofs about its execution), and shape analysis (mapping the program’s memory signature through its data structures). Inputs: Program or code fragment to analyse Outputs: List or visualisation of possible code defects Strengths: Automated tools analyse the structure of the code rather than just paths through the code like testing; they can therefore provide a deeper analysis with better code coverage. Static analysis is faster and more reliable than the alternative of manual code review. Though performance depends on the programming language and static analysis tool, performing static analysis is usually totally automated with easy-to-understand output suitable for a non-expert in verification. Weaknesses: Static analysers have no understanding of developer intent (e.g., they can detect that a function may access unallocated memory but will not find that the function does a different task than intended/labeled in the comments). They cannot enforce full coding standards, e.g., because some coding rules are open to interpretation, like those concerning comments, good documentation, and understandable variable names. The above ambiguities (in programmer intent, coding standards, use cases, etc.) can lead to false positive and false negative defect reports.\nFalse negatives mean that the analyser has failed to identify an error, can lead to fallacious trust in the software.\nFalse positives mean that the analyser reports an error where there is none, can cause a lot of wasted work at the developer’s side. Applicability:\nStatic analysis covers a range of different methods.\nBasic checks, i.e., whether each used variable was previously initialised, are included in most modern compilers.\nMore sophisticated checks, e.g., whether pointer access may lead to an exception, are realised by widely used tools such as Lint .\nHigh-end static analysers such as Polyspace  and Astrée  are able to check for general assertions.\nA review of 40 static code analysis tools updated to November 2019 is available via the Software Testing Help blog .\nAccording to CENELEC EN50128, static analysis is highly recommended for all safety-relevant software.\nConsequently, static analysis has been applied for most safety-critical software in avionics, railway and automotive; e.g., NASA regularly releases its own home-grown static analysis tools including (Java) Symbolic PathFinder, IKOS (Inference Kernel for Open\nStatic Analyzers) 9, and C Global Surveyor (CGS) .\nStatic analysis techniques generally analyze software in the Rules Layer. The Reactions Layer requires analysis techniques able to deal with continuous physical models, and the Principles Layer requires the ability to deal with potentially complex reasoning. Although static analysis can analyze autonmous systems’ software, its key weakness, being non-exhaustive, limits the utility of applying it.\n Runtime verification (RV) is a semi-formal method aimed at checking the\ncurrent run of the system. RV is most often run online (e.g., embedded on-board\nthe system during operation), and stream-based (e.g., evaluating requirements\ncontinuously throughout system operation), though it can also be run offline\n(e.g., with recorded data for post-mission analysis). Inputs:\n(1) Input data stream or streams containing time-stamped\nsensor or software values, e.g., sent over a system bus to the RV engine; (2) A requirement to verify, expressed in the form of a temporal logic formula, most commonly Mission-time Linear Temporal Logic (MLTL)    or First-Order Linear Temporal Logic (FOLTL) 8  whose variables are set by the input data\nor operationally via finite state automata, trace expressions , etc. (see Sect 2.1 of ). Outputs: For online, stream-based RV, the output is a stream of\n⟨time,verdict⟩timeverdict\\left<\\mathrm{time},\\mathrm{verdict}\\right> tuples containing a time stamp\nand the verdict from the valuation of the temporal logic requirement (true or\nfalse), evaluated over the data stream starting from that time step. Variations\non RV include extensions to non-Boolean (e.g., Bayesian or otherwise probabilistic) results, and evaluations of only part of the data stream of interest. Strengths: RV is the only formal verification technique that can run\nduring system deployment. Sanity checks can provide near-instant confirmation\nthat a system is obeying its requirements or that a failure has occurred,\nproviding an efficient mitigation trigger. Like simulation, RV can also provide useful characterisations of system runs, though RV analyzes runs in terms of violations of formal specifications; see  for a detailed comparison. Weaknesses: Specification of the requirement to monitor at runtime\nis the biggest bottleneck to successful deployment of RV . Specifications are\ndifficult to write for the same reasons as for model checking; further\ncomplications include noisiness of sensor data and challenges of real-world\nenvironments. Limitations and constraints of embedded systems and certification processes challenge RV implementations, though to date two tools have risen to these real-world-deployment challenges and more are sure to follow  .\nOf course, violation of certain critial safety properties is not acceptable, even if this is detected. Since RV operates at (or after) runtime, it\nis not suitable for such properties. Applicability: RV is widely used for\nsoftware in Rules Layer as discussed for example by Rosu12 [ref]198 and by HavelundR17 1; see TRV12 4 for another example.\nRV of multiagent systems using trace expressions   is also a viable approach to cope with the Rules Layer.\nRV has been used for the Reactions Layer, for\nexample by BartocciBBMS17 . The R2U2 framework  [ref]88 8   is a real-time, Realisable, Responsive, Unobtrusive Unit for runtime system analysis, including security threat detection, implemented in either FPGA hardware or software. R2U2 analyzes rules and reactions spanning the Rules Layerand (with the addition of a parallel system model) the Reactions Layerof our reference framework.\nFor the Principles Layer, DBLP:journals/ker/DastaniTY18  survey the use of norms for monitoring of the behaviour of autonomous agents: monitoring of norms is indeed foundational for processes of accountability, enforcement, regulation and sanctioning.\nThe idea\nof a runtime ethical monitor, or governor, is also commonly used  . Software Testing (ST) is not a formal method, though formal methods are often used for test-case geeration with provable qualities, such as coverage metrics. ST amounts to observing the execution of a software system to validate whether it behaves as intended and identify potential malfunctions 0. Testing can take place at different levels including Unit Testing, which tests individual units of source code; Integration Testing, aimed at testing a collection of software modules seen as a group; System Testing, carried out on a complete integrated system to evaluate its compliance with respect to its functional and/or system requirements; and User Acceptance Testing, meant to determine if the systems meets its requirements in terms of a specification or contract, and allow the user to accept the system, or not. Inputs: Depending on the test level and on the testing technique adopted for the level, the inputs that feed the ST application can change dramatically. For example, Unit Testing tests source code, while System Testing may follow a black-box approach, with the stucture of the system unknown to the tester. Outputs: As for RV, ST always consists of observing a sample of executions, and giving a verdict over them. The output may be an association between a test case, an execution run, and the respective verdict. Strengths: ST can be performed in an agile way  and can be directly integrated in the software development method , allowing the software to be developed and tested in the same development cycle. ST techniques may be very efficient; coverage analysis and reliability measures can help in performing an ‘as complete as possible’ testing. Weaknesses: Software Testing is not exhaustive. Identifying the most relevant units, functions, or functionalities to test is not easy, and finding the right balance between testing enough, but not too much, requires a deep understanding of the system under test. In particular, testing autonomous systems is hard since\nthe range of possible behaviours that can be exhibited by an autonomous agent can be vast 9 8.\nThe design and development of test cases is expensive; if the system under test changes, test cases developed for a previous version might not be reusable for the new one. Importantly, ST can never verify the absence of bad or undesirable behaviours, only the (partial) existance of desirable ones. Also unlike formal methods, ST requires a complete, executable system to test, pushing ST verification later in the design lifecycle than formal verification, which easily analyzes partial, early system designs. Applicability: While software testing is commonly used in\nindustry for both the Reactions Layer and Rules Layer, and is applied\nfor testing features at both levels in autonomous\n4 9 and multiagent systems\n, we are not aware of proposals\nfor testing the Principles Layer of an autonomous system. The analysis that we carried out shows that no golden bullet exists for addressing the certification, via formal and semi-formal verification, of autonomous systems. All the methods considered in this section show strengths and weaknesses.\nAlso, the applicability to the Principles Layer is not widespread yet. While a few proposals exist for some formal methods,\nthe challenges to face are many and the initial ideas sprouting from the research community must still consolidate and turn into practical and usable applications.\n\nReference markers found: [198]\n\nContext windows for each [ref]:\n[ref]198: if this is detected. Since RV operates at (or after) runtime, it\nis not suitable for such properties. Applicability: RV is widely used for\nsoftware in Rules Layer as discussed for example by Rosu12 [ref]198 and by HavelundR17 1; see TRV12 4 for another example.\nRV of multiagent systems using trace expressions   is also a viable approach to cope with the Rules Layer.\nRV has been used for the\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 198,\n            \"titles\": [\n                \"\\\"LTL\\\"_𝑓 Satisfiability Checking\",\n                \"인공지능 기반 공격 그래프 생성\",\n                \"24me Smart Personal Assistant\",\n                \"Quantitative Automata Model Checking of Autonomous Stochastic Hybrid Systems\",\n                \"Stream runtime monitoring on UAS\",\n                \"Certification of autonomous systems\",\n                \"Discrete abstractions of hybrid systems\",\n                \"Reliable Decision-Making in Autonomous Vehicles\",\n                \"Can You Trust Your Robotic Assistant?\",\n                \"Comparing trace expressions and Linear Temporal Logic for runtime verification\",\n                \"Parametric runtime verification of multiagent systems\",\n                \"EthEl: Toward a principled ethical eldercare robot\",\n                \"Machine Ethics\",\n                \"Every Planar Map is Four-Colorable, volume 98 of Contemporary Mathematics\",\n                \"Formal validation and verification of a medical software critical component\",\n                \"Towards certification of automotive software\",\n                \"Governing lethal behavior: embedding ethics in a hybrid deliberative/reactive robot architecture\",\n                \"Robot vacuums undergo a security check: trustworthy helpers around the house or chatty cleaning appliances?\",\n                \"Principles of Model Checking\",\n                \"A deep learning framework for financial time series using stacked autoencoders and long-short term memory\",\n                \"Policy learning in continuous-time markov decision processes using gaussian processes\",\n                \"Runtime monitoring of metric first-order temporal properties\",\n                \"Agent UML: A formalism for specifying multiagent software systems\",\n                \"Test-driven development: by example\",\n                \"Manifesto for agile software development, 2001\",\n                \"An overview of SAL\",\n                \"Moral Permissibility of Actions in Smart Home Systems\",\n                \"Analysis of a kernel verification\",\n                \"Challenges of platooning on public motorways\",\n                \"PTIME: personalized assistance for calendaring\",\n                \"Software testing research: Achievements, challenges, dreams\",\n                \"Fully autonomous driving: Where technology and ethics meet\",\n                \"Safety and assurance cases: Past, present and possible future – an adelard perspective\",\n                \"The Unified Modeling Language User Guide\",\n                \"Model checking AgentSpeak\",\n                \"Model checking rational agents\",\n                \"Verifying multi-agent programs by model checking\",\n                \"Formal design and safety analysis of air6110 wheel brake system\",\n                \"IKOS: A framework for static analysis based on abstract interpretation\",\n                \"Precise and scalable static program analysis of NASA flight software\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]198 based on the context and research area of Towards a Framework for Certification of Reliable Autonomous Systems. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}