{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks\n\nPaper Abstract: \n\nPaper Section Text:\nTo understand how stochastic collapse can benefit generalization, we will theoretically analyze learning dynamics in a teacher-student model for linear networks, a well established framework for studying generalization  [ref]51 .\nThis analysis not only provably connects the phenomenon of stochastic collapse to generalization benefits, but also suggests a stochastic collapse based mechanism for why successful learning rate schedules improve generalization. We then provide direct evidence for this theoretically suggested mechanism in deep neural networks. Insights from a two-layer linear neural network.\nWe build on the analysis of full-batch gradient learning dynamics of training error  and test error [ref]51 in the limit of infinitesimal learning rates for two-layer linear neural networks in a student-teacher setting. In our new analysis here we incorporate the important new ingredient of stochastic gradients, which dramatically alters the learning dynamics. The details of our analysis are presented fully in App.Â I. A teacher neural network with a low-rank weight matrix WÂ¯Â¯ğ‘Š\\bar{W} generates a training dataset by providing noise corrupted outputs to a set of random Gaussian inputs. The input-output data covariance matrix Î£yâ€‹xsubscriptÎ£ğ‘¦ğ‘¥\\Sigma_{yx} then drives the learning dynamics of a two-layer student with composite weight matrix W^=W2â€‹W1^ğ‘Šsubscriptğ‘Š2subscriptğ‘Š1\\hat{W}=W_{2}W_{1}. We analyze the learning dynamics of the student under a set of four assumptions precisely stated in App.Â I. These assumptions are similar to the ones made in  [ref]51, and are roughly stated as: (A1) whitened inputs to the teacher; (A2) structured gradient noise related to input-output covariance; (A3) spectral initialization with student singular vectors aligned to input-output covariance singular vectors; (A4) balanced initialization with equal power in the two weight layers of the student. Under these assumptions, the learning dynamics of the student weight matrix W^=W2â€‹W1^ğ‘Šsubscriptğ‘Š2subscriptğ‘Š1\\hat{W}=W_{2}W_{1} can be decomposed into independent learning dynamics for each singular value s^isubscript^ğ‘ ğ‘–\\hat{s}_{i} of W^^ğ‘Š\\hat{W}.\nAn associated singular value s~isubscript~ğ‘ ğ‘–\\tilde{s}_{i} of Î£yâ€‹xsubscriptÎ£ğ‘¦ğ‘¥\\Sigma_{yx} drives the dynamics of s^isubscript^ğ‘ ğ‘–\\hat{s}_{i} via the SDE, where Î¶>0ğœ0\\zeta>0 is the amplitude of the gradient noise.\nIn the small noise limit of Î¶â†’0â†’ğœ0\\zeta\\to 0, this SDE aligns with the nonlinear ODE corresponding to the student networkâ€™s dynamics under gradient flow, where exact solutions exist  [ref]51.\nImportant insights gained from this previous analysis are that larger data singular modes are learned earlier  and larger data singular modes, when learned, help the student generalize, while smaller singular modes, when learned, impede generalization by overfitting to noise in the training data [ref]51.\nBoth these insights will apply in our analysis of SGF.\nHowever, we will also uncover a third behavior unique to SGF: stochastic collapse effectively governs the minimum learnable singular mode, serving as a natural mechanism to prevent overfitting.\nOur analysis in App.Â I reveals that the student singular values s^isubscript^ğ‘ ğ‘–\\hat{s}_{i} evolve according to the process: Under assumptions A1 - A4, the dynamics of s^iâ€‹(t)subscript^ğ‘ ğ‘–ğ‘¡\\hat{s}_{i}(t) given by Eq.Â 7 are governed by the stochastic process, In terms of the per layer balanced student weight wi=s^isubscriptğ‘¤ğ‘–subscript^ğ‘ ğ‘–w_{i}=\\sqrt{\\hat{s}_{i}}, the above process can be thought of as originating from the quartic loss landscape â„“iâ€‹(wi)=14â€‹(wi2âˆ’si~)2subscriptâ„“ğ‘–subscriptğ‘¤ğ‘–14superscriptsuperscriptsubscriptğ‘¤ğ‘–2~subscriptğ‘ ğ‘–2\\ell_{i}(w_{i})=\\frac{1}{4}(w_{i}^{2}-\\tilde{s_{i}})^{2}, equivalent to the example presented in Sec.Â 4.\nThus, there exists an invariant set for each student singular value corresponding to the affine space s^i=wi=0subscript^ğ‘ ğ‘–subscriptğ‘¤ğ‘–0\\hat{s}_{i}=w_{i}=0 where both drift and diffusion terms vanish. Stochastic collapse to this invariant set would thus induce a low-rank regularization on the student in which some number of singular modes in the data are never learned despite having nonzero data singular values s~isubscript~ğ‘ ğ‘–\\tilde{s}_{i}. To confirm stochastic collapse to this invariant set, we derive: In the limit s^iâ€‹(0)â†’0â†’subscript^ğ‘ ğ‘–00\\hat{s}_{i}(0)\\rightarrow 0, for any t>0ğ‘¡0t>0, then s^iâ€‹(t)/s^iâ€‹(0)â†’a.s.e(2â€‹s~iâˆ’Î·â€‹Î¶2)â€‹t+2â€‹Î·â€‹Î¶2â€‹Btsubscriptâ†’formulae-sequenceğ‘ğ‘ subscript^ğ‘ ğ‘–ğ‘¡subscript^ğ‘ ğ‘–0superscriptğ‘’2subscript~ğ‘ ğ‘–ğœ‚superscriptğœ2ğ‘¡2ğœ‚superscriptğœ2subscriptğµğ‘¡\\hat{s}_{i}(t)/\\hat{s}_{i}(0)\\mathrel{\\mathop{\\rightarrow}\\limits_{a.s.}}e^{(2\\tilde{s}_{i}-\\eta\\zeta^{2})t+2\\sqrt{\\eta\\zeta^{2}}B_{t}}. CorollaryÂ 6.1 has important implications. First, if s~i<Î·â€‹Î¶22subscript~ğ‘ ğ‘–ğœ‚superscriptğœ22\\tilde{s}_{i}<\\frac{\\eta\\zeta^{2}}{2}, the distribution will exhibit stochastic collapse, converging to a delta distribution at the origin, consistent with TheoremÂ 4.1 (Fig.Â 5: middle right panel). This in turn implies that early training with large learning rates promotes stochastic collapse of more student singular modes.\nIndeed, even after the training loss plateaus, further extended training with a large learning rate Î·ğœ‚\\eta will continue to drive stochastic collapse of small singular modes. After a subsequent learning rate drop, some of these modes will cease to obey the collapsing condition and as a consequence, the associated invariant set will repulse rather than attract the dynamics. Nonetheless, the repulsive escape will take longer the closer it starts from the invariant set (Fig.Â 5: rightmost panel).\nThis results in dynamics in which generalization error is reduced by the student not learning many data singular modes with large Î·ğœ‚\\eta, because s~i<Î·â€‹Î¶22subscript~ğ‘ ğ‘–ğœ‚superscriptğœ22\\tilde{s}_{i}<\\frac{\\eta\\zeta^{2}}{2}, and then learning only the subset with large s~~ğ‘ \\tilde{s} that have time to escape the vicinity of the invariant set after a learning rate drop. The overall effect is that the student selectively learns modes with higher signal-to-noise ratios, resulting in improved generalization after the learning rate drop as shown in Fig.Â 5: middle left panel.\nIn summary, the linear teacher-student model highlights that with an appropriate learning rate schedule, a noisy student can effectively avoid overfitting through stochastic collapse and achieve superior generalization compared to a noiseless student. Stochastic collapse explains why early large learning rates helps generalization.\nThe analyses in the linear teacher student network provide valuable insights into how stochastic collapsing can enhance generalization. Importantly, it sheds light on the mystery of why training at large learning rates for a long period of time (even after training and test loss have plateaued) helps generalization after a subsequent learning rate drop.\nThe key prediction is that a large learning rate induces stronger stochastic collapse, thereby regularizing the model complexity. Furthermore, remaining in a phase of larger learning rates for a prolonged period drives SGD closer to the invariant set. Consequently, when the learning rate is eventually dropped, overfitting in these specific directions is mitigated. To test this predicted mechanism, we trained VGG-16 and ResNet-16 on CIFAR-10 and CIFAR-100, respectively. The training loss and test accuracy had already plateaued at 104superscript10410^{4} steps (Fig.Â 6: left column).\nWe dropped the learning rate after different lengths of the initial high learning rate training phase and confirmed that training with large learning rates for longer periods helps subsequent generalization (Fig.Â 6: middle column).\nWe then tested our prediction that stochastic collapse is occurring during the early high learning rate training, thereby enhancing an implicit bias towards simpler subnetworks. In particular, we computed the fraction of independent neurons at each step where we drop the learning rate. We found, as predicted, that models with a longer initial training phase collapse towards invariant sets with fewer independent neurons (Fig.Â 6: right column).\n\nReference markers found: [51]\n\nContext windows for each [ref]:\n[ref]51: collapse can benefit generalization, we will theoretically analyze learning dynamics in a teacher-student model for linear networks, a well established framework for studying generalization  [ref]51 .\nThis analysis not only provably connects the phenomenon of stochastic collapse to generalization benefits, but also suggests a stochastic collapse based mechanism for why successful\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 51,\n            \"titles\": [\n                \"Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup\",\n                \"Understanding deep learning (still) requires rethinking generalization\",\n                \"Control batch size and learning rate to generalize well: Theoretical and empirical evidence\",\n                \"Implicit Gradient Regularization\",\n                \"Stochastic Training is Not Necessary for Generalization\",\n                \"Limiting dynamics of sgd: Modified loss, phase space oscillations, and anomalous diffusion\",\n                \"Shape Matters: Understanding the Implicit Bias of the Noise Covariance\",\n                \"Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process\",\n                \"Label Noise SGD Provably Prefers Flat Global Minimizers\",\n                \"What Happens after SGD Reaches Zero Loss? â€“A Mathematical Framework\",\n                \"An Alternative View: When Does SGD Escape Local Minima?\",\n                \"The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects\",\n                \"A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima\",\n                \"On large-batch training for deep learning: Generalization gap and sharp minima\",\n                \"The asymmetric maximum margin bias of quasi-homogeneous neural networks\",\n                \"Kernel and Rich Regimes in Overparametrized Models\",\n                \"Implicit bias of the step size in linear diagonal neural networks\",\n                \"Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity\",\n                \"SGD with Large Step Sizes Learns Sparse Features\",\n                \"Label noise (stochastic) gradient descent implicitly solves the Lasso for quadratic parametrisation\",\n                \"Loss Landscapes of Regularized Linear Autoencoders\",\n                \"Exact Solutions of a Deep Linear Network\",\n                \"Sgd and weight decay provably induce a low-rank bias in neural networks\",\n                \"Implicit bias of sgd in l2subscriptğ‘™2l_{2}-regularized linear dnns: One-way jumps from high to low rank\",\n                \"Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions\",\n                \"The low-rank simplicity bias in deep networks\",\n                \"Gradient Descent Happens in a Tiny Subspace\",\n                \"What shapes the loss landscape of self supervised learning?\",\n                \"SGD with a Constant Large Learning Rate Can Converge to Local Maxima\",\n                \"The probabilistic stability of stochastic gradient descent\",\n                \"Rectified linear units improve restricted boltzmann machines\",\n                \"Rectifier nonlinearities improve neural network acoustic models\",\n                \"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\",\n                \"Searching for Activation Functions\",\n                \"Gaussian Error Linear Units (GELUs)\",\n                \"Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics\",\n                \"Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape\",\n                \"Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances\",\n                \"Dynamics of learning near singularities in layered networks\",\n                \"Stochastic modified equations and adaptive stochastic gradient algorithms\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]51 based on the context and research area of Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}