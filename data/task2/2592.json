{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Framework for Supervised Heterogeneous Transfer Learning using Dynamic Distribution Adaptation and Manifold Regularization\n\nPaper Abstract: \n\nPaper Section Text:\nTLF consists of five main steps as follows (also shown in Algorithm 1). Step-1: Build forests and find label distributions and centroids from both source Dssubscript𝐷𝑠D_{s} and target Dtsubscript𝐷𝑡D_{t} domains. Step-2: Remove duplicate label distributions and find the aggregated values of the corresponding centroids. Step-3: Find pivots between source and target domains. Step-4: Calculate projection matrix between source and target domains. Step-5: Find and transfer projected source data to the target domain and build the final classifier. We also present an overall flow diagram of TLF as shown in Fig. 5. We now discuss the steps as follows. Step-1: Build forests and find label distributions and centroids from both source Dssubscript𝐷𝑠D_{s} and target Dtsubscript𝐷𝑡D_{t} domains. In TLF, we take two datasets, namely source domain dataset Dssubscript𝐷𝑠D_{s} and target dataset Dtsubscript𝐷𝑡D_{t} as input. Dssubscript𝐷𝑠D_{s} has nssubscript𝑛𝑠n_{s} rows, dssubscript𝑑𝑠d_{s} attributes and Cssubscript𝐶𝑠C_{s} labels, and Dtsubscript𝐷𝑡D_{t} has ntsubscript𝑛𝑡n_{t} rows, dtsubscript𝑑𝑡d_{t} attributes and Ctsubscript𝐶𝑡C_{t} labels. In this step, we build two forests Fssubscript𝐹𝑠F_{s} and Ftsubscript𝐹𝑡F_{t} (where each forest has τ𝜏\\tau trees) by applying a decision forest algorithm such as Random Forest (RF) [ref]30 on Dssubscript𝐷𝑠D_{s} and Dtsubscript𝐷𝑡D_{t}, respectively as shown in the Step 1 of Algorithm 1 and Fig. 5. Using Fssubscript𝐹𝑠F_{s}, we find the leaves Lssubscript𝐿𝑠L_{s} where a leaf lsi∈Lssubscriptsuperscript𝑙𝑖𝑠subscript𝐿𝑠l^{i}_{s}\\in L_{s} is represented by the i𝑖ith rule of Fssubscript𝐹𝑠F_{s}. For the i𝑖ith leaf lsisubscriptsuperscript𝑙𝑖𝑠l^{i}_{s}, we find records (from Dssubscript𝐷𝑠D_{s}) that belong to lsisubscriptsuperscript𝑙𝑖𝑠l^{i}_{s}. We then calculate the label distribution vsisubscriptsuperscript𝑣𝑖𝑠v^{i}_{s}, the centroid wsisubscriptsuperscript𝑤𝑖𝑠w^{i}_{s} and centroid’s label rsi∈Cssubscriptsuperscript𝑟𝑖𝑠subscript𝐶𝑠r^{i}_{s}\\in C_{s}. The vsi​jsubscriptsuperscript𝑣𝑖𝑗𝑠v^{ij}_{s} is the probability of the j𝑗jth label in lsisubscriptsuperscript𝑙𝑖𝑠l^{i}_{s}. If the j𝑗jth attribute is a numerical attribute then the value of wsi​jsubscriptsuperscript𝑤𝑖𝑗𝑠w^{ij}_{s} is calculated as follows. where usi​jsubscriptsuperscript𝑢𝑖𝑗𝑠u^{ij}_{s} and s​t​d𝑠𝑡𝑑std are the j𝑗jth attribute mean and standard deviation, respectively, of the records that belong to leaf lsisubscriptsuperscript𝑙𝑖𝑠l^{i}_{s}. For categorical attributes, we use the mode values. Thus, for all leaves of Fssubscript𝐹𝑠F_{s} we obtain a label distribution matrix Vs[Ls×Cs]subscriptsuperscript𝑉delimited-[]subscript𝐿𝑠subscript𝐶𝑠𝑠V^{[L_{s}\\times C_{s}]}_{s}, a centroid matrix Ws[Ls×ds]subscriptsuperscript𝑊delimited-[]subscript𝐿𝑠subscript𝑑𝑠𝑠W^{[L_{s}\\times d_{s}]}_{s} and a matrix for centroids label Rs[Ls×1]subscriptsuperscript𝑅delimited-[]subscript𝐿𝑠1𝑠R^{[L_{s}\\times 1]}_{s}. Similarly, using Dtsubscript𝐷𝑡D_{t} and Ftsubscript𝐹𝑡F_{t}, we find a label distribution matrix Vt[Lt×Ct]subscriptsuperscript𝑉delimited-[]subscript𝐿𝑡subscript𝐶𝑡𝑡V^{[L_{t}\\times C_{t}]}_{t}, a centroid matrix Wt[Lt×dt]subscriptsuperscript𝑊delimited-[]subscript𝐿𝑡subscript𝑑𝑡𝑡W^{[L_{t}\\times d_{t}]}_{t} and a matrix for centroids label Rt[Lt×1]subscriptsuperscript𝑅delimited-[]subscript𝐿𝑡1𝑡R^{[L_{t}\\times 1]}_{t}. The centroids are considered as the attribute contributions which are calculated differently than SHDA . Step-2: Remove duplicate label distributions and find the aggregated values of the corresponding centroids. Since label distribution of a leaf is calculated based on the number of records, of each label, belonging to the leaf, the existence of duplicate label distributions at the leaves is common. In this step, we remove duplicates from Vssubscript𝑉𝑠V_{s} and Vtsubscript𝑉𝑡V_{t} as shown in Step 2 of Algorithm 1. For every duplicate entry in Vssubscript𝑉𝑠V_{s} and Vtsubscript𝑉𝑡V_{t}, the corresponding centroids in Wssubscript𝑊𝑠W_{s} and Wtsubscript𝑊𝑡W_{t} are aggregated. For numerical and categorical attributes we take the average and mode values, respectively. This procedure helps to keep attribute contributions for the aggregated vectors. Let Ls′subscriptsuperscript𝐿′𝑠L^{\\prime}_{s} and Lt′subscriptsuperscript𝐿′𝑡L^{\\prime}_{t} be the number of entries remained after the aggregation. Now, the matrices are: Vs[Ls′×Cs]subscriptsuperscript𝑉delimited-[]subscriptsuperscript𝐿′𝑠subscript𝐶𝑠𝑠V^{[{L^{\\prime}_{s}}\\times{C_{s}}]}_{s}, Vt[Lt′×Ct]subscriptsuperscript𝑉delimited-[]subscriptsuperscript𝐿′𝑡subscript𝐶𝑡𝑡V^{[{L^{\\prime}_{t}}\\times{C_{t}}]}_{t}, Ws[Ls′×ds]subscriptsuperscript𝑊delimited-[]subscriptsuperscript𝐿′𝑠subscript𝑑𝑠𝑠W^{[{L^{\\prime}_{s}}\\times{d_{s}}]}_{s}, Wt[Lt′×dt]subscriptsuperscript𝑊delimited-[]subscriptsuperscript𝐿′𝑡subscript𝑑𝑡𝑡W^{[{L^{\\prime}_{t}}\\times{d_{t}}]}_{t}, Rs[Ls′×1]subscriptsuperscript𝑅delimited-[]subscriptsuperscript𝐿′𝑠1𝑠R^{[{L^{\\prime}_{s}}\\times 1]}_{s} and Rt[Lt′×1]subscriptsuperscript𝑅delimited-[]subscriptsuperscript𝐿′𝑡1𝑡R^{[{L^{\\prime}_{t}}\\times 1]}_{t}. Step-3: Find pivots between source and target domains. In most real-world HeTL scenarios, the feature discrepancy is a challenging issue since the correspondences between Dssubscript𝐷𝑠D_{s} and Dtsubscript𝐷𝑡D_{t} are not available. To minimize the discrepancy, some common information between Dssubscript𝐷𝑠D_{s} and Dtsubscript𝐷𝑡D_{t} can be identified to bridge the domains. Since we assume that both Dssubscript𝐷𝑠D_{s} and Dtsubscript𝐷𝑡D_{t} share some label space, we exploit the shared label space to identify correspondences between domains. Thus, in Step 3, we determine the leaves with similar label distributions across the domains as shown in Step 3 of Algorithm 1 and Fig. 5. Let SLs′×Lt′superscript𝑆subscriptsuperscript𝐿′𝑠subscriptsuperscript𝐿′𝑡S^{L^{\\prime}_{s}\\times L^{\\prime}_{t}} be the similarity matrix of Vs[Ls′×ds]subscriptsuperscript𝑉delimited-[]subscriptsuperscript𝐿′𝑠subscript𝑑𝑠𝑠V^{[{L^{\\prime}_{s}}\\times{d_{s}}]}_{s} and Vt[Lt′×dt]subscriptsuperscript𝑉delimited-[]subscriptsuperscript𝐿′𝑡subscript𝑑𝑡𝑡V^{[{L^{\\prime}_{t}}\\times{d_{t}}]}_{t}, where each value varies between 0 and 1. A lower value indicates a higher similarity. The similarity of two label distributions is computed using the Jensen Shannon divergence (JSD) method . A source label distribution Vsisubscriptsuperscript𝑉𝑖𝑠V^{i}_{s} is said to be similar to a target label distribution Vtksubscriptsuperscript𝑉𝑘𝑡V^{k}_{t} if the divergence between them is less than 10%. Since Dtsubscript𝐷𝑡D_{t} generally contains small labeled data, it is common to have a small number of common labels. So, we my have only a small number of similar labeled distributions across the domains. Each shared labeled distribution (i.e. (Vsi,Vtk)subscriptsuperscript𝑉𝑖𝑠subscriptsuperscript𝑉𝑘𝑡(V^{i}_{s},V^{k}_{t})) is considered as a pivot that acts as the bridge between Dssubscript𝐷𝑠D_{s} and Dtsubscript𝐷𝑡D_{t}. Let Npsubscript𝑁𝑝N_{p} be the number of pivots between Dssubscript𝐷𝑠D_{s} and Dtsubscript𝐷𝑡D_{t}. For each pivotal label distribution, we also have the attribute contribution which is calculated in Step 1 and refined in Step 2. Based on the similar source and target label distributions, we then determine the attribute contribution matrices Ws[Np×ds]subscriptsuperscript𝑊delimited-[]subscript𝑁𝑝subscript𝑑𝑠𝑠W^{[N_{p}\\times d_{s}]}_{s} and Wt[Np×dt]subscriptsuperscript𝑊delimited-[]subscript𝑁𝑝subscript𝑑𝑡𝑡W^{[N_{p}\\times d_{t}]}_{t}, and the associated label matrices Rs[Np×1]subscriptsuperscript𝑅delimited-[]subscript𝑁𝑝1𝑠R^{[N_{p}\\times 1]}_{s} and Rt[Np×1]subscriptsuperscript𝑅delimited-[]subscript𝑁𝑝1𝑡R^{[N_{p}\\times 1]}_{t}. Step-4: Calculate projection matrix between source and target domains. In this step, we calculate a projection matrix Pssubscript𝑃𝑠P_{s} for Dssubscript𝐷𝑠D_{s} so that the data can be projected to Dtsubscript𝐷𝑡D_{t} as shown in Step 4 of Algorithm 1 and Fig. 5. We calculate Pssubscript𝑃𝑠P_{s} by applying structural risk minimization (SRM) principle and regularization theory 1 on the attribute contribution matrices Ws[Np×ds]subscriptsuperscript𝑊delimited-[]subscript𝑁𝑝subscript𝑑𝑠𝑠W^{[N_{p}\\times d_{s}]}_{s} and Wt[Np×dt]subscriptsuperscript𝑊delimited-[]subscript𝑁𝑝subscript𝑑𝑡𝑡W^{[N_{p}\\times d_{t}]}_{t}. Let Jssubscript𝐽𝑠J_{s} and Jtsubscript𝐽𝑡J_{t} be the source and target joint probability distributions, respectively and Pssubscript𝑃𝑠P_{s} and Ptsubscript𝑃𝑡P_{t} be the source and target marginal distributions, respectively. To calculate Pssubscript𝑃𝑠P_{s} we particularly, aim to optimize the following three objective functions: To minimize structural risk function on Wssubscript𝑊𝑠W_{s} and Wtsubscript𝑊𝑡W_{t}; To minimize distribution discrepancy between Jssubscript𝐽𝑠J_{s} and Jtsubscript𝐽𝑡J_{t}; and To maximize manifold consistency underlying Pssubscript𝑃𝑠P_{s} and Ptsubscript𝑃𝑡P_{t}. Let Dcsubscript𝐷𝑐D_{c} be the combined dataset of Wssubscript𝑊𝑠W_{s}, Rssubscript𝑅𝑠R_{s}, Wtsubscript𝑊𝑡W_{t} and Rtsubscript𝑅𝑡R_{t} i.e. Dc={(ws1,rs1),…,(wsNp,rsNp),(wt1,rt1),…,(wtNp,rtNp)}subscript𝐷𝑐subscript𝑤subscript𝑠1subscript𝑟subscript𝑠1…subscript𝑤subscript𝑠subscript𝑁𝑝subscript𝑟subscript𝑠subscript𝑁𝑝subscript𝑤subscript𝑡1subscript𝑟subscript𝑡1…subscript𝑤subscript𝑡subscript𝑁𝑝subscript𝑟subscript𝑡subscript𝑁𝑝D_{c}=\\left\\{(w_{s_{1}},r_{s_{1}}),...,(w_{s_{N_{p}}},r_{s_{N_{p}}}),(w_{t_{1}},r_{t_{1}}),...,(w_{t_{N_{p}}},r_{t_{N_{p}}})\\right\\}. By ignoring source and destination, we can simply write Dcsubscript𝐷𝑐D_{c} as {(w1,r1),…,(wz,rz)}subscript𝑤1subscript𝑟1…subscript𝑤𝑧subscript𝑟𝑧\\left\\{(w_{1},r_{1}),...,(w_{z},r_{z})\\right\\}, where z=2​Np𝑧2subscript𝑁𝑝z=2N_{p}. Also let f=βT​ϕ​(w)𝑓superscript𝛽𝑇italic-ϕ𝑤f=\\beta^{T}\\phi(w) be the classifier having parameters w𝑤w and feature mapping function ϕ:w↦ℋ:italic-ϕmaps-to𝑤ℋ\\phi:w\\mapsto\\mathcal{H} that projects the actual feature vector to a Hilbert space ℋℋ\\mathcal{H}. Using the SRM principle and regularization theory, the framework of TLF can be formulated as: where K𝐾K is the kernel function represented by ϕitalic-ϕ\\phi such that ⟨ϕ​(wi),ϕ​(wi)⟩=K​(wi,wj)italic-ϕsubscript𝑤𝑖italic-ϕsubscript𝑤𝑖𝐾subscript𝑤𝑖subscript𝑤𝑗\\left\\langle\\phi(w_{i}),\\phi(w_{i})\\right\\rangle=K(w_{i},w_{j}), and l(.,.)l(.,.), ‖f‖K2subscriptsuperscriptnorm𝑓2𝐾\\left\\|f\\right\\|^{2}_{K} and Df,K(.,.)D_{f,K}(.,.) represent loss function, ridge regularization and dynamic distribution adaptation (DDA), respectively. Moreover, we introduce Mf,K​(Ps,Pt)subscript𝑀𝑓𝐾subscript𝑃𝑠subscript𝑃𝑡M_{f,K}(P_{s},P_{t}) as a Laplacian regularization to further exploit intrinsic properties of nearest points in Manifold 𝒢𝒢\\mathcal{G} . The symbols σ𝜎\\sigma, λ𝜆\\lambda, and γ𝛾\\gamma are positive regularization parameters for ridge, DDA, and Manifold, respectively. In Eq. (3), the loss function l(.)l(.) measures the fitness of f𝑓f for classifying labels on Dcsubscript𝐷𝑐D_{c}. We use SVM 1 to solve l(.)l(.). Moreover, since the kernel mapping ϕ:w↦ℋ:italic-ϕmaps-to𝑤ℋ\\phi:w\\mapsto\\mathcal{H} may have infinite dimensions, we solve Eq. (3) by reformulating it with the following Representer Theorem. (Representer Theorem) [ref]3 1 If K𝐾K is a kernel function induced by ϕitalic-ϕ\\phi then the optimization problem in Eq. (3) for Dcsubscript𝐷𝑐D_{c}, can be represented by where αisubscript𝛼𝑖\\alpha_{i} is a coefficient. Using the SRM principle and equations (4), we can reformulate loss function, ridge regularization of Eq. (3) as follows. We now discuss the process of minimizing distribution divergence through dynamic distribution adaptation and manifold regularization as follows. Dynamic Distribution Adaptation (DDA): Since the joint distributions Jssubscript𝐽𝑠J_{s} and Jtsubscript𝐽𝑡J_{t} are different, the loss function l(.)l(.) with only the ridge regularization in Eq. (3) may not generalize well to Wtsubscript𝑊𝑡W_{t} [ref]3. Thus, we aim to minimize joint distribution divergence 1) between the marginal distributions Pssubscript𝑃𝑠P_{s} and Ptsubscript𝑃𝑡P_{t}, and 2) between the conditional distributions Qssubscript𝑄𝑠Q_{s} and Qtsubscript𝑄𝑡Q_{t}, simultaneously. Following ARTL [ref]3 and MEDA , we adopt MMD 3  to handle the distribution divergence. Based on the samples drawn from two domains, MMD is capable of determining whether two distributions are different or not. The MMD between marginal distributions Pssubscript𝑃𝑠P_{s} and Ptsubscript𝑃𝑡P_{t} is computed as where ϕ:w↦ℋ:italic-ϕmaps-to𝑤ℋ\\phi:w\\mapsto\\mathcal{H} is a feature mapping. The MMD can be regularized for the classifier f𝑓f as where f=βT​ϕ​(w)𝑓superscript𝛽𝑇italic-ϕ𝑤f=\\beta^{T}\\phi(w). Since we have labeled data in both domains, we now minimize the distance Df,K​(Ps,Pt)subscript𝐷𝑓𝐾subscript𝑃𝑠subscript𝑃𝑡D_{f,K}(P_{s},P_{t}) between conditional distributions Qssubscript𝑄𝑠Q_{s} and Qtsubscript𝑄𝑡Q_{t}. Based on the intra-class centroids of two distributions Qs​(ws|rs)subscript𝑄𝑠conditionalsubscript𝑤𝑠subscript𝑟𝑠Q_{s}(w_{s}|r_{s}) and Qt​(wt|rt)subscript𝑄𝑡conditionalsubscript𝑤𝑡subscript𝑟𝑡Q_{t}(w_{t}|r_{t}), the MMD of a class c∈C𝑐𝐶c\\in C is computed as where Ws(c)={wi:wi∈Ws∧r​(wi)=c}subscriptsuperscript𝑊𝑐𝑠conditional-setsubscript𝑤𝑖subscript𝑤𝑖subscript𝑊𝑠𝑟subscript𝑤𝑖𝑐W^{(c)}_{s}=\\left\\{w_{i}:w_{i}\\in W_{s}\\wedge r(w_{i})=c\\right\\} is the set records associated to class c𝑐c in Dssubscript𝐷𝑠D_{s} and Wt(c)={wj:wj∈Wt∧r​(wj)=c}subscriptsuperscript𝑊𝑐𝑡conditional-setsubscript𝑤𝑗subscript𝑤𝑗subscript𝑊𝑡𝑟subscript𝑤𝑗𝑐W^{(c)}_{t}=\\left\\{w_{j}:w_{j}\\in W_{t}\\wedge r(w_{j})=c\\right\\} is the set records associated to class c𝑐c in Dtsubscript𝐷𝑡D_{t}. If |Wk(c)|=0subscriptsuperscript𝑊𝑐𝑘0|W^{(c)}_{k}|=0 then we ignore the part of |Wk|subscript𝑊𝑘|W_{k}| in Eq. (8). In real-world scenarios, the importance of marginal (P𝑃P) and conditional (Q𝑄Q) are different . Thus, the divergence for joint joint distributions Jssubscript𝐽𝑠J_{s} and Jtsubscript𝐽𝑡J_{t} is adapted dynamically by integrating Eq. (7) and Eq. (8) as follows. where μ∈ 𝜇01\\mu\\in  is the adaptive factor which is calculated dynamically by following MEDA . μ→0→𝜇0\\mu\\rightarrow 0 indicates the distribution distance between the domains is large, and thus, the marginal distribution adaptation is important. Besides, μ→1→𝜇1\\mu\\rightarrow 1 indicates the class distribution is dominant than feature distribution, and thus, the conditional distribution adaptation is important. Using the Representer Theorem (see Theorem 1 and Eq. (4)), Eq. (9) becomes where M[z×z]=(1−μ)​M0+μ​∑c=1CMcsuperscript𝑀delimited-[]𝑧𝑧1𝜇subscript𝑀0𝜇subscriptsuperscript𝐶𝑐1subscript𝑀𝑐M^{[z\\times z]}=(1-\\mu)M_{0}+\\mu\\sum^{C}_{c=1}{M_{c}} is the MMD matrix. The components of the MMD matrix are calculated as follows. where nc=|Ws(c)|subscript𝑛𝑐subscriptsuperscript𝑊𝑐𝑠n_{c}=|W^{(c)}_{s}| and mc=|Wt(c)|subscript𝑚𝑐subscriptsuperscript𝑊𝑐𝑡m_{c}=|W^{(c)}_{t}|. Manifold Regularization: In addition to DDA, we further exploit intrinsic properties of the marginal distributions Pssubscript𝑃𝑠P_{s} and Ptsubscript𝑃𝑡P_{t} by manifold regularization  due to its effectiveness in mitigating influence of feature distortion [ref]3 . The features of a domain can be transformed into manifold space which is suitable for exploiting more detailed property and structure of the domain. Considering the manifold assumption , the manifold regularization is computed as where L[z×z]superscript𝐿delimited-[]𝑧𝑧L^{[z\\times z]} is the normalized Laplacian regularization matrix and B𝐵B is the graph affinity matrix which is calculated as where cos⁡(wi,wj)=wi.wj‖wi‖.‖wj‖subscript𝑤𝑖subscript𝑤𝑗formulae-sequencesubscript𝑤𝑖subscript𝑤𝑗formulae-sequencenormsubscript𝑤𝑖normsubscript𝑤𝑗\\cos(w_{i},w_{j})=\\frac{w_{i}.w_{j}}{\\|w_{i}\\|.\\|w_{j}\\|} is a similarity function which is used to calculate distance between two records wisubscript𝑤𝑖w_{i} and wjsubscript𝑤𝑗w_{j}. It is worth noting that any similarity functions such as guassian, laplacian and cosine, can be incorporated in TLF. We empirically test the effectiveness of TLF, by using three kernels separately, on a real-world dataset Reuters-215782 (RT)  which has 3 cross-domain pairs as follows: orgs↦maps-to\\mapstopeople, orgs↦maps-to\\mapstoplaces and people↦maps-to\\mapstoplaces (see Section 5.1 for details). For each cross-domain pair, we separately use guassian, laplacian and cosine kernels in Eq. (14) and calculate the corresponding classification accuracy. We present the results in Fig. 6 which shows that TLF performs the best when cosine kernel is used in Eq. (14). Therefore, we use cosine kernel in TLF to calculate distance between two records wisubscript𝑤𝑖w_{i} and wjsubscript𝑤𝑗w_{j}. In Eq. (14), k​(wj)𝑘subscript𝑤𝑗k(w_{j}) represents the k𝑘k nearest-neighbors (kNN) of the record wjsubscript𝑤𝑗w_{j}. While existing methods including ARTL [ref]3 and MEDA  use a user-defined value for k𝑘k, we propose a novel approach to determine k𝑘k automatically so that the intrinsic properties of a record can be exploited more accurately resulting in a high transfer performance. In TLF, we determine k𝑘k NN of a record wisubscript𝑤𝑖w_{i} having label risubscript𝑟𝑖r_{i} as follows. We first calculate distance between wisubscript𝑤𝑖w_{i} and other records one by one and then sort the records based on the distance in ascending order. We set the minimum of value of k𝑘k to 4. That is, the first\nfour records are added to kNN set. Following the order, we continue to add a record wusubscript𝑤𝑢w_{u} to kNN set iteratively until ri≠ru;∀usubscript𝑟𝑖subscript𝑟𝑢for-all𝑢r_{i}\\neq{r_{u}};\\forall u. Fig. 7 illustrates the difference between user-defined and automatic k𝑘k, the automatic one finds the nearest-neighbors of a record more accurately. We now test the influences of automatic k𝑘k on the RT dataset. We first apply TLF on each pair for different user-defined k𝑘k values such as 4, 8, 16, 32 and 64, and then calculate classification accuracy for each k𝑘k. After that we apply TLF on each pair where k𝑘k is determined automatically and then calculate classification accuracy. We present the results of the people↦maps-to\\mapstoplace pair in Fig. 8. The figure clearly shows the effectiveness of automatic k𝑘k since TLF achieves the best results when we determine k𝑘k automatically. We also obtain similar results for other two pairs. The Laplacian matrix, L𝐿L of Eq. (13) is calculated as where D𝐷D is a diagonal matrix with each element Di​i=∑j=1zBi​jsubscript𝐷𝑖𝑖subscriptsuperscript𝑧𝑗1subscript𝐵𝑖𝑗D_{ii}=\\sum^{z}_{j=1}{B_{ij}}. Thus, by incorporating Eq. (4) into Eq. (13), we get Using equations (5), (10) and (16), we can reformulate f𝑓f in Eq. (3) as follows. Setting derivative ∂f∂α=0𝑓𝛼0\\frac{\\partial f}{\\partial\\alpha}=0, we obtain the solution as We now transform the attribute contribution matrices Ws[Np×ds]subscriptsuperscript𝑊delimited-[]subscript𝑁𝑝subscript𝑑𝑠𝑠W^{[N_{p}\\times d_{s}]}_{s} and Wt[Np×dt]subscriptsuperscript𝑊delimited-[]subscript𝑁𝑝subscript𝑑𝑡𝑡W^{[N_{p}\\times d_{t}]}_{t} into Gs[z×ds]subscriptsuperscript𝐺delimited-[]𝑧subscript𝑑𝑠𝑠G^{[z\\times d_{s}]}_{s} and Gt[z×dt]subscriptsuperscript𝐺delimited-[]𝑧subscript𝑑𝑡𝑡G^{[z\\times d_{t}]}_{t}, respectively as follows. Using equations (18), (19) and (20), we obtain the projection matrix Pssubscript𝑃𝑠P_{s} as follows. Step-5: Find and transfer projected source data to the target domain and build the final classifier. In this step, we first identify transferable data Ds′subscriptsuperscript𝐷′𝑠D^{\\prime}_{s} from Dssubscript𝐷𝑠D_{s} by using the projection matrix Pssubscript𝑃𝑠P_{s} (obtained in Eq. (21)) and source pivots Vssubscript𝑉𝑠V_{s} (obtained in Step 3). We then project Ds′subscriptsuperscript𝐷′𝑠D^{\\prime}_{s} into Ds′′subscriptsuperscript𝐷′′𝑠D^{\\prime\\prime}_{s} i.e. Ds′′=Ds′×Pssubscriptsuperscript𝐷′′𝑠subscriptsuperscript𝐷′𝑠subscript𝑃𝑠D^{\\prime{\\prime}}_{s}=D^{\\prime}_{s}\\times{P_{s}}. After that we merge Ds′′subscriptsuperscript𝐷′′𝑠D^{\\prime\\prime}_{s} with Dtsubscript𝐷𝑡D_{t} and obtain Dt′subscriptsuperscript𝐷′𝑡D^{\\prime}_{t} (see Step 5 of Algorithm 1 and Fig. 5). We finally build the final forest F𝐹F by applying RF on Dt′subscriptsuperscript𝐷′𝑡D^{\\prime}_{t}. The final classifier is then used to classify unlabeled target data.\n\nReference markers found: [30]\n\nContext windows for each [ref]:\n[ref]30: labels. In this step, we build two forests Fssubscript𝐹𝑠F_{s} and Ftsubscript𝐹𝑡F_{t} (where each forest has τ𝜏\\tau trees) by applying a decision forest algorithm such as Random Forest (RF) [ref]30 on Dssubscript𝐷𝑠D_{s} and Dtsubscript𝐷𝑡D_{t}, respectively as shown in the Step 1 of Algorithm 1 and Fig. 5. Using Fssubscript𝐹𝑠F_{s}, we find the leaves Lssubscript𝐿𝑠L_{s} where a leaf\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 30,\n            \"titles\": [\n                \"Balanced Distribution Adaptation for Transfer Learning\",\n                \"A decade survey of transfer learning (2010–2020)\",\n                \"Supervised heterogeneous feature transfer via random forests\",\n                \"Adaptation regularization: A general framework for transfer learning\",\n                \"Transfer Learning with Dynamic Distribution Adaptation\",\n                \"Domain adaptation problems: A dasvm classification technique and a circular validation strategy\",\n                \"Easy Transfer Learning By Exploiting Intra-domain Structures\",\n                \"Semi-supervised heterogeneous domain adaptation: Theory and algorithms\",\n                \"Deep supervised cross-modal retrieval\",\n                \"Heterogeneous Domain Adaptation via Soft Transfer Network\",\n                \"Heterogeneous domain adaptation via covariance structured feature translators\",\n                \"Transfer neural trees for heterogeneous domain adaptation\",\n                \"A survey on transfer learning\",\n                \"Transfer learning via dimensionality reduction.\",\n                \"Learning Deep Kernels for Non-Parametric Two-Sample Tests\",\n                \"Manifold regularization: A geometric framework for learning from labeled and unlabeled examples.\",\n                \"Label-noise robust domain adaptation\",\n                \"Understanding self-training for gradual domain adaptation\",\n                \"Transfer Adaptation Learning: A Decade Survey\",\n                \"Learning from a complementary-label source domain: theory and algorithms\",\n                \"Domain adaptation via transfer component analysis\",\n                \"Statistical Learning Theory\",\n                \"Transfer feature learning with joint distribution adaptation\",\n                \"Transferable representation learning with deep adaptation networks\",\n                \"Deep Transfer Learning with Joint Adaptation Networks\",\n                \"Learning kernels for unsupervised domain adaptation with applications to visual object recognition\",\n                \"Heterogeneous domain adaptation: an unsupervised approach\",\n                \"TOHAN: A One-step Approach towards Few-shot Hypothesis Adaptation\",\n                \"Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation\",\n                \"Source Data-absent Unsupervised Domain Adaptation through Hypothesis Transfer and Labeling Transfer\",\n                \"Random forests\",\n                \"Multiple task transfer learning with small sample sizes\",\n                \"Transfer learning resources\",\n                \"Everything about transfer learning and domain adapation\",\n                \"Visual Domain Adaptation with Manifold Embedded Distribution Alignment\",\n                \"Return of Frustratingly Easy Domain Adaptation\",\n                \"Knowledge discovery through sysfor: a systematically developed forest of multiple decision trees\",\n                \"The weka workbench. online appendix for ”data mining: Practical machine learning tools and techniques”\",\n                \"Majority voting: Material classification by tactile sensing using surface texture\",\n                \"Statistics: an introduction\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]30 based on the context and research area of A Framework for Supervised Heterogeneous Transfer Learning using Dynamic Distribution Adaptation and Manifold Regularization. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}