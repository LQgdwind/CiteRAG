{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Cut-and-Paste Object Insertion by Enabling Deep Image Prior for Reshading\n\nPaper Abstract: \n\nPaper Section Text:\nCP IH  DIPR (ours) Real Scene Scenes and objects. We collected about 100100100 diverse images with\nspatially varying illumination, both indoors-outdoors, day-night, to act as target scenes in our experiments. 252525 of these scenes are captured by placing a spherical lampshade, which we use as ground truth for quantitative evaluation. We first test DIPR by inserting simple 2D bright disks at various locations in target scenes (Fig. 9).\nWe also show reshading results for real cars inserted into our target scenes (Fig. 14). We also tested DIPR with complex surface properties – a lego dozer, a plant, a chair, and a set of sixteen materials with different reflective properties used in NeRF  (Fig. 1). Other objects from  are in our Appendix.\nWe use ADE20K validation set  for supplying real residual loss to our image decomposition network and\nalso to train our shading consistency network. ADE20K does not have ground truth normals and we use normals from [ref]41. Network architecture.\nOur DIP network is very similar to the U-Net used in 3DPhotoInpainting [ref]48.\nWe use their provided model in our implementation as DIP.\nThe only difference is our use of flipped partial convolution instead of standard partial convolution as described in our Sec 3. Training details. We used U-Net for DIP, Image Decomposition and Shading Consistency Network.\nNetwork architecture and other training details are in our supplementary. We update our DIP for a fixed 101010k iterations and this takes about 900900900 seconds using our image decomposition network\nand 160016001600 seconds when using CGIntrinsic . IH baseline. We use Cong et al.’s DoveNet  and their provided pretrained model for our IH baseline. Quantitative comparison to ground truth.\nWe compare 25 photographs of a spherical\nlampshade with DIPR based insertions. We get insertions\nby placing a white disk over the lampshade, reshading\nthe disk using DIPR, then multiplying by lampshade’s albedo\n(a favorable case as it correctly rendered cast shadows). We then quantitatively compare results with ground\ntruth (the real lampshade) using PSNR, LPIPS or MSE (Tab. 1; Fig. LABEL:fig:teaser, Fig. 6). Fig. LABEL:fig:teaser (b) and (c) are real. Fooling users. The gold standard evaluation here is user studies (after all, the goal is to fool people). However, user studies are a poor\nway to polish a method, and a proxy would be valuable. For images of the spherical lampshade used in the quantitative evaluation of\nTab. 1, we asked users to identify which of the two presented images (showing distinct scenes, but both containing the lampshade) was real. One image\npresented was always a rendering, the other always real. Users each see\n16 pairs, and there were 727272 participants performing this study. Our DIPR renderings are very good at fooling users (50%percent5050\\% is a chance). They pick DIPR 42.7%percent42.742.7\\% of the time. We then use logistic regression to predict each rendered image’s probability of being marked real against MSE, PSNR and LPIPS. An accurate regression would mean that we had a score of “realness”. However, such a model explains almost none of the variation of the data (null deviance: 17.8717.8717.87, residual deviance: 171717).\nThis means that, while it is pleasant that DIPR has strong MSE, PSNR, and LPIPS scores, these scores can not be used to predict user preferences for our task. For other object when we did not have ground truth for objects like cars, lego dozer, plant and chair, we conducted another user study to compare DIPR, CP, and IH renderings. Each study comprises a pre-qualifying process, followed by 999 pair-wise comparisons, where the user is asked which of two images are more realistic. The result is 109109109 pre-qualified studies. The comparisons are balanced. Each study is 333 DIPR-IH pairs, 333 CP-IH pairs, and 333 DIPR-CP pairs, in random order. We collected data from a total of 122 unique users in 500 studies from Amazon Mechanical Turk. Each study consists of a prequalifying process,\nfollowed by 9 pair-wise comparisons, where the user is asked which of two images are more realistic. The prequalifying\nprocess presents the user with five tests; each consists of an image with inserted white spheres which are not reshaded (i.e. bright\nwhite disks) and an image with inserted spheres which have been reshaded (see Fig 9). We ignore any study where the user does not correctly identify all five reshaded images, on the grounds that the difference is very obvious and the user must not have\nbeen paying attention. The simplest analysis strongly supports DIPR is preferred over both alternatives. One compares the probability that DIPR is preferred to IH (.673.673.673, over 327327327 comparisons, so standard error is .026.026.026, and the difference from 0.50.50.5 is clearly significant); DIPR is preferred to CP (.645.645.645, over 327 comparisons, so the standard error is .026.026.026, and the difference from 0.50.50.5 is clearly significant); IH is preferred to CP (.511.511.511, over 327327327 comparisons, so standard error is .027.027.027, and there is no significant difference from 0.50.50.5). An alternative is a Bradley-Terry model   used in IH evaluation, regressing the quality predicted by the Bradley-Terry model against the class of algorithm. This yields coefficients of 00 for IH, −0.3470.347-0.347 for CP, and 0.0390.0390.039 for DIPR, implying again that DIPR is preferred over IH and strongly preferred over CP. Target Scene CP IH  DIPR (ours) Consistent Instance Segmentation. We cannot quantitatively evaluate our reshading method when we do not know the ground truth. But we can test whether standard image tasks (which likely benefit from structural consistency in images) perform better on our images. We observe image segmentation methods (we used [ref]8 ) seem to prefer our images\n(Fig 8) compared to the naïve cut-and-paste. We believe the instance segmentation network inherently “knows” if the object’s placement is natural or not and hence requires the foreground object to be consistent with the background. If not, the segmentation would produce inconsistent results. Our findings are also consistent with Ghiasi et al., who show cut-and-paste is a strong data augmentation method for the instance segmentation. Previous models trained without this augmentation, such as  [ref]8 are sensitive to cut-and-paste images if the inserted fragments contradict the background’s shading. This suggests downstream standard image analysis tasks can serve as a proxy evaluation to further polish reshading methods and also DIPR data augmentations could further improve various recognition tasks.\n\nReference markers found: [41]\n\nContext windows for each [ref]:\n[ref]41: set  for supplying real residual loss to our image decomposition network and\nalso to train our shading consistency network. ADE20K does not have ground truth normals and we use normals from [ref]41. Network architecture.\nOur DIP network is very similar to the U-Net used in 3DPhotoInpainting [ref]48.\nWe use their provided model in our implementation as DIP.\nThe only difference is our use\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 41,\n            \"titles\": [\n                \"Learning Physics-guided Face Relighting under Directional Light\",\n                \"Shapes and Context: In-the-Wild Image Synthesis & Manipulation\",\n                \"Shape, Illumination, and Reflectance from Shading\",\n                \"What is the set of images of an object under all possible illumination conditions?\",\n                \"Intrinsic images in the wild\",\n                \"Remote effects of highlights on gloss perception\",\n                \"An l 1 image transform for edge-preserving smoothing and scene-level intrinsic decomposition\",\n                \"Tracking multiple targets with multifocal attention\",\n                \"Rethinking atrous convolution for semantic image segmentation\",\n                \"DoveNet: Deep Image Harmonization via Domain Verification\",\n                \"Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography\",\n                \"Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection\",\n                \"Revisiting Deep Intrinsic Image Decompositions\",\n                \"Intrinsic Image Decomposition Using Paradigms\",\n                \"Deep Parametric Indoor Lighting Estimation\",\n                \"Fast spatially-varying indoor lighting estimation\",\n                \"Training ‘greeble’experts: a framework for studying expert object recognition processes\",\n                \"Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation\",\n                \"Image harmonization with transformer\",\n                \"Deep Sky Modeling for Single Image Outdoor Lighting Estimation\",\n                \"Drag-and-drop pasting\",\n                \"Rendering Synthetic Objects into Legacy Photographs\",\n                \"Automatic Scene Inference for 3D Object Compositing\",\n                \"Photo clip art\",\n                \"Color vision and the natural image. part i\",\n                \"Color vision and the natural image part ii\",\n                \"The retinex theory of color vision\",\n                \"Lightness and retinex theory\",\n                \"Context-Aware Synthesis and Placement of Object Instances\",\n                \"Inverse Rendering for Complex Indoor Scenes: Shape, Spatially-Varying Lighting and SVBRDF from a Single Image\",\n                \"Cgintrinsics: Better intrinsic image decomposition through physically-based rendering\",\n                \"Learning Intrinsic Image Decomposition from Watching the World\",\n                \"Learning to reconstruct shape and spatially-varying reflectance from a single image\",\n                \"Building a dictionary of image fragments\",\n                \"An approximate shading model for object relighting\",\n                \"An Approximate Shading Model with Detail Decomposition for Object Relighting\",\n                \"Microsoft COCO: Common Objects in Context\",\n                \"Region-aware Adaptive Instance Normalization for Image Harmonization\",\n                \"Image Inpainting for Irregular Holes Using Partial Convolutions\",\n                \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]41 based on the context and research area of Cut-and-Paste Object Insertion by Enabling Deep Image Prior for Reshading. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}