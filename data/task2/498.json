{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Formal Language Approach to Explaining RNNsThe source code is available at https://github.com/bishwamittra/LEXR\n\nPaper Abstract: \n\nPaper Section Text:\nIn this section, we evaluate the explanatory capabilities of LEXRÂ and compare it to the current state-of-the-art explanation method of Mayr and YovineÂ , which produces explanations in terms of DFAs.\nTo this end, we have implemented a Python prototype based on the LTL learning algorithm by Neider and GavranÂ [ref]26.\nMoreover, we have modified Mayr and Yovineâ€™s method such that it can handle queries.\nTo simplify the implementation of the verifier, we did not use the dynamic programming solution described in SectionÂ 4.1 but a similar technique based on a translation of LTL into DFAsÂ .\nBoth implementations use the same verifier (though Mayr and Yovineâ€™s method also asks for the classification of individual test inputs). Since assessing the quality of an explanation is difficult (as we do not know one), we use a statistical test that computes the accuracy of an explanation on a separate test set TâŠ‚Î£âˆ—ğ‘‡superscriptÎ£âˆ—T\\subset\\Sigma^{\\ast}.\nSlightly deviating from the standard definition, we define accuracy as the fraction of words uâˆˆTğ‘¢ğ‘‡u\\in T that satisfy uâˆˆLâ€‹(Ï†)ğ‘¢ğ¿ğœ‘u\\in L(\\varphi) if and only if uâˆˆLâ€‹(â„›)âˆ©Lâ€‹(Ïˆ)ğ‘¢ğ¿â„›ğ¿ğœ“u\\in L(\\mathcal{R})\\cap L(\\psi) (i.e., Ï†ğœ‘\\varphi and â„›â„›\\mathcal{R} agree).\nTo determine whether an LTL formula is easier or harder to interpret as a DFA, we compare their sizes (i.e., number of sub-formulas and states), following the principle â€œsmaller is easier to interpretâ€œ typically used in the literatureÂ    [ref]26 . We have conducted experiments with six different RNN acceptors, three RNN acceptors for synthetic languages and three RNN acceptors for real-word problems (valid email addressesÂ , an alternating bit protocolÂ , and words of balanced parenthesesÂ ).\nWe here report summaries of our experiments on two RNN acceptors for synthetic languages and one for the language of balanced parentheses. Details for all experiments can be found in the Appendix. We have conducted our experiments on an Intel Xeon E7-8857 v2 CPU with 48 cores and 1.51.51.5â€‰TB of RAM running on a 64bit Linux distribution based on Debian.\nWe have set Îµğœ€\\varepsilon and Î´ğ›¿\\delta to 0.050.050.05 and the timeout to 400400400â€‰s.\nSince the verifier implements a probabilistic algorithm, we have repeated each experiment 250250250 times and report an average of all numerical quantities.222\nAlthough we assume the RNN acceptors to be black-boxes, we briefly provide information about their training.\nAll RNNs were LSTM networks with 333 layers and 101010 hidden units, while the linear classifier was a linear threshold function with threshold 0.50.50.5.\nFor each synthetic (real-world) problem, we have generated 6,00060006,000 (20,0002000020,000) random words and split them into training and test sets with an 8:2:828:2 ratio.\nWe used the Adam optimizer with a learning rate of 0.0010.0010.001 and a stopping threshold of 0.0050.0050.005 to train the networks.\nOn the test set, the RNN acceptors have achieved more than 99.9%percent99.999.9\\% accuracy.\n Synthetic BenchmarksÂ Â \nWe have trained two RNN acceptors for the languages ğ–¥â¡(a)ğ–¥ğ‘\\operatorname{\\mathsf{F}}(a) and ğ–¥â¡(aâˆ§ğ–·â¡b)ğ–¥ğ‘ğ–·ğ‘\\operatorname{\\mathsf{F}}(a\\land\\operatorname{\\mathsf{X}}b) over the alphabet Î£={a,b,c}Î£ğ‘ğ‘ğ‘\\Sigma=\\{a,b,c\\}, respectively.\nFor each problem, we have considered four simple queries, as shown in TableÂ 1.\nThis table also shows the averaged results of LEXRÂ as well as one explanation that has an accuracy close to the average accuracy of all 250250250 repetitions.\nFor explanations in form of DFAs, we report the average results as well. If the query is true (âŠ¤top\\top), the explanation has to describe the language of the RNN acceptor.\nIn contrast, when the query is false (âŠ¥bottom\\bot), the explanation should be false as well.\nIn all synthetic benchmarks, we have found the LTL explanations to be 100%percent100100\\% accurate for these two queries.\nIn fact, we observe that LEXRÂ produced an explanation with 100%percent100100\\% accuracy for all but one query.\nThe average accuracy, size of explanations, and running times are slightly in favor of LEXR.\nHowever, there is one case where the DFA explanation has a higher average accuracy than the LTL explanation: for the language ğ–¥â¡(aâˆ§ğ–·â¡(b))ğ–¥ğ‘ğ–·ğ‘\\operatorname{\\mathsf{F}}(a\\wedge\\operatorname{\\mathsf{X}}(b)) and the query ğ–¥â¡(c)ğ–¥ğ‘\\operatorname{\\mathsf{F}}(c), the DFA has on average 43.743.743.7 states with 99%percent9999\\% accuracy, whereas the LTL explanation has on average a size of 6.96.96.9 with 56.9%percent56.956.9\\% accuracy. Let us now analyze one query in detail.\nFor the language ğ–¥â¡ağ–¥ğ‘\\operatorname{\\mathsf{F}}a and the query ğ–¥â¡(b)ğ–¥ğ‘\\operatorname{\\mathsf{F}}(b), the learned (and expected) LTL explanation is ğ–¥â¡(a)âˆ§ğ–¥â¡(b)ğ–¥ğ‘ğ–¥ğ‘\\operatorname{\\mathsf{F}}(a)\\wedge\\operatorname{\\mathsf{F}}(b).\nThe average size of this explanation is 555 and its accuracy is 100%percent100100\\%.\nLEXRÂ has on average taken around 13.513.513.5â€‰s for this explanation (11.111.111.1â€‰s for the learner and 2.42.42.4â€‰s for the verifier), and the verifier has generated a total of around 1,40014001,400 random words before it certifies the PAC-guarantee.\nIn contrast, Mayr and Yovineâ€™s method takes on average 218218218â€‰s and generates a DFA with 27.327.327.3 states with 98.5%percent98.598.5\\% accuracy. Balanced ParenthesesÂ Â \nFor this benchmark, the RNN acceptor is trained to predict whether a word has balanced parentheses or not.\nWe have used the alphabet Î£={l,r,a}Î£ğ‘™ğ‘Ÿğ‘\\Sigma=\\{l,r,a\\} where lğ‘™l and rğ‘Ÿr are placeholders for the left parenthesis â€œ(â€ and the right parenthesis â€œ)â€, respectively, and ağ‘a stands for non-parenthesis symbols.\nFor our evaluation, we have considered twelve queries, which define different valid and invalid properties of balanced parentheses: The results of LEXRÂ and Mayr and Yovineâ€™s method are shown in FigureÂ 2.\nAs can be seen from the bar plot on the left, LEXRÂ produces explanation with 100%percent100100\\% accuracy in nine out of twelve casesâ€”in the remaining three, the average accuracy is still greater than 50%percent5050\\%.\nMoreover, for nine queries, the average accuracy of the LTL explanations was greater than the one of the DFAs.\nMayr and Yovineâ€™s method, on the other hand, was able to learn more accurate DFAs for three of the twelve queries. The bar plot in the center of FigureÂ 2 shows the size of the generated explanations.\nLEXRâ€™s LTL explanations were relatively small.\nThe largest explanation was of size 888.\nOn the other hand, the DFA explanations of Mayr and Yovineâ€™s method were comparatively large.\nIn fact, all DFAs had an average of 444444 or more states, making them arguably challenging to interpret. Finally, the running times of both methods are shown in the right bar plot of FigureÂ 2.\nFor three queries, LEXRÂ always times out and produces explanations with an average accuracy of 54%percent5454\\%.\nFor the remaining nine queries, however, LEXRÂ is on average significantly faster than Mayr and Yovineâ€™s method (note the logarithmic axis) and produces explanations with 100%percent100100\\% accuracy. SummaryÂ Â \nOur experimental results show a similar pattern for all other RNNs (discussed in the Appendix).\nIn total, we observed that LEXRÂ is able to generate human-interpretable explanations for a variety of different RNN acceptors and queries.\nIts explanations are comparable to the explanations of Mayr and Yovineâ€™s method in terms of average accuracy.\nHowever, LEXRâ€™s explanations are on average much smaller and, hence, easier to interpret.\n\nReference markers found: [26]\n\nContext windows for each [ref]:\n[ref]26: explanation method of Mayr and YovineÂ , which produces explanations in terms of DFAs.\nTo this end, we have implemented a Python prototype based on the LTL learning algorithm by Neider and GavranÂ [ref]26.\nMoreover, we have modified Mayr and Yovineâ€™s method such that it can handle queries.\nTo simplify the implementation of the verifier, we did not use the dynamic programming solution described\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 26,\n            \"titles\": [\n                \"Weighted Automata Extraction from Recurrent Neural Networks via Regression on State Spaces\",\n                \"Learning regular sets from queries and counterexamples\",\n                \"Principles of model checking\",\n                \"On the synthesis of finite-state machines from samples of their behavior\",\n                \"Learning interpretable models expressed in linear temporal logic\",\n                \"Handbook of Model Checking\",\n                \"A deep learning architecture for image representation, visual interpretability and automated basal-cell carcinoma cancer detection\",\n                \"Techniques for Interpretable Machine Learning\",\n                \"Comprehensible classification models: a position paper\",\n                \"Ltlf2dfa\",\n                \"Test selection based on finite state models\",\n                \"Classification rules in relaxed logical form\",\n                \"IMLI: An incremental framework for MaxSAT-based learning of interpretable classification rules\",\n                \"Synthesis for LTL and LDL on finite traces\",\n                \"European Union regulations on algorithmic decision-making and a â€œright to explanationâ€\",\n                \"Speech recognition with deep recurrent neural networks\",\n                \"Recurrent neural networks for time series forecasting: Current status and future directions\",\n                \"A sat-based approach to learn explainable decision sets\",\n                \"Rule extraction from recurrent neural networks: Ataxonomy and review\",\n                \"LTL path checking is efficiently parallelizable\",\n                \"Interpretable decision sets: A joint framework for description and prediction\",\n                \"Multilayer feedforward networks with a nonpolynomial activation function can approximate any function\",\n                \"Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model\",\n                \"A Unified Approach to Interpreting Model Predictions\",\n                \"A system for robotic heart surgery that learns to tie knots using recurrent neural networks\",\n                \"Regular inference on artificial neural networks\",\n                \"Learning linear temporal properties\",\n                \"The building blocks of interpretability\",\n                \"Extraction of rules from discrete-time recurrent neural networks\",\n                \"The temporal logic of programs\",\n                \"A proof of kampâ€™s theorem\",\n                \"â€œWhy Should I Trust You?â€ Explaining the Predictions of Any Classifier\",\n                \"Diagnosis of automata failures: A calculus and a method\",\n                \"Learning Interpretable Models in the Property Specification Language\",\n                \"Evolino: Hybrid neuroevolution/optimal linear search for sequence prediction\",\n                \"Program synthesis by sketching\",\n                \"A theory of the learnable\",\n                \"Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples\",\n                \"Generated Reference Title 38 for [ref]26\",\n                \"Generated Reference Title 39 for [ref]26\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]26 based on the context and research area of A Formal Language Approach to Explaining RNNsThe source code is available at https://github.com/bishwamittra/LEXR. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}