{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Spectrum Sharing Among Multiple-Seller and Multiple-Buyer Operators of A Mobile Network: A Stochastic Geometry Approach\n\nPaper Abstract: \n\nPaper Section Text:\nIn this subsection, we prove the convergence of the proposed Algorithm 1 to a stationary point and also calculate its computational complexity. The convergence conditions of the CSSCA method are provided in [ref]34 and . In this subsection, we show that the sufficient conditions for the convergence of the proposed algorithm are satisfied. To achieve this, we confirm that the adopted surrogate functions fit¯​(𝐀,𝐏)¯superscriptsubscript𝑓𝑖𝑡𝐀𝐏\\bar{f_{i}^{t}}(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}}) hold properties that are essential for convergence as explained in the following. First, the following assumptions have to be made on the step sizes ρtsuperscript𝜌𝑡\\rho^{t} and βtsuperscript𝛽𝑡\\beta^{t}. We define the following assumptions on the sequence of step sizes ρtsuperscript𝜌𝑡\\rho^{t} and βtsuperscript𝛽𝑡\\beta^{t}: ρt→0→superscript𝜌𝑡0\\rho^{t}\\rightarrow 0, ∑tρt=∞subscript𝑡superscript𝜌𝑡\\sum_{t}\\rho^{t}=\\infty, ∑t(ρt)2<∞subscript𝑡superscriptsuperscript𝜌𝑡2\\sum_{t}{(\\rho^{t})}^{2}<\\infty, limt→∞ρt​t−1/2<∞subscript→𝑡superscript𝜌𝑡superscript𝑡12\\lim_{t\\rightarrow\\infty}\\rho^{t}t^{-1/2}<\\infty, βt→0→superscript𝛽𝑡0\\beta^{t}\\rightarrow 0, ∑tβt=∞subscript𝑡superscript𝛽𝑡\\sum_{t}\\beta^{t}=\\infty, ∑t(βt)2<∞subscript𝑡superscriptsuperscript𝛽𝑡2\\sum_{t}{(\\beta^{t})}^{2}<\\infty, limt→∞βt/ρt=0subscript→𝑡superscript𝛽𝑡superscript𝜌𝑡0\\lim_{t\\rightarrow\\infty}{\\beta^{t}}/{\\rho^{t}}=0. Now, based on Assumption 1, we can observe that the adopted surrogate functions, i.e., f¯it​(𝐀,𝐏)superscriptsubscript¯𝑓𝑖𝑡𝐀𝐏\\bar{f}_{i}^{t}(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}}), have the following properties described in Lemma 6. For all t={0,1,…}𝑡01…t=\\{0,1,...\\}, the properties of the surrogate function fi¯t​(𝐀,𝐏)superscript¯subscript𝑓𝑖𝑡𝐀𝐏\\bar{f_{i}}^{t}(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}}) are as follows fi¯t​(𝐀,𝐏)superscript¯subscript𝑓𝑖𝑡𝐀𝐏\\bar{f_{i}}^{t}(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}}) is uniformly strongly convex in 𝐀𝐀\\boldsymbol{\\rm{A}} and 𝐏𝐏\\boldsymbol{\\rm{P}}. fi¯t​(𝐀,𝐏)superscript¯subscript𝑓𝑖𝑡𝐀𝐏\\bar{f_{i}}^{t}(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}}) is a Lipschitz continuous function with respect to 𝐀𝐀\\boldsymbol{\\rm{A}} and 𝐏𝐏\\boldsymbol{\\rm{P}}. Furthermore, limsupt1,t2→∞fi¯t1​(𝐀,𝐏)−fi¯t2​(𝐀,𝐏)≤K​(‖𝐀t1−𝐀t2‖+‖𝐏t1−𝐏t2‖),∀(𝐀,𝐏)∈𝒳formulae-sequencesubscriptsupremum→subscript𝑡1subscript𝑡2superscript¯subscript𝑓𝑖subscript𝑡1𝐀𝐏superscript¯subscript𝑓𝑖subscript𝑡2𝐀𝐏𝐾normsuperscript𝐀subscript𝑡1superscript𝐀subscript𝑡2normsuperscript𝐏subscript𝑡1superscript𝐏subscript𝑡2for-all𝐀𝐏𝒳\\lim\\sup_{t_{1},t_{2}\\rightarrow\\infty}\\bar{f_{i}}^{t_{1}}(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}})-\\bar{f_{i}}^{t_{2}}(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}})\\leq K(||\\boldsymbol{\\rm{A}}^{t_{1}}-\\boldsymbol{\\rm{A}}^{t_{2}}||+||\\boldsymbol{\\rm{P}}^{t_{1}}-\\boldsymbol{\\rm{P}}^{t_{2}}||),\\forall(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}})\\in\\mathcal{X} for some constant K>0𝐾0K>0. For any (𝐀,𝐏)∈𝒳𝐀𝐏𝒳(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}})\\in\\mathcal{X}, fi¯t​(𝐀,𝐏)superscript¯subscript𝑓𝑖𝑡𝐀𝐏\\bar{f_{i}}^{t}(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}}), its derivative, and its second-order derivative are uniformly bounded. limt→∞|f¯it​(𝐀t,𝐏t)−fi​(𝐀t,𝐏t)|=0subscript→𝑡superscriptsubscript¯𝑓𝑖𝑡superscript𝐀𝑡superscript𝐏𝑡subscript𝑓𝑖superscript𝐀𝑡superscript𝐏𝑡0\\lim_{t\\rightarrow\\infty}|\\bar{f}_{i}^{t}(\\boldsymbol{\\rm{A}}^{t},\\boldsymbol{\\rm{P}}^{t})-f_{i}(\\boldsymbol{\\rm{A}}^{t},\\boldsymbol{\\rm{P}}^{t})|=0 and limt→∞‖∇fi¯t​(𝐀t,𝐏t)−∇fi​(𝐀t,𝐏t)‖=0subscript→𝑡norm∇superscript¯subscript𝑓𝑖𝑡superscript𝐀𝑡superscript𝐏𝑡∇subscript𝑓𝑖superscript𝐀𝑡superscript𝐏𝑡0\\lim_{t\\rightarrow\\infty}\\allowbreak||\\nabla\\bar{f_{i}}^{t}(\\boldsymbol{\\rm{A}}^{t},\\boldsymbol{\\rm{P}}^{t})-\\nabla f_{i}(\\boldsymbol{\\rm{A}}^{t},\\boldsymbol{\\rm{P}}^{t})||=0. The surrogate functions fi¯t​(𝐀,𝐏)superscript¯subscript𝑓𝑖𝑡𝐀𝐏\\bar{f_{i}}^{t}(\\boldsymbol{\\rm{A}},\\boldsymbol{\\rm{P}}) are a special case of the structured surrogate function with a zero convex component in [ref]34. As a result, under Assumption 1, all properties mentioned in this lemma are met for all the adopted surrogate functions.\n∎ (Convergence of Algorithm 1): Suppose that Assumption 1 and Lemma 6 are satisfied, and the initial point {𝐀0,𝐏0}superscript𝐀0superscript𝐏0\\{\\boldsymbol{\\rm{A}}^{0},\\boldsymbol{\\rm{P}}^{0}\\} is a feasible point. Let {𝐀t,𝐏t}t=1∞superscriptsubscriptsuperscript𝐀𝑡superscript𝐏𝑡𝑡1\\{\\boldsymbol{\\rm{A}}^{t},\\boldsymbol{\\rm{P}}^{t}\\}_{t=1}^{\\infty} denote the iterates generated by Algorithm 1. Then, every limiting point {𝐀∗,𝐏∗}superscript𝐀superscript𝐏\\{\\boldsymbol{\\rm{A}}^{*},\\boldsymbol{\\rm{P}}^{*}\\} of {𝐀t,𝐏t}t=1∞superscriptsubscriptsuperscript𝐀𝑡superscript𝐏𝑡𝑡1\\{\\boldsymbol{\\rm{A}}^{t},\\boldsymbol{\\rm{P}}^{t}\\}_{t=1}^{\\infty} satisfying the Slater condition is a stationary point of problem (LABEL:Problem_5) almost surely. It has been shown in [ref]34 – that under Lemma 6 and Assumption 1, structured surrogate functions  converge to a stationary point. Due to the fact that our adopted surrogate functions are structured surrogate functions and Lemma 6 and Assumption 1 are satisfied, Algorithm 1 converges to a stationary point. Thus, this theorem is proven.\n∎ We analyze the computational complexity of the proposed Algorithm 1 for each iteration in the following. As mentioned above, we solve the optimization problems (51) and (52) by the CVX toolbox; CVX uses the interior point method  –. The optimization problems (51) and (52) consist of C=2​B+S+S​B+2​L+L​B+1𝐶2𝐵𝑆𝑆𝐵2𝐿𝐿𝐵1C=2B+S+SB+2L+LB+1 convex constraints, and thus, the number of iterations required for the interior point method is obtained by log⁡((C)/(i0​Λ))log⁡(ς)𝐶superscript𝑖0Λ𝜍\\dfrac{\\log\\left((C)/(i^{0}\\Lambda)\\right)}{\\log(\\varsigma)} , where i0superscript𝑖0i^{0} is the initial point, 0<Λ<<10Λmuch-less-than10<\\Lambda<\\!\\!<1 is the stopping criterion, and ς𝜍\\varsigma is used to update the accuracy of the interior point method. With a polynomial presentation, the number of required iterations grows as C𝐶\\sqrt{C} (asymptotically ≈L​Babsent𝐿𝐵\\approx\\sqrt{LB}) , . Moreover, for each iteration, additional computational complexity is incurred to update the surrogate functions employing the CSSCA method in step 4, which is O​((S+B)​L​B)𝑂𝑆𝐵𝐿𝐵O((S+B)LB) . Therefore, the computational complexity of the proposed Algorithm 1 at each iteration is O​((S+B)​(L​B)1.5)𝑂𝑆𝐵superscript𝐿𝐵1.5O((S+B)(LB)^{1.5}), which is polynomial time complexity.\n\nReference markers found: [34]\n\nContext windows for each [ref]:\n[ref]34: subsection, we prove the convergence of the proposed Algorithm 1 to a stationary point and also calculate its computational complexity. The convergence conditions of the CSSCA method are provided in [ref]34 and . In this subsection, we show that the sufficient conditions for the convergence of the proposed algorithm are satisfied. To achieve this, we confirm that the adopted surrogate functions\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 34,\n            \"titles\": [\n                \"Stochastic Successive Convex Optimization for Two-timescale Hybrid Precoding in Massive MIMO\",\n                \"Stochastic geometry analysis of spectrum sharing among seller and buyer mobile operators\",\n                \"Cisco visual networking index: Global mobile data traffic forecast update 2017-2022 white paper\",\n                \"A survey of dynamic spectrum access\",\n                \"Dynamics of multiple-seller and multiple-buyer spectrum trading in cognitive radio networks: A game-theoretic modeling approach\",\n                \"Incentive mechanism based cooperative spectrum sharing for OFDM cognitive IoT network\",\n                \"Co-primary spectrum sharing for denser networks in local area\",\n                \"Co-primary Spectrum Sharing for Inter-operator Device-to-Device Communication\",\n                \"Enhanced co-primary spectrum sharing method for multi-operator networks\",\n                \"Spectrum sharing using licensed shared access: The concept and its workflow for LTE-advanced networks\",\n                \"Economics of strategic network infrastructure sharing: A backup reservation approach\",\n                \"5G spectrum GSMA public policy position\",\n                \"Multi-operator spectrum sharing in next generation wireless communications networks: A short review and roadmap to future\",\n                \"Dynamic inter-operator spectrum sharing via lyapunov optimization\",\n                \"Sharing of unlicensed spectrum by strategic operators\",\n                \"Distributed resource allocation for network slicing over licensed and unlicensed bands\",\n                \"Dynamic spectrum sharing optimization and post-optimization analysis with multiple operators in cellular networks\",\n                \"On Spectrum Sharing Among Micro-Operators in 5G\",\n                \"Gains of Restricted Secondary Licensing in Millimeter Wave Cellular Systems\",\n                \"Spectrum Sharing Scenarios and Resulting Technical Requirement for 5G Systems\",\n                \"Multi-operator spectrum sharing for small cell networks: a matching game perspective\",\n                \"A multi-leader multi-follower stackelberg game for resource management in LTE unlicensed\",\n                \"QoE-enabled unlicensed spectrum sharing in 5G: A game-theoretic approach\",\n                \"A game-theoretic approach for fair coexistence between LTE-U and Wi-Fi systems\",\n                \"An efficient transmit power control strategy for underlay spectrum sharing networks with spatially random primary users\",\n                \"Interference in large wireless networks\",\n                \"Modeling and Analyzing the Coexistence of Wi-Fi and LTE in Unlicensed Spectrum\",\n                \"A tractable approach to coverage and rate in cellular networks\",\n                \"Cooperative infrastructure and spectrum sharing in heterogeneous mobile networks\",\n                \"Joint optimization of cooperative beamforming and relay assignment in multi-user wireless relay networks\",\n                \"Energy Efficiency Maximization Via Joint Sub-Carrier Assignment and Power Control for OFDMA Full Duplex Networks\",\n                \"CVX: MATLAB Software for Disciplined Convex Programming, Version 2.1\",\n                \"Convex optimization\",\n                \"Nonlinear Multiobjective Optimization\",\n                \"Stochastic Successive Convex Approximation for Non-Convex Constrained Stochastic Optimization\",\n                \"Interior point polynomial time methods in convex programming\",\n                \"A parallel decomposition method for nonconvex stochastic multi-agent optimization problems\",\n                \"Poisson Porcesses\",\n                \"Stochastic Geometry and its Applications. 3rd ed.\",\n                \"Generated Reference Title 39 for [ref]34\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]34 based on the context and research area of Spectrum Sharing Among Multiple-Seller and Multiple-Buyer Operators of A Mobile Network: A Stochastic Geometry Approach. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}