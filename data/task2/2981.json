{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A unification of least-squares and Green-Gauss gradients under a common projection-based gradient reconstruction framework\n\nPaper Abstract: \n\nPaper Section Text:\nAs mentioned, the results plotted in Fig. 25 revealed that some\ngradients exhibited a numerical instability at finer meshes. The instability disappeared when the\ncalculations were performed in higher precision arithmetic, which indicates that roundoff errors\nplay a role. Factors that were observed to either trigger or aggravate the instability are the grid\nfineness and the weighting exponent q𝑞q. Of course, it is reasonable to assume that the high aspect\nratio is also a necessary ingredient, since no such instabilities were observed in the tests of\nSec. 7. When it comes to least-squares methods, it is well known  that solution of the\nover-determined system by means of the normal equations, which is what is practiced in the present\nwork, can lead to numerical instabilities. For orthogonal projections (Eq. (11))) our solution method, Eq. (10), gives 𝐱=(𝐀T​𝐀)−1​𝐀T​𝐲𝐱superscriptsuperscript𝐀T𝐀1superscript𝐀T𝐲\\mathbf{x}=(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A})^{-1}\\mathbf{A}^{\\mathrm{T}}\\mathbf{y}. These operations return 𝐱𝐱\\mathbf{x} but can amplify\nround-off errors by a factor of κ​(𝐀)2𝜅superscript𝐀2\\kappa(\\mathbf{A})^{2}, where κ​(𝐀)𝜅𝐀\\kappa(\\mathbf{A}) is the condition number\nof the matrix 𝐀𝐀\\mathbf{A}, so depending on this number the error of the calculation can be very large\n. The remedy, in the orthogonal projection case, is to do a QR or SVD\ndecomposition of the matrix 𝐀𝐀\\mathbf{A} which leads to alternative algorithms \nthat do not solve the normal equations but involve cancellations between orthogonal matrices done\nbeforehand in exact arithmetic (not applicable in oblique projection cases). These algorithms\namplify roundoff errors in 𝐀𝐀\\mathbf{A} and 𝐲𝐲\\mathbf{y} only by a factor κ​(𝐀)𝜅𝐀\\kappa(\\mathbf{A}) and are therefore\nmuch more stable. The effect of the condition number on the accuracy of least squares gradients has not been\nthoroughly investigated in the literature, but some studies do exist. In [ref]33 a\nleast-squares procedure was devised to construct gradients of first- and higher-order accuracy,\nusing SVD; the higher-order variants had much larger condition numbers (up to 1012superscript101210^{12} reported for\naspect ratios of 1000), but this could be brought down significantly by scaling the columns of the\nmatrix 𝐀𝐀\\mathbf{A}. Seo et al.  compared what is effectively our LS(2) and LSX(2)\ngradients, computed via solution of the normal equations, against a couple of Green-Gauss gradients.\nTests were performed on grids of aspect ratios up to 500, and it was found that there is a positive\ncorrelation between the condition number κ​(𝐀T​𝐀)∼κ​(𝐀)2similar-to𝜅superscript𝐀T𝐀𝜅superscript𝐀2\\kappa(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A})\\sim\\kappa(\\mathbf{A})^{2} and the\ngradient error. Condition numbers of nearly up to 20,000 were reported for the product 𝐀T​𝐀superscript𝐀T𝐀\\mathbf{A}^{\\mathrm{T}}\\mathbf{A} of the LS(2) gradient, but those of the LSX(2) gradient were much smaller; this gave rise to\nthe idea of using κ​(𝐀T​𝐀)𝜅superscript𝐀T𝐀\\kappa(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}) as a criterion for switching between the cheaper\nLS(2) gradient and the more expensive LSX(2) gradient. To find out whether our instability is related to the normal equations, we repeated the LSA gradient\ncalculations (the most unstable ones) using the SVD algorithm instead. The DGELSD subroutine of the\nLAPACK  library was used for this purpose; it returns also the singular values of\n𝐀𝐀\\mathbf{A}, from which κ​(𝐀)𝜅𝐀\\kappa(\\mathbf{A}) can be easily computed as the ratio of largest to smallest\nsingular value. The LSA gradient can be brought into the form (11) with\n𝐀=𝐃12​𝐑𝐀superscript𝐃12𝐑\\mathbf{A}=\\mathbf{D}^{\\frac{1}{2}}\\mathbf{R} and 𝐲=𝐃12​𝐛𝐲superscript𝐃12𝐛\\mathbf{y}=\\mathbf{D}^{\\frac{1}{2}}\\mathbf{b}, in the notation\nof Sec. 2.1 (Eqs. (7) and (8)), where 𝐃12superscript𝐃12\\mathbf{D}^{\\frac{1}{2}} is a diagonal matrix with elements Df​f=(‖S¯f‖/‖R¯f‖q)12subscript𝐷𝑓𝑓superscriptnormsubscript¯𝑆𝑓superscriptnormsubscript¯𝑅𝑓𝑞12D_{ff}=(\\|\\underline{S}_{f}\\|/\\|\\underline{R}_{f}\\|^{q})^{\\frac{1}{2}}. Unfortunately, using the SVD did not improve the\nstability: the results were exactly the same as for the normal equations. Furthermore, while the\ncondition numbers κ​(𝐀)𝜅𝐀\\kappa(\\mathbf{A}) were indeed found to increase with increasing q𝑞q (e.g. κ​(𝐀)𝜅𝐀\\kappa(\\mathbf{A}) has mean values of roughly 1, 32 and 1000 for q𝑞q = 1, 2 and 3, respectively, and\nthe maximum κ​(𝐀)𝜅𝐀\\kappa(\\mathbf{A}) are not much larger than the mean), they were not large enough to\nexplain the instability (even the LSA(3) became unstable under certain conditions, described\nbelow). Finally, the condition numbers were found to be nearly independent of the grid fineness,\nwhereas the instability shows a strong dependence on it. So, the origin of the instability must be sought elsewhere. In fact, experiments showed a very\nstrong dependence on the algorithm used to calculate the cell centroids. For all the experiments of\nSec. 7, and for the triangular grid experiments of Sec. 8, the\ncoordinates of the cell centroids C¯0=(x0,y0)subscript¯𝐶0subscript𝑥0subscript𝑦0\\underline{C}_{0}=(x_{0},y_{0}) were calculated with formulae derived\nusing the Gauss theorem: and similarly for y0subscript𝑦0y_{0}, where Ω0subscriptΩ0\\Omega_{0} and S0subscript𝑆0S_{0} are the volume and bounding surface of the\ncell, e¯^xsubscript^¯𝑒𝑥\\hat{\\underline{e}}_{x} is the unit vector in the x𝑥x direction, and xf,1subscript𝑥𝑓1x_{f,1} and xf,2subscript𝑥𝑓2x_{f,2} are the\nx𝑥x-coordinates of the two vertices of face f𝑓f. Another method that was tried was simply averaging\nthe coordinates of the cell’s vertices. Strictly speaking, this is not equal to the geometric\ncentroid, but the two converge with grid refinement on structured quadrilateral grids such as the\npresent ones. The great difference in stability resulting from these two methods can be seen in\nFig. 25. The HARC and HARCO quadrilateral grid results of Sec. 8.1 were obtained with the simple averaging method. Yet another method that was\ntried was splitting each quadrilateral into two triangles, and computing the centroid as an\narea-weighted average of the centroids of the two triangles. In exact precision arithmetic, this\nmethod would give exactly the same result as formula (34). This method resulted in\nbetter stability than Eq. 34, but worse than simple averaging. To provide a better feel for the potential severity of this instability, Fig. 26 shows the errors of all gradients, with q=3𝑞3q=3, on the HARCO quadrilateral grids, for\nboth the radial and azimuthal functions, with centroids calculated via (34). To\nbetter test the stability of the gradients, two additional refinement levels are included (l=10𝑙10l=10\nand 111111, with 204820482048 and 409640964096 cells in each direction, respectively). Both 8- and 10-byte\ncalculations are included, so that the effects of the instability can be evident. The instability is\nmore prominent in the radial function case, but the azimuthal function case is not immune to it\neither. In the radial function case, the 8-byte LSA gradient becomes unstable already at level l=4𝑙4l=4, and even the 10-byte version does not manage to remain stable all the way down to the finest\ngrid. All the other gradients become unstable at l=8𝑙8l=8, except the TG gradient which, although\nstable, has a huge error. To locate the source of the instability, we performed downscaling tests [ref]39, which\nuse only a tiny fraction of the grid (the stencil of Fig. 19) and allow\nrefinement down to very fine levels. The cell configuration was representative of the HARC\nquadrilateral grid, as in Fig. 19, and the radial function was considered.\nThe findings that helped pinpoint the source of the instability are the following. If all calculations are performed in quadruple (16 byte) precision, then the instabilities\nvanish. The formula (34), in 8-byte arithmetic, which results in more intense instability,\nintroduces larger misalignment between the ideally colinear nodes C¯1subscript¯𝐶1\\underline{C}_{1}, C¯0subscript¯𝐶0\\underline{C}_{0} and\nC¯3subscript¯𝐶3\\underline{C}_{3} (Fig. 19), than the more stable node averaging method. If only the centroid calculations are performed in 8-byte precision and all other operations\n(calculation of the R¯fsubscript¯𝑅𝑓\\underline{R}_{f} and V¯fsubscript¯𝑉𝑓\\underline{V}_{f} vectors, assembly and solution of the normal\nequations etc.) are performed in 16-byte precision, then there is no improvement in stability\ncompared to the case that all calculations are performed in 8-byte precision. Misalignment between the colinear centroids correlates with the production of a large spurious\nazimuthal component of the gradient, whereas the error in the radial component remains small. Observation 3 is important because it means that the errors that we labeled as\nstability errors thus far, seen in Figs. 20, 25\nand 26, are actually not round-off errors but truncation errors, since\nthey occur even if all gradient-related operations are essentially exact (quadruple precision).\nCalculation of the cell centroids cannot be considered to be part of the gradient calculation. If\nthe gradient schemes have a small truncation error then they should accurately compute the gradient\nwhether the centroids are aligned or not. So, all these observations taken together point to the\nfollowing explanation for the instabilities. To keep things simple, consider the model stencil of Fig. 27. Grid\ncurvature is not necessary for the instability to occur, so it has been omitted. We have perturbed\nonly one centroid, C¯3subscript¯𝐶3\\underline{C}_{3}, a distance δ𝛿\\delta horizontally. The R¯fsubscript¯𝑅𝑓\\underline{R}_{f} vectors are\ntherefore R¯1=(0,−h)subscript¯𝑅10ℎ\\underline{R}_{1}=(0,-h), R¯2=(A​h,0)subscript¯𝑅2𝐴ℎ0\\underline{R}_{2}=(Ah,0), R¯3=(δ,h)subscript¯𝑅3𝛿ℎ\\underline{R}_{3}=(\\delta,h) and R¯4=(−A​h,0)subscript¯𝑅4𝐴ℎ0\\underline{R}_{4}=(-Ah,0). Hence the 𝐑𝐑\\mathbf{R} and 𝐕𝐕\\mathbf{V} matrices for the LS(q𝑞q) gradient are: where, for simplicity, we assumed in V¯3subscript¯𝑉3\\underline{V}_{3} that ‖R¯3‖≈hnormsubscript¯𝑅3ℎ\\|\\underline{R}_{3}\\|\\approx h. Now, assume that we are differentiating a radially-varying function such as (32), with the origin of the polar coordinate system located on the line joining C¯0subscript¯𝐶0\\underline{C}_{0} and\nC¯1subscript¯𝐶1\\underline{C}_{1}, as in Fig. 19. Then ϕ2=ϕ4subscriptitalic-ϕ2subscriptitalic-ϕ4\\phi_{2}=\\phi_{4} and the vector 𝐛𝐛\\mathbf{b}\nin Eq. (10) is (ϕfsubscriptitalic-ϕ𝑓\\phi_{f} being short for ϕ​(C¯f)italic-ϕsubscript¯𝐶𝑓\\phi(\\underline{C}_{f})): Then, the solution (10) to the system (9) is: (𝗑1subscript𝗑1\\mathsf{x}_{1} and 𝗑2subscript𝗑2\\mathsf{x}_{2} must not be confused with the x𝑥x coordinate; they are the two\ncomponents of the vector 𝐱𝐱\\mathbf{x} of Eq. (10) which are intended\nto hold approximations to ∂ϕ/∂xitalic-ϕ𝑥\\partial\\phi/\\partial x and ∂ϕ/∂yitalic-ϕ𝑦\\partial\\phi/\\partial y,\nrespectively). It is noted that if ϕitalic-ϕ\\phi were a linear function, ϕ=ϕ0+G​yitalic-ϕsubscriptitalic-ϕ0𝐺𝑦\\phi=\\phi_{0}+Gy (it must have\nthis form for ϕ​(C¯2)italic-ϕsubscript¯𝐶2\\phi(\\underline{C}_{2}) to equal ϕ​(C¯4)italic-ϕsubscript¯𝐶4\\phi(\\underline{C}_{4})), then 𝗑1=0subscript𝗑10\\mathsf{x}_{1}=0 and 𝗑2=Gsubscript𝗑2𝐺\\mathsf{x}_{2}=G, which are the exact values. If ϕitalic-ϕ\\phi is a non-linear, radial function (ϕ=ϕ​(r)italic-ϕitalic-ϕ𝑟\\phi=\\phi(r)) but\nδ=0𝛿0\\delta=0, then 𝗑1=0subscript𝗑10\\mathsf{x}_{1}=0 (the exact value) and 𝗑2=(ϕ3−ϕ1)/2​hsubscript𝗑2subscriptitalic-ϕ3subscriptitalic-ϕ12ℎ\\mathsf{x}_{2}=(\\phi_{3}-\\phi_{1})/2h\n(a very good approximation). Now suppose that ϕ=ϕ​(r)italic-ϕitalic-ϕ𝑟\\phi=\\phi(r) and δ≠0𝛿0\\delta\\neq 0. The value of 𝗑2subscript𝗑2\\mathsf{x}_{2} can be seen from\n(39) to lie between (ϕ3−ϕ1)/2​hsubscriptitalic-ϕ3subscriptitalic-ϕ12ℎ(\\phi_{3}-\\phi_{1})/2h and (ϕ0−ϕ1)/hsubscriptitalic-ϕ0subscriptitalic-ϕ1ℎ(\\phi_{0}-\\phi_{1})/h; when\nδ𝛿\\delta is small such that δ2≪4​A2−q​h2much-less-thansuperscript𝛿24superscript𝐴2𝑞superscriptℎ2\\delta^{2}\\ll 4A^{2-q}h^{2}, then 𝗑2→(ϕ3−ϕ1)/2​h→subscript𝗑2subscriptitalic-ϕ3subscriptitalic-ϕ12ℎ\\mathsf{x}_{2}\\rightarrow(\\phi_{3}-\\phi_{1})/2h, which, in this case, is a good approximation to ∂ϕ/∂yitalic-ϕ𝑦\\partial\\phi/\\partial y. At the\nother extreme, when δ2≫4​A2−q​h2much-greater-thansuperscript𝛿24superscript𝐴2𝑞superscriptℎ2\\delta^{2}\\gg 4A^{2-q}h^{2} then 𝗑2→(ϕ0−ϕ1)/h→subscript𝗑2subscriptitalic-ϕ0subscriptitalic-ϕ1ℎ\\mathsf{x}_{2}\\rightarrow(\\phi_{0}-\\phi_{1})/h, which is still a good approximation to ∂ϕ/∂yitalic-ϕ𝑦\\partial\\phi/\\partial y. In general, no matter what\nthe value of δ𝛿\\delta, 𝗑2subscript𝗑2\\mathsf{x}_{2} will return acceptable approximations to ∂ϕ/∂yitalic-ϕ𝑦\\partial\\phi/\\partial y. However, this is not the case for 𝗑1subscript𝗑1\\mathsf{x}_{1}, the approximation to the azimuthal component of\nthe gradient, which should ideally be zero. Let ϕ¯∗subscript¯italic-ϕ\\bar{\\phi}_{*} be the linearly extrapolated value\nof ϕitalic-ϕ\\phi at point C¯∗=(0,h)subscript¯𝐶0ℎ\\underline{C}_{*}=(0,h) (Fig. 27), assuming ∂ϕ/∂y≈(ϕ0−ϕ1)/hitalic-ϕ𝑦subscriptitalic-ϕ0subscriptitalic-ϕ1ℎ\\partial\\phi/\\partial y\\approx(\\phi_{0}-\\phi_{1})/h: Noting that ϕ3−2​ϕ0+ϕ1=ϕ3−ϕ¯∗subscriptitalic-ϕ32subscriptitalic-ϕ0subscriptitalic-ϕ1subscriptitalic-ϕ3subscript¯italic-ϕ\\phi_{3}-2\\phi_{0}+\\phi_{1}=\\phi_{3}-\\bar{\\phi}_{*}, Eq. (38) can be\nwritten in a form similar to (39) as The first term on the right-hand side is zero, but it has been included because it represents the\nfinite difference (ϕ2−ϕ4)/(2​A​h)=0subscriptitalic-ϕ2subscriptitalic-ϕ42𝐴ℎ0(\\phi_{2}-\\phi_{4})/(2Ah)=0. Thus, the value of 𝗑1subscript𝗑1\\mathsf{x}_{1} lies somewhere\nbetween the finite difference (ϕ2−ϕ4)/(2​A​h)subscriptitalic-ϕ2subscriptitalic-ϕ42𝐴ℎ(\\phi_{2}-\\phi_{4})/(2Ah) and the finite difference (ϕe−ϕ¯∗)/δsubscriptitalic-ϕ𝑒subscript¯italic-ϕ𝛿(\\phi_{e}-\\bar{\\phi}_{*})/\\delta. To which one it is closer is determined by the relative magnitude of\nδ2superscript𝛿2\\delta^{2} compared to 4​A2−q​h24superscript𝐴2𝑞superscriptℎ24A^{2-q}h^{2}. Inverse-distance weighting can lead to the contributions of\npoints C¯2subscript¯𝐶2\\underline{C}_{2} and C¯4subscript¯𝐶4\\underline{C}_{4} to the LS gradient being suppressed, if the aspect ratio A𝐴A and\nthe weight exponent q𝑞q are large enough so that 4​A2−q​h2<δ24superscript𝐴2𝑞superscriptℎ2superscript𝛿24A^{2-q}h^{2}<\\delta^{2}. In that case\n𝗑1subscript𝗑1\\mathsf{x}_{1} will be closer to (ϕe−ϕ¯∗)/δsubscriptitalic-ϕ𝑒subscript¯italic-ϕ𝛿(\\phi_{e}-\\bar{\\phi}_{*})/\\delta. In the extreme case of very\nlarge A𝐴A and q𝑞q the contributions of C¯2subscript¯𝐶2\\underline{C}_{2} and C¯4subscript¯𝐶4\\underline{C}_{4} are completely eliminated and we\nare left only with C¯0subscript¯𝐶0\\underline{C}_{0}, C¯1subscript¯𝐶1\\underline{C}_{1} and C¯3subscript¯𝐶3\\underline{C}_{3}. These are three non-colinear points, so\nthe system (7) is not overdetermined but has a unique solution.\nThis solution is the same as that of Eqs. (41) and (39) if\nwe set 4​A2−q​h2→0→4superscript𝐴2𝑞superscriptℎ204A^{2-q}h^{2}\\rightarrow 0: As mentioned, the error is small for 𝗑2subscript𝗑2\\mathsf{x}_{2}. However, it can be huge for 𝗑1subscript𝗑1\\mathsf{x}_{1}; the\nproblem is that, if δ→0→𝛿0\\delta\\rightarrow 0, ϕ3subscriptitalic-ϕ3\\phi_{3} tends to the value ϕ​(C¯∗)italic-ϕsubscript¯𝐶\\phi(\\underline{C}_{*}), which is\ndifferent from ϕ¯∗subscript¯italic-ϕ\\bar{\\phi}_{*} because the latter is just an extrapolated value. Hence, as δ→0→𝛿0\\delta\\rightarrow 0 the difference ϕ3−ϕ¯∗subscriptitalic-ϕ3subscript¯italic-ϕ\\phi_{3}-\\bar{\\phi}_{*} remains finite and the ratio 𝗑1=(ϕ3−ϕ¯∗)/δsubscript𝗑1subscriptitalic-ϕ3subscript¯italic-ϕ𝛿\\mathsf{x}_{1}=(\\phi_{3}-\\bar{\\phi}_{*})/\\delta goes to infinity. For a quantitative estimate of this error, we can expand ϕ​(r)italic-ϕ𝑟\\phi(r) as Taylor series in (x,y)𝑥𝑦(x,y)\ncoordinates around point C¯0subscript¯𝐶0\\underline{C}_{0}. If ∂ϕ/∂x=0italic-ϕ𝑥0\\partial\\phi/\\partial x=0 and ∂ϕ/∂y=Gitalic-ϕ𝑦𝐺\\partial\\phi/\\partial y=G is the exact gradient, it is not hard to show444One needs that, at\nC¯0subscript¯𝐶0\\underline{C}_{0}, ∂r/∂x=0𝑟𝑥0\\partial r/\\partial x=0, ∂r/∂y=1𝑟𝑦1\\partial r/\\partial y=1, ∂2r/∂x2=1/rsuperscript2𝑟superscript𝑥21𝑟\\partial^{2}r/\\partial x^{2}=1/r, ∂2r/∂y2=0superscript2𝑟superscript𝑦20\\partial^{2}r/\\partial y^{2}=0, and ∂2r/∂x​∂y=0superscript2𝑟𝑥𝑦0\\partial^{2}r/\\partial x\\partial y=0; r=x2+y2𝑟superscript𝑥2superscript𝑦2r=\\sqrt{x^{2}+y^{2}}. that also ∂2ϕ/∂x2=G/rsuperscript2italic-ϕsuperscript𝑥2𝐺𝑟\\partial^{2}\\phi/\\partial x^{2}=G/r, ∂2ϕ/∂y2=d2​ϕ/d​r2superscript2italic-ϕsuperscript𝑦2superscriptd2italic-ϕdsuperscript𝑟2\\partial^{2}\\phi/\\partial y^{2}=\\mathrm{d}^{2}\\phi/\\mathrm{d}r^{2} and ∂2ϕ/∂x​∂y=0superscript2italic-ϕ𝑥𝑦0\\partial^{2}\\phi/\\partial x\\partial y=0 (all\nderivatives evaluated at C¯0subscript¯𝐶0\\underline{C}_{0}). Expanding ϕ1subscriptitalic-ϕ1\\phi_{1} and ϕ3subscriptitalic-ϕ3\\phi_{3} thus we obtain where ϕ′′=∂2ϕ/∂r2superscriptitalic-ϕ′′superscript2italic-ϕsuperscript𝑟2\\phi^{\\prime\\prime}=\\partial^{2}\\phi/\\partial r^{2}. It is obvious that these two values do not converge\nif δ→0→𝛿0\\delta\\rightarrow 0. Substituting in (42) we get for\n𝗑1subscript𝗑1\\mathsf{x}_{1}: If δ→0→𝛿0\\delta\\rightarrow 0 then the second term on the right-hand side tends to zero, but the first\nterm tends to infinity, hence we get very large errors in the azimuthal component of the gradient.\nIt is noteworthy that, perhaps counterintuitively at first sight, the smaller δ𝛿\\delta is, i.e. the better the alignment of the cell centroids, the larger the error; it is only when δ𝛿\\delta is\nexactly zero that the error becomes zero too. In practice though, we observed that better alignment\nof the centroids results in smaller error (e.g. node averaging vs. the formulae (34)). This is because if δ𝛿\\delta becomes too small then the assumption on which Eq. (45) is based, that C¯2subscript¯𝐶2\\underline{C}_{2} and C¯4subscript¯𝐶4\\underline{C}_{4} do not contribute to\nthe gradient, is invalidated. As seen from Eq. (41), formula (45) is an accurate approximation to the solution 𝗑1subscript𝗑1\\mathsf{x}_{1} if δ2≫4​A2−q​h2much-greater-thansuperscript𝛿24superscript𝐴2𝑞superscriptℎ2\\delta^{2}\\gg 4A^{2-q}h^{2}. So, the peak error would be expected to occur roughly at a value δ∗subscript𝛿\\delta_{*} for\nwhich δ∗2=4​A2−q​h2superscriptsubscript𝛿24superscript𝐴2𝑞superscriptℎ2\\delta_{*}^{2}=4A^{2-q}h^{2}, or where δ~∗subscript~𝛿\\tilde{\\delta}_{*} is a non-dimensional version of δ∗subscript𝛿\\delta_{*} that compares it to the\ndistance 2​A​h2𝐴ℎ2Ah between C¯2subscript¯𝐶2\\underline{C}_{2} and C¯4subscript¯𝐶4\\underline{C}_{4}. If δ𝛿\\delta is increased beyond δ∗subscript𝛿\\delta_{*} then\nequation (45) describes the error well, but since δ𝛿\\delta is\nincreasing, the error is falling; on the other hand, if δ𝛿\\delta is decreased below δ∗subscript𝛿\\delta_{*},\nthen Eq. 45 does not describe the error accurately, and\n𝗑1subscript𝗑1\\mathsf{x}_{1} becomes increasingly dominated by the finite difference (ϕ2−ϕ4)/(2​A​h)subscriptitalic-ϕ2subscriptitalic-ϕ42𝐴ℎ(\\phi_{2}-\\phi_{4})/(2Ah),\nwhich has a small error (zero, in this case). This theoretical analysis is confirmed by the downscaling tests. Figure 28 shows results of such tests, which include grid curvature as in Fig. 19, with A=1000𝐴1000A=1000 and r=1𝑟1r=1. The maximum errors do indeed occur at about δ∗subscript𝛿\\delta_{*}. The\nlarger the values of q𝑞q, the larger the possible errors. The errors can be huge for q=4𝑞4q=4 and\n555, and large for q=3𝑞3q=3. Use of q≤2𝑞2q\\leq 2 is recommended for safety. The errors decrease with\ngrid refinement; in Figs. 25 and 26 the\nerrors are seen to actually increase with refinement up to a certain grid level, and decrease\nsubsequently. This can be attributed to the fact that grid refinement also affects δ𝛿\\delta, and\ncan bring it closer to δ∗subscript𝛿\\delta_{*}.\n\nReference markers found: [33]\n\nContext windows for each [ref]:\n[ref]33: are therefore\nmuch more stable. The effect of the condition number on the accuracy of least squares gradients has not been\nthoroughly investigated in the literature, but some studies do exist. In [ref]33 a\nleast-squares procedure was devised to construct gradients of first- and higher-order accuracy,\nusing SVD; the higher-order variants had much larger condition numbers (up to\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 33,\n            \"titles\": [\n                \"Numerical Nonlinear Algebra\",\n                \"A robust and efficient finite volume scheme for the discretization of diffusive flux on extremely skewed meshes in complex geometries\",\n                \"On the discretization of the diffusion term in finite-volume continuum mechanics\",\n                \"An efficient implicit unstructured finite volume solver for generalised Newtonian fluids\",\n                \"Computational methods for fluid dynamics, 3rd Edition\",\n                \"The finite volume method in computational rheology\",\n                \"Stabilization of an open-source finite-volume solver for viscoelastic fluid flows\",\n                \"A finite volume method for the simulation of elastoviscoplastic flows and its application to the lid-driven cavity case\",\n                \"A new finite element formulation for viscoelastic flows: Circumventing simultaneously the LBB condition and the high-Weissenberg number problem\",\n                \"An efficient stabilized finite element scheme for simulating viscoelastic flows\",\n                \"The design and application of upwind schemes on unstructured meshes\",\n                \"Error analysis and estimation for the finite volume method with application to fluid flows\",\n                \"An implicit finite-volume method using nonmatching blocks of structured grid\",\n                \"Similarity and comparison of three finite-volume methods for diffusive fluxes computation on nonorthogonal meshes\",\n                \"The Finite Volume Method in Computational Fluid Dynamics\",\n                \"A 3-D upwind Euler solver for unstructured meshes\",\n                \"Finite-volume CFD procedure and adaptive error control strategy for grids of arbitrary topology\",\n                \"A high-order-accurate unstructured mesh finite-volume scheme for the advection–diffusion equation\",\n                \"An adaptive multiscale finite volume solver for unsteady and steady state flow computations\",\n                \"A critical analysis of some popular methods for the discretisation of the gradient operator in finite volume methods\",\n                \"Gradient calculation methods on arbitrary polyhedral unstructured meshes for cell-centered CFD solvers\",\n                \"A new Green–Gauss reconstruction on unstructured meshes. part i: Gradient reconstruction\",\n                \"From hyperbolic diffusion scheme to gradient method: Implicit Green–Gauss gradients for unstructured grids\",\n                \"Accuracy analysis of gradient reconstruction on isotropic unstructured meshes and its effects on inviscid flow simulation\",\n                \"A new auxiliary volume-based gradient algorithm for triangular and tetrahedral meshes\",\n                \"Discretization and parallel performance of an unstructured finite volume Navier–Stokes solver\",\n                \"An accurate gradient and Hessian reconstruction method for cell-centered finite volume discretizations on general unstructured grids\",\n                \"Unsteady flows about bodies in relative motion\",\n                \"Multiple-correction hybrid k-exact schemes for high-order compressible RANS-LES simulations on fully unstructured grids\",\n                \"Revisiting the least-squares procedure for gradient reconstruction on unstructured meshes\",\n                \"Accuracy of gradient reconstruction on grids with high aspect ratio\",\n                \"A family of first-order accurate gradient schemes for finite volume methods\",\n                \"A comparison of gradient estimation methods for volume rendering on unstructured meshes\",\n                \"Higher-order finite volume solution reconstruction on highly anisotropic meshes\",\n                \"Gmsh: A 3-D finite element mesh generator with built-in pre-and post-processing facilities\",\n                \"Least-square switching process for accurate and efficient gradient estimation on unstructured grid\",\n                \"New gradient calculation method for MUSCL type CFD schemes in arbitrary polyhedra\",\n                \"LAPACK Users’ Guide, 3rd Edition\",\n                \"Towards verification of unstructured-grid solvers\",\n                \"Numerical experiments on the efficiency of local grid refinement based on truncation error estimates\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]33 based on the context and research area of A unification of least-squares and Green-Gauss gradients under a common projection-based gradient reconstruction framework. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}