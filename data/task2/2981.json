{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A unification of least-squares and Green-Gauss gradients under a common projection-based gradient reconstruction framework\n\nPaper Abstract: \n\nPaper Section Text:\nAs mentioned, the results plotted in Fig.Â 25 revealed that some\ngradients exhibited a numerical instability at finer meshes. The instability disappeared when the\ncalculations were performed in higher precision arithmetic, which indicates that roundoff errors\nplay a role. Factors that were observed to either trigger or aggravate the instability are the grid\nfineness and the weighting exponent qğ‘q. Of course, it is reasonable to assume that the high aspect\nratio is also a necessary ingredient, since no such instabilities were observed in the tests of\nSec.Â 7. When it comes to least-squares methods, it is well known  that solution of the\nover-determined system by means of the normal equations, which is what is practiced in the present\nwork, can lead to numerical instabilities. For orthogonal projections (Eq.Â (11))) our solution method, Eq.Â (10), gives ğ±=(ğ€Tâ€‹ğ€)âˆ’1â€‹ğ€Tâ€‹ğ²ğ±superscriptsuperscriptğ€Tğ€1superscriptğ€Tğ²\\mathbf{x}=(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A})^{-1}\\mathbf{A}^{\\mathrm{T}}\\mathbf{y}. These operations return ğ±ğ±\\mathbf{x} but can amplify\nround-off errors by a factor of Îºâ€‹(ğ€)2ğœ…superscriptğ€2\\kappa(\\mathbf{A})^{2}, where Îºâ€‹(ğ€)ğœ…ğ€\\kappa(\\mathbf{A}) is the condition number\nof the matrix ğ€ğ€\\mathbf{A}, so depending on this number the error of the calculation can be very large\n. The remedy, in the orthogonal projection case, is to do a QR or SVD\ndecomposition of the matrix ğ€ğ€\\mathbf{A} which leads to alternative algorithms \nthat do not solve the normal equations but involve cancellations between orthogonal matrices done\nbeforehand in exact arithmetic (not applicable in oblique projection cases). These algorithms\namplify roundoff errors in ğ€ğ€\\mathbf{A} and ğ²ğ²\\mathbf{y} only by a factor Îºâ€‹(ğ€)ğœ…ğ€\\kappa(\\mathbf{A}) and are therefore\nmuch more stable. The effect of the condition number on the accuracy of least squares gradients has not been\nthoroughly investigated in the literature, but some studies do exist. In [ref]33 a\nleast-squares procedure was devised to construct gradients of first- and higher-order accuracy,\nusing SVD; the higher-order variants had much larger condition numbers (up to 1012superscript101210^{12} reported for\naspect ratios of 1000), but this could be brought down significantly by scaling the columns of the\nmatrix ğ€ğ€\\mathbf{A}. Seo et al.Â  compared what is effectively our LS(2) and LSX(2)\ngradients, computed via solution of the normal equations, against a couple of Green-Gauss gradients.\nTests were performed on grids of aspect ratios up to 500, and it was found that there is a positive\ncorrelation between the condition number Îºâ€‹(ğ€Tâ€‹ğ€)âˆ¼Îºâ€‹(ğ€)2similar-toğœ…superscriptğ€Tğ€ğœ…superscriptğ€2\\kappa(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A})\\sim\\kappa(\\mathbf{A})^{2} and the\ngradient error. Condition numbers of nearly up to 20,000 were reported for the product ğ€Tâ€‹ğ€superscriptğ€Tğ€\\mathbf{A}^{\\mathrm{T}}\\mathbf{A} of the LS(2) gradient, but those of the LSX(2) gradient were much smaller; this gave rise to\nthe idea of using Îºâ€‹(ğ€Tâ€‹ğ€)ğœ…superscriptğ€Tğ€\\kappa(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}) as a criterion for switching between the cheaper\nLS(2) gradient and the more expensive LSX(2) gradient. To find out whether our instability is related to the normal equations, we repeated the LSA gradient\ncalculations (the most unstable ones) using the SVD algorithm instead. The DGELSD subroutine of the\nLAPACK  library was used for this purpose; it returns also the singular values of\nğ€ğ€\\mathbf{A}, from which Îºâ€‹(ğ€)ğœ…ğ€\\kappa(\\mathbf{A}) can be easily computed as the ratio of largest to smallest\nsingular value. The LSA gradient can be brought into the form (11) with\nğ€=ğƒ12â€‹ğ‘ğ€superscriptğƒ12ğ‘\\mathbf{A}=\\mathbf{D}^{\\frac{1}{2}}\\mathbf{R} and ğ²=ğƒ12â€‹ğ›ğ²superscriptğƒ12ğ›\\mathbf{y}=\\mathbf{D}^{\\frac{1}{2}}\\mathbf{b}, in the notation\nof Sec.Â 2.1 (Eqs.Â (7) and (8)), where ğƒ12superscriptğƒ12\\mathbf{D}^{\\frac{1}{2}} is a diagonal matrix with elements Dfâ€‹f=(â€–SÂ¯fâ€–/â€–RÂ¯fâ€–q)12subscriptğ·ğ‘“ğ‘“superscriptnormsubscriptÂ¯ğ‘†ğ‘“superscriptnormsubscriptÂ¯ğ‘…ğ‘“ğ‘12D_{ff}=(\\|\\underline{S}_{f}\\|/\\|\\underline{R}_{f}\\|^{q})^{\\frac{1}{2}}. Unfortunately, using the SVD did not improve the\nstability: the results were exactly the same as for the normal equations. Furthermore, while the\ncondition numbers Îºâ€‹(ğ€)ğœ…ğ€\\kappa(\\mathbf{A}) were indeed found to increase with increasing qğ‘q (e.g.Â Îºâ€‹(ğ€)ğœ…ğ€\\kappa(\\mathbf{A}) has mean values of roughly 1, 32 and 1000 for qğ‘q = 1, 2 and 3, respectively, and\nthe maximum Îºâ€‹(ğ€)ğœ…ğ€\\kappa(\\mathbf{A}) are not much larger than the mean), they were not large enough to\nexplain the instability (even the LSA(3) became unstable under certain conditions, described\nbelow). Finally, the condition numbers were found to be nearly independent of the grid fineness,\nwhereas the instability shows a strong dependence on it. So, the origin of the instability must be sought elsewhere. In fact, experiments showed a very\nstrong dependence on the algorithm used to calculate the cell centroids. For all the experiments of\nSec.Â 7, and for the triangular grid experiments of Sec.Â 8, the\ncoordinates of the cell centroids CÂ¯0=(x0,y0)subscriptÂ¯ğ¶0subscriptğ‘¥0subscriptğ‘¦0\\underline{C}_{0}=(x_{0},y_{0}) were calculated with formulae derived\nusing the Gauss theorem: and similarly for y0subscriptğ‘¦0y_{0}, where Î©0subscriptÎ©0\\Omega_{0} and S0subscriptğ‘†0S_{0} are the volume and bounding surface of the\ncell, eÂ¯^xsubscript^Â¯ğ‘’ğ‘¥\\hat{\\underline{e}}_{x} is the unit vector in the xğ‘¥x direction, and xf,1subscriptğ‘¥ğ‘“1x_{f,1} and xf,2subscriptğ‘¥ğ‘“2x_{f,2} are the\nxğ‘¥x-coordinates of the two vertices of face fğ‘“f. Another method that was tried was simply averaging\nthe coordinates of the cellâ€™s vertices. Strictly speaking, this is not equal to the geometric\ncentroid, but the two converge with grid refinement on structured quadrilateral grids such as the\npresent ones. The great difference in stability resulting from these two methods can be seen in\nFig.Â 25. The HARC and HARCO quadrilateral grid results of Sec.Â 8.1 were obtained with the simple averaging method. Yet another method that was\ntried was splitting each quadrilateral into two triangles, and computing the centroid as an\narea-weighted average of the centroids of the two triangles. In exact precision arithmetic, this\nmethod would give exactly the same result as formula (34). This method resulted in\nbetter stability than Eq.Â 34, but worse than simple averaging. To provide a better feel for the potential severity of this instability, Fig.Â 26 shows the errors of all gradients, with q=3ğ‘3q=3, on the HARCO quadrilateral grids, for\nboth the radial and azimuthal functions, with centroids calculated via (34). To\nbetter test the stability of the gradients, two additional refinement levels are included (l=10ğ‘™10l=10\nand 111111, with 204820482048 and 409640964096 cells in each direction, respectively). Both 8- and 10-byte\ncalculations are included, so that the effects of the instability can be evident. The instability is\nmore prominent in the radial function case, but the azimuthal function case is not immune to it\neither. In the radial function case, the 8-byte LSA gradient becomes unstable already at level l=4ğ‘™4l=4, and even the 10-byte version does not manage to remain stable all the way down to the finest\ngrid. All the other gradients become unstable at l=8ğ‘™8l=8, except the TG gradient which, although\nstable, has a huge error. To locate the source of the instability, we performed downscaling tests [ref]39, which\nuse only a tiny fraction of the grid (the stencil of Fig.Â 19) and allow\nrefinement down to very fine levels. The cell configuration was representative of the HARC\nquadrilateral grid, as in Fig.Â 19, and the radial function was considered.\nThe findings that helped pinpoint the source of the instability are the following. If all calculations are performed in quadruple (16 byte) precision, then the instabilities\nvanish. The formula (34), in 8-byte arithmetic, which results in more intense instability,\nintroduces larger misalignment between the ideally colinear nodes CÂ¯1subscriptÂ¯ğ¶1\\underline{C}_{1}, CÂ¯0subscriptÂ¯ğ¶0\\underline{C}_{0} and\nCÂ¯3subscriptÂ¯ğ¶3\\underline{C}_{3} (Fig.Â 19), than the more stable node averaging method. If only the centroid calculations are performed in 8-byte precision and all other operations\n(calculation of the RÂ¯fsubscriptÂ¯ğ‘…ğ‘“\\underline{R}_{f} and VÂ¯fsubscriptÂ¯ğ‘‰ğ‘“\\underline{V}_{f} vectors, assembly and solution of the normal\nequations etc.) are performed in 16-byte precision, then there is no improvement in stability\ncompared to the case that all calculations are performed in 8-byte precision. Misalignment between the colinear centroids correlates with the production of a large spurious\nazimuthal component of the gradient, whereas the error in the radial component remains small. Observation 3 is important because it means that the errors that we labeled as\nstability errors thus far, seen in Figs.Â 20, 25\nand 26, are actually not round-off errors but truncation errors, since\nthey occur even if all gradient-related operations are essentially exact (quadruple precision).\nCalculation of the cell centroids cannot be considered to be part of the gradient calculation. If\nthe gradient schemes have a small truncation error then they should accurately compute the gradient\nwhether the centroids are aligned or not. So, all these observations taken together point to the\nfollowing explanation for the instabilities. To keep things simple, consider the model stencil of Fig.Â 27. Grid\ncurvature is not necessary for the instability to occur, so it has been omitted. We have perturbed\nonly one centroid, CÂ¯3subscriptÂ¯ğ¶3\\underline{C}_{3}, a distance Î´ğ›¿\\delta horizontally. The RÂ¯fsubscriptÂ¯ğ‘…ğ‘“\\underline{R}_{f} vectors are\ntherefore RÂ¯1=(0,âˆ’h)subscriptÂ¯ğ‘…10â„\\underline{R}_{1}=(0,-h), RÂ¯2=(Aâ€‹h,0)subscriptÂ¯ğ‘…2ğ´â„0\\underline{R}_{2}=(Ah,0), RÂ¯3=(Î´,h)subscriptÂ¯ğ‘…3ğ›¿â„\\underline{R}_{3}=(\\delta,h) and RÂ¯4=(âˆ’Aâ€‹h,0)subscriptÂ¯ğ‘…4ğ´â„0\\underline{R}_{4}=(-Ah,0). Hence the ğ‘ğ‘\\mathbf{R} and ğ•ğ•\\mathbf{V} matrices for the LS(qğ‘q) gradient are: where, for simplicity, we assumed in VÂ¯3subscriptÂ¯ğ‘‰3\\underline{V}_{3} that â€–RÂ¯3â€–â‰ˆhnormsubscriptÂ¯ğ‘…3â„\\|\\underline{R}_{3}\\|\\approx h. Now, assume that we are differentiating a radially-varying function such as (32), with the origin of the polar coordinate system located on the line joining CÂ¯0subscriptÂ¯ğ¶0\\underline{C}_{0} and\nCÂ¯1subscriptÂ¯ğ¶1\\underline{C}_{1}, as in Fig.Â 19. Then Ï•2=Ï•4subscriptitalic-Ï•2subscriptitalic-Ï•4\\phi_{2}=\\phi_{4} and the vector ğ›ğ›\\mathbf{b}\nin Eq.Â (10) is (Ï•fsubscriptitalic-Ï•ğ‘“\\phi_{f} being short for Ï•â€‹(CÂ¯f)italic-Ï•subscriptÂ¯ğ¶ğ‘“\\phi(\\underline{C}_{f})): Then, the solution (10) to the system (9) is: (ğ—‘1subscriptğ—‘1\\mathsf{x}_{1} and ğ—‘2subscriptğ—‘2\\mathsf{x}_{2} must not be confused with the xğ‘¥x coordinate; they are the two\ncomponents of the vector ğ±ğ±\\mathbf{x} of Eq.Â (10) which are intended\nto hold approximations to âˆ‚Ï•/âˆ‚xitalic-Ï•ğ‘¥\\partial\\phi/\\partial x and âˆ‚Ï•/âˆ‚yitalic-Ï•ğ‘¦\\partial\\phi/\\partial y,\nrespectively). It is noted that if Ï•italic-Ï•\\phi were a linear function, Ï•=Ï•0+Gâ€‹yitalic-Ï•subscriptitalic-Ï•0ğºğ‘¦\\phi=\\phi_{0}+Gy (it must have\nthis form for Ï•â€‹(CÂ¯2)italic-Ï•subscriptÂ¯ğ¶2\\phi(\\underline{C}_{2}) to equal Ï•â€‹(CÂ¯4)italic-Ï•subscriptÂ¯ğ¶4\\phi(\\underline{C}_{4})), then ğ—‘1=0subscriptğ—‘10\\mathsf{x}_{1}=0 and ğ—‘2=Gsubscriptğ—‘2ğº\\mathsf{x}_{2}=G, which are the exact values. If Ï•italic-Ï•\\phi is a non-linear, radial function (Ï•=Ï•â€‹(r)italic-Ï•italic-Ï•ğ‘Ÿ\\phi=\\phi(r)) but\nÎ´=0ğ›¿0\\delta=0, then ğ—‘1=0subscriptğ—‘10\\mathsf{x}_{1}=0 (the exact value) and ğ—‘2=(Ï•3âˆ’Ï•1)/2â€‹hsubscriptğ—‘2subscriptitalic-Ï•3subscriptitalic-Ï•12â„\\mathsf{x}_{2}=(\\phi_{3}-\\phi_{1})/2h\n(a very good approximation). Now suppose that Ï•=Ï•â€‹(r)italic-Ï•italic-Ï•ğ‘Ÿ\\phi=\\phi(r) and Î´â‰ 0ğ›¿0\\delta\\neq 0. The value of ğ—‘2subscriptğ—‘2\\mathsf{x}_{2} can be seen from\n(39) to lie between (Ï•3âˆ’Ï•1)/2â€‹hsubscriptitalic-Ï•3subscriptitalic-Ï•12â„(\\phi_{3}-\\phi_{1})/2h and (Ï•0âˆ’Ï•1)/hsubscriptitalic-Ï•0subscriptitalic-Ï•1â„(\\phi_{0}-\\phi_{1})/h; when\nÎ´ğ›¿\\delta is small such that Î´2â‰ª4â€‹A2âˆ’qâ€‹h2much-less-thansuperscriptğ›¿24superscriptğ´2ğ‘superscriptâ„2\\delta^{2}\\ll 4A^{2-q}h^{2}, then ğ—‘2â†’(Ï•3âˆ’Ï•1)/2â€‹hâ†’subscriptğ—‘2subscriptitalic-Ï•3subscriptitalic-Ï•12â„\\mathsf{x}_{2}\\rightarrow(\\phi_{3}-\\phi_{1})/2h, which, in this case, is a good approximation to âˆ‚Ï•/âˆ‚yitalic-Ï•ğ‘¦\\partial\\phi/\\partial y. At the\nother extreme, when Î´2â‰«4â€‹A2âˆ’qâ€‹h2much-greater-thansuperscriptğ›¿24superscriptğ´2ğ‘superscriptâ„2\\delta^{2}\\gg 4A^{2-q}h^{2} then ğ—‘2â†’(Ï•0âˆ’Ï•1)/hâ†’subscriptğ—‘2subscriptitalic-Ï•0subscriptitalic-Ï•1â„\\mathsf{x}_{2}\\rightarrow(\\phi_{0}-\\phi_{1})/h, which is still a good approximation to âˆ‚Ï•/âˆ‚yitalic-Ï•ğ‘¦\\partial\\phi/\\partial y. In general, no matter what\nthe value of Î´ğ›¿\\delta, ğ—‘2subscriptğ—‘2\\mathsf{x}_{2} will return acceptable approximations to âˆ‚Ï•/âˆ‚yitalic-Ï•ğ‘¦\\partial\\phi/\\partial y. However, this is not the case for ğ—‘1subscriptğ—‘1\\mathsf{x}_{1}, the approximation to the azimuthal component of\nthe gradient, which should ideally be zero. Let Ï•Â¯âˆ—subscriptÂ¯italic-Ï•\\bar{\\phi}_{*} be the linearly extrapolated value\nof Ï•italic-Ï•\\phi at point CÂ¯âˆ—=(0,h)subscriptÂ¯ğ¶0â„\\underline{C}_{*}=(0,h) (Fig.Â 27), assuming âˆ‚Ï•/âˆ‚yâ‰ˆ(Ï•0âˆ’Ï•1)/hitalic-Ï•ğ‘¦subscriptitalic-Ï•0subscriptitalic-Ï•1â„\\partial\\phi/\\partial y\\approx(\\phi_{0}-\\phi_{1})/h: Noting that Ï•3âˆ’2â€‹Ï•0+Ï•1=Ï•3âˆ’Ï•Â¯âˆ—subscriptitalic-Ï•32subscriptitalic-Ï•0subscriptitalic-Ï•1subscriptitalic-Ï•3subscriptÂ¯italic-Ï•\\phi_{3}-2\\phi_{0}+\\phi_{1}=\\phi_{3}-\\bar{\\phi}_{*}, Eq.Â (38) can be\nwritten in a form similar to (39) as The first term on the right-hand side is zero, but it has been included because it represents the\nfinite difference (Ï•2âˆ’Ï•4)/(2â€‹Aâ€‹h)=0subscriptitalic-Ï•2subscriptitalic-Ï•42ğ´â„0(\\phi_{2}-\\phi_{4})/(2Ah)=0. Thus, the value of ğ—‘1subscriptğ—‘1\\mathsf{x}_{1} lies somewhere\nbetween the finite difference (Ï•2âˆ’Ï•4)/(2â€‹Aâ€‹h)subscriptitalic-Ï•2subscriptitalic-Ï•42ğ´â„(\\phi_{2}-\\phi_{4})/(2Ah) and the finite difference (Ï•eâˆ’Ï•Â¯âˆ—)/Î´subscriptitalic-Ï•ğ‘’subscriptÂ¯italic-Ï•ğ›¿(\\phi_{e}-\\bar{\\phi}_{*})/\\delta. To which one it is closer is determined by the relative magnitude of\nÎ´2superscriptğ›¿2\\delta^{2} compared to 4â€‹A2âˆ’qâ€‹h24superscriptğ´2ğ‘superscriptâ„24A^{2-q}h^{2}. Inverse-distance weighting can lead to the contributions of\npoints CÂ¯2subscriptÂ¯ğ¶2\\underline{C}_{2} and CÂ¯4subscriptÂ¯ğ¶4\\underline{C}_{4} to the LS gradient being suppressed, if the aspect ratio Ağ´A and\nthe weight exponent qğ‘q are large enough so that 4â€‹A2âˆ’qâ€‹h2<Î´24superscriptğ´2ğ‘superscriptâ„2superscriptğ›¿24A^{2-q}h^{2}<\\delta^{2}. In that case\nğ—‘1subscriptğ—‘1\\mathsf{x}_{1} will be closer to (Ï•eâˆ’Ï•Â¯âˆ—)/Î´subscriptitalic-Ï•ğ‘’subscriptÂ¯italic-Ï•ğ›¿(\\phi_{e}-\\bar{\\phi}_{*})/\\delta. In the extreme case of very\nlarge Ağ´A and qğ‘q the contributions of CÂ¯2subscriptÂ¯ğ¶2\\underline{C}_{2} and CÂ¯4subscriptÂ¯ğ¶4\\underline{C}_{4} are completely eliminated and we\nare left only with CÂ¯0subscriptÂ¯ğ¶0\\underline{C}_{0}, CÂ¯1subscriptÂ¯ğ¶1\\underline{C}_{1} and CÂ¯3subscriptÂ¯ğ¶3\\underline{C}_{3}. These are three non-colinear points, so\nthe system (7) is not overdetermined but has a unique solution.\nThis solution is the same as that of Eqs.Â (41) and (39) if\nwe set 4â€‹A2âˆ’qâ€‹h2â†’0â†’4superscriptğ´2ğ‘superscriptâ„204A^{2-q}h^{2}\\rightarrow 0: As mentioned, the error is small for ğ—‘2subscriptğ—‘2\\mathsf{x}_{2}. However, it can be huge for ğ—‘1subscriptğ—‘1\\mathsf{x}_{1}; the\nproblem is that, if Î´â†’0â†’ğ›¿0\\delta\\rightarrow 0, Ï•3subscriptitalic-Ï•3\\phi_{3} tends to the value Ï•â€‹(CÂ¯âˆ—)italic-Ï•subscriptÂ¯ğ¶\\phi(\\underline{C}_{*}), which is\ndifferent from Ï•Â¯âˆ—subscriptÂ¯italic-Ï•\\bar{\\phi}_{*} because the latter is just an extrapolated value. Hence, as Î´â†’0â†’ğ›¿0\\delta\\rightarrow 0 the difference Ï•3âˆ’Ï•Â¯âˆ—subscriptitalic-Ï•3subscriptÂ¯italic-Ï•\\phi_{3}-\\bar{\\phi}_{*} remains finite and the ratio ğ—‘1=(Ï•3âˆ’Ï•Â¯âˆ—)/Î´subscriptğ—‘1subscriptitalic-Ï•3subscriptÂ¯italic-Ï•ğ›¿\\mathsf{x}_{1}=(\\phi_{3}-\\bar{\\phi}_{*})/\\delta goes to infinity. For a quantitative estimate of this error, we can expand Ï•â€‹(r)italic-Ï•ğ‘Ÿ\\phi(r) as Taylor series in (x,y)ğ‘¥ğ‘¦(x,y)\ncoordinates around point CÂ¯0subscriptÂ¯ğ¶0\\underline{C}_{0}. If âˆ‚Ï•/âˆ‚x=0italic-Ï•ğ‘¥0\\partial\\phi/\\partial x=0 and âˆ‚Ï•/âˆ‚y=Gitalic-Ï•ğ‘¦ğº\\partial\\phi/\\partial y=G is the exact gradient, it is not hard to show444One needs that, at\nCÂ¯0subscriptÂ¯ğ¶0\\underline{C}_{0}, âˆ‚r/âˆ‚x=0ğ‘Ÿğ‘¥0\\partial r/\\partial x=0, âˆ‚r/âˆ‚y=1ğ‘Ÿğ‘¦1\\partial r/\\partial y=1, âˆ‚2r/âˆ‚x2=1/rsuperscript2ğ‘Ÿsuperscriptğ‘¥21ğ‘Ÿ\\partial^{2}r/\\partial x^{2}=1/r, âˆ‚2r/âˆ‚y2=0superscript2ğ‘Ÿsuperscriptğ‘¦20\\partial^{2}r/\\partial y^{2}=0, and âˆ‚2r/âˆ‚xâ€‹âˆ‚y=0superscript2ğ‘Ÿğ‘¥ğ‘¦0\\partial^{2}r/\\partial x\\partial y=0; r=x2+y2ğ‘Ÿsuperscriptğ‘¥2superscriptğ‘¦2r=\\sqrt{x^{2}+y^{2}}. that also âˆ‚2Ï•/âˆ‚x2=G/rsuperscript2italic-Ï•superscriptğ‘¥2ğºğ‘Ÿ\\partial^{2}\\phi/\\partial x^{2}=G/r, âˆ‚2Ï•/âˆ‚y2=d2â€‹Ï•/dâ€‹r2superscript2italic-Ï•superscriptğ‘¦2superscriptd2italic-Ï•dsuperscriptğ‘Ÿ2\\partial^{2}\\phi/\\partial y^{2}=\\mathrm{d}^{2}\\phi/\\mathrm{d}r^{2} and âˆ‚2Ï•/âˆ‚xâ€‹âˆ‚y=0superscript2italic-Ï•ğ‘¥ğ‘¦0\\partial^{2}\\phi/\\partial x\\partial y=0 (all\nderivatives evaluated at CÂ¯0subscriptÂ¯ğ¶0\\underline{C}_{0}). Expanding Ï•1subscriptitalic-Ï•1\\phi_{1} and Ï•3subscriptitalic-Ï•3\\phi_{3} thus we obtain where Ï•â€²â€²=âˆ‚2Ï•/âˆ‚r2superscriptitalic-Ï•â€²â€²superscript2italic-Ï•superscriptğ‘Ÿ2\\phi^{\\prime\\prime}=\\partial^{2}\\phi/\\partial r^{2}. It is obvious that these two values do not converge\nif Î´â†’0â†’ğ›¿0\\delta\\rightarrow 0. Substituting in (42) we get for\nğ—‘1subscriptğ—‘1\\mathsf{x}_{1}: If Î´â†’0â†’ğ›¿0\\delta\\rightarrow 0 then the second term on the right-hand side tends to zero, but the first\nterm tends to infinity, hence we get very large errors in the azimuthal component of the gradient.\nIt is noteworthy that, perhaps counterintuitively at first sight, the smaller Î´ğ›¿\\delta is, i.e.Â the better the alignment of the cell centroids, the larger the error; it is only when Î´ğ›¿\\delta is\nexactly zero that the error becomes zero too. In practice though, we observed that better alignment\nof the centroids results in smaller error (e.g.Â node averaging vs.Â the formulae (34)). This is because if Î´ğ›¿\\delta becomes too small then the assumption on which Eq.Â (45) is based, that CÂ¯2subscriptÂ¯ğ¶2\\underline{C}_{2} and CÂ¯4subscriptÂ¯ğ¶4\\underline{C}_{4} do not contribute to\nthe gradient, is invalidated. As seen from Eq.Â (41), formula (45) is an accurate approximation to the solution ğ—‘1subscriptğ—‘1\\mathsf{x}_{1} if Î´2â‰«4â€‹A2âˆ’qâ€‹h2much-greater-thansuperscriptğ›¿24superscriptğ´2ğ‘superscriptâ„2\\delta^{2}\\gg 4A^{2-q}h^{2}. So, the peak error would be expected to occur roughly at a value Î´âˆ—subscriptğ›¿\\delta_{*} for\nwhich Î´âˆ—2=4â€‹A2âˆ’qâ€‹h2superscriptsubscriptğ›¿24superscriptğ´2ğ‘superscriptâ„2\\delta_{*}^{2}=4A^{2-q}h^{2}, or where Î´~âˆ—subscript~ğ›¿\\tilde{\\delta}_{*} is a non-dimensional version of Î´âˆ—subscriptğ›¿\\delta_{*} that compares it to the\ndistance 2â€‹Aâ€‹h2ğ´â„2Ah between CÂ¯2subscriptÂ¯ğ¶2\\underline{C}_{2} and CÂ¯4subscriptÂ¯ğ¶4\\underline{C}_{4}. If Î´ğ›¿\\delta is increased beyond Î´âˆ—subscriptğ›¿\\delta_{*} then\nequation (45) describes the error well, but since Î´ğ›¿\\delta is\nincreasing, the error is falling; on the other hand, if Î´ğ›¿\\delta is decreased below Î´âˆ—subscriptğ›¿\\delta_{*},\nthen Eq.Â 45 does not describe the error accurately, and\nğ—‘1subscriptğ—‘1\\mathsf{x}_{1} becomes increasingly dominated by the finite difference (Ï•2âˆ’Ï•4)/(2â€‹Aâ€‹h)subscriptitalic-Ï•2subscriptitalic-Ï•42ğ´â„(\\phi_{2}-\\phi_{4})/(2Ah),\nwhich has a small error (zero, in this case). This theoretical analysis is confirmed by the downscaling tests. Figure 28 shows results of such tests, which include grid curvature as in Fig.Â 19, with A=1000ğ´1000A=1000 and r=1ğ‘Ÿ1r=1. The maximum errors do indeed occur at about Î´âˆ—subscriptğ›¿\\delta_{*}. The\nlarger the values of qğ‘q, the larger the possible errors. The errors can be huge for q=4ğ‘4q=4 and\n555, and large for q=3ğ‘3q=3. Use of qâ‰¤2ğ‘2q\\leq 2 is recommended for safety. The errors decrease with\ngrid refinement; in Figs.Â 25 and 26 the\nerrors are seen to actually increase with refinement up to a certain grid level, and decrease\nsubsequently. This can be attributed to the fact that grid refinement also affects Î´ğ›¿\\delta, and\ncan bring it closer to Î´âˆ—subscriptğ›¿\\delta_{*}.\n\nReference markers found: [33]\n\nContext windows for each [ref]:\n[ref]33: are therefore\nmuch more stable. The effect of the condition number on the accuracy of least squares gradients has not been\nthoroughly investigated in the literature, but some studies do exist. In [ref]33 a\nleast-squares procedure was devised to construct gradients of first- and higher-order accuracy,\nusing SVD; the higher-order variants had much larger condition numbers (up to\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 33,\n            \"titles\": [\n                \"Numerical Nonlinear Algebra\",\n                \"A robust and efficient finite volume scheme for the discretization of diffusive flux on extremely skewed meshes in complex geometries\",\n                \"On the discretization of the diffusion term in finite-volume continuum mechanics\",\n                \"An efficient implicit unstructured finite volume solver for generalised Newtonian fluids\",\n                \"Computational methods for fluid dynamics, 3rd Edition\",\n                \"The finite volume method in computational rheology\",\n                \"Stabilization of an open-source finite-volume solver for viscoelastic fluid flows\",\n                \"A finite volume method for the simulation of elastoviscoplastic flows and its application to the lid-driven cavity case\",\n                \"A new finite element formulation for viscoelastic flows: Circumventing simultaneously the LBB condition and the high-Weissenberg number problem\",\n                \"An efficient stabilized finite element scheme for simulating viscoelastic flows\",\n                \"The design and application of upwind schemes on unstructured meshes\",\n                \"Error analysis and estimation for the finite volume method with application to fluid flows\",\n                \"An implicit finite-volume method using nonmatching blocks of structured grid\",\n                \"Similarity and comparison of three finite-volume methods for diffusive fluxes computation on nonorthogonal meshes\",\n                \"The Finite Volume Method in Computational Fluid Dynamics\",\n                \"A 3-D upwind Euler solver for unstructured meshes\",\n                \"Finite-volume CFD procedure and adaptive error control strategy for grids of arbitrary topology\",\n                \"A high-order-accurate unstructured mesh finite-volume scheme for the advectionâ€“diffusion equation\",\n                \"An adaptive multiscale finite volume solver for unsteady and steady state flow computations\",\n                \"A critical analysis of some popular methods for the discretisation of the gradient operator in finite volume methods\",\n                \"Gradient calculation methods on arbitrary polyhedral unstructured meshes for cell-centered CFD solvers\",\n                \"A new Greenâ€“Gauss reconstruction on unstructured meshes. part i: Gradient reconstruction\",\n                \"From hyperbolic diffusion scheme to gradient method: Implicit Greenâ€“Gauss gradients for unstructured grids\",\n                \"Accuracy analysis of gradient reconstruction on isotropic unstructured meshes and its effects on inviscid flow simulation\",\n                \"A new auxiliary volume-based gradient algorithm for triangular and tetrahedral meshes\",\n                \"Discretization and parallel performance of an unstructured finite volume Navierâ€“Stokes solver\",\n                \"An accurate gradient and Hessian reconstruction method for cell-centered finite volume discretizations on general unstructured grids\",\n                \"Unsteady flows about bodies in relative motion\",\n                \"Multiple-correction hybrid k-exact schemes for high-order compressible RANS-LES simulations on fully unstructured grids\",\n                \"Revisiting the least-squares procedure for gradient reconstruction on unstructured meshes\",\n                \"Accuracy of gradient reconstruction on grids with high aspect ratio\",\n                \"A family of first-order accurate gradient schemes for finite volume methods\",\n                \"A comparison of gradient estimation methods for volume rendering on unstructured meshes\",\n                \"Higher-order finite volume solution reconstruction on highly anisotropic meshes\",\n                \"Gmsh: A 3-D finite element mesh generator with built-in pre-and post-processing facilities\",\n                \"Least-square switching process for accurate and efficient gradient estimation on unstructured grid\",\n                \"New gradient calculation method for MUSCL type CFD schemes in arbitrary polyhedra\",\n                \"LAPACK Usersâ€™ Guide, 3rd Edition\",\n                \"Towards verification of unstructured-grid solvers\",\n                \"Numerical experiments on the efficiency of local grid refinement based on truncation error estimates\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]33 based on the context and research area of A unification of least-squares and Green-Gauss gradients under a common projection-based gradient reconstruction framework. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}