{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: From Solitary Directives to Interactive Encouragement! LLM Secure Code Generation by Natural Language Prompting\n\nPaper Abstract: \n\nPaper Section Text:\nThis section reviews closely related works. A more comprehensive review of LLMs on code analysis-related topics can be found in [ref]7  . LLM for Code Generation.\nChen et al.  propose and evaluate Codex, a GPT model fine-tuned on GitHub code, and find that it solves 28.8% of HumanEval benchmark , significantly outperforming GPT-3 and GPT-J, with repeated sampling improving results.\nChoi and Lee  propose CodePrompt, a Task-Agnostic prompt tuning method for code generation, combining Input-Dependent Prompt Template and Corpus-Specific Prefix Tuning to improve efficiency and effectiveness in various settings.\nLi et al.  propose Chain of Code (CoC), enhancing LLM reasoning by integrating code-writing and interpretation, achieving 84% on BIG-Bench Hard tasks, surpassing Chain of Thought by 12%.\nShen et al. 9 present RRTF and PanGu-Coder2 for code generation, achieving 62.20% pass@1 on HumanEval, and outperforming previous models on CoderEval  and LeetCode benchmarks 9.\nDong et al.  propose a self-collaboration framework for code generation using LLMs like ChatGPT, where multiple LLM agents act as experts for specific subtasks, improving pass@1 by 29.9%-47.1%.\nLi et al.  fine-tune StarCoder and StarCoderBase for code generation; these 15.5B parameter models, trained on large-scale datasets, outperform open Code LLMs and ensure safe, open-access release.\nAhmed et al.  study the impact of adding semantic facts to code summarization prompts and find that this approach improves performance across different LLMs.\nRidnik et al.  develop AlphaCodium, a test-based, multistage, code-oriented iterative approach, improving GPT-4 accuracy on CodeContests from 19% to 44%, demonstrating its effectiveness in code generation tasks.\nDenny et al.  explore GitHub Copilot’s performance on 166 programming problems, suggesting that, for code generation, prompt engineering can enhance learning and computational thinking skills, improving initial failure interactions. However, all these studies only deal with functional correctness, neglecting the generated code’s security. LLM for Secure Code Generation.\nHe and Vechev 2 study LLMs for secure code generation and proposed SVEN, which significantly enhances security control without compromising functional correctness.\nKhoury et al. 4 investigate the impact of LLMs like ChatGPT on code generation, suggesting that while capable of translating natural language into code, ChatGPT’s generated programs often lack robust security against specific attacks, raising ethical concerns.\nAlrashedy and Aljasser  claim that LLMs excel in code generation but can inadvertently introduce security vulnerabilities; they propose Feedback-Driven Security Patching, leveraging LLMs with feedback from Bandit333Bandit is a tool designed to find common security issues in Python code. for refining solutions to enhance Python code security.\nLiu et al. 0 argue that LLMs have limitations in assessing the functional correctness of generated code and proposed EvalPlus, an enhanced evaluation framework using automated test generation to address these limitations for benchmarking LLM-synthesized code.\nUllah et al.  experimentally investigate ChatGPT’s capability to generate computer programs and suggest that while it is aware of potential vulnerabilities, prompted improvements are necessary for robust code security.\nHowever, these works are focused either on functional correctness or on the evaluation of the security issues of the generated code using LLMs. The work that is most closely related to ours is SVEN 2, which primarily focuses on fine-tuning LLM models to generate secure code using mixed prompts of NL and programming languages. In contrast, our work focuses on code generation using NL prompts only and automatically fixing potential vulnerabilities by incorporating rich knowledge provisioned from both LLMs and CodeQL. To the best of our knowledge, we are the first to propose an NL prompt-based approach for secure code generation, reflecting the real-world need for accessible and easy-to-use tools for the wider developer community.\n\nReference markers found: [7]\n\nContext windows for each [ref]:\n[ref]7: This section reviews closely related works. A more comprehensive review of LLMs on code analysis-related topics can be found in [ref]7  . LLM for Code Generation.\nChen et al.  propose and evaluate Codex, a GPT model fine-tuned on GitHub code, and find that it solves 28.8% of HumanEval benchmark , significantly outperforming\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 7,\n            \"titles\": [\n                \"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language\",\n                \"Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)\",\n                \"Can LLMs Patch Security Issues?\",\n                \"Programming is hard-or at least it used to be: Educational opportunities and challenges of ai code generation\",\n                \"Evaluating Large Language Models Trained on Code\",\n                \"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\",\n                \"Codeprompt: Task-agnostic prefix tuning for program and language generation\",\n                \"Code generation using machine learning: A systematic review\",\n                \"Self-collaboration code generation via chatgpt\",\n                \"Codeql\",\n                \"DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence\",\n                \"Large language models for code: Security hardening and adversarial testing\",\n                \"A Survey on Large Language Models for Code Generation\",\n                \"How Secure is Code Generated by ChatGPT?\",\n                \"A Survey on Data-driven Software Vulnerability Assessment and Prioritization\",\n                \"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator\",\n                \"StarCoder: may the source be with you!\",\n                \"Evaluating c/c++ vulnerability detectability of query-based static application security testing tools\",\n                \"Software vulnerability detection using deep neural networks: a survey\",\n                \"Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation\",\n                \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing\",\n                \"Automatic Programming: Large Language Models and Beyond\",\n                \"Assessing the security of github copilot’s generated code-a targeted replication study\",\n                \"Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities\",\n                \"Show Your Work: Scratchpads for Intermediate Computation with Language Models\",\n                \"Asleep at the keyboard? assessing the security of github copilot’s code contributions\",\n                \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\n                \"Android source code vulnerability detection: a systematic literature review\",\n                \"PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\",\n                \"Large Language Model Alignment: A Survey\",\n                \"Securityeval dataset: Mining vulnerability examples to evaluate machine learning-based code generation techniques\",\n                \"LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations\",\n                \"Llms cannot reliably identify and reason about security vulnerabilities (yet?): A comprehensive evaluation, framework, and benchmarks\",\n                \"Combining structured static code information and dynamic symbolic traces for software vulnerability prediction\",\n                \"Chain-of-thought prompting elicits reasoning in large language models\",\n                \"Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review\",\n                \"How Effective Are Neural Networks for Fixing Security Vulnerabilities\",\n                \"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models\",\n                \"Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x\",\n                \"Large Language Model for Vulnerability Detection: Emerging Results and Future Directions\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]7 based on the context and research area of From Solitary Directives to Interactive Encouragement! LLM Secure Code Generation by Natural Language Prompting. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}