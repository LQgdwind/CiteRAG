{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Framework for dynamically meeting performance objectives on a service mesh\n\nPaper Abstract: \n\nPaper Section Text:\nStarting from an overall management objective and intent, the goal is to find an optimal (or close-to-optimal) policy that provides the control settings for a given state of the target system (in this work target system and testbed are used interchangeably). Moreover, these settings should adapt as the system state and the environment evolve. In this section, we present the solution framework for this engineering task.\n Compared with the recent literature, our framework has the following distinguishing features. (See Section VIII for more details on related work).\nFirst, the control policies are determined by end-to-end management objectives, which can be of varying complexity. They are defined using a set of metrics that can be observed on the target system.\nSecond, the framework includes the target system on which the RL agent acts, as well as a simulator where effective policies are computed in an efficient way.\nThe key reason for including a simulator is the long and often unfeasible training time for an RL agent operating on the target system. Agent training includes a series of interaction steps, each of which is composed of the settling time after a control action followed by the observation time of the new system state. For the scenarios considered in this work, an interaction step takes seconds to minutes on the target system, and thousands of such steps are typically needed to fully train an agent. In the simulation environment, a step can be executed within milliseconds, which speeds up the training time by several orders of magnitude. In addition, the simulator allows us to study the robustness or generalization properties of a learned policy, for instance by observing the effect of changes in the load pattern, the system configuration, or environmental parameters, while keeping the policy constant.\nThird, we explicitly learn the system model from monitoring the target system. This opens up the possibility to learn different policies for different management objectives for the same system model.\nFinally, the framework allows any number of control actions (e.g., scaling, routing, and blocking), which can be executed in sequence or concurrently and which allows for a rich set of control actions in support of meeting the management objectives. In comparison, many recent works focus on studying a single type of control action for a given performance objective. Figure 2 shows the solution framework of our method including processes and the data flows between them. To find optimal control actions in support of an end-to-end management objective, we perform the following tasks in a sequential fashion. 111 Describe use case, management objective(s), control actions. We describe the use case by giving details about the infrastructure, the services, the stakeholders, etc. Then, we formulate the management objective (of the service provider) and the control functions in the infrastructure that are available to achieve the management objective. (See Section II.) 222 Develop the RL model including the reward function. Here we formalize the use case and the management objective using the reinforcement learning approach. This includes defining the key elements of the RL model, namely, states, actions, and a reward function that expresses the management objective. (See Section II.) 333 Define a scenario and run it on the target system. Within the context of the use case, we define several scenarios which approximate operational conditions and run them on the target system. In this paper, we report on four scenarios. (See section V.) A scenario description contains the characterization of the offered load, the service configuration, and the management objective. We set up a scenario on the target system, run it, and collect data traces through periodic monitoring of the system state and service metrics. A data item has the format of (st,at,st+1)subscript𝑠𝑡subscript𝑎𝑡subscript𝑠𝑡1(s_{t},a_{t},s_{t+1}), whereby stsubscript𝑠𝑡s_{t} describes the system state at time t𝑡t, atsubscript𝑎𝑡a_{t} the value of the control parameters at time t𝑡t, and st+1subscript𝑠𝑡1s_{t+1} the new state of the system after the control parameters have been applied. In the RL model, time evolves in discrete steps. We choose the duration of a time step on the target system in such a way that it includes the settling time of the control action and the time it takes to estimate the new system state. (By settling time we understand the time it takes to reach an equilibrium state after taking a control action ). Figure 3 shows the time steps of the reinforcement learning model on the target system. The settling time depends on the type of control function. In the experiments for this work, the settling time for routing action is typically about one second, and the settling time for scaling is generally about 303030 seconds. 444 Estimate the system mode and the operating region. The system model is a function that maps the system state to a new state after applying a specific control function . In this work, the system model is given by Eq. (3)3(\\ref{eq:system_simple}). We estimate the system model through supervised learning using random forest regression  from the collected data traces. To achieve predictable and stable service quality, we want the system to be stable and perform within its operating region . We say that a tuple consisting of the system state s𝑠s and the setting of control parameters a𝑎a is within the operating region Opsubscript𝑂𝑝O_{p} of the system if the state is stationary. For our use case, the system state is stationary if the mean response time remains constant and its variance is small. (In this work, we consider the variance to be small if it is smaller than 50%percent5050\\% of the mean.) Formally, Figure 4 illustrates an example of the operating region for the services running on the target. 555 Train and evaluate the RL agent on the simulator. The upper part of Figure 2 shows the structure and the main components of the simulator. The system model receives as input the offered load from the load generator and the actions from the RL agent at each time step, and it predicts the response time of the services for the next time step. The RL agent computes the reward and updates the control policy.\nNote that the action that the RL agent takes is constrained by the operating region which is expressed by the action mask in Figure 2 [ref]11. 666 Evaluate the RL agent on the target system. We evaluate the learned policy with respect to robustness and generality and with respect to its effectiveness on the target system. To assess the generality of the policy, we first evaluate the learned policy on the simulator using a different load pattern (i.e., an operational load pattern) than the agent has been trained on. This informs us about the generalization error of the learned policy. Second, and more important, we evaluate the learned policy on the target system for the load pattern used to train the agent as well as the operational load pattern. The gap in effectiveness between the simulation results and the results from the target system is determined by the accuracy of the system model. Finally, we assess the robustness of the policy by evaluating the learned policy on the target system using a different load pattern than the agent has been trained on.\n\nReference markers found: [11]\n\nContext windows for each [ref]:\n[ref]11: The RL agent computes the reward and updates the control policy.\nNote that the action that the RL agent takes is constrained by the operating region which is expressed by the action mask in Figure 2 [ref]11. 666 Evaluate the RL agent on the target system. We evaluate the learned policy with respect to robustness and generality and with respect to its effectiveness on the target system. To assess\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 11,\n            \"titles\": [\n                \"A Closer Look at Invalid Action Masking in Policy Gradient Algorithms\",\n                \"“Simplify observability\",\n                \"Simplify observability, traffic management, security, and policy with the leading service mesh\",\n                \"Production-grade container orchestration\",\n                \"Dynamically meeting performance objectives for multiple services on a service mesh\",\n                \"Markov decision processes: discrete stochastic dynamic programming\",\n                \"Reinforcement learning: An introduction\",\n                \"Feedback control of dynamic systems\",\n                \"Reinforcement learning and optimal control\",\n                \"Random Forests\",\n                \"sklearn.ensemble.RandomForestClassifier\",\n                \"Feedback control of computing systems\",\n                \"Maskable ppo\",\n                \"인공지능 기반 공격 그래프 생성\",\n                \"Python\",\n                \"Flask\",\n                \"Pods\",\n                \"Proximal policy optimization algorithms\",\n                \"Ppo\",\n                \"Towards goal-oriented development of self-adaptive systems\",\n                \"A survey of formal methods in self-adaptive systems\",\n                \"A survey on engineering approaches for self-adaptive systems\",\n                \"The vision of autonomic computing\",\n                \"Autonomic computing: An overview\",\n                \"Self-driving network and service coordination using deep reinforcement learning\",\n                \"Intent-based networking with proactive load distribution in data center using ibn manager and smart path manager\",\n                \"Intent based network operations\",\n                \"Reinforcement Learning-based Application Autoscaling in the Cloud: A Survey\",\n                \"Self-adaptive container deployment in the fog: A survey\",\n                \"Coscal: Multi-faceted scaling of microservices with reinforcement learning\",\n                \"Self-learning multi-objective service coordination using deep reinforcement learning\",\n                \"Heuristic and Reinforcement Learning Algorithms for Dynamic Service Placement on Mobile Edge Cloud\",\n                \"Self-adaptive threshold-based policy for microservices elasticity\",\n                \"FIRM: An Intelligent Fine-Grained Resource Management Framework for SLO-Oriented Microservices\",\n                \"Miras: Model-based reinforcement learning for microservice resource allocation over scientific workflows\",\n                \"Graf: A graph neural network based proactive resource allocation framework for slo-oriented microservices\",\n                \"Horizontal scaling in cloud using contextual bandits\",\n                \"A lightweight workload-aware microservices autoscaling with qos assurance\",\n                \"Microscaler: Cost-effective scaling for microservice applications in the cloud with an online learning approach\",\n                \"Throughput-aware partitioning and placement of applications in fog computing\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]11 based on the context and research area of A Framework for dynamically meeting performance objectives on a service mesh. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}