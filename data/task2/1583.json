{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: HINT: Hierarchical Interaction Network for Clinical Trial Outcome Prediction\n\nPaper Abstract: \n\nPaper Section Text:\nImplementation Details\nWe use our TOP benchmark for model evaluation.\nThe implementation details are described below.\nThe curated dataset and code repository is available at 888https://github.com/futianfan/clinical-trial-outcome-prediction. âˆ™âˆ™\\bullet Molecule Embedding (SectionÂ 3.3). Regarding the molecule embedding function fmâ€‹(â‹…)subscriptğ‘“ğ‘šâ‹…f_{m}(\\cdot) in Eq.Â (5), HINT supports Morgan fingerprintÂ , SMILES encoderÂ [ref]10, graph message passing neural network (MPNN)Â  [ref]19  , we choose MPNN because it usually works better in our experiments.\nThe depth of MPNN is 3, with hidden layer dimension 100. The input feature of MPNN is molecular graph with atom and bond features. FollowingÂ  [ref]19 , atom features is a 38 dimensional vector, including its atom type (23 dim, 22 frequent atoms and 1 unknown indicator), degree (6-dim one-hot vector, {0,1,2,3,4,5}012345\\{0,1,2,3,4,5\\}), its formal charge (5-dim one-hot vector, {âˆ’1,âˆ’2,1,2,0}12120\\{-1,-2,1,2,0\\}) and its chiral configuration (4-dim one-hot vector, {0,1,2,3}0123\\{0,1,2,3\\}). Bond feature is a 11 dim vector, which is concatenation of its bond type (4-dim one-hot vector, {single, double, triple, aromatic}single, double, triple, aromatic\\{\\text{single, double, triple, aromatic}\\}), whether the bond is in a ring (1-dim), and its cis-trans configuration (6-dim one-hot vector, {0,1,2,3,4,5}012345\\{0,1,2,3,4,5\\}).\n\nâˆ™âˆ™\\bullet Disease Embedding (SectionÂ 3.3). Disease embedding is obtained by GRAMÂ  , as defined in Eq.Â (6). The embedding size of both ğ¡dsubscriptğ¡ğ‘‘\\mathbf{h}_{d} (Eq.Â 6) and ğğ\\mathbf{e} (Eq.Â 7) are both 100. Following , the attention model defined in Eq.Â (8) is feed-forward network with a single hidden layer (with dimension 100).\n\nâˆ™âˆ™\\bullet Protocol Embedding (SectionÂ 3.3). ğ¬iIsubscriptsuperscriptğ¬ğ¼ğ‘–\\mathbf{s}^{I}_{i} and ğ¬iEsubscriptsuperscriptğ¬ğ¸ğ‘–\\mathbf{s}^{E}_{i} in Eq.Â (9) are both 768 dimensional vectors. FollowingÂ , the kernel size k1,k2,k3,k4subscriptğ‘˜1subscriptğ‘˜2subscriptğ‘˜3subscriptğ‘˜4k_{1},k_{2},k_{3},k_{4} in Eq.Â (10) is set to 1,3,5,7, respectively.\nâˆ™âˆ™\\bullet Interaction Graph (SectionÂ 3.5). All the node embedding in Interaction Graph have same dimension to support graph neural network reasoning. The dimension is set to 100. The neural architecture of the connections between different nodes are one-layer fully-connected neural network followed by a two-layer highway network. The input dimension of fully-connected neural network is determined by the number of input node number while the output dimension is 100, equal to the hidden dimension of highway network and node dimension in interaction graph. For example, to obtain pharmaco-kinetics nodeâ€™s embedding in Eq.Â (20), the input dimension of one-layer fully-connected neural network is 500=5âˆ—1005005100500=5*100 (ğ¡A,ğ¡D,ğ¡M,ğ¡E,ğ¡Tsubscriptğ¡ğ´subscriptğ¡ğ·subscriptğ¡ğ‘€subscriptğ¡ğ¸subscriptğ¡ğ‘‡\\mathbf{h}_{A},\\mathbf{h}_{D},\\mathbf{h}_{M},\\mathbf{h}_{E},\\mathbf{h}_{T}), the output dimension is 100. To obtain Interaction nodeâ€™s embedding in Eq.Â (21), the input dimension of one-layer fully-connected NN is 300=3âˆ—1003003100300=3*100 (ğ¡d,ğ¡m,ğ¡psubscriptğ¡ğ‘‘subscriptğ¡ğ‘šsubscriptğ¡ğ‘\\mathbf{h}_{d},\\mathbf{h}_{m},\\mathbf{h}_{p}).\n\nâˆ™âˆ™\\bullet Graph Neural Architecture (SectionÂ 3.5).\nThe hidden size of GCN in Eq.Â (25) is set to 200. The dropout rate is set to 0.6. The feature dimension of GCN is 100, equal to nodeâ€™s embedding size. The graph attention model defined in Eq.Â (26) is a two-layer fully-connected neural network with output dimension 1 with sigmoid activation. The input size is 200=2âˆ—1002002100200=2*100 (concatenation of two nodesâ€™ embedding), the hidden size is 50, the output dimension is 1. As shown in Eq.Â (26), the output is a scalar, ğ•i,jâˆˆâ„+subscriptğ•ğ‘–ğ‘—subscriptâ„\\mathbf{V}_{i,j}\\in\\mathbb{R}_{+}. The depth of GNN Lğ¿L is set to 3.\n\nâˆ™âˆ™\\bullet Learning. During both pre-training and training procedure, we use the Adam as optimizerÂ [ref]28. The learning rate is selected from {1â€‹eâˆ’4,5â€‹eâˆ’4,1â€‹eâˆ’3}1superscriptğ‘’45superscriptğ‘’41superscriptğ‘’3\\{1e^{-4},5e^{-4},1e^{-3}\\} and tuned on validation set. When pretraining ADMET models, the learning rate is set to 5â€‹eâˆ’45superscriptğ‘’45e^{-4}. When pretraining disease risk model, learning rate is set to 1â€‹eâˆ’31superscriptğ‘’31e^{-3}. When training HINT, the learning rate is set to 5â€‹eâˆ’45superscriptğ‘’45e^{-4}. The maximal epochs are set to 10. We observe all the models converged within maximal epochs.\nWe save the model every epoch and choose the model that performs best on validation set. All hyperparameters are tuned on validation set. Evaluation Settings We consider two realistic evaluation setups. The first is phase-level evaluation where we predict the outcome of a single phase study. Since each phase has different goals (e.g. Phase I is for safety whereas Phase II and III is for efficacy), we conduct evaluation for Phase I, II and III individually. We create the test datasets using the FDA guideline999https://www.fda.gov/patients/drug-development-process/step-3-clinical-research on the success-failure ratio for each phase, specifically 70% success rate for Phase I, 33% for Phase II and 30% for Phase III. Second, we also consider indication-level evaluation where we test if the drug can pass all three phases for the final market approval. To imitate it, we assemble all phase studies related to the drug and disease of the study and then use the latest phase protocol as the input to our model. Drugs that have Phase III success are labelled positive and other drugs that fail in any of the three phases are labelled negative. Data statistics are shown in TableÂ 2. Evaluation Metrics\nWe use the following metrics to measure the performance of all methods.\n\nâˆ™âˆ™\\bullet PR-AUC (Precision-Recall Area Under Curve).\n\nâˆ™âˆ™\\bullet F1. The F1 score is the harmonic mean of the precision and recall.\n\nâˆ™âˆ™\\bullet ROC-AUC (Area Under the Receiver Operating Characteristic Curve).\n\nâˆ™âˆ™\\bullet p-value. We report the results of hypothesis testing in terms of p-value to showcase the statistical significance of our method over the best baseline results. If p-value is smaller than 0.05, we claim our method significantly outperforms the best baseline method. For PR-AUC, F1, and ROC-AUC, higher value represent better performance.\nWe split the dataset based on registration date. The earlier trials are used for learning while the later trials are used for inference. For example, for Phase I dataset, we learn the model using the trials before Aug 13th, 2014 and make inference using the trials after that date, as shown in TableÂ 2.\nWe use BootstrapÂ  to estimate the mean and standard deviation of accuracy on test set. Baselines. We compare HINT with several baselines, including both conventional machine learning models and deep learning methods. LR (Logistic Regression). It was used in  on trial prediction with disease features only. For fair comparison, we adapt it such that the input features include (i) 1024 dimensional Morgan fingerprint featureÂ , (ii) GRAM embedding (Eq.Â 7, GRAM is pretrained using disease risk moduleÂ Eq.Â 18) and (iii) BERT embedding of eligibility criteria for protocol. Then these three features are concatenated as the input of LR model. RF (Random Forest). Similarly to LR, it was used in  on trial prediction and we adapt it to use the same feature set. XGBoost. An implementation of gradient boosted decision trees designed for speed and performance. It was used in context of individual patient trial outcome prediction inÂ . We adapt it to use the same feature set for general trial outcome prediction. AdaBoost (Adaptive Boosting). It was used inÂ  for individual Alzheimerâ€™s patientâ€™s trial result prediction. We adapt it to use the same feature set. kNN+RF.  leverages statistical imputation techniques to handle missing data, and finds using (1) kNN (k-Nearest Neighbor) as imputation technique and (2) Random Forest as classifier would achieve best performance. We adapt this method to use the same feature set. FFNN (Feed-Forward Neural Network)Â . It uses the same feature with LR. The feature vectors are fed into a three-layer feedforward neural network. DeepEnrollÂ . DeepEnroll was originally designed for patient trial matching, it uses (1) pre-trained BERT modelÂ  to encode eligibility criteria into sentence embedding; (2) a hierarchical embedding model to disease information and (3) alignment model to capture the protocol-disease interaction information. To adapt it to our scenario, molecule embedding (ğ¡msubscriptğ¡ğ‘š\\mathbf{h}_{m}) is concatenated to the output of alignment model to make prediction. COMPOSE (cross-modal pseudo-siamese network)Â . COMPOSE was also originally designed for patient trial matching, it uses convolutional highway network and memory network to encode eligibility criteria and diseases respectively and an alignment model to model the interaction. COMPOSE incorporate the molecule information in the same way with DeepEnroll, as mentioned above.\n\nReference markers found: [10]\n\nContext windows for each [ref]:\n[ref]10: âˆ™âˆ™\\bullet Molecule Embedding (SectionÂ 3.3). Regarding the molecule embedding function fmâ€‹(â‹…)subscriptğ‘“ğ‘šâ‹…f_{m}(\\cdot) in Eq.Â (5), HINT supports Morgan fingerprintÂ , SMILES encoderÂ [ref]10, graph message passing neural network (MPNN)Â  [ref]19  , we choose MPNN because it usually works better in our experiments.\nThe depth of MPNN is 3, with hidden layer dimension 100. The input\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 10,\n            \"titles\": [\n                \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n                \"Blood-brain barrier permeation models: discriminating between potential cns and non-cns drugs including p-glycoprotein substrates\",\n                \"Publicly Available Clinical BERT Embeddings\",\n                \"Welcome to the icd-10 code for sarcopenia\",\n                \"Integrated deep learned transcriptomic and structure-based predictor of clinical trials outcomes\",\n                \"Doctor2Vec: Dynamic Doctor Representation Learning for Clinical Trial Recruitment\",\n                \"Molecular fingerprint similarity search in virtual screening\",\n                \"Bootstrap methods\",\n                \"GRAM: Graph-based Attention Model for Healthcare Representation Learning\",\n                \"Convolutional embedding of attributed molecular graphs for physical property prediction\",\n                \"Discriminative embeddings of latent variable models for structured data\",\n                \"Admetlab: a platform for systematic admet evaluation based on a comprehensively collected admet database\",\n                \"Interpretable Molecular Graph Generation via Monotonic Constraints\",\n                \"Application of kpca and adaboost algorithm in classification of functional magnetic resonance imaging of alzheimerâ€™s disease\",\n                \"Fundamentals of clinical trials\",\n                \"Differentiable Scaffolding Tree for Molecular Optimization\",\n                \"MIMOSA: Multi-constraint molecule sampling for molecule optimization\",\n                \"Probabilistic and dynamic molecule-disease interaction modeling for drug discovery\",\n                \"CORE: Automatic Molecule Optimization Using Copy & Refine Strategy\",\n                \"COMPOSE: Cross-Modal Pseudo-Siamese Network for Patient Trial Matching\",\n                \"A data-driven approach to predicting successes and failures of clinical trials\",\n                \"Modeling admet\",\n                \"Clinical development success rates for investigational drugs\",\n                \"Predicting successes and failures of clinical trials with an ensemble ls-svr\",\n                \"ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission\",\n                \"DeepPurpose: A Deep Learning Library for Drug-Target Interaction Prediction\",\n                \"Junction Tree Variational Autoencoder for Molecular Graph Generation\",\n                \"Adam: A method for stochastic optimization\",\n                \"Semi-Supervised Classification with Graph Convolutional Networks\",\n                \"4 ways to fix the clinical trial: clinical trials are crumbling under modern economic and scientific pressures. nature looks at ways they might be saved\",\n                \"Machine learning with statistical imputation for predicting drug approvals\",\n                \"Prediction models of human plasma protein binding rate and oral bioavailability derived by using gaâ€“cgâ€“svm method\",\n                \"How much do clinical trials cost?\",\n                \"Clinical trial methodology\",\n                \"e-drug3d: 3d structure collections dedicated to drug repurposing and fragment-based drug design\",\n                \"Predicting phase 3 clinical trial results by modeling phase 2 clinical trial subject level data using deep learning\",\n                \"Evaluation of a Machine Learning Model Based on Pretreatment Symptoms and Electroencephalographic Features to Predict Outcomes of Antidepressant Treatment in Adults With Depression: A Prespecified Secondary Analysis of a Randomized Clinical Trial\",\n                \"Toxcast chemical landscape: paving the road to 21st century toxicology\",\n                \"Training very deep networks\",\n                \"Clinical development success rates 2006â€“2015\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]10 based on the context and research area of HINT: Hierarchical Interaction Network for Clinical Trial Outcome Prediction. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}