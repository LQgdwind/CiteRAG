{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Is Simple Uniform Sampling Effective for Center-Based Clustering with Outliers: When and Why?\n\nPaper Abstract: \n\nPaper Section Text:\nClustering has many important applications in real worldÂ . An important type of clustering problems is called â€œcenter-based clusteringâ€, such as the well-known kğ‘˜k-center/median/means clustering problemsÂ .\nIn general, a center-based clustering problem aims to find kğ‘˜k cluster centers so as to minimize the induced clustering cost.\nBut in practice our datasets often contain outliers which can seriously destroy the final clustering resultsÂ .\nA key obstacle is that the outliers can be arbitrarily located in the space. Outlier removal is a topic that has been extensively studied beforeÂ . The existing outlier removal methods (like DBSCANÂ  [ref]6) yet cannot simply replace the center-based clustering with outliers approaches, since we often need to use the obtained cluster centers to represent the data or perform other tasks, e.g., data compression and data selection for large-scale machine learningÂ [ref]7 . Clustering with outliers can be viewed as a generalization of the vanilla clustering problems, however, the presence of arbitrary outliers makes the problems to be much more challenging. In particular, the problem of clustering with outliers is often a challenging combinatorial optimization problemÂ  0.\nEven for their approximate solutions, most existing quality-guaranteed algorithms have super-linear time complexities.\nSo a number of sampling methods have been proposed for reducing their complexities. Namely, we take a small sample (uniformly or non-uniformly) from the input and run an existing approximation algorithm on the sample. A number of non-uniform sampling methods have been proposed, such as the kğ‘˜k-means++ seedingÂ 0 1 2 3 and the successive sampling methodÂ 4. However, these non-uniform sampling approaches may suffer several drawbacks in practice. For instance, they need to read the input dataset in multiple passes with high computational complexities, and/or may have to discard more than zğ‘§z outliers (zğ‘§z is the pre-specified number of outliers). For example, the methodsÂ 0 1 need to run the kğ‘˜k-means++ seeding procedure Î©â€‹(k+z)Î©ğ‘˜ğ‘§\\Omega(k+z) rounds, and discard Oâ€‹(kâ€‹zâ€‹logâ¡n)ğ‘‚ğ‘˜ğ‘§ğ‘›O(kz\\log n) and Oâ€‹(kâ€‹z)ğ‘‚ğ‘˜ğ‘§O(kz) outliers respectively. Compared with the non-uniform sampling methods, uniform sampling enjoys several significant advantages, e.g., it is very easy to implement in practice.\nWe need to emphasize that, beyond the clustering quality and asymptotic complexity, the simplicity of implementation is also a major concern in algorithm engineering, especially for dealing with big dataÂ 5.\nHowever, uniform sampling usually requires a large sample size depending\non n/zğ‘›ğ‘§n/z and the dimensionality dğ‘‘d, which could be very high (e.g., dğ‘‘d could be high and zğ‘§z could be much smaller than nğ‘›n). For example, as the result proposed byÂ Huang etÂ al. 6, the sample size should be at least\nÎ©â€‹((nÎ´â€‹z)2â€‹kâ€‹dâ€‹logâ¡k)Î©superscriptğ‘›ğ›¿ğ‘§2ğ‘˜ğ‘‘ğ‘˜\\Omega\\big{(}(\\frac{n}{\\delta z})^{2}kd\\log k\\big{)}, if Î´âˆˆ(0,1)ğ›¿01\\delta\\in(0,1) and we allow to discard (1+Î´)â€‹z1ğ›¿ğ‘§(1+\\delta)z outliers (which is slightly larger than the pre-specified number zğ‘§z). If z=nğ‘§ğ‘›z=\\sqrt{n}, their sample size will be even larger than nğ‘›n. Actually, for the vanilla kğ‘˜k-median/means clustering (without outliers) problems, Â Meyerson etÂ al. [17, Section 4 and 5] considered using uniform sampling to solve the practical case that each optimal cluster is assumed to have size Î©â€‹(Ïµâ€‹nk)Î©italic-Ïµğ‘›ğ‘˜\\Omega(\\frac{\\epsilon n}{k}), where Ïµitalic-Ïµ\\epsilon is a fixed parameter in (0,1)01(0,1). But unfortunately their result cannot be easily extended to the version with outliers. As studied in the same paperÂ [17, Section 6], it was shown that the sample size relies on nzğ‘›ğ‘§\\frac{n}{z}, and moreover, the solution has a large error on the number of discarded outliers (it needs to discard >16â€‹zabsent16ğ‘§>16z outliers). Our contributions and key ideas. In this paper, we aim to provide the theoretical analysis to explain the effectiveness of uniform sampling in practice.\nOur algorithmic framework actually is very simple, where we just need to take a small sample uniformly at random from the input and run an existing approximation algorithm (as the black-box algorithm) on the sample. Our results are partly inspired by the work fromÂ Meyerson etÂ al. 7, but as discussed above, we need to go deeper and develop significantly new ideas for analyzing the influence of outliers.\nWe briefly introduce our ideas and results below. Suppose {C1âˆ—,C2âˆ—,â‹¯,Ckâˆ—}subscriptsuperscriptğ¶1subscriptsuperscriptğ¶2â‹¯subscriptsuperscriptğ¶ğ‘˜\\{C^{*}_{1},C^{*}_{2},\\cdots,C^{*}_{k}\\} are the kğ‘˜k optimal clusters.\nWe consider the ratio between two values: the size of the smallest optimal cluster min1â‰¤jâ‰¤kâ¡|Cjâˆ—|subscript1ğ‘—ğ‘˜subscriptsuperscriptğ¶ğ‘—\\min_{1\\leq j\\leq k}|C^{*}_{j}|111|Cjâˆ—|subscriptsuperscriptğ¶ğ‘—|C^{*}_{j}| indicates the number of points in Cjâˆ—subscriptsuperscriptğ¶ğ‘—C^{*}_{j}. and the number of outliers zğ‘§z.\nWe show this ratio actually plays a key role to the effectiveness of uniform sampling.\nIn real applications, a cluster usually represents a certain scale of population and it is rare to have a cluster with the size much smaller than the number of outliers.\nSo it is reasonable to assume that each cluster Cjâˆ—subscriptsuperscriptğ¶ğ‘—C^{*}_{j} has a size at least comparable to zğ‘§z, i.e., min1â‰¤jâ‰¤kâ¡|Cjâˆ—|/z=Î©â€‹(1)subscript1ğ‘—ğ‘˜subscriptsuperscriptğ¶ğ‘—ğ‘§Î©1\\min_{1\\leq j\\leq k}|C^{*}_{j}|/z=\\Omega(1) (note this assumption allows the ratio to be a value smaller than 111, say 0.50.50.5). Under such a realistic assumption, our framework can output k+Oâ€‹(logâ¡k)ğ‘˜ğ‘‚ğ‘˜k+O(\\log k) cluster centers that yield a 444-approximation for kğ‘˜k-center clustering with outliers.\nIn some scenarios, we may insist on returning exactly kğ‘˜k cluster centers. We prove that, if min1â‰¤jâ‰¤kâ¡|Cjâˆ—|z>1subscript1ğ‘—ğ‘˜subscriptsuperscriptğ¶ğ‘—ğ‘§1\\frac{\\min_{1\\leq j\\leq k}|C^{*}_{j}|}{z}>1, our framework can return exactly kğ‘˜k cluster centers that yield a (c+2)ğ‘2(c+2)-approximate solution, through running an existing cğ‘c-approximation kğ‘˜k-center with outliers algorithm (with some câ‰¥1ğ‘1c\\geq 1) on the sample. The framework also yields similar results for kğ‘˜k-median/means clustering with outliers. We prove that the sample size can be independent of the ratio n/zğ‘›ğ‘§n/z and the dimensionality dğ‘‘d, which is fundamentally different from the previous results on uniform samplingÂ 8 6 9. Our sample size only depends on kğ‘˜k, min1â‰¤jâ‰¤kâ¡|Cjâˆ—|subscript1ğ‘—ğ‘˜subscriptsuperscriptğ¶ğ‘—\\min_{1\\leq j\\leq k}|C^{*}_{j}|, and several parameters controlling the clustering error and success probability.\nMoreover, different from the previous methods which often have the errors on the number of discarded outliers, our method allows to discard exactly zğ‘§z outliers.\nAlso, if we only require to output the cluster centers, our uniform sampling approach has the sublinear time complexity that is independent of the input size. It is worth noting that Gupta 0 proposed a similar uniform sampling approach to solve kğ‘˜k-means clustering with outliers.\nHowever, the analysis and results ofÂ Gupta 0 are quite different from ours. Also the assumption inÂ 0 is stronger: it requires that each optimal cluster has size roughly Î©â€‹(zÎ³2â€‹logâ¡k)Î©ğ‘§superscriptğ›¾2ğ‘˜\\Omega(\\frac{z}{\\gamma^{2}}\\log k) (i.e., min1â‰¤jâ‰¤kâ¡|Cjâˆ—|z=Î©â€‹(logâ¡kÎ³2)subscript1ğ‘—ğ‘˜subscriptsuperscriptğ¶ğ‘—ğ‘§Î©ğ‘˜superscriptğ›¾2\\frac{\\min_{1\\leq j\\leq k}|C^{*}_{j}|}{z}=\\Omega(\\frac{\\log k}{\\gamma^{2}})) where Î³ğ›¾\\gamma is a small parameter in (0,1)01(0,1).\n\nReference markers found: [6]\n\nContext windows for each [ref]:\n[ref]6: key obstacle is that the outliers can be arbitrarily located in the space. Outlier removal is a topic that has been extensively studied beforeÂ . The existing outlier removal methods (like DBSCANÂ  [ref]6) yet cannot simply replace the center-based clustering with outliers approaches, since we often need to use the obtained cluster centers to represent the data or perform other tasks, e.g.,\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 6,\n            \"titles\": [\n                \"Active Learning for Convolutional Neural Networks: A Core-Set Approach\",\n                \"Data clustering: 50 years beyond k-means\",\n                \"Center based clustering: A foundational perspective\",\n                \"Robust k-means: a theoretical revisit\",\n                \"Anomaly detection: A survey\",\n                \"A density-based algorithm for discovering clusters in large spatial databases with noise\",\n                \"A survey of density based clustering algorithms\",\n                \"Selection via Proxy: Efficient Data Selection for Deep Learning\",\n                \"Algorithms for facility location problems with outliers\",\n                \"Local search methods for k-means with outliers\",\n                \"Fast noise removal for k-means clustering\",\n                \"Greedy sampling for approximate clustering in the presence of outliers\",\n                \"Robust k-means++\",\n                \"A Practical Algorithm for Distributed Clustering and Outlier Detection\",\n                \"Algorithm engineering for big data\",\n                \"Epsilon-coresets for clustering (with outliers) in doubling metrics\",\n                \"A k-median algorithm with running time independent of data size\",\n                \"Better streaming algorithms for clustering problems\",\n                \"Greedy Strategy Works for ğ‘˜-Center Clustering with Outliers and Coreset Construction\",\n                \"Approximation algorithms for clustering and facility location problems\",\n                \"Streaming algorithms for k-center clustering with outliers and with anonymity\",\n                \"The non-uniform k-center problem\",\n                \"Approximate clustering via core-sets\",\n                \"Clustering to minimize the maximum intercluster distance\",\n                \"Solving k-center Clustering (with Outliers) in MapReduce and Streaming, almost as Accurately as Sequentially.\",\n                \"Trimmed k-means: An attempt to robustify quantizers\",\n                \"A constant factor approximation algorithm for k-median clustering with outliers\",\n                \"Constant Approximation for ğ‘˜-Median and ğ‘˜-Means with Outliers via Iterative Rounding\",\n                \"Approximation Schemes for Clustering with Outliers\",\n                \"k-meansâ€“: A unified approach to clustering and outlier detection\",\n                \"On integrated clustering and outlier detection\",\n                \"Clustering with Outlier Removal\",\n                \"Uniform Concentration Bounds toward a Unified Framework for Robust Clustering\",\n                \"k-means++: The advantages of careful seeding\",\n                \"Optimal time bounds for approximate clustering\",\n                \"Layered Sampling for Robust Optimization Problems\",\n                \"Near-optimal Coresets for Robust Clustering\",\n                \"The Power of Uniform Sampling for Coresets\",\n                \"Sublinear time algorithms for metric space problems\",\n                \"Sublinear time approximate clustering\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]6 based on the context and research area of Is Simple Uniform Sampling Effective for Center-Based Clustering with Outliers: When and Why?. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}