{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Augmenting Zero-Shot Detection Training with Image Labels\n\nPaper Abstract: \n\nPaper Section Text:\nZero-shot detection (ZSD) [ref]1 is a special form of object detection. Contrary to object detection, not every object class a ZSD detector should categorize during testing is present in the training data set. Those classes are called unseen classes while the classes present during training are called seen classes.\nTo evaluate the performance of a ZSD model, a data set is needed which contains only seen classes in the training data set and unseen classes in the test data set. For this purpose, commonly used object detection data sets are split in seen and unseen classes. One example is the COCO [ref]17 data set. Currently, two splits are widely used: The 48/17 split [ref]1 and the 65/15 split .\nFor the 48/17 split, word vector embeddings are calculated for all classes of COCO. They are then clustered into 10 clusters using cosine similarity as a measure. From each cluster, 80% of the classes are randomly assigned to the seen classes, while the other 20% are assigned to the unseen classes. But only those classes are used, which have a synset associated with them in the WordNet hierarchy , resulting in the 48 seen classes and 17 unseen classes.\nRahman et al.  identify some problems with the 48/17 split. On the one hand, the 48/17 split only contains 65 classes while COCO contains 80 classes. Therefore, they do not utilize the full potential of COCO. On the other hand, the chosen clustering does not guarantee a diverse set of unseen classes. To tackle these issues, they proposed the 65/15 split. Based on the total number of classes, the classes of each superclass from COCO are sorted in ascending order. Then, 20% of the rare classes from each superclass are selected as unseen classes, the rest as seen classes. This ensures a diverse nature of unseen classes while utilizing the complete COCO data set. Because of these advantages, we only use the 65/15 split in this paper.\nThe problem of ZSD is especially interesting due to the fact that object detection data sets are more difficult to annotate than image classification data sets, because not only a classification label has to be provided but also the location of the object. Therefore, object detection data sets are quite small compared to object image classification data sets. The 2014 release of COCO [ref]17 contains roughly 83,000 images divided into 80 classes. Compared to this, the ImageNet1000 [ref]31 data set contains roughly 1,200,000 images divided into 1000 classes.\nOne way to tackle ZSD is to use an embedding space [ref]1. The idea is to not directly predict a classification for a given image but rather an embedding vector, which is an abstract representation of the classification label. A currently used embedding space is the CLIP embedding space . More embedding spaces, like ALIGN , GloVE , ConSE , and DeViSE , exist but we focus on CLIP in this paper.\nCLIP consists of an image encoder and a text encoder which both output n-dimensional embedding vectors. During training, a contrastive loss  is used, meaning that image embeddings are computed using the image encoder and the corresponding text labels are fed through the text encoder to compute text embeddings. Then, both the image encoder and the text encoder are tuned in a way to maximize the cosine similarity of corresponding image text pairs while minimizing the cosine similarity of non-corresponding image text pairs. This way, images are related to their labels through the embedding space.\nCLIP was developed for the task of zero-shot classification, i.e., predicting a label for an image of unseen classes. The idea is, that the trained embedding space is not only able to relate images and labels of seen classes but also of unseen classes. To make a zero-shot classification prediction, the labels of unseen classes are fed through the text encoder, generating embedding vectors. Then, an image of an unseen class is fed through the image encoder, generating an embedding vector as well. To make a prediction, the cosine similarity between the image embedding vector and all the text embedding vectors of seen and unseen classes is calculated. The label embedding with the highest cosine similarity is then the prediction, which ideally corresponds to the correct label of the unseen class. This way, CLIP is not restricted to the classes it was trained on, enabling zero-shot classification.\nThis idea can be used for ZSD as well.\nXie and Zheng propose ZSD-YOLO , which builds upon YOLOv5222https://github.com/ultralytics/yolov5, a single-stage detector. They alter the detection head in a way to output an embedding vector instead of class probabilities in addition to bounding box and objectness outputs. During training, they align this embedding vector to the output of the pretrained CLIP  text encoder as well as to the pretrained image encoder. This allows them to use CLIP as a classifier and make predictions for unseen classes. Additionally, they propose a self-labeling strategy to increase the number of examples the model can learn from. For this, they use their model, only trained on seen classes, and run inference on the images of the training set. Non-maximum suppression is applied, where ground truth bounding boxes are prioritized. The new bounding boxes are then treated as ground truth boxes in further training. Before they train their model on the ZSD task, they train the unaltered YOLOv5 model only on the seen classes.\nAnother approach is ViLD , proposed by Gu et al., which builds upon maskRCNN , a two-stage detector. They work on the task of zero-shot instance segmentation, where a pixel-wise mask for unseen classes is predicted, instead of just bounding boxes. But since implicitly bounding boxes are predicted as well, we mention this method here. They alter the classification part of their model to an embedding vector and align it to the text embeddings and image embeddings from CLIP . For the text embedding part of their model, they introduce a background category for predictions that do not match any of the ground truth labels. The background embedding is learned during training. The text embedding part is trained only using class names of seen classes, while the image part is also trained on embeddings of unseen classes. Additionally, they introduce an ensembling approach, where they weight the predictions of the text part and the image part based on if a class is seen or unseen. ViLD is trained from scratch without any pretraining.\nZareian et al.  propose OVR-CNN. They pretrain their model on an image-caption data set to learn an embedding space. They then finetune their model on an object detection data set which only contains some of the classes of the image-caption data set. By this, they are able to also make bounding box predictions for the classes from the image-caption data set, achieving zero-shot predictions.\nOur approach follows this line of work. We build upon YOLOX and change its classification output to an embedding vector. But in contrast to ViLD and ZSD-YOLO, we do not align to the image embeddings from CLIP but only to the text embeddings. Additionally, we follow the idea of YOLO9000 and augment our training with ImageNet1000 classification data to get a better alignment to CLIP.\n\nReference markers found: [1]\n\nContext windows for each [ref]:\n[ref]1: Zero-shot detection (ZSD) [ref]1 is a special form of object detection. Contrary to object detection, not every object class a ZSD detector should categorize during testing is present in the training data set. Those classes\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 1,\n            \"titles\": [\n                \"YOLOv4: Optimal Speed and Accuracy of Object Detection\",\n                \"Zero-Shot Object Detection\",\n                \"End-to-end object detection with transformers\",\n                \"R-FCN: Object Detection via Region-based Fully Convolutional Networks\",\n                \"Devise: A deep visual-semantic embedding model\",\n                \"OTA: Optimal Transport Assignment for Object Detection\",\n                \"YOLOX: Exceeding YOLO Series in 2021\",\n                \"Fast R-CNN\",\n                \"Rich feature hierarchies for accurate object detection and semantic segmentation\",\n                \"Open-vocabulary Object Detection via Vision and Language Knowledge Distillation\",\n                \"Mask R-CNN\",\n                \"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition\",\n                \"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\n                \"Scaling up visual and vision-language representation learning with noisy text supervision\",\n                \"Feature Pyramid Networks for Object Detection\",\n                \"Focal Loss for Dense Object Detection\",\n                \"Microsoft COCO: Common Objects in Context\",\n                \"SSD: Single Shot MultiBox Detector\",\n                \"Swin transformer: Hierarchical vision transformer using shifted windows\",\n                \"Wordnet: a lexical database for english\",\n                \"Zero-shot learning by convex combination of semantic embeddings\",\n                \"Glove: Global vectors for word representation\",\n                \"DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution\",\n                \"Learning Transferable Visual Models From Natural Language Supervision\",\n                \"Improved visual-semantic alignment for zero-shot object detection\",\n                \"Dlwl: Improving detection for lowshot classes with weakly labelled data\",\n                \"You Only Look Once: Unified, Real-Time Object Detection\",\n                \"YOLO9000: Better, Faster, Stronger\",\n                \"Yolov3: An incremental improvement\",\n                \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\n                \"ImageNet Large Scale Visual Recognition Challenge\",\n                \"Optimizing the Trade-off between Single-Stage and Two-Stage Deep Object Detectors using Image Difficulty Prediction\",\n                \"EfficientDet: Scalable and Efficient Object Detection\",\n                \"Attention Is All You Need\",\n                \"Zsd-yolo: Zero-shot yolo detection using vision-language knowledgedistillation\",\n                \"A Survey of Modern Deep Learning based Object Detection Models\",\n                \"Open-Vocabulary Object Detection Using Captions\",\n                \"mixup: Beyond Empirical Risk Minimization\",\n                \"Contrastive Learning of Medical Visual Representations from Paired Images and Text\",\n                \"Deformable DETR: Deformable Transformers for End-to-End Object Detection\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]1 based on the context and research area of Augmenting Zero-Shot Detection Training with Image Labels. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}