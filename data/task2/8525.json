{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: No-Regret Learning in Harmonic Games: Extrapolation in the Face of Conflicting Interests\n\nPaper Abstract: \n\nPaper Section Text:\nThe most basic rationality postulate in the context of online learning is the minimization of a player’s (external) regret, i.e., the difference between a player’s cumulative payoff and that of the player’s best possible strategy in hindsight.\nIn more detail, assuming for the moment that play evolves in continuous time, the regret of player i∈𝒩𝑖𝒩i\\in\\mathcal{N} relative to a sequence of play x​(t)∈𝒳𝑥𝑡𝒳x(t)\\in\\mathcal{X} is defined as and we say that the player has no regret under x​(t)𝑥𝑡x(t) if Regi⁡(T)=o​(T)subscriptReg𝑖𝑇𝑜𝑇\\operatorname{Reg}_{i}(T)=o(T) as T→∞→𝑇T\\to\\infty. The most widely used scheme for attaining no regret is the family of policies known as follow-the-regularized-leader (FTRL) [ref]58 .\nAt a high level, the idea behind FTRL is that, at all times t≥0𝑡0t\\geq 0, each player i∈𝒩𝑖𝒩i\\in\\mathcal{N} plays a mixed strategy xi​(t)∈𝒳isubscript𝑥𝑖𝑡subscript𝒳𝑖x_{i}(t)\\in\\mathcal{X}_{i} that maximizes the player’s cumulative payoff up to time t𝑡t minus a certain regularization penalty.\nIn our continuous-time setting, this gives rise to the FTRL dynamics or, more compactly, where\nhi:𝒳i→ℝ:subscriptℎ𝑖→subscript𝒳𝑖ℝh_{i}\\colon\\mathcal{X}_{i}\\to\\mathbb{R} is a convex penalty function known as the regularizer of the method,\nQisubscript𝑄𝑖Q_{i} denotes the regularized choice map of player i𝑖i,\nand\nQ=(Q1,…,QN)𝑄subscript𝑄1…subscript𝑄𝑁Q=(Q_{1},\\dotsc,Q_{N})\ndenotes the profile thereof.\nFormally, writing 𝒴i≡ℝAisubscript𝒴𝑖superscriptℝsubscript𝐴𝑖\\mathcal{Y}_{i}\\equiv\\mathbb{R}^{A_{i}} for the payoff space of player i∈𝒩𝑖𝒩i\\in\\mathcal{N}\n– that is, the space of all possible payoff vectors visubscript𝑣𝑖v_{i} of player i𝑖i – the regularized choice map Qi:𝒴i→𝒳i:subscript𝑄𝑖→subscript𝒴𝑖subscript𝒳𝑖Q_{i}\\colon\\mathcal{Y}_{i}\\to\\mathcal{X}_{i} is defined as In essence, Qisubscript𝑄𝑖Q_{i} is a “soft” version of the arg​maxargmax\\operatorname*{arg\\,max} correspondence yi↦arg​maxxi∈𝒳i⁡⟨yi,xi⟩maps-tosubscript𝑦𝑖subscriptargmaxsubscript𝑥𝑖subscript𝒳𝑖subscript𝑦𝑖subscript𝑥𝑖y_{i}\\mapsto\\operatorname*{arg\\,max}_{x_{i}\\in\\mathcal{X}_{i}}\\langle y_{i},x_{i}\\rangle, suitably regularized by a penalty term intended to incentivize exploration.\nFor technical reasons, we will also assume that each hisubscriptℎ𝑖h_{i} is strongly convex, i.e., for some Ki>0subscript𝐾𝑖0K_{i}>0 (commonly referred to as the strong convexity modulus of hisubscriptℎ𝑖h_{i}), and for all xi,xi′∈𝒳subscript𝑥𝑖subscriptsuperscript𝑥′𝑖𝒳x_{i},x^{\\prime}_{i}\\in\\mathcal{X}, t∈[ref]0 𝑡01t\\in[ref]0 .\nIn plain words, this simply means that hisubscriptℎ𝑖h_{i} has “enough curvature” in the sense that it can be bounded from below by a (positive) quadratic function which agrees with hisubscriptℎ𝑖h_{i} to first order. The go-to example of this setup is the entropic regularizer which yields the so-called logit choice map By Pinsker’s inequality, the entropic regularizer is 111-strongly convex relative to the L1superscript𝐿1L^{1}-norm on 𝒳isubscript𝒳𝑖\\mathcal{X}_{i} , and by a standard calculation  [ref]37, the induced sytem (FTRL-D) boils down to the replicator dynamics of Taylor & Jonker .\nSome other standard examples of (FTRL-D) include\nthe Euclidean projection dynamics of Friedman 7 when hi​(xi)=(1/2)​∥xi∥22subscriptℎ𝑖subscript𝑥𝑖12superscriptsubscriptdelimited-∥∥subscript𝑥𝑖22h_{i}(x_{i})=(1/2)\\lVert x_{i}\\rVert_{2}^{2},\nthe q𝑞q-replicator dynamics  ,\netc.\nTo streamline our presentation, we defer a detailed discussion of these examples to Appendix C,\nand we proceed below to state the main regret guarantee of (FTRL-D), originally due to : Under (FTRL-D), each player’s regret is bounded as Regi⁡(T)≤Hi≔max⁡hi−min⁡hisubscriptReg𝑖𝑇subscript𝐻𝑖≔subscriptℎ𝑖subscriptℎ𝑖\\operatorname{Reg}_{i}(T)\\leq H_{i}\\coloneqq\\max h_{i}-\\min h_{i}. Theorem 1 showcases the strong no-regret properties of (FTRL-D):\nit is not possible to guarantee less than constant, 𝒪⁡(1)𝒪1\\operatorname{\\mathcal{O}}(1) regret, so (FTRL-D) is optimal in this regard.\nIn turn, by standard results , Theorem 1 implies further that the players’ (correlated) empirical frequencies z#​1​(t)≔(1/t)​∫0t∏ix#​1​(τ)​d​τ≔subscript𝑧#1𝑡1𝑡superscriptsubscript0𝑡subscriptproduct𝑖subscript𝑥#1𝜏𝑑𝜏z_{#1}(t)\\coloneqq(1/t)\\int_{0}^{t}\\prod_{i}x_{#1}(\\tau)\\>d\\tau converge to the game’s set of  coarse correlated equilibria (CCE) at a rate of 𝒪⁡(1/t)𝒪1𝑡\\operatorname{\\mathcal{O}}(1/t). Importantly, this result makes no assumptions about the underlying game, but it does not carry the same predictive power in all games:\nfor one thing, a game’s set of coarse correlated equilibria may include highly non-rationalizable outcomes (such as dominated strategies and the like) ;\nfor another, the time-averaging that is inherent in the definition of empirical distributions may conceal a wide range of non-convergence phenomena, from cycles to chaos  .\nOn that account, the day-to-day behavior of (FTRL-D) in harmonic games cannot be understood from Theorem 1 alone, and requires a closer, more in-depth look. Our first result below provides such a lense and shows that (FTRL-D) is almost-periodic in harmonic games, a property known as Poincaré recurrence. Suppose ΓΓ\\Gamma is harmonic.\nThen almost every orbit x​(t)𝑥𝑡x(t) of (FTRL-D) returns arbitrarily close to its starting point infinitely often:\nspecifically, for (Lebesgue) almost every initial condition x​(0)=Q​(y​(0))∈𝒳𝑥0𝑄𝑦0𝒳x(0)=Q(y(0))\\in\\mathcal{X}, there exists an increasing sequence of times tn↑∞↑subscript𝑡𝑛t_{n}\\uparrow\\infty such that x​(tn)→x​(0)→𝑥subscript𝑡𝑛𝑥0x(t_{n})\\to x(0). An immediate consequence of Theorem 2 is that no-regret learning under (FTRL-D) fails to converge in any harmonic game;\nin particular, since the orbits of (FTRL-D) eventually return to (almost) where they started, it is debatable if the players have learned anything at all,\ndespite the fact that they incur at most constant regret.\nThis cyclic, non-convergent landscape is the polar opposite of the long-run behavior of (FTRL-D) in potential games, where the dynamics are known to converge globally .\nThus, in addition to the strategic viewpoint of the previous section, Theorem 2 shows that harmonic games are orthogonal to potential games also from a dynamic viewpoint. Theorem 2 also provides a far-reaching generalization of existing results on Poincaré recurrence in (possibly networked) two-player zero-sum games with an interior equilibrium  to general-sum, N𝑁N-player games.\nCombined with our previous remark, and given that the zero-sum property is not as meaningful for N𝑁N players as it is for two,666Recall that any N𝑁N-player game can be turned into an equivalent zero-sum game by adding a fictitious player.\nthe class of harmonic games can be seen as the more natural N𝑁N-player generalization of two-player zero-sum games from a learning viewpoint. To the best of our knowledge, the only comparable result to Theorem 2 in the literature is the very recent paper of Legacci et al.  who showed that the replicator dynamics – a special case of (FTRL-D) – are Poincaré recurrent in uniform harmonic games, that is, in harmonic games where the uniform distribution is a Nash equilibrium, cf. (A.1) and the discussion surrounding Definition 1.\nIn this regard, Theorem 2 extends the recent results of Legacci et al.  along two axes:\n\n\n\\edefnn(\\edefnit\\selectfonti \\edefnn) it applies to the entire class of FTRL dynamics (not only the replicator dynamics);\nand\n\n\\edefnn(\\edefnit\\selectfonti \\edefnn) it applies to the entire class of harmonic games (and not only uniformly harmonic games).\n\n In terms of techniques, Legacci et al.  obtained their result through a surprising connection between a certain Riemannian metric underlying the replicator dynamics and the defining relation of uniformly harmonic games.\nThis relation no longer holds for different instances of (FTRL-D) or for non-uniform harmonic games, so the techniques of  cannot be extended – and, in fact, Legacci et al.  stated this generalization as an open problem.\nOur techniques instead rely on the fact that the orbits y​(t)𝑦𝑡y(t) of (FTRL-D) comprise a volume-preserving flow in the game’s payoff space 𝒴≡∏i𝒴i𝒴subscriptproduct𝑖subscript𝒴𝑖\\mathcal{Y}\\equiv\\prod_{i}\\mathcal{Y}_{i} (though not necessarily on 𝒳𝒳\\mathcal{X}), and then deriving a suitable constant of motion.\nIn the case of the logit map (9), this constant of motion can be written as where μ=(μi​αi)αi∈𝒜i,i∈𝒩𝜇subscriptsubscript𝜇𝑖subscript𝛼𝑖formulae-sequencesubscript𝛼𝑖subscript𝒜𝑖𝑖𝒩\\mu=(\\mu_{i\\alpha_{i}})_{\\alpha_{i}\\in\\mathcal{A}_{i},i\\in\\mathcal{N}} is the harmonic measure on 𝒳𝒳\\mathcal{X} defining ΓΓ\\Gamma.\nIn the more general case,\nthe construction of a constant of motion for (FTRL-D) involves a characterization of harmonic games in terms of a “strategic center”,\nwhich we carry out in detail in Appendix C.\n\nReference markers found: [58]\n\nContext windows for each [ref]:\n[ref]58: as T→∞→𝑇T\\to\\infty. The most widely used scheme for attaining no regret is the family of policies known as follow-the-regularized-leader (FTRL) [ref]58 .\nAt a high level, the idea behind FTRL is that, at all times t≥0𝑡0t\\geq 0, each player i∈𝒩𝑖𝒩i\\in\\mathcal{N} plays a mixed strategy xi​(t)∈𝒳isubscript𝑥𝑖𝑡subscript𝒳𝑖x_{i}(t)\\in\\mathcal{X}_{i}\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 58,\n            \"titles\": [\n                \"Fast Convergence of Regularized Learning in Games\",\n                \"Decomposition of games: some strategic considerations\",\n                \"Mathematical Methods of Classical Mechanics\",\n                \"The rate of convergence of Bregman proximal methods: Local geometry vs. regularity vs. sharpness\",\n                \"Convex optimization algorithms\",\n                \"Tight last-iterate convergence of the extragradient and the optimistic gradient descent-ascent algorithm for constrained monotone variational inequalities\",\n                \"Flows and decompositions of games: Harmonic and potential games\",\n                \"Convergence analysis of a proximal-like minimization algorithm using Bregman functions\",\n                \"Hedging in games: Faster convergence of external and swap regrets\",\n                \"Online optimization with gradual variations\",\n                \"Quasi-Fejérian analysis of some optimization algorithms\",\n                \"Last-Iterate Convergence: Zero-Sum Games and Constrained Min-Max Optimization\",\n                \"The complexity of computing a Nash equilibrium\",\n                \"Near-Optimal No-Regret Learning in General Games\",\n                \"Near-optimal no-regret learning dynamics for general convex games\",\n                \"Forward Looking Best-Response Multiplicative Weights Update Methods for Bilinear Zero-sum Games\",\n                \"NO-REGRET LEARNING AND MIXED NASH EQUILIBRIA: THEY DO NOT MIX\",\n                \"Evolutionary games in economics\",\n                \"A Variational Inequality Perspective on Generative Adversarial Networks\",\n                \"Tight last-iterate convergence rates for no-regret learning in multi-player games\",\n                \"Last-iterate convergence of optimistic gradient method for monotone variational inequalities\",\n                \"Escort evolutionary game theory\",\n                \"A simple adaptive procedure leading to correlated equilibrium\",\n                \"Stochastic uncoupled dynamics and Nash equilibrium\",\n                \"Learning with bandit feedback in potential games\",\n                \"Evolutionary Games and Population Dynamics\",\n                \"On the Convergence of Single-Call Stochastic Extra-Gradient Methods\",\n                \"Adaptive Learning in Continuous Games: Optimal Regret Bounds and Convergence to Nash Equilibrium\",\n                \"No-Regret Learning in Games with Noisy Feedback: Faster Rates and Adaptivity via Learning Rate Separation\",\n                \"Multi-agent online optimization with delays: Asynchronicity, adaptivity, and optimism\",\n                \"Statistical ranking and combinatorial Hodge theory\",\n                \"Solving variational inequalities with stochastic mirror-prox algorithm\",\n                \"The extragradient method for finding saddle points and other problems\",\n                \"A Continuous-Time Approach to Online Optimization\",\n                \"Introduction to Smooth Manifolds\",\n                \"A geometric decomposition of finite games: Convergence vs. recurrence under exponential weights\",\n                \"Finite-time last-iterate convergence for multi-agent learning in games\",\n                \"The emergence of rational behavior in the presence of stochastic perturbations\",\n                \"Learning in games via reinforcement and regularization\",\n                \"Riemannian Game Dynamics\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]58 based on the context and research area of No-Regret Learning in Harmonic Games: Extrapolation in the Face of Conflicting Interests. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}