{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: A Comprehensive Survey of Continual Learning: Theory, Method and Application\n\nPaper Abstract: \n\nPaper Section Text:\nWe group the methods for approximating and recovering old data distributions into this category (see Fig. 5). Depending on the content of replay, these methods can be further divided into three sub-directions, each with its own targets and challenges. The first is experience replay, which typically stores a few old training samples within a small memory buffer. Due to the extremely limited storage space, the key challenges consist of how to construct and how to exploit the memory buffer. As for construction, the preserved old training samples should be carefully selected, compressed, augmented, and updated, in order to recover adaptively the past information. Earlier work adopts fixed principles for sample selection. For example, Reservoir Sampling   9 randomly preserves a fixed number of old training samples obtained from each training batch. Ring Buffer [ref]281 further ensures an equal number of old training samples per class. Mean-of-Feature  selects an equal number of old training samples that are closest to the feature mean of each class. There are many other fixed principles, such as k-means , plane distance 9 and entropy 9, but all perform mediocrely 9 . More advanced strategies are typically gradient-based or optimizable, by maximizing such as\nthe sample diversity in terms of parameter gradients (GSS ), performance of corresponding tasks with cardinality constraints (CCBO ), mini-batch gradient similarity and cross-batch gradient diversity (OCS ), ability of optimizing latent decision boundaries (ASER ), diversity of robustness against perturbations (RM [ref]27), similarity to the gradients of old training samples with respect to the current parameters (GCR ), etc. To improve storage efficiency, AQM  performs online continual compression based on a VQ-VAE framework  and saves compressed data for replay. MRDC  formulates experience replay with data compression as determinantal point processes (DPPs) , and derives a computationally efficient way for online determination of an appropriate compression rate. RM [ref]27 adopts conventional and label mixing-based strategies of data augmentation to enhance the diversity of old training samples. RAR  synthesizes adversarial samples near the forgetting boundary and performs MixUp  for data augmentation.\nThe auxiliary information with low storage cost, such as class statistics (IL2M , SNCL ) and attention maps (RRR , EPR ), can be further incorporated to maintain old knowledge. Besides, the old training samples could be continually modified to accommodate incremental changes, such as by making them more representative (Mnemonics ) or more challenging (GMED ) for separation. As for exploitation, experience replay requires an adequate use of the memory buffer to recover the past information. There are many different implementations, closely related to other continual learning strategies.\nFirst, the effect of old training samples in optimization can be constrained to avoid catastrophic forgetting and facilitate knowledge transfer. For example, GEM [ref]281 constructs individual constraints based on the old training samples for each task to ensure non-increase in their losses.\nA-GEM  replaces the individual constraints with a global loss of all tasks to improve training efficiency.\nLOGD  decomposes the gradient of each task into task-sharing and task-specific components to leverage inter-task information. To achieve a good trade-off in interference-transfer 9 (i.e., forgetting-generalization ), MER 9 employs meta-learning for gradient alignment in experience replay. BCL  explicitly pursues a saddle point of the cost of old and new training samples. MetaSP 7 leverages the Pareto optimum of example influence on stability-plasticity to control the model and storage updates.\nTo selectively utilize the memory buffer, MIR  prioritizes the old training samples that subject to more forgetting, while HAL  uses them as “anchors” and stabilizes their predictions. On the other hand, experience replay can be naturally combined with knowledge distillation (KD), which additionally incorporates the past information from the old model. iCaRL  and EEIL  are two early works that perform KD on both old and new training samples. Some subsequent improvements focus on different issues in experience replay. To mitigate data imbalance of the limited old training samples, LUCIR 5 encourages similar feature orientation of the old and new models, while performing cosine normalization of the last layer and mining hard negatives of the current task. BiC  and WA  attribute this issue to the bias of the last fully connected layer, and resolve it by either learning a bias correction layer with a balanced validation set  or normalizing the output weights . SS-IL  adopts separated softmax in the last layer and task-wise KD to mitigate the bias.\nDRI  trains a generative model to supplement the old training samples with additional generated data.\nTo alleviate dramatic representation shifts, PODNet 2 employs a spatial distillation loss to preserve representations throughout the model. Co2L  introduces a self-supervised distillation loss to obtain robust representations against catastrophic forgetting. GeoDL  performs KD along a path that connects the low-dimensional projections of the old and new feature spaces for a smooth transition between them.\nELI  learns an energy manifold with the old and new models to realign the representation shifts for optimizing incremental tasks.\nTo adequately exploit the past information, AU  incorporates uncertainty and self-attention into the distillation loss, while CSC  additionally leverages the structure of the feature space.\nDDE  distills colliding effects from the features of the new training samples, which is causally equivalent to replaying more old training samples.\nTAMiL  adds task-specific attention in the feature space and performs consistency regularization to better preserve task-relevant information.\nTo further enhance learning plasticity, D+R 4 performs KD from an additional model dedicated to each new task.\nFOSTER  dynamically expands new modules to fit the residuals of the old model and then distills them into a single model.\nBesides, weight regularization approaches can be combined with experience replay to achieve better performance and generality  . It is worth noting that the merits and potential limitations of experience replay remain largely open. In addition to the intuitive benefits of staying in the low-loss region of the old tasks , theoretical analysis has demonstrated its contribution to resolving the NP-hard problem of optimal continual learning . However, it risks overfitting to only a few old training samples retained in the memory buffer, which potentially affects generalizability .\nTo alleviate overfitting, LiDER  takes inspirations from adversarial robustness and enforces the Lipschitz continuity of the model to its inputs.\nMOCA  enlarges the variation of representations to prevent the old ones from shrinking in their space.\nInterestingly, several simple baselines of experience replay can achieve considerable performance. DER/DER++  and X-DER  preserve old training samples together with their logits, and perform logit-matching throughout the optimization trajectory. GDumb  greedily collects incoming training samples in a memory buffer and then uses them to train a model from scratch for testing. These efforts can serve as an evaluation criterion for more advanced strategies in this sub-direction. The second is generative replay or pseudo-rehearsal, which usually requires training an additional generative model to replay generated data. This is closely related to continual learning of generative models themselves, as they also require incremental updates.\nDGR 2 provides an initial framework in which learning each generation task is accompanied with replaying generated data sampled from the old generative model, so as to inherit the previously-learned knowledge. MeRGAN 4 further employs replay alignment to enforce consistency of the generative data sampled with the same random noise between the old and new generative models, similar to the role of function regularization. Besides, other continual learning strategies can be incorporated into generative replay.\nWeight regularization  8   and experience replay   have been shown to be effective in mitigating catastrophic forgetting of generative models.\nDGMa/DGMw  adopts binary masks to allocate task-specific parameters for overcoming inter-task interference, and an extendable network to ensure scalability.\nIf pre-training is available, it can provide a relatively stable and strong reference model for continual learning. For example, FearNet 1 and ILCAN  additionally preserves statistical information of the old features acquired from a pre-trained feature extractor, while GAN-Memory  continually adjusts a pre-trained generative model with task-specific parameters. The generative models for pseudo-rehearsal can be of various types, such as generative adversarial networks (GANs) and (variational) autoencoder (VAE).\nA majority of state-of-the-art approaches have focused on GANs to enjoy its advantages in fine-grained generation, but usually suffer from label inconsistency in continual learning  . In contrast, autoencoder-based strategies, such as FearNet 1, SRM , CLEER , EEC , GMR  and Flashcards , can explicitly control the labels of the generated data, albeit with relatively blurred granularity. L-VAEGAN 3 instead employs a hybrid model for both high-quality generation and accurate inference.\nHowever, since continual learning of generative models is extremely difficult and requires significant resource overhead, generative replay is often limited to relatively simple datasets  . An alternative is to convert the target of generative replay from data level to feature level, which can largely reduce the complexity of conditional generation and more adequately exploit semantic information. For example, GFR  trains conditional GANs to replay generated features after the feature extractor. BI-R  incorporates context-modulated feedback connections in a standard VAE to replay internal representations. In fact, maintaining feature-level rather than data-level distributions enjoys numerous benefits in terms of efficiency and privacy. We refer to this sub-direction as feature replay. However, a central challenge is the representation shift caused by sequentially updating the feature extractor, which reflects the feature-level catastrophic forgetting. To address this issue, GFR , FA  and DSR  perform feature distillation between the old and new models. IL2M  and SNCL  recover statistics of feature representations (e.g., mean and covariance) on the basis of experience replay.\nRER 9 explicitly estimates the representation shift to update the preserved old features.\nREMIND  and ACAE-REMIND  instead fix the early layers of the feature extractor and reconstruct the intermediate representations to update the latter layers.\nFeTrIL  employs a fixed feature extractor learned from the initial task and replays generated features afterwards. For continual learning from scratch, the required changes in representation are often dramatic, while stabilizing the feature extractor may interfere with accommodating new representations.\nIn contrast, the use of strong pre-training can provide robust representations that are generalizable to downstream tasks and remain relatively stable in continual learning.\nAn empirical study  has systematically investigated feature replay for continual learning with large-scale pre-training. A more in-depth discussion of this topic is presented in Sec. 4.4.\n\nReference markers found: [281]\n\nContext windows for each [ref]:\n[ref]281: work adopts fixed principles for sample selection. For example, Reservoir Sampling   9 randomly preserves a fixed number of old training samples obtained from each training batch. Ring Buffer [ref]281 further ensures an equal number of old training samples per class. Mean-of-Feature  selects an equal number of old training samples that are closest to the feature mean of each class. There\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 281,\n            \"titles\": [\n                \"DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps\",\n                \"Conditional channel gated networks for task-aware continual learning\",\n                \"IIRC: Incremental Implicitly-Refined Classification\",\n                \"Metaplasticity: tuning synapses and networks for plasticity\",\n                \"Metaplasticity: the plasticity of synaptic plasticity\",\n                \"Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies\",\n                \"GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning\",\n                \"Continual Learning with Adaptive Weights (CLAW)\",\n                \"Semantics-driven generative replay for few-shot class incremental learning\",\n                \"Uncertainty-based continual learning with adaptive regularization\",\n                \"Ss-il: Separated softmax for incremental learning\",\n                \"Subspace Regularizers for Few-Shot Class Incremental Learning\",\n                \"Memory Aware Synapses: Learning what (not) to forget\",\n                \"Online continual learning with maximal interfered retrieval\",\n                \"Expert Gate: Lifelong Learning with a Network of Experts\",\n                \"Task-Free Continual Learning\",\n                \"Gradient based sample selection for online continual learning\",\n                \"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System\",\n                \"Varigrow: Variational architecture growing for task-agnostic continual learning based on bayesian novelty\",\n                \"Activity-dependent gating of lateral inhibition in the mouse olfactory bulb\",\n                \"A Closer Look at Memorization in Deep Networks\",\n                \"Class-Incremental Learning with Cross-Space Clustering and Controlled Transfer\",\n                \"Dopaminergic neurons write and update memories with cell-type-specific rules\",\n                \"Mushroom body output neurons encode valence and guide memory-based action selection in drosophila\",\n                \"Few-Shot Continual Active Learning by a Robot\",\n                \"EEC: Learning to Encode and Regenerate Images for Continual Learning\",\n                \"Structural components of synaptic plasticity and memory consolidation\",\n                \"Rainbow Memory: Continual Learning with a Memory of Diverse Samples\",\n                \"Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries\",\n                \"Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models\",\n                \"Learning to Continually Learn\",\n                \"Il2m: Class incremental learning with dual memory\",\n                \"ScaIL: Classifier Weights Scaling for Class Incremental Learning\",\n                \"Generalisation Guarantees for Continual Learning with Orthogonal Gradient Descent\",\n                \"Unifying importance based regularisation methods for continual learning\",\n                \"Comps: Continual meta policy search\",\n                \"Task-Aware Information Routing from Common Representation Space in Lifelong Learning\",\n                \"Doodle it yourself: Class incremental learning by drawing a few sketches\",\n                \"Continual Lifelong Learning in Natural Language Processing: A Survey\",\n                \"On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]281 based on the context and research area of A Comprehensive Survey of Continual Learning: Theory, Method and Application. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}