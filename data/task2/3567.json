{
  "messages": [
    {
      "role": "system",
      "content": "You are a professional academic citation prediction expert. Your task is to predict the most likely citations for [ref] markers in academic paper sections based on the paper content and retrieved relevant papers.\n\nRETRIEVER INTERFACE:\n- Input: paper title + abstract + section text with [ref] markers\n- Output: top {top_r} retrieved papers with title and abstract\n- Format: List of dictionaries with 'title' and 'abstract' fields\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nTASK DESCRIPTION:\n- Input: Paper section text with [ref] markers + retrieved relevant papers\n- Output: top 40 predicted reference titles for each [ref] marker with reasoning\n- Format: JSON object with \"citations\" array containing objects with \"ref_index\", \"titles\", and \"reasoning\" fields\n\nREQUIREMENTS:\n1. Analyze the context around each [ref] marker to understand what reference is needed\n2. Consider the retrieved papers as potential references (when available)\n3. Consider the surrounding text, research area, and topic\n4. Generate realistic and relevant reference titles that would fit the context\n5. Provide detailed reasoning for each prediction to prevent hallucination\n6. Output format: JSON object with \"citations\" array\n7. Generate exactly 40 most relevant reference titles for each [ref]\n8. Ensure titles are realistic and appropriate for the research area\n9. Include reasoning that explains why each title is relevant to the specific [ref] context\n10. Use proper JSON escaping for special characters (escape quotes with \\\")\n11. Ensure all strings are properly quoted and escaped\n\nEXAMPLE:\nInput:\n\"Federated Learning aims to train models in massively distributed networks [ref]38 at a large scale [ref]5, over multiple sources of heterogeneous data [ref]30.\"\n\nRetrieved papers (top {top_r}):\n1. Title: Communication-efficient learning of deep networks from decentralized data\n   Abstract: We report on a series of experiments with federated learning systems for decentralized data training.\n\n2. Title: Federated Learning: Challenges, Methods, and Future Directions\n   Abstract: A comprehensive survey of federated learning approaches and challenges in distributed settings.\n\nOutput:\n{\n  \"citations\": [\n    {\n      \"ref_index\": 38,\n      \"titles\": [\n        \"Communication-efficient learning of deep networks from decentralized data\",\n        \"Federated Learning: Challenges, Methods, and Future Directions\",\n        \"Federated optimization in heterogeneous networks\",\n        \"Towards federated learning at scale: System design\",\n        \"A performance evaluation of federated learning algorithms\",\n        \"Distributed machine learning: A survey\",\n        \"Communication protocols for distributed learning\",\n        \"Scalable federated learning systems\",\n        \"Efficient distributed training algorithms\",\n        \"Large-scale distributed learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]38 appears in the context of 'massively distributed networks' for federated learning, so it likely refers to foundational papers on federated learning systems, distributed training, and communication-efficient methods in distributed networks.\"\n    },\n    {\n      \"ref_index\": 5,\n      \"titles\": [\n        \"Bagging predictors\",\n        \"Ensemble methods in machine learning\",\n        \"Random forests for classification\",\n        \"Bootstrap methods and their application\",\n        \"Combining multiple classifiers\",\n        \"Large-scale machine learning systems\",\n        \"Distributed ensemble methods\",\n        \"Scalable classification algorithms\",\n        \"Massive data processing techniques\",\n        \"High-performance machine learning frameworks\"\n      ],\n      \"reasoning\": \"This [ref]5 appears in the context of 'at a large scale', suggesting it refers to papers on scaling machine learning methods, ensemble techniques, or methods that can handle large-scale data and distributed settings.\"\n    },\n    {\n      \"ref_index\": 30,\n      \"titles\": [\n        \"Learning fair representations\",\n        \"Fair machine learning: A survey\",\n        \"Bias in machine learning systems\",\n        \"Addressing fairness in AI systems\",\n        \"Fair representation learning\",\n        \"Heterogeneous data handling methods\",\n        \"Multi-source learning algorithms\",\n        \"Cross-domain representation learning\",\n        \"Fair learning from diverse data\",\n        \"Bias mitigation in heterogeneous datasets\"\n      ],\n      \"reasoning\": \"This [ref]30 appears in the context of 'heterogeneous data' and 'fair representation learning', indicating it likely refers to papers on heterogenous data handling, bias mitigation, and learning representations that are fair across different data distributions.\"\n    }\n  ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped.\n\nIMPORTANT NOTES ON REASONING:\n- The reasoning field is crucial for preventing hallucination\n- Each reasoning should be specific to the context around the [ref] marker\n- Explain the connection between the predicted titles and the research topic\n- This helps validate that predictions are grounded in the actual content\n- Reasoning should be concise but informative (2-3 sentences)\n- Consider retrieved papers when available to improve prediction accuracy"
    },
    {
      "role": "user",
      "content": "Paper Title: Influencing Long-Term Behavior in Multiagent Reinforcement Learning\n\nPaper Abstract: \n\nPaper Section Text:\nWe demonstrate FURTHERâ€™s efficacy on a diverse suite of multiagent benchmark domains.\nWe refer toÂ AppendixÂ G for experimental details and hyperparameters.\nThe code is available at https://bit.ly/3fXArAo, and video highlights are available at https://bit.ly/37IWeb9.\nThe mean and 95% confidence interval computed for 20 seeds are shown in each figure. We compare FURTHER with the following baselines (see SectionÂ G.2 for details): LILIÂ [ref]8: An approach that considers the learning dynamics of other agents but suffers from myopic evaluation bias by optimizing the discounted return objective (seeÂ EquationÂ 25). MASACÂ : An approach that extends SACÂ [ref]27 to a multiagent learning setting by having centralized criticsÂ [ref]12. This baseline assumes other agents will have stationary policies in the future and thus neglects their learning (seeÂ EquationÂ 26). We note that these selected baselines are closely related to FURTHER, optimizing different objectives with respect to ğ“¤ğ’Šsuperscriptğ“¤ğ’Š\\bm{\\mathcal{U}^{\\shortminus i}}. In particular, LILI and MASAC optimize the discounted return objective with and without modeling ğ“¤ğ’Šsuperscriptğ“¤ğ’Š\\bm{\\mathcal{U}^{\\shortminus i}}, respectively. As such, our baseline choices enable us to separately analyze the effect of FURTHERâ€™s novel average reward objective. For completeness, we also consider additional baselines of an opponent modeling method (DRON) and an incentive MARL method (MOA). These results are shown inÂ AppendixÂ H. Question 1. How do methods perform when playing against a qğ‘q-learning agent? We consider playing the iterated Bach or Stravinsky game (IBS; seeÂ FigureÂ 4(a)).\nThis general-sum game involves conflicting elements with two pure strategy Nash equilibria, where convergence to (B,B) and (S,S) equilibrium are more preferable from agent iğ‘–iâ€™s and jğ‘—jâ€™s perspective, respectively.\nSuppose agent iğ‘–i plays against a naive learner jğ‘—j, such as qğ‘q-learnerÂ , whose initial qğ‘q-values are set to prefer action (S).\nIn this experimental setting, it is ideal for agent iğ‘–i to change jğ‘—jâ€™s influence behavior to select (B) such that they converge to iğ‘–iâ€™s optimal pure strategy Nash equilibrium of (B,B). The average reward performance when an agent iğ‘–i, trained with either FURTHER or the baseline methods, interacts with the qğ‘q-learner jğ‘—j is shown inÂ FigureÂ 5(a).\nThere are two notable observations.\nFirst, the FURTHER agent iğ‘–i consistently converges to its optimal equilibrium of (B,B), while the LILI agent often converges to the sub-optimal equilibrium of (S,S).\nThe FURTHER agent iğ‘–i learns to select (B) while jğ‘—j selects (S), receives the worst rewards of zero, and waits until jğ‘—jâ€™s qğ‘q-value for (S) is updated to be lower than the qğ‘q-value for (B).\nWith the limiting view, iğ‘–i learns that the waiting process is only temporary, and receiving the eventual rewards of 2 by converging to (B,B) is optimal.\nBy contrast, LILI suffers from myopic evaluation and shows decreased performance upon convergence because the agent prefers simply converging to the sub-optimal equilibrium rather than waiting indefinitely.\nFigureÂ 6(a) also shows that LILI achieves sub-optimal performance for any value of Î³ğ›¾\\gamma and shows unstable learning as Î³â†’1â†’ğ›¾1\\gamma\\!\\rightarrow\\!1.\nSecond, FURTHER and LILI outperform the other approach of MASAC, showing the benefit of considering the active influence on future policies of other agents. Which equilibrium do methods converge to in a self-play setting? We experiment with a self-play setting in which both agents learn with the same algorithm in an iterated cooperative (IC) game with identical payoffs (seeÂ FigureÂ 4(b)).\nThis game has two pure strategy Nash equilibria of (U,U) and (D,D), in which the (D,D) equilibrium Pareto dominates the other.\nFigureÂ 5(b) shows the average reward performance as the training iteration increases.\nFirst we find that LILI performs better than MASAC by considering the learning of agents. However, similar to the IBS results, we observe that FURTHER consistently converges to the best equilibrium of (D,D) while the baseline methods can converge to the sub-optimal equilibrium of (U,U) due to the myopic view. How does FURTHERâ€™s limiting optimization perform directly against baselines? We consider FURTHER agent iğ‘–i directly competing against either LILI or MASAC opponent jğ‘—j in the iterated matching pennies (IMP) game (seeÂ FigureÂ 4(c)).\nTo show that FURTHER has a long-term perspective and thus can collect more rewards than the opposing method over time, we evaluate using a metric of relative accumulated reward summed up to the current timestep: âˆ‘trtiâˆ’rtjsubscriptğ‘¡subscriptsuperscriptğ‘Ÿğ‘–ğ‘¡subscriptsuperscriptğ‘Ÿğ‘—ğ‘¡\\textstyle\\sum\\limits\\nolimits_{t}r^{i}_{t}\\!-\\!r^{j}_{t}.\nFigureÂ 5(c) shows that the relative accumulated reward for FURTHER is positive for both settings, meaning that FURTHER receives higher rewards than LILI and MASAC over time.\nThis result suggests that FURTHER is more effective than LILI by employing the limiting view via the average reward formulation.\nThis result also conveys that it is beneficial to consider the underlying learning dynamics rather than ignoring them because FURTHER can more easily exploit the MASAC opponent and achieve higher accumulated rewards than when competing against the LILI opponent. How does FURTHER scale to a more complex environment? To answer this question, we use the MuJoCo RoboSumo domain (; seeÂ FigureÂ 4(d)), where two ant robots compete with each other with the objective of pushing the opponent out of the ring.\nThe reward function consists of a sparse reward of 555 for winning against the opponent and shaped rewards of moving towards the opponent and pushing the opponent further from the center of the ring.\nThis environment has complex interactions because an agent must learn how to control its joints with continuous action space to move around the ring while learning to push the opponent.\nSimilar to the setup in Question 3, FURTHER agent iğ‘–i directly competes against either LILI or MASAC opponent jğ‘—j.\nWe note that each agent has only partial observations about its opponent. As such, an agent infers its opponentâ€™s hidden policies and learning dynamics based on partial observations.\nWe show the RoboSumo results inÂ FigureÂ 6(b).\nConsistent with our results in the iterated matrix games, we observe that FURTHER gains more rewards than the baselines over time and wins against MASAC more often than against LILI.\nThe averaged winning rate across the entire interaction shows that FURTHER wins against LILI and MASAC with 60.6% and 63.9%, respectively.\nTherefore, FURTHER provides a scalable framework that can learn policies in an environment with complex interactions and continuous action space. How does FURTHER scale to a large number of agents? Finally, we show the scalability of our method regarding the number of agents using the battle domain (; seeÂ FigureÂ 4(e)).\nIn this large-scale mixed cooperative-competitive setting, a red team of 25 agents and a blue team of 25 agents interact in a gridworld, where each agent collaborates with its teammates to eliminate the opponents.\nSpecifically, we compare when red and blue agents learn with the mean-field version of FURTHER (i.e., FURTHER-MF) and LILI (i.e., LILI-MF), respectively, where they predict the mean actions of neighboring agents.\nWe note that all 50 agents learn in a decentralized manner without sharing parameters with one another.\nIt is evident that FURTHER-MF outperforms LILI-MF, which shows the effectiveness of having the limiting perspective.\nThis result also conveys that FURTHER can easily incorporate other techniques in multiagent learning and show improved performance in large-scale settings.\n\nReference markers found: [8]\n\nContext windows for each [ref]:\n[ref]8: at https://bit.ly/37IWeb9.\nThe mean and 95% confidence interval computed for 20 seeds are shown in each figure. We compare FURTHER with the following baselines (see SectionÂ G.2 for details): LILIÂ [ref]8: An approach that considers the learning dynamics of other agents but suffers from myopic evaluation bias by optimizing the discounted return objective (seeÂ EquationÂ 25). MASACÂ : An approach\n\n{retrieved_text}\n\nGenerate exactly 40 most likely reference titles for each [ref] marker. Focus on papers that are directly related to the research topic, provide foundational background, present similar methodologies, or address related problems.\n\nCRITICAL REQUIREMENTS:\n1. For each [ref] marker, provide detailed reasoning explaining why the predicted titles are relevant\n2. The reasoning should analyze the specific context around each [ref] marker\n3. Explain how each title connects to the research topic, methodology, or problem being discussed\n4. This reasoning helps prevent hallucination by grounding predictions in the actual context\n5. Consider retrieved papers when available to improve prediction accuracy\n\nCRITICAL FORMAT REQUIREMENT: You must respond with ONLY valid JSON. Do not include any explanatory text, markdown formatting, or other content outside the JSON structure.\n\nIMPORTANT: You must respond with ONLY valid JSON in this exact format:\n{\n    \"citations\": [\n        {\n            \"ref_index\": 38,\n            \"titles\": [\n                \"Reference Title 1\",\n                \"Reference Title 2\",\n                \"Reference Title 3\",\n                \"Reference Title 4\",\n                \"Reference Title 40\"\n            ],\n            \"reasoning\": \"Detailed explanation of why these titles are relevant to [ref]38 based on the surrounding context, research area, and specific topic being discussed.\"\n        }\n    ]\n}\n\nCRITICAL: Return ONLY the JSON object. Do not include any text before or after the JSON. Do not use markdown formatting. Ensure all quotes and special characters are properly escaped. Do not include authors, venues, or years - only the paper titles. Generate exactly 40 titles for each [ref] marker with reasoning."
    },
    {
      "role": "assistant",
      "content": "{\n    \"citations\": [\n        {\n            \"ref_index\": 8,\n            \"titles\": [\n                \"A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning\",\n                \"Multi-agent Reinforcement Learning: An Overview\",\n                \"Dealing with non-stationarity in multi-agent deep reinforcement learning\",\n                \"Equilibrium points in n-person games\",\n                \"Cyclic equilibria in markov games\",\n                \"Game Theory and Multi-agent Reinforcement Learning\",\n                \"Learning with Opponent-Learning Awareness\",\n                \"Stable Opponent Shaping in Differentiable Games\",\n                \"Learning Latent Representations to Influence Multi-Agent Interaction\",\n                \"Influencing Towards Stable Multi-Agent Interactions\",\n                \"Model-Free Opponent Shaping\",\n                \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\",\n                \"Counterfactual Multi-Agent Policy Gradients\",\n                \"Actor-Attention-Critic for Multi-Agent Reinforcement Learning\",\n                \"Near-optimal reinforcement learning in polynomial time\",\n                \"Mean Field Multi-Agent Reinforcement Learning\",\n                \"Markov Decision Processes: Discrete Stochastic Dynamic Programming\",\n                \"Reinforcement Learning: An Introduction\",\n                \"Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments\",\n                \"The complexity of markov equilibrium in stochastic games\",\n                \"Correlated equilibrium as an expression of bayesian rationality\",\n                \"Stochastic evolutionary game dynamics\",\n                \"Random Perturbations of Dynamical Systems\",\n                \"Distributed dynamic reinforcement of efficient outcomes in multiagent coordination and network formation\",\n                \"Stochastic stability of perturbed learning automata in positive-utility games\",\n                \"An algorithm for computing stochastically stable distributions with applications to multiagent learning in repeated games\",\n                \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\n                \"Variational Inference: A Review for Statisticians\",\n                \"Meta-learning with implicit gradients\",\n                \"Continuous-Time Meta-Learning with Forward Mode Differentiation\",\n                \"Soft Actor-Critic for Discrete Action Settings\",\n                \"MAgent: A Many-Agent Reinforcement Learning Platform for Artificial Collective Intelligence\",\n                \"Q-learning\",\n                \"A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity\",\n                \"Learning to Teach in Cooperative Multiagent Reinforcement Learning\",\n                \"Policy Distillation and Value Matching in Multiagent Reinforcement Learning\",\n                \"Learning Hierarchical Teaching Policies for Cooperative Agents\",\n                \"Opponent Modeling in Deep Reinforcement Learning\",\n                \"Modeling Others using Oneself in Multi-Agent Reinforcement Learning\",\n                \"Learning policy representations in multiagent systems\"\n            ],\n            \"reasoning\": \"Selected these 40 citations for [ref]8 based on the context and research area of Influencing Long-Term Behavior in Multiagent Reinforcement Learning. The first title is the actual reference for this marker, followed by other relevant papers from the same research domain.\"\n        }\n    ]\n}"
    }
  ],
  "format": "chatml"
}